Recommendations for capacity planning - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for capacity planning
Article
2023-11-15
3 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:02
Conduct capacity planning. Capacity planning should be done before there are predicted changes in usage patterns. Predicted changes include as seasonal variations, product updates, marketing campaigns, special events, or regulatory changes.
This guide describes the recommendations for capacity planning. Capacity planning refers to the process of determining the resources required to meet workload performance targets. It involves estimating the amount of computing resources such as CPU, memory, storage, and network bandwidth needed to support the workload's performance requirements. Capacity planning helps avoid underprovisioning and ensures the workload has sufficient resources to handle the expected workload demands without experiencing performance degradation or bottlenecks. It also helps prevent overprovisioning and unnecessary costs. A lack of capacity planning can lead to performance issues, resource bottlenecks, increased costs, inefficient allocation, scalability challenges, and unpredictable workload performance.
Definitions
Term
Definition
Capacity planning
The process of predicting the resources a workload needs to meet its performance targets.
Functional requirements
The features and capabilities a workload must have to fulfill its intended purpose.
Technical requirements
The code and infrastructure needed to meet functional requirements.
Trend analysis
Historical data analysis to forecast future demand.
Key design strategies
Capacity planning is a forward-looking process that involves making decisions based on anticipated workload demands and patterns. Its goal is to optimize workload performance across both continuous and peak load scenarios. By understanding changes in usage, such as seasonal shifts or product releases, you can allocate resources strategically, preventing system strain during high demand periods. This proactive strategy reduces disruptions and bolsters performance efficiency. By analyzing past usage trends and growth data, you can forecast short and long-term needs. You can pinpoint potential bottlenecks and scaling issues, ensuring consistent and efficient workload performance.
Gather capacity data
Gathering workload utilization data entails collecting and analyzing information on how a workload uses resources. You should collect data on historical patterns for existing workloads and predictive measures for new workloads. This process helps translate business objectives into technical requirements and is essential for forecasting capacity. Consider the following recommendations:
Understand an existing workload
Understanding an existing workload for capacity planning involves analyzing historical data related to how the workload utilizes resources. It encompasses metrics like resource utilization, performance data, and workload patterns. This understanding ensures efficient resource allocation, translates business goals into technical requirements, and helps identify potential bottlenecks.
Understand the data
: Review the available historical data and understand its structure, format, and relevance to capacity planning. The review might include resource utilization metrics, workload patterns, performance metrics, and other relevant data points. Understand the business processes and the criticality of the applications. Identify the peak usage times, user load, transaction rates, and other relevant metrics.
Clean and preprocess the data
: Prepare the data for analysis by removing any inconsistencies, errors, or outliers. Preparing the data might involve data cleaning techniques like data imputation, the handling of missing values, or normalization.
Identify key metrics
: Identify the metrics that are relevant for capacity planning. Metrics can include CPU utilization, memory usage, network throughput, and response times.
Identify bottlenecks
: Measure throughput and response times to identify the specific components of your system that might become bottlenecks as the workload grows. Requests per second and database CPU usage can be good indicators of capacity.
Visualize the data
: Create visualizations, like charts or plots, to gain better insights into historical data. Visualizations can help you identify patterns, trends, and anomalies in data to give you a clearer understanding of workload behavior.
Understand a new workload
Understanding a new workload for capacity planning refers to predicting the resource requirements of a future task without historical data. Predicting the future needs of new a workload without historical data can be more challenging. This process ensures you allocate resources efficiently and align allocations with workload objectives when the workload is introduced. Consider the following recommendations:
Market research
: Conducting market research to understand the demand for similar products or services can provide valuable insights into the potential demand for a new workload. The research can involve analyzing market trends, conducting surveys, or studying competitor offerings.
Expert judgment
: Input from subject matter experts or professionals who have experience in the industry can help you estimate the demand for a new workload. Their expertise and insights can provide valuable inputs for forecasting.
Pilot projects or prototypes
: Small-scale pilot projects or prototypes can help you gather real-time data and feedback. You can then use this data to inform the capacity planning process and adjust the forecasted demand.
External data sources
: External data sources like industry reports, market studies, or customer surveys can provide additional information for estimating demand for a new workload. These sources can offer valuable insights into customer preferences, market trends, and potential demand drivers.
Forecast demand
Forecasting demand involves using workload data to predict future needs for a service or product. It's essential for capacity planning to ensure efficient resource allocation, anticipate growth patterns, and prepare for potential surges in demand. When you forecast future demand, you use data to get a sense of future needs. You apply statistical analysis, trend analysis, or predictive modeling techniques to the data you have to forecast future demand. These methods take into account historical or anticipated patterns and project them into the future to provide estimates of the expected workload demand. To forecast demand, consider these strategies:
Account for various scenarios
When you perform capacity planning, you need to plan for different scenarios that might occur. This planning should include both predictable growth patterns and unexpected surges in demand. Usage patterns can grow or shrink. They can be organic (more or less users) or inorganic (an event or security incident). You need to conduct capacity planning before usage changes, at key times:
Design (prediction)
Regular spikes (8:00 AM sign-in rush)
Launch (prediction validation)
Business model change
Acquisition or merger
Marketing push
Seasonal change
Feature launch
Periodically
Use prediction techniques
Forecasting future demand for a service or product involves using techniques like statistical analysis, trend analysis, and predictive modeling. Here's an overview of how you can use these techniques:
Statistical analysis
: Statistical methods can you help uncover patterns and relationships within historical data. You can use these patterns to forecast future demand. You can use techniques like time series analysis, regression analysis, and moving averages to identify trends, seasonality, and other patterns in the data.
Trend analysis
: Trend analysis involves examining historical data to identify consistent patterns and extrapolating those patterns into the future. For example, if workload demand increased by 10 percent during the past year, you might forecast a continuation of this trend. When you analyze historical demand data over a period of time, you can identify growth or reduction trends. Use these trends as a basis for forecasting future demand. Trend analysis can also identify the effects of one-time events that cause rapid shifts in traffic (inorganic). For example, feature releases might consistently increase demand by 5 percent. If you have four major releases a year, you should plan for 5 percent growth each time.
Predictive modeling
: Predictive modeling is the process of building mathematical models that use historical data and other relevant variables to make predictions about future demand. You can use techniques like machine learning algorithms, neural networks, or decision trees. These models can take into account multiple factors and variables to provide more accurate forecasts.
Align forecasts with workload objectives
Aligning forecasts with workload objectives involves adjusting predictive capacity models to ensure they meet the specific goals and demands of a given workload. This alignment ensures resources are adequately provisioned, preventing both underutilization and potential workload overloads. For example, if you aim to support an API for 1 million users to upload 1-MB files in a second, but current data shows slow write speeds, you need to adjust your system. It's essential to talk with stakeholders to grasp the workload's requirements. Make sure your plans align with the promises (SLAs) of your service providers. This alignment ensures your capacity meets the expected demand and helps pinpoint areas of the system that might need changes.
Determine resource requirements
Determining resource requirements for capacity planning involves assessing the resources that you need to meet forecasted demand. For example, if an application anticipates a 50% increase in users during a promotional campaign, it might need to allocate more cloud instances or adjust its autoscaling parameters to handle the increased load.
A workload can have many resources, so there's no one metric to observe to determine resource requirements. You need to measure capacity at the resource level to get meaningful results. Estimate the expected demand for your resources based on historical data, market trends, and business projections. Consider the number of transactions, concurrent users, or any other relevant metrics.
Based on the forecasted demand, calculate the resources needed to meet that demand. Consider factors such as server capacity, network bandwidth, storage capacity, and personnel:
Server capacity
: Determine the required server capacity based on the estimated number of concurrent users or transactions. Consider factors like CPU, memory, and disk space requirements to ensure that your servers can handle the expected workload.
Network bandwidth
: Evaluate the network bandwidth that you need to support the anticipated level of traffic. You should include both inbound and outbound data transfer rates to ensure smooth and efficient communication between servers and clients.
Storage capacity
: Estimate the amount of data that the workload generates or processes during the forecasted demand. Consider factors like database size, file storage requirements, and any other data storage needs that are specific to your application.
Personnel
: Assess the human resources that are required to manage and maintain the infrastructure, handle customer support, perform system maintenance, and ensure smooth operations. Take into account factors like workload distribution, skill set, and required expertise.
Understand resource limitations
Resources in your workload have performance limitations. Performance limitations apply to services and SKUs within each service. You need to understand the limitations of the resources in your workload and factor those limitations into your design decisions. For example, you should know whether resource limitations require you to change SKUs or to change resources altogether.
You also need to identify reachable limits. It refers to pinpointing the maximum thresholds or boundaries of a workload. These limits usually apply to infrastructure (compute, memory, storage, network), application (concurrent database connections, response times, availability), service (requests per second), and scaling. When capacity planning identifies reachable limits, you need to modify the workload before the limit creates a performance problem. Performance baselines, continuous monitoring, and testing are essential to validating the limits and the solution.
Tradeoff
: Misjudged capacity planning can lead to over-provisioning or under-provisioning of resources. Over-provisioning leads to higher costs. Under-provisioning can result in poor performance. Try to find the right balance.
Azure facilitation
Gathering capacity data and forecasting demand
:
Azure Monitor
enables you to collect and analyze telemetry data from your applications and infrastructure. It supports the monitoring of various Azure resources, including virtual machines, containers, and storage accounts. Key tools include
Application Insights
and
Log Analytics
. By configuring data collection and defining metrics and logs that you want to monitor, you can gather valuable workload data for analysis. For
network monitoring
, combine Azure Monitor with Azure Network Watcher, Azure Monitor network insights, and Azure ExpressRoute monitoring.
Azure Monitor allows you to analyze historical data and apply forecasting techniques to predict future workload trends and capacity requirements. You can generate forecasts that can help you with capacity planning. These forecasts help estimate server capacity, network bandwidth, storage capacity, and other resource needs by using predicted demand patterns.
Determining resource requirements
: Because they provide a wide range of configurations, Azure tools and services can help you define technical requirements. You can align your workload requirements with available Azure resources, ensuring that you select the appropriate components and settings to meet your functional needs.
Understanding resource limitations
: Azure provides documentation and resources to help you understand the performance limitations of various
Azure services and SKUs
. Taking into consideration these limitations can help you make informed design decisions and optimize your workload architecture for performance and cost-effectiveness.
Azure provides scalability options like autoscaling, which can automatically adjust resources based on workload demand. You can scale vertically by increasing the capacity of a resource by using a larger virtual machine size, or you can scale horizontally by adding new instances of a resource. Azure services that have autoscaling capabilities can automatically scale out to ensure capacity during workload peaks and return to normal when the load decreases. There are scaling limits within your configuration and services that you should be aware of. You can read the documentation or run tests. Azure provides tools like
Azure Load Testing
, which can simulate load and different usage patterns to help you gather relevant data about your workload.
Related links
Azure Monitor
Application Insights
Log Analytics
Network monitoring services
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Design review checklist for Performance Efficiency - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Design review checklist for Performance Efficiency
Article
2023-11-15
2 contributors
Feedback
In this article
This checklist presents a set of recommendations for you to scale your system so it can grow and meet your workload usage demand. The goal of
performance
is to maintain the efficiency of every interaction with a healthy system as demand increases. When you design and implement for performance, focus on the
efficiency and effectiveness
of cost, complexity, supporting new requirements, technical debt, reporting, and toil.
For every system, there's a limit to how much you can scale it without redesigning, introducing a workaround, or incorporating human involvement. If you don't include performance efficiency practices and consider the tradeoffs, your design is potentially at risk. Carefully consider all the points covered in the checklist to instill confidence in your system's success.
Checklist
Code
Recommendation
☐
PE:01
Define performance targets.
Performance targets should be numerical values that are tied to workload requirements. You should implement performance targets for all workload flows.
☐
PE:02
Conduct capacity planning.
Capacity planning should be done before there are predicted changes in usage patterns, such as seasonal variations, product updates, marketing campaigns, special events, or regulatory changes.
☐
PE:03
Select the right services.
The services, infrastructure, and tier selections must support your ability to reach the workload's performance targets and accommodate expected capacity changes. The selections should also weigh the benefits of using platform features or building a custom implementation.
☐
PE:04
Collect performance data.
Workload components and flows should provide automatic, continuous, and meaningful metrics and logs. Collect data at different levels of the workload, such as the application, platform, data, and operating system levels.
☐
PE:05
Optimize scaling and partitioning.
Incorporate reliable and controlled scaling and partitioning. The scale unit design of the workload is the basis of the scaling and partitioning strategy.
☐
PE:06
Test performance.
Perform regular testing in an environment that matches the production environment. Compare results against the performance targets and the performance benchmark.
☐
PE:07
Optimize code and infrastructure.
Use code that's performant, and ensure that it offloads responsibilities to the platform. Use code and infrastructure only for their core purpose and only when necessary.
☐
PE:08
Optimize data usage.
Optimize data stores, partitions, and indexes for their intended and actual use in the workload.
☐
PE:09
Prioritize the performance of critical flows.
The allocation of workload resources and performance optimization efforts should prioritize the flows that support the most important business processes, users, and operations.
☐
PE:10
Optimize operational tasks.
Monitor and minimize the effects of the software development lifecycle and other routine operations on workload performance. These operations include virus scans, secret rotations, backups, reindexing databases, and deployments.
☐
PE:11
Respond to live performance issues.
Plan how to address performance problems by incorporating clear lines of communication and responsibilities. When a problematic situation occurs, use what you learn to identify preventive measures and incorporate them in your workload. Implement methods to return to normal operations faster when similar situations occur.
☐
PE:12
Continuously optimize performance.
Focus on components that show deteriorating performance over time, such as databases and networking features.
Next steps
We recommend that you review the Performance Efficiency tradeoffs to explore other concepts.
Performance Efficiency tradeoffs
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Collect workload performance data - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for collecting performance data
Article
2023-11-15
3 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:04
Collect performance data. Workload components and flows should provide automatic, continuous, and meaningful metrics and logs. Collect data at different levels of the workload, such as the application, platform, data, and operating system levels.
Collecting performance data is the process of gathering metrics and logs that provide information about the performance of a workload. This data includes numerical values, which are known as
metrics
. Metrics describe the state of the system at a particular point in time. It also includes logs that contain different types of data organized into records.
By collecting performance data, you can monitor and analyze the performance of a workload. You can use this information to identify performance bottlenecks, to troubleshoot issues, to optimize resource allocation, and to make data-driven decisions to improve the overall performance efficiency of the workload.
Without data-driven insights, you might be unaware of underlying performance issues or opportunities for optimization. Potential results include slower response times, decreased throughput, increased resource usage, and ultimately, a suboptimal user experience. Additionally, the lack of performance data makes it difficult to diagnose and troubleshoot issues in a timely manner, leading to prolonged downtime and reduced productivity.
Definitions
Term
Definition
Activity logs
Logs that track management operations on resources, such as deleting a resource.
Application logs
Logs that track information about application events, errors, and other activities, such use sign-ins and database connection failures.
Application performance monitoring (APM) tool
A tool that monitors and reports the performance of an application.
Code instrumentation
The direct or indirect capture of performance metrics from the perspective of the application code. Captured metrics include flow metrics, resource use, and metrics specific to the language or runtime.
Distributed tracing
Gathering and correlating metrics across distributed workload components.
Metrics sink
A storage destination for your metrics that correlates time series data for analysis.
Platform logs
Diagnostic and auditing data that includes resource logs, activity logs, and audit logs.
Platform metrics
Numerical values that record workload performance at a particular time.
Resource logs
Data that a system generates. It provides information about the state of the system.
Rx/Tx errors
The number of receive errors and transmit errors on a network interface.
Structured logging
Defining a meaningful format to log messages, typically as key-value pairs.
Key design strategies
Performance optimization requires data to measure the current performance of a workload or a flow against its performance targets. You need to collect the right amount and diversity of data to measure the performance of the code and the infrastructure against
performance targets
. Ensure that every component and flow within the workload automatically generates continuous and meaningful metrics and logs. You need to source this data from diverse levels like the application, platform, storage, and operating system. Comprehensive performance data collection allows for a holistic understanding of performance, enabling precise identification of inefficiencies and avenues for improvement.
Centralize the collection of performance data
Centralizing performance metrics and logs is the process of collecting performance metrics and logs from various sources and storing them in a central location. Create a central metrics sink and a central log sink. This centralization allows for easy access, analysis, and monitoring of performance metrics and logs across different systems and components. By centralizing metrics and logs, you gain visibility into the performance of your workload. Choose a suitable platform or tool that can aggregate and store workload performance metrics and logs.
Tradeoff
: Understand the cost of collecting metrics and logs. In general, the more metrics and logs you collect, the higher the cost.
Segment performance data
Segmenting performance data involves organizing and categorizing metrics and logs based on their origin, purpose, or environment. For example, you should separate production data from nonproduction data or distinguish between performance targets and business metrics. Segmenting data helps with optimizing specific environments, facilitates troubleshooting, and limits inaccuracies in performance monitoring. By maintaining a clear distinction between different data types, you can capture, analyze, and respond to relevant metrics more efficiently and better align workload health with workload objectives. To segment performance data, consider the following recommendations:
Keep production data and nonproduction data separate
. By separating data by environment, you can ensure focused monitoring and optimization of each environment. In production environments, you can better identify and address performance issues that directly affect users and business operations. In nonproduction environments, the data separation facilitates effective troubleshooting and fine-tuning during the testing phase before you deploy to production.
Use one set of data within each environment
. Don't use one set of data for performance targets and another set of data for alerts related to the performance targets. Using different sets of data leads to inaccurate alerts that undermine the effectiveness of performance monitoring.
Separate performance targets and business metrics
. The operations and development teams use performance targets to monitor workload health and meet business targets. Business metrics relate to business goals or customer reporting. Capture business metrics in a separate data stream, even if the data directly overlaps. The separation gives you flexibility to capture the right data and independently analyze the data.
Define retention policies
Retention policies dictate how long performance data should be kept. Establishing these policies helps manage storage efficiently and ensures only necessary data is accessible for analysis. Such policies support better performance and meet compliance standards. You should configure retention policies for the log and metrics data to enable effective troubleshooting and monitoring in all environments. For example, the logs and metrics might need to be kept for longer time in a production environment than in the testing environment. The retention period should match your organization's requirements and compliance regulations. Decide how long to retain the data for analysis and audit purposes. Archive the data that you don't need for immediate analysis.
Collect application performance data
Collecting application data involves monitoring and analyzing an application's performance metrics, such as throughput, latency, and completion times, primarily gathered through instrumenting code. Application performance data provides valuable insights into the health and performance of an application. By monitoring and analyzing performance data, you can identify and troubleshoot issues, optimize application performance, and make informed decisions for your application.
Instrument code
Instrumentation refers to the process of embedding code snippets or integrating tools into an application code. The purpose of instrumentation is to capture performance data while the application runs. It's essential to gather metrics that highlight the application's critical operations. Focus on metrics like throughput, latency, and completion time. It's important to differentiate between business-related operations and operations that aren't. For data pertaining to business operations, make sure its metadata is structured in a way that allows distinct tracking and storage. The primary reason for code instrumentation is to collect data on how the application handles its workload. It provides the following benefits:
Identifying performance bottlenecks:
By tracking metrics such as CPU use and memory use, you can identify bottlenecks and optimize the code accordingly.
Evaluating system behavior under a load:
You can see how the application performs under different workloads and stress scenarios. This data can help you identify issues related to scalability, concurrency, and resource use.
Tracking application health and availability:
Because key performance indicators are monitored in real time, you can get alerts about potential issues that affect the application's performance and availability.
Improve user experience:
You can gain insights into how users interact with the application. Use this information to optimize the user experience and identify areas for improvement.
Plan capacity and allocate resources:
The performance data that instrumentation gathers can provide valuable insights into the resource requirements of an application. This information can inform your decisions about planning capacity and allocating resources.
When you instrument code for performance monitoring, consider the following strategies:
Use APM tools
: APM tools can collect and analyze performance data, including metrics, traces, and logs. APM tools offer features like code-level instrumentation, transaction tracing, and performance profiling.
Use logging and tracing frameworks
: Logging and tracing frameworks are tools or libraries that developers integrate into their applications to facilitate logging and tracing. These frameworks provide functions to generate logs, trace requests, and sometimes even format or transport the generated data. By incorporating logging and tracing frameworks into the code base, developers can capture relevant data during runtime. The data can include information about the running path, I/O, and performance.
Custom instrumentation
: Developers can add custom code to collect performance metrics that are unique to their application and workload. The custom instrumentation can measure runtimes, track resource usage, or capture specific events. Write custom code instrumentation only when platform metrics are insufficient. In some situations, the platform resource can measure aggregate or even granular perspectives of your application. Weigh the question of whether to duplicate that effort by using custom code against excess code tradeoffs or dependency on a platform feature.
Capture transaction times.
Capturing transaction times relates to measuring the end-to-end times for key technical functions as a part of performance monitoring. Application-level metrics should include end-to-end transaction times. These transaction times should cover key technical functions such as database queries, response times for external API calls, and failure rates of processing steps.
Use telemetry standards.
Consider using APM tool instrumentation libraries and tools that are built around a telemetry standard, such as OpenTelemetry.
Enable distributed tracing
Distributed tracing is a technique used to track and monitor requests as they flow through a distributed system. It allows you to trace the path of a request as it travels across multiple services and components, providing valuable insights into the performance and efficiency of your workload. Distributed tracing is important for performance efficiency because it helps identify bottlenecks, latency issues, and areas for optimization within a distributed system. You can pinpoint where delays or inefficiencies occur and take appropriate actions to improve performance by visualizing the flow of a request. Follow these steps to enable distributed tracing:
Start by instrumenting your applications and services to generate trace data. Use libraries or frameworks that support distributed tracing, such as OpenTelemetry.
Ensure that trace information is propagated across service boundaries. You should typically pass a unique trace ID and other contextual information with each request.
Set up a centralized trace collection system. This system collects and stores the trace data generated by your applications and services.
Use the trace data collected to visualize the end-to-end flow of requests and analyze the performance characteristics of your distributed system.
Collect application logs
When you instrument code, one of the primary outputs should be application logs. Logging helps you understand how the application runs in various environments. Application logs record the conditions that produce application events. Collect application logs across all application environments. Corresponding log entries across the application should capture a correlation ID for their respective transactions. The correlation ID should correlate application log events across critical application flows such as user sign-in. Use this correlation to assess the health of key scenarios in the context of targets and nonfunctional requirements.
You should use structured logging. Structured logging speeds up log parsing and analysis. It makes the logs easier to index, query, and report without complexity. Add and use a structured logging library in your application code. Sometimes log entries can help you correlate data that you couldn't correlate by other means.
Collect resource performance data
By collecting resource performance data, you can gain insights into the health and behavior of your workload. Resource performance data provides information about resource use, which is key for capacity planning. This data also provides insights into the health of a workload and can help you detect issues and troubleshoot. Consider the following recommendations:
Collect metrics and logs for every resource.
Each Azure service has a set of metrics that's unique to the functionality of the resource. These metrics help you understand the resource's health and performance. Add a
diagnostic setting
for each resource to send metrics to a location that your workload team can access as they build alerts and dashboards. Metric data is available for short-term access. For long-term access or for access from a system that's outside of Azure Monitor, send the metric data to your unified sink to the access location.
Use platform tooling.
Gather inspiration from built-in and integrated monitoring solutions, such as Azure Monitor Insights. This tooling streamlines performance operations. Consider platform tooling as you select a platform and invest in custom tooling or reporting.
Monitor network traffic.
Monitoring network traffic means to track and analyze the flow and patterns of data as it moves across network pathways. Collect traffic analytics and monitor the traffic that traverses subnet boundaries. Your goal is to analyze and optimize network performance.
Collect database and storage data
Many database and storage systems provide their own monitoring tools. These tools collect performance data specific to those systems. Database and storage systems often generate logs that contain performance-related events and indicators. Collect database data and storage performance data so you can identify bottlenecks, diagnose issues, and make informed decisions to improve the overall performance and reliability of your workload. Consider collecting the following types of performance data:
Throughput
: Throughput measure the amount of data read from or written to the storage system over a period of time. Throughput data indicates the data transfer capabilities.
Latency
: Latency measures how long storage operations last. Latency data indicates the responsiveness of the storage system.
IOPS (I/O operations per second)
: Data about the number of read operations or write operations that the storage system can perform in a second. IOPS data indicates the storage system's throughput and responsiveness.
Capacity use
: Capacity use is the amount of storage capacity used and the amount that's available. Capacity-use data helps organizations plan for future storage needs.
For databases, you should also collect database-specific metrics:
Query performance
: Data about the execution time, resource usage, and efficiency of database queries. Slow or inefficient database queries can significantly slow down a workload. Look for queries that are slow and that run frequently.
Transaction performance
: Data about the performance of database transactions, such as transaction duration, concurrency, and lock contention.
Index performance
: Data about the performance of database indexes, such as index fragmentation, usage statistics, and query optimization.
Resource use
: Data that includes CPU, memory, disk space, I/O, and network bandwidth.
Connection metrics
: Metrics that track the number of active, aborted, and failed connections. High failure rates could indicate network issues or could indicate that the database reached its maximum number of connections.
Transaction rates
: The number of transactions that a database runs per second. A change in transaction rates can indicate performance issues.
Error rates
: Data that indicates a database performance. High error rates might indicate a performance issue. Collect and analyze database errors.
Collect operating system data
A platform as a service (PaaS) solution eliminates the need to collect operating system performance data. However, if your workload runs on virtual machines (infrastructure as a service), you need to collect performance data about the operating system. You need to understand the demand on your operating system and virtual machine. Frequently sample operating system performance counters. For example, you could sample the performance counters every minute.
At a minimum, collect data about the following performance areas.
Performance area
Process or function
CPU
- CPU usage (user mode or privileged mode)
- CPU queue length (number of processes that are waiting for CPU time)
Process
- Process thread count
- Process handle count
Memory
- Committed memory
- Available memory
- Pages per second
- Swap space usage
Disk
- Disk read
- Disk writes
- Disk throughput
- Disk space usage
Network
- Network interface throughput
- Network interface Rx/Tx errors
Validate and analyze data
Your performance data should align with the performance targets. The data needs to represent workload or flow performance completely and accurately as it relates to performance targets. For example, the response time for a web service has a performance target of 500 ms. Make it a routine to analyze the data, as frequent evaluations allow for early detection and mitigation of performance issues.
Create alerts.
It's beneficial to have alerts that are actionable, enabling prompt identification and rectification of performance problems. These alerts should clearly indicate the breached performance threshold, the potential business effect, and the involved components. Start by setting common and recommended alert. Over time, you can modify these criteria based on your specific needs. The primary objective of these alerts should be to forecast potential performance drops before they escalate into significant issues. If you can't set an alert for an external dependency, consider devising a method to gather indirect measurements, like the duration of a dependency call.
Set data collection limits.
Determine and set logical limits on the volume of data you collect and its retention duration. Telemetry can sometimes produce overwhelming amounts of data. It's essential to focus on capturing only the most vital performance indicators or have an efficient system in place to extract meaningful insights from your performance data.
Azure facilitation
Centralizing, segmenting, and retaining performance data
:
Azure Monitor
collects and aggregates data from every layer and component of your workload across multiple Azure and non-Azure subscriptions and tenants. It stores the data in a common data platform for consumption by a common set of tools that can correlate, analyze, visualize, and/or respond to the data.
You need at least one
Log Analytics workspace
to enable Azure Monitor Logs. You can use a single workspace for all your data collection. You can also create multiple workspaces based on requirements to segment performance data. It also allows you to define
retention policies
.
Collecting application performance data
:
Application Insights
is a feature of Azure Monitor that helps you monitor the performance and availability of your application. It provides application-level insights by collecting telemetry data such as request rates, response times, and exception details. You can enable Application Insights for your application and configure it to collect the necessary performance data. Application Insights also supports
distributed tracing
. Configure distributed tracing for all flows. To build end-to-end transaction flows, correlate events that come from different application components or tiers.
Performance counters are a powerful way to monitor the performance of your application. Azure provides various performance counters that you can use to collect data about CPU usage, memory usage, disk I/O, network traffic, and more. If you configure your application to emit performance counter data, Azure Monitor collects and stores the data for analysis.
Collecting resource performance data
: Most Azure services generate platform logs and metrics that provide diagnostic and auditing information. By enabling diagnostic settings, you can specify the platform logs and metrics to collect and store. For correlation purposes, enable diagnostics for all supported services and then send the logs to the same destination as your application logs.
Collecting database and storage performance data
: Azure Monitor allows you to collect performance data for databases in Azure. You can enable monitoring for Azure SQL Database, Azure Database for MySQL, Azure Database for PostgreSQL, and other database services. Azure Monitor provides metrics and logs for monitoring database performance, including CPU use, memory use, and query performance. To be notified of issues, you can set up alerts based on performance thresholds.
Azure offers performance recommendations for databases, such as SQL Server on Azure Virtual Machines. These recommendations help you optimize the performance of your database workloads. They include suggestions for collecting performance counters, capturing wait statistics, and gathering performance data during peak hours.
Azure Storage Analytics allows you to collect performance data for Azure Storage services like Blob Storage, Table Storage, and Queue Storage. You can enable logging and metrics for your storage accounts to monitor key performance indicators, such as the number of read/write operations, throughput, and latency.
Collecting operating system performance data:
The Azure Diagnostics extension enables you to collect detailed performance data from your virtual machines (VMs), including CPU, memory, disk I/O, and network traffic. This data can be sent to Azure Monitor or other storage services for analysis and alerting.
Validating and analyzing performance data
: Within Azure Monitor, you can use Azure Monitor Logs to collect, analyze, and visualize log data from your applications and systems. You can configure Azure Monitor Logs to ingest logs from your application, including application-level logs and infrastructure logs. By aggregating logs, you can cross-query events and gain insights into the performance of your application. For more information, see
Azure Monitor Logs cost calculations and options
and
Pricing for Azure Monitor
.
In Azure Monitor, you can define alert rules to monitor specific performance metrics and trigger alerts based on predefined conditions. For example, you can create an alert rule to notify you when CPU usage exceeds a certain threshold or when response time goes above a specified limit. Configure the alert rule to send notifications to the desired recipients.
When you create an alert rule, you can define the criteria that determine when an alert should be triggered. You can set thresholds, aggregation methods, time windows, and the frequency of evaluation. Define the criteria based on your performance monitoring requirements. In addition to sending notifications, you can specify actions to be taken when an alert is triggered. Actions can include sending emails, calling webhooks, or running Azure functions. Choose the appropriate actions to respond to the specific alert scenario.
Examples
Baseline highly available zone-redundant app services web application
Monitor a microservices application in Azure Kubernetes Service (AKS)
Enterprise monitoring with Azure Monitor
Related links
Platform metrics
Diagnostic settings
Audit logs
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for continuous performance optimization - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for continuous performance optimization
Article
2023-11-15
4 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:12
Continuously optimize performance. Focus on components that show deteriorating performance over time, such as databases and networking features.
This guide describes the recommendations for continuous performance optimization. Continuous performance optimization is the process of constantly monitoring, analyzing, and improving performance efficiency. Performance efficiency adapts to increases and decreases in demand. Performance optimization needs to be an ongoing activity throughout the life of the workload. Workload performance often degrades or becomes excessive over time, and factors to consider include changes in usage patterns, demand, features, and technical debt.
Definitions
Term
Definition
Data tiering
A storage strategy that involves categorizing data based on its access frequency and storing it on storage tiers accordingly.
Technical debt
The accumulated inefficiencies, suboptimal design choices, or shortcuts intentionally taken during the development process to deliver code faster.
Time-to-live
A mechanism that sets an expiration time for data.
Key design strategies
Performance efficiency is when workload capacity aligns to actual usage. A workload that overperforms is as problematic as one that underperforms. The tradeoffs differ. Overperformance affects cost optimization. Poor performance affects users. The key to performance efficiency is monitoring, adjusting, and testing over time. You need to regularly review performance metrics and make adjustments as necessary to ensure that the workload is efficient. Testing all changes pre- and post-implementation is required to reach performance targets.
Develop a performance culture
A performance culture is an environment in which continuous improvement is expected and the team learns from production. Performance optimization requires specialized skills. Workload teams need the right skills and mindset to optimize their performance to meet increases and decreases in demand. You also need to allocate their time to support the required monitoring and remediation of performance issues as they arise. These teams need clear expectations. For example, performance targets, baselines, and deviation thresholds (how far from baseline is acceptable) need to be highly visible and socialized.
Tradeoff
: Continuous performance optimizations require a team that has the right skills and time to find and fix performance issues. Dedicating personnel to performance adds operational cost. If you have limited personnel resources, continuous performance optimization could take time away from other operational tasks.
Evaluate new platform features
Evaluating new platform features involves examining the new functionalities and tools of a platform that can improve performance efficiency, such as optimized storage solutions, caching mechanisms, or resource management tools. New platform features can open avenues for enhancing performance efficiency. Keep your platform and tools up-to-date to ensure you're using the latest innovations and best practices. Consistently monitor feedback and performance metrics from these new additions to refine your approach.
Prioritize optimization efforts
Proactively optimizing performance means taking proactive measures to improve and enhance the performance of the workload before any performance issues arise. Using proactive measures involves identifying potential bottlenecks, monitoring performance metrics, and implementing optimizations to ensure that the workload operates efficiently and meets the desired performance goals. Based on the analysis of deteriorating components, critical flows, and technical debt, you can implement performance optimizations specific to each area. Improvements might involve code changes, infrastructure adjustments, or configuration updates.
Prioritize deteriorating components
A workload often has components such as databases and networking components that are prone to performance degradations over time. As the workload evolves and usage patterns change, these changes often affect the performance of individual components in the workload. Increased data in databases can lead to longer query run times and slower data retrieval. Changes in usage patterns might result in suboptimal query design. Queries that were once efficient can become inefficient as the workload evolves. Inefficient queries can consume excessive resources and degrade database performance. Increased workload usage can lead to higher network traffic, causing congestion and latency issues.
It's important to make continuous efforts to optimize the performance of these components. Proactively identify and address performance issues in your workload. By prioritizing known deteriorating components, you can proactively address potential performance issues and ensure the smooth operation of your workload. It might involve implementing performance tuning techniques, optimizing resource allocation, or upgrading hardware or software components as needed.
Prioritize critical flows
Critical flows are the most important and high-priority processes or workflows in the workload. By prioritizing these critical flows, you ensure that the most essential parts of the workload are optimized for performance. Knowing which flows are critical helps prioritize optimization efforts. Optimizing the performance efficiency of the most important areas of your application provides the highest return on investment. You should monitor critical flows and the most popular pages. Look for ways to make them more efficient.
Automate performance optimization
Automation can eliminate repetitive and time-consuming manual processes, allowing them to be performed efficiently. Automation reduces the chances of human error and ensures consistency in running optimization tasks. By automating these tasks, you can also free up people to focus on more complex activities and activities that add value. You can apply automation to various tasks, such as performance testing, deployment, and monitoring:
Automated performance testing
: Use automated performance testing tools like JMeter, K6, or Selenium to simulate different workloads and scenarios.
Automated deployment
: Implement automated deployment processes to ensure consistent and error-free deployments. Use CI/CD tools to automate the deployment process. These tools can help you identify performance bottlenecks as you use them to test against endpoints, check HTTP statuses, and even validate data quality and variations.
Monitoring and alerting
: Set up automated monitoring and alerting systems to continuously monitor performance metrics and detect any deviations or anomalies. When performance issues are detected, automated alerts can be triggered to notify the appropriate teams or individuals.
Incident management
: Implement an automated incident management system that can receive alerts, create tickets, and assign tickets to the appropriate teams for resolution. These steps help ensure that performance issues are promptly addressed and assigned to the right resources.
Automated diagnostics
: Develop automated diagnostic tools or scripts that can analyze performance data and identify the root causes of performance issues. These tools can help pinpoint specific areas or components of the system that are causing performance problems.
Automated remediation actions
: Define and implement automated remediation actions that can be triggered when specific performance issues are detected. These actions can include restarting services, adjusting resource allocation, clearing caches, or implementing other performance optimization techniques.
Self-healing systems
: Build self-healing capabilities into your system by automating the recovery process for known performance issues. This capability can involve automatically fixing or adjusting the system configuration to restore optimal performance.
Address technical debt
Technical debt refers to the accumulated inefficiencies, suboptimal design choices, or shortcuts taken during the development process that can affect performance. Technical debt, unclear code, and overly complex implementations can make performance efficiency more difficult to attain. Addressing technical debt involves identifying and resolving these issues to improve the overall performance and maintainability of the workload. This work might include refactoring code, optimizing database queries, improving architectural design, or implementing best practices. Perhaps you introduced technical debt to meet a deadline, but you need to address the technical debt as you optimize performance efficiency over time.
Optimize databases
Continuously optimizing databases involves identifying and implementing optimizations to ensure that databases can handle loads, deliver fast response times, and minimize resource utilization. By regularly optimizing databases, you can improve application performance, reduce downtime, and enhance the overall user experience.
Optimize database queries
: Poorly written SQL statements can degrade database performance. Inefficient JOIN conditions can cause unneeded data processing. Complex subqueries, nested queries, and excessive functions can reduce running speed. Queries that retrieve too much data should be rewritten. You should identify your most common or critical database queries and optimize them. The optimization helps ensure faster queries.
Maintain indexes
: Evaluate your indexing strategy to ensure that indexes are properly designed and maintained. Index maintenance includes identifying unused or redundant indexes and creating indexes that align with the query patterns. Database indexes help accelerate data retrieval operations. For relational databases, you need to monitor index fragmentation. You should rebuild or reorganize indexes regularly. For nonrelational databases, you need to pick the correct indexing policy for your workload. Use automatic tuning on databases where available. These features include automatically creating missing indexes, dropping unused indexes, and plan correction. For more information, see
Maintaining indexes to improve performance
.
Review model design
: Review the data model to ensure you optimize it for the specific requirements of the application. Improving query performance and data retrieval might involve denormalization, partitioning, or other techniques.
Optimize database configuration
: Optimize database configuration settings such as memory allocation, disk I/O, and concurrency settings to maximize performance and resource utilization.
Optimize data efficiency
Optimizing data efficiency is the process of ensuring that data is stored, processed, and accessed in the most efficient way possible. Data tiering and using time-to-live (TTL) are techniques that can be used to optimize data efficiency. You can apply these techniques in various data storage scenarios, such as databases, file systems, or object storage.
Use data tiering
: Data tiering involves categorizing data based on its importance or frequency of access and storing data in different tiers accordingly. Setting up data tiering allows for more efficient use of storage resources and improves performance. Frequently accessed or critical data can be stored in high-performance tiers, while less frequently accessed or less critical data can be stored in lower-cost tiers. The goal is to review data usage over time to ensure data is in the correct tier. As data priorities change, data should move from one tier to another.
Implement time-to-live
: Time-to-live is a mechanism that sets an expiration time for data. Time-to-live allows data to be automatically deleted or archived after a certain period, reducing storage requirements and improving data management. By setting an appropriate time-to-live, you allow unnecessary data to be removed, freeing up storage space and improving overall efficiency. Session data, temporary files, and cache data are frequent targets for the time-to-live. Database entries can also have a time-to-live.
Risk
: A time-to-live that's too short can create performance issues.
Azure facilitation
Automating performance optimization
: Azure Advisor provides automatic
performance recommendations
based on workload telemetry. You should review and address these recommendations regularly. Azure Monitor provides real-time insights into the performance of your system and allows you to set up alerts based on specific performance metrics. Azure Log Analytics provides automated diagnostics and analysis on collected logs and metrics. Tools like Azure Application Insights provide insights and recommendations for optimizing performance.
To automate remediation, use automation tools or scripts to execute remediation actions automatically when the alerts are triggered. You can use Azure Automation, Azure Functions, or custom automation solutions.
Azure lets performance testing to simulate different user scenarios and workloads. Automated testing can help you identify performance bottlenecks and optimize your system accordingly. Tools like Azure DevOps can automate performance testing.
Optimizing databases
: The SQL family of products has many
built-in features
that allow you to monitor and remediate SQL database performance. You should use these features to maintain database performance. Azure SQL Database has an
automatic tuning
feature that continuously monitors and improves queries. You should use this feature to improve SQL queries automatically.
You can
customize your indexing policies
by using the features of Azure Cosmos DB. Customize the policies to meet the performance needs of your workload.
Optimizing data efficiency
: Data tiering allows you to store data in different tiers based on its access frequency and importance. It helps optimize storage costs and performance. Azure provides different storage tiers, such as hot, cool, and archive tiers for blob data. Hot tiers are optimized for frequently accessed data, cool tiers are for infrequently accessed data, and archive tiers are for rarely accessed data. By using the storage access tier best suited to your data, you can ensure efficient data storage and retrieval.
Related links
Optimize index maintenance to improve query performance and reduce resource consumption
Improve the performance of Azure applications by using Azure Advisor
Automatic tuning in Azure SQL Database and Azure SQL Managed Instance
Indexing policies in Azure Cosmos DB
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Cloud design patterns that support performance efficiency - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Cloud design patterns that support performance efficiency
Article
2024-10-10
2 contributors
Feedback
In this article
When you design workload architectures, you should use industry patterns that address common challenges. Patterns can help you make intentional tradeoffs within workloads and optimize for your desired outcome. They can also help mitigate risks that originate from specific problems, which can impact reliability, security, cost, and operations. If not mitigated, risks will eventually lead to performance inefficiencies. These patterns are backed by real-world experience, are designed for cloud scale and operating models, and are inherently vendor agnostic. Using well-known patterns as a way to standardize your workload design is a component of operational excellence.
Many design patterns directly support one or more architecture pillars. Design patterns that support the Performance Efficiency pillar address scalability, performance tuning, task prioritization, and the removal of bottlenecks.
Design patterns for performance efficiency
The following table summarizes cloud design patterns that support the goals of performance efficiency.
Pattern
Summary
Asynchronous Request-Reply
Improves the responsiveness and scalability of systems by decoupling the request and reply phases of interactions for processes that don't need immediate answers. By using an asynchronous pattern, you can maximize concurrency on the server side. You can use this pattern to schedule work to be completed as capacity allows.
Backends for Frontends
Individualizes the service layer of a workload by creating separate services that are exclusive to a specific frontend interface. This separation enables you to optimize in ways that might not be possible with a shared service layer. When you handle individual clients differently, you can optimize performance for a specific client's constraints and functionality.
Bulkhead
Introduces segmentation between components to isolate the blast radius of malfunctions. This design enables each bulkhead to be individually scalable to meet the needs of the task that's encapsulated in the bulkhead.
Cache-Aside
Optimizes access to frequently read data by introducing a cache that's populated on demand. The cache is then used on subsequent requests for the same data. This pattern is especially useful with read-heavy data that doesn't change often and can tolerate a certain amount of staleness. The goal of this implementation is to provide better performance in the system overall by offloading this type of data to a cache instead of sourcing it from its data store.
Choreography
Coordinates the behavior of autonomous distributed components in a workload by using decentralized, event-driven communication. This pattern can provide an alternative when performance bottlenecks occur in a centralized orchestration topology.
Circuit Breaker
Prevents continuous requests to a malfunctioning or unavailable dependency. A retry-on-error approach can lead to excessive resource utilization during dependency recovery and can also overload performance on a dependency that's attempting recovery.
Claim Check
Separates data from the messaging flow, providing a way to separately retrieve the data related to a message. This pattern improves the efficiency and performance of message publishers, subscribers, and the message bus itself when the system handles large data payloads. It works by decreasing the size of messages and ensuring that consumers retrieve payload data only if necessary and at an opportune time.
Competing Consumers
Applies distributed and concurrent processing to efficiently handle items in a queue. This model supports distributing load across all consumer nodes and dynamic scaling that's based on queue depth.
Compute Resource Consolidation
Optimizes and consolidates compute resources by increasing density. This pattern combines multiple applications or components of a workload on a shared infrastructure. This consolidation maximizes the utilization of computing resources by using spare node capacity to reduce overprovisioning. Container orchestrators are a common example. Large (vertically scaled) compute instances are often used in the resource pool for these infrastructures.
Command and Query Responsibility Segregation (CQRS)
Separates the read and write operations of an application's data model. This separation enables targeted performance and scaling optimizations for each operation's specific purpose. This design is most helpful in applications that have a high read-to-write ratio.
Deployment Stamps
Provides an approach for releasing a specific version of an application and its infrastructure as a controlled unit of deployment, based on the assumption that the same or different versions will be deployed concurrently. This pattern often aligns to the defined scale units in your workload: as additional capacity is needed beyond what a single scale unit provides, an additional deployment stamp is deployed for scaling out.
Event Sourcing
Treats state change as series of events, capturing them in an immutable, append-only log. Depending on your workload, this pattern, usually combined with CQRS, an appropriate domain design, and strategic snapshotting, can improve performance. Performance improvements are due to the atomic append-only operations and the avoidance of database locking for writes and reads.
Federated Identity
Delegates trust to an identity provider that's external to the workload for managing users and providing authentication for your application. When you offload user management and authentication, you can devote application resources to other priorities.
Gatekeeper
Offloads request processing that's specifically for security and access control enforcement before and after forwarding the request to a backend node. This pattern is often used to implement throttling at a gateway level rather than implementing rate checks at the node level. Coordinating rate state among all nodes isn't inherently performant.
Gateway Aggregation
Simplifies client interactions with your workload by aggregating calls to multiple backend services in a single request. This design can incur less latency than a design in which the client establishes multiple connections. Caching is also common in aggregation implementations because it minimizes calls to backend systems.
Gateway Offloading
Offloads request processing to a gateway device before and after forwarding the request to a backend node. Adding an offloading gateway to the request process enables you to use less resources per-node because functionality is centralized at the gateway. You can optimize the implementation of the offloaded functionality independently of the application code. Offloaded platform-provided functionality is already likely to be highly performant.
Gateway Routing
Routes incoming network requests to various backend systems based on request intents, business logic, and backend availability. Gateway routing enables you to distribute traffic across nodes in your system to balance load.
Geode
Deploys systems that operate in active-active availability modes across multiple geographies. This pattern uses data replication to support the ideal that any client can connect to any geographical instance. You can use it to serve your application from a region that's closest to your distributed user base. Doing so reduces latency by eliminating long-distance traffic and because you share infrastructure only among users that are currently using the same geode.
Health Endpoint Monitoring
Provides a way to monitor the health or status of a system by exposing an endpoint that's specifically designed for that purpose. You can use these endpoints to improve load balancing by routing traffic to only nodes that are verified as healthy. With additional configuration, you can also get metrics on available node capacity.
Index Table
Optimizes data retrieval in distributed data stores by enabling clients to look up metadata so that data can be directly retrieved, avoiding the need to do full data store scans. Clients are pointed to their shard, partition, or endpoint, which can enable dynamic data partitioning for performance optimization.
Materialized View
Uses precomputed views of data to optimize data retrieval. The materialized views store the results of complex computations or queries without requiring the database engine or client to recompute for every request. This design reduces overall resource consumption.
Priority Queue
Ensures that higher-priority items are processed and completed before lower-priority items. Separating items based on business priority enables you to focus performance efforts on the most time-sensitive work.
Publisher/Subscriber
Decouples components of an architecture by replacing direct client-to-service or client-to-services communication with communication via an intermediate message broker or event bus. The decoupling of publishers from consumers enables you to optimize the compute and code specifically for the task that the consumer needs to perform for the specific message.
Queue-Based Load Leveling
Controls the level of incoming requests or tasks by buffering them in a queue and letting the queue processor handle them at a controlled pace. This approach enables intentional design on throughput performance because the intake of requests doesn't need to correlate to the rate in which they're processed.
Scheduler Agent Supervisor
Efficiently distributes and redistributes tasks across a system based on factors that are observable in the system. This pattern uses performance and capacity metrics to detect current utilization and route tasks to an agent that has capacity. You can also use it to prioritize the execution of higher priority work over lower priority work.
Sharding
Directs load to a specific logical destination to handle a specific request, enabling colocation for optimization. When you use sharding in your scaling strategy, the data or processing is isolated to a shard, so it competes for resources only with other requests that are directed to that shard. You can also use sharding to optimize based on geography.
Sidecar
Extends the functionality of an application by encapsulating non-primary or cross-cutting tasks in a companion process that exists alongside the main application. You can move cross-cutting tasks to a single process that can scale across multiple instances of the main process, which reduces the need to deploy duplicate functionality for each instance of the application.
Static Content Hosting
Optimizes the delivery of static content to workload clients by using a hosting platform that's designed for that purpose. Offloading responsibility to an externalized host helps mitigate congestion and enables you to use your application platform only to deliver business logic.
Throttling
Imposes limits on the rate or throughput of incoming requests to a resource or component. When the system is under high demand, this pattern helps mitigate congestion that can lead to performance bottlenecks. You can also use it to proactively avoid noisy neighbor scenarios.
Valet Key
Grants security-restricted access to a resource without using an intermediary resource to proxy the access. Doing so offloads processing as an exclusive relationship between the client and the resource without requiring an ambassador component that needs to handle all client requests in a performant way. The benefit of using this pattern is most significant when the proxy doesn't add value to the transaction.
Next steps
Review the cloud design patterns that support the other Azure Well-Architected Framework pillars:
Cloud design patterns that support reliability
Cloud design patterns that support security
Cloud design patterns that support operational excellence
Cloud design patterns that support cost optimization
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Performance efficiency quick links - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Read in English
Table of contents
Read in English
Edit
Share via
Facebook
x.com
LinkedIn
Email
Table of contents
Performance efficiency quick links
Apply performance efficiency guidance to your architecture to efficiently meet workload demands.
Learn key points
Quickstart
Design principles
Checklist
Tradeoffs
Performance efficiency patterns
Azure Well-Architected Review assessment
Training
Performance efficiency
video
Performance efficiency: Fast & furious
Design principles
Concept
Negotiate realistic performance targets
Design to meet capacity requirements
Achieve and sustain performance
Improve efficiency through optimization
Achieve performance targets
How-To Guide
Set performance targets
Select the right services
Conduct capacity planning
Collect performance data
Improve efficiency
How-To Guide
Scaling and partitioning
Code and infrastructure
Data
Critical flows
Operational tasks
Continuous optimization
Implement testing and incident response
How-To Guide
Run performance testing
Address performance incidents
Explore related resources
Reference
Azure Advisor: Performance recommendations
Application Insights
Azure Load Testing
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for optimizing code and infrastructure - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for optimizing code and infrastructure
Article
2023-11-15
5 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:07
Optimize code and infrastructure. Use code that's performant, and ensure that it offloads responsibilities to the platform. Use code and infrastructure only for their core purpose and only when necessary.
This guide describes the recommendations for optimizing code and infrastructure performance. To optimize your code and infrastructure, you should use your components only for their core purpose and only when necessary. When you overuse code and infrastructure, it creates unnecessary resource consumption, bottlenecks, and slow responses. To compensate for those inefficiencies, you must add more resources to accomplish the same tasks.
Definitions
Term
Definition
Concurrency
When multiple tasks or processes are performed at once but not necessarily at the exact same time.
CPU architecture
The components and principles that affect how the computer works.
Data compression
The action of reducing the size of files by minimizing redundant data.
Heap
An area in memory used for runtime memory allocations.
Memory leak
When a workload fails to release allocated memory after the memory is no longer needed.
Parallelism
When multiple tasks or processes are performed at the same time.
Key design strategies
Optimizing code and infrastructure entails fine-tuning the code and the supporting infrastructure to improve performance efficiency. It requires performant code that executes tasks quickly and doesn’t waste resources. It requires a well-designed infrastructure that is streamlined to avoid unnecessary complexity. A workload should use the inherent capabilities of the platform. It's an approach that helps ensure both code and infrastructure are used primarily for their core purposes and only when necessary.
Optimize code performance
To optimize code performance, modify code to reduce resource usage, minimize runtime, and enhance performance. You can modify code to improve the efficiency and speed of a software program. Don't mask performance issues with brute force. Brute force means adding compute resources to compensate for code performance, like adding extra capacity instead of addressing the source. You need to fix performance issues with optimization. When you optimize code performance, it helps maximize the utilization of system resources, improves response time, reduces latency, and enhances the user experience.
Instrument your code
Instrumenting code refers to the practice of adding code snippets or libraries to code that collect data and monitor code performance during runtime. Code instrumentation allows developers to gather information about key metrics such as resource consumption (CPU, memory usage) and execution time. By instrumenting code, developers can gain insights into code hot paths, identify performance bottlenecks, and optimize the code for better performance efficiency.
In an ideal environment, you should do code analysis early in the software development lifecycle. The earlier you catch a code issue, the cheaper it's to fix it. You want to automate as much of this code analysis as possible. Use dynamic and static code analysis tools to reduce the manual effort. However, keep in mind that this testing is still a simulation of production. Production provides the clearest understanding of code optimization.
Tradeoff
: Code monitoring tools are likely to increase costs.
Identify hot paths
By instrumenting your code, you can measure the resource consumption for different code paths. These measurements help you identify hot paths. Hot paths have a significant effect on performance and resource usage. They're critical or frequently executed sections of a program that require high performance and low latency. To identify code hot paths, consider these steps:
Analyze runtime data
: Collect runtime data and analyze it to identify areas of the code that consume significant resources, such as CPU, memory, or I/O operations. Look for patterns or sections of code that are frequently executed or take a long time to complete.
Measure performance
: Use profiling tools or performance testing frameworks to measure the execution time and resource consumption of different code paths. It helps identify bottlenecks and areas for improvement.
Consider business logic and user effect
: Evaluate the importance of different code paths based on their relevance to the application's functionality or critical business operations. Determine which code paths are crucial for delivering value to users or meeting performance requirements.
Optimize code logic
Optimizing code logic is about refining the structure and design of code to perform tasks with fewer resources. Improved logic reduces unnecessary operations. It creates faster execution with less resource consumption. You should remove any unnecessary operations within the code path that might affect performance. Prioritize optimizing hot paths to see the greatest performance efficiency gains. To optimize code logic, consider the following strategies:
Remove unnecessary function calls
: Review your code and identify any functions that aren't essential for the desired functionality and might affect performance negatively. For example, if a function call performs a validation completed earlier in the code, you can remove the unnecessary validation function call.
Minimize logging operations
: Logging can be helpful for debugging and analysis, but excessive logging can affect performance. Evaluate the necessity of each logging operation and remove any unnecessary logging calls that aren't critical for performance analysis.
Optimize loops and conditionals
: Analyze loops and conditionals in your code and identify any unnecessary iterations or conditions that can be eliminated. Simplifying and optimizing these structures can improve the performance of your code. Minimize function calls within loops, and eliminate redundant calculations. Consider moving computations outside the loop or using loop unrolling.
Reduce unnecessary data processing
: Review your code for any unnecessary data processing operations, such as redundant calculations or transformations. Eliminate these unnecessary operations to improve the efficiency of your code.
Optimize data structures.
To efficiently store and retrieve data, select appropriate data structures, such as arrays, linked lists, trees, and hash tables. Choose the best data structure for a specific problem. A suitable data structure improves application performance.
Minimize network requests
: If your code involves making network requests, minimize the number of requests and optimize their usage. Batch requests when possible and avoid unnecessary round trips to improve performance.
Minimize allocations
: Identify areas where excessive memory allocation occurs. Optimize the code by reducing unnecessary allocations and reusing existing resources when possible. By minimizing allocations, you can improve memory efficiency and overall performance. Use the appropriate memory management and garbage collection strategies for your programming language.
Reduce data structure size
: Assess the size of your data structures, such as classes, and identify areas where reduction is possible. Review the data requirements and eliminate any unnecessary fields or properties. Optimize memory usage by selecting appropriate data types and packing data efficiently.
Use performance-optimized SDKs and libraries.
Use native SDKs or performance-optimized libraries. Native SDKs are designed to interact with the services and resources on a platform or within a framework. For example, cloud-native SDKs work better with cloud service data planes than with custom API access. SDKs excel at handling network requests and optimizing interactions. Performance-optimized libraries, such as Math.NET, contain performance-optimized functions. When you apply the functions appropriately, you can improve your workload's performance.
Cross-cutting implementation
: Consider the effects of cross-cutting implementations, such as middleware or token checks, and assess whether they negatively affect performance.
Review the performance recommendations specific to the programming language you're working with. Evaluate your code against these recommendations to identify areas for improvement.
Tradeoffs
:
Optimizing code and hot paths requires developer expertise in identifying code inefficiencies is subjective and might be highly skilled individual required for other tasks.
SDKs provide convenience and eliminate the complexities of interacting with APIs. But SDKs might limit your control and customization options for custom code.
Optimize memory management
Optimizing memory management involves refining the way a workload uses, allocates, and releases memory resources to improve efficiency. Proper memory management improves code performance because it reduces the overhead of memory operations. Efficient memory usage reduces latency, prevents system slowdowns or crashes, and maximizes the throughput of computational tasks. Consider the following strategies to optimize memory management.
Debug memory issues.
Memory dumps are application memory snapshots. They capture the memory state of an application at a specific point in time. Memory dumps enable retrospective analysis of memory-related issues. Select the appropriate type of memory dump based on the nature of the problem you're trying to diagnose and the resources available. You should use miniature dumps for routine debugging and full dumps for complex, critical issues. This strategy provides a balance between resource usage and diagnostic capabilities. Many code hosting services support memory debugging. You should prefer services that support memory analysis over those services that don't. Here are the basic steps to debugging memory issues:
Capture memory dumps
: Begin by setting up a mechanism to capture memory dumps during your application's runtime. The capture can be triggered manually, automatically, or when specific conditions (like excessive memory consumption) are met. Some cloud services might already offer this process.
Analyze memory dumps
: After you collect the memory dumps, analyze them. Numerous tools can assist you in inspecting these dumps, such as WinDbg for Windows applications or GDB for Unix-based systems.
Identify memory leaks
: Focus on identifying memory leaks during the analysis. Memory leaks arise when your application allocates memory but fails to release it when the memory is no longer required. Search for objects or data structures that remain in memory even when they should be deallocated.
Fix and test
: Upon identifying the problematic code, concentrate on resolving the memory issues. Resolutions might involve releasing memory correctly, optimizing data structures, or reevaluating memory management practices. Confirm that your solutions undergo rigorous testing to ensure their efficacy.
Iterate and monitor
: Memory management is a continuous process. Routinely monitor your application's memory usage and persist in collecting memory dumps in production. Regularly revisit the analysis and optimization stages to make sure memory issues don't reappear with subsequent code modifications.
By incorporating memory dump analysis into your software development lifecycle, you can amplify the reliability and efficiency of your applications. It helps to reduce the likelihood of memory-related issues in production.
Reduce memory allocations.
Minimize memory allocations to reduce the overall memory footprint of the code. Your workload can utilize the available memory efficiently. There's less need for the garbage collector to reclaim unused memory, and it reduces the frequency and duration of garbage collection cycles. Memory allocations can be costly, especially if you perform them frequently. Minimize memory allocations, so the code can run quickly and efficiently.
Caches store frequently accessed data close to the processor, which improves performance. When you minimize memory allocations, there's less contention for cache space, so you can effectively utilize the cache. A high number of memory allocations can degrade application performance and generate errors. Other ways to minimize memory allocations include:
Local variables
: Use local variables instead of global variables to minimize memory consumption.
Lazy initialization
: Implement lazy initialization to defer the creation of objects or resources until they're needed.
Buffers
: Manage buffers effectively to avoid allocating large memory buffers.
Object pooling
: Consider object pooling to reuse large objects instead of allocating and deallocating them.
For more information, see
Reduce memory allocations
and
The large object heap on Windows systems
.
Use concurrency and parallelism
Using concurrency and parallelism involves executing multiple tasks or processes either simultaneously or in an overlapping manner to make efficient use of computing resources. These techniques increase the overall throughput and the number of tasks that a workload can process. When you run tasks concurrently or in parallel, it reduces the runtime of the application and decreases latency and increases response times. Concurrency and parallelism enable efficient utilization of computing resources, such as CPU cores or distributed systems. Concurrency and parallelism effectively distribute the workload among the computing resources.
Use parallelism.
Parallelism is the ability of a system to simultaneously trigger multiple tasks or processes on multiple computing resources. Parallelism divides a workload into smaller tasks that are run in parallel. You can achieve parallelism by using techniques like multiprocessing or distributed computing. Distribute tasks across multicore processors to optimize workload management. Optimize code to take advantage of the CPU architecture, threading models, and multicore processors. When you run code in parallel, performance improves because the workload is distributed across multiple cores.
Use concurrency.
Concurrency is the ability of a system to run multiple tasks or processes. Concurrency enables different parts of a program to make progress independently, which can improve overall performance. You can implement concurrency by using techniques like multithreading, in which multiple threads run concurrently within a single process. You can also use asynchronous programming, in which tasks are triggered concurrently.
Asynchronous programming
: Asynchronous programming is an approach to trigger tasks without blocking the main thread. Asynchronous programming enables a program to trigger tasks while waiting for long-running operations to finish. With asynchronous programming, the program can initiate multiple tasks and wait for them to complete asynchronously. The program doesn't have to wait for each task to finish before moving on to the next one.
There are many asynchronous programming techniques and patterns, depending on the programming language and platform. One common approach is to use asynchronous keywords and constructs, such as
async
and
await
, in languages like C#. With these keywords, you can define asynchronous methods. For HTTP traffic, consider using the
Asynchronous Request-Reply pattern
.
Many frameworks and libraries provide built-in support for asynchronous programming. For example, in the .NET platform, you can implement asynchronous operations by using patterns like
Task-Based Asynchronous pattern
and
Event-Based Asynchronous pattern
. The specific implementation of asynchronous programming varies depending on the programming language, platform, and requirements of the application.
Queues
: A queue is a storage buffer located between a requesting component (producer) and the processing component (consumer) of the workload. There can be multiple consumers for a single queue. As the tasks increase, you should scale the consumers to meet the demand. The producer places tasks in a queue. The queue stores the tasks until a consumer has capacity. A queue is often the best way to hand off work to a processing service that experiences peaks in demand. For more information, see
Queue-Based Load Leveling pattern
and
Storage queues and Service Bus queues
.
Use connection pooling
Connection pooling is the practice of reusing established database connections instead of creating a new connection for every request. It can be expensive to establish a connection to a database. You have to create an authenticated network connection to the remote database server. Database connections are especially expensive for applications that frequently open new connections. Connection pooling reuses existing connections and eliminates the expense of opening a new connection for each request. Connection pooling reduces connection latency and enables high database throughput (transactions per second) on the server. You should choose a pool size that can handle more connections than you currently have. The goal is to have the connection pool quickly handle new incoming requests.
Understand connection pooling limits.
Some services limit the number of network connections. When you exceed this limit, connections might slow down or terminate. You can use connection pooling to establish a fixed set of connections at startup time and then maintain those connections. In many cases, a default pool size might consist of only a few connections that perform quickly in basic test scenarios. Your application might exhaust the default pool size under scale and create a bottleneck. You should establish a pool size that maps to the number of concurrent transactions that are supported on each application instance.
Test the connection pool.
Each database and application platform has slightly different requirements for setting up and using a pool. Test your connection pool to ensure it works efficiently under load.
Risk
: Connection pooling can create
pool fragmentation
and degrade performance.
Optimize background jobs
Many applications require background tasks that run independently of the UI. The application can start the job and continue to process interactive requests from users. Examples of background jobs include batch jobs, processor-intensive tasks, and long-running processes, such as workflows. Background tasks shouldn't block the application or cause inconsistencies due to delayed operation when the system is under load. To improve performance, you can scale compute instances that host background tasks. For more information, see
Background jobs
and
Scaling and performance considerations
.
Optimize infrastructure performance
Optimizing infrastructure performance means enhancing and adjusting infrastructure elements to ensure peak operation and the best use of resources for a workload. By fine-tuning infrastructure, you can minimize waste, reduce lags, and achieve more with the available resources. It ensures that workloads run reliably and swiftly, leading to improved user experiences and cost savings. To optimize infrastructure performance, consider the following strategies:
Add usage limits.
You can implement usage limits on some workload components. For example, to remove unstable pods, you can
define pod CPU and memory limits
in Azure Kubernetes Service (AKS). To optimize performance, you can
define memory limits in Java virtual machines (VMs)
.
Streamline infrastructure.
Simplify your workload to reduce the potential for interaction, dependency, and compatibility issues. When you simplify your workload, you optimize resource utilization of memory, processing power, and storage.
Reduce load.
To reduce load on a workload, minimize the demand placed on an application and enable resources to perform their primary tasks. For example, it's common practice to avoid running security solutions within your code or on individual compute instances. Instead, web servers should serve HTTP requests. Web application firewalls and gateway resources can handle security checks. The following strategies help reduce the load on your workload:
Eventual consistency
: Adopt an eventual consistency model to enhance performance by allowing data to be slightly dated. Eventual consistency reduces the immediate demand on CPU cycles and network bandwidth for constant data updates.
Delegate tasks
: Delegate server tasks to clients or intermediaries, such as search indexes and caches. Delegate tasks like sorting data, filtering data, or rendering views. When you offload these tasks, you reduce the workload on your servers and improve performance.
Optimize the network.
To optimize a workload network for performance, configure and fine-tune the network infrastructure. Ensure that the workload can operate at its highest level of efficiency.
Network protocols
: Upgrade to modern protocols like HTTP/2, which enable multiple requests to be sent over a single connection. Modern protocols reduce the overhead of establishing new connections.
Tradeoff
: Modern protocols might exclude older clients.
Network chattiness
: Batch network requests together to reduce the number of requests. Instead of making multiple small requests, combine them into larger requests to reduce network overhead.
Database queries
: Ensure that database queries retrieve only the necessary information. Avoid retrieving large amounts of unnecessary data, which can lead to increased network traffic and slow performance.
Static data
: Utilize a content delivery network to cache frequently accessed static content that's close to the users. When you cache data, it doesn't have to travel over long distances. Caching improves response times and reduces network traffic.
Log collection
: Collect and retain only the log data that's necessary to support your requirements. Configure data collection rules and implement design considerations to optimize your Log Analytics costs.
Data compression
: Compress and bundle
HTTP content
and
file data
to allow fast transmission between clients and servers. Compression shrinks the data that a page or API returns and sends back to the browser or client app. Compression optimizes network traffic, which can accelerate application communication.
Tradeoff
: Compression adds server-side and client-side processing. The application must compress, send, and decompress data. Multicast communication, or communication to multiple recipients, can create decompression overhead. You need to test and measure the performance variations before and after implementing data compression to determine if it's a good fit for your workload. For more information, see
Response compression in ASP.NET Core
.
Azure facilitation
Instrumenting code
: Azure Monitor Application Insights supports automatic instrumentation (autoinstrumentation) and manual instrumentation of application code. Autoinstrumentation enables telemetry collection without touching the application's code. Manual instrumentation requires code changes to implement the Application Insights or OpenTelemetry API. You can use
Application Insights Profiler
to help optimize hot paths.
Optimizing code logic
: Azure offers
SDKs
and libraries for various programming languages to interact with Azure services. Use SDKs to simplify interactions between applications and Azure resources. SDKs provide optimal interaction with Azure services, which reduces latency and enhances efficiency.
Optimizing memory management
: Use
the smart detection feature of Application Insights
to analyze memory consumption and help to identify and address memory leaks.
Azure App Service
has a profiler and memory dump collection and analysis feature. The App Service
autohealing feature
can automatically take memory dumps and profile traces of .NET and Java apps.
Using concurrency and parallelism
: Different Azure services provide unique support for concurrency, such as
Azure Cosmos DB
,
Azure Functions
and
Blob storage
. For parallelism, services
AKS
supports deploying containerized applications, which improves parallel processing.
Azure Batch
is a cloud-based job scheduling service that you can use to enable parallel and high-performance computing without the need for infrastructure setup. For more information, see
Background jobs
.
Optimizing infrastructure performance
: Implement
Azure Resource Manager templates
to define and deploy infrastructure by using code. Use these templates to implement efficient, repeatable, and consistent resource deployments.
Azure Policy
provides governance capabilities to ensure that resource deployments adhere to organizational best practices and standards.
For asynchronous programming, use scalable queuing services, like
Azure Queue Storage
and
Azure Service Bus
, to facilitate asynchronous programming. You can queue tasks and independently process them. To support asynchronous operations, Azure Marketplace offers third-party queues and tools that you can integrate with Azure services.
Related links
AKS
Application Insights smart detection feature
Asynchronous Request-Reply pattern
Avoid memory allocations
Azure Batch
Azure Policy
Azure Resource Manager templates
Azure SDKs
Background jobs
Background jobs scaling and performance considerations
Compress file data
Compress HTTP content
Define pod CPU and memory limits
Event-Based Asynchronous pattern
Java virtual machines (VMs)
Large object heap
Pool fragmentation
Queue-Based Load Leveling pattern
Response compression in ASP.NET Core
Storage queues and Service Bus queues
Task-Based Asynchronous pattern
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for optimizing data performance - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for optimizing data performance
Article
2023-11-15
6 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:08
Optimize data performance. Optimize data stores, partitions, and indexes for their intended and actual use in the workload.
This guide describes the recommendations for optimizing data performance. Optimizing data performance is about refining the efficiency with which the workload processes and stores data. Every workload operation, transaction, or computation typically relies on the quick and accurate retrieval, processing, and storage of data. When data performance is optimized, the workload runs smoothly. Compromised data performance creates a domino effect of poor performance efficiency. Failure to optimize data performance results in response delays, heightened latency, and curtailed scalability. It jeopardizes the efficiency of the entire workload.
Definitions
Term
Definition
CAP theorem
A framework used to consider consistency, availability, and partition tolerance to help explain the tradeoffs in data consistency.
Database index rebuilding
A maintenance activity that drops and recreates an index.
Database index reorganization
A maintenance activity that optimizes the current database index.
Data store
A resource that stores data such as a database, object store, or file share.
Eventual consistency
A data synchronization model that allows for temporary inconsistency in data replicas before they eventually sync.
Index
A database structure that provides quick access to items.
Online analytical processing (OLAP)
A technology that organizes large business databases, supports complex analysis, and performs complex analytical queries without negatively affecting transactional systems.
Online transaction processing (OLTP)
A technology that records business interactions as they occur in day-to-day operations of an organization.
Optimistic concurrency
An approach for updating databases that uses snapshots to make updates instead of traditional locking mechanisms, improving performance and scalability.
PACELC theorem
A framework used to consider partition tolerance, availability, consistency, and latency to help explain the tradeoffs in data consistency.
Partitioning
The process of physically dividing data into separate data stores.
Query tuning
A process that optimizes the speed of a database query.
Read replica
A live copy of a primary database that enables you to offload read traffic from a write database.
Key design strategies
To optimize data usage, ensure that data stores, partitions, and indexes are optimized for their intended use and for their actual use in a workload. Optimized data usage can improve query performance, reduce resource consumption, and enhance overall system efficiency. Consider the following strategies:
Profile data.
Understand your data and ensure that your data model is well-suited for your workload. Consider factors such as data normalization, indexing strategies, and partitioning techniques. For efficient data retrieval, ensure that you select appropriate data types, define relationships between entities, and determine an optimal indexing strategy.
Fine-tune your data storage configuration.
Configure your data storage infrastructure to align with your workload requirements. Select an appropriate storage technology, for example relational databases, NoSQL databases, and data warehouses. Optimize storage settings, such as buffer size, caching mechanisms, and compression.
Optimize query performance.
Analyze and optimize queries that run in the workload. Use techniques such as query optimization, indexing, and caching. To identify bottlenecks, use query plans and performance monitoring tools, and then make necessary improvements.
Regularly monitor and tune the system.
Continuously monitor the performance of your workload and iterate on the data storage configuration and query optimizations. Based on performance tuning best practices, analyze system metrics, identify areas of improvement, and implement changes.
Profile data
Data profiling involves examining the data from a source and gathering information about it. The objective is to understand the quality, structure, and characteristics of workload data. This process allows for the identification of issues such as missing values, duplicates, inconsistent formats, and other anomalies. For effective data profiling, consider the following strategies:
Understand the data structure.
Examine the structure of your data, including tables, columns, and relationships. Determine the data types, lengths, and constraints that are applied to each column. Data structure evaluation helps you understand how the data is organized and how it relates to other data elements.
Analyze the data volume.
Assess the volume of your data to understand the overall size and growth patterns. Determine the number of records or documents and the size of individual tables or collections. This information helps you estimate storage requirements and identify scalability issues.
Identify data relationships.
Explore the relationships between data elements, such as primary and foreign key relationships. Understand how data is connected, so you can determine how changes in one table or document might affect related data.
Assess data quality.
Evaluate the quality of your data by examining factors such as completeness, accuracy, consistency, and uniqueness. Identify data anomalies, missing values, or duplicate records that might affect data integrity and query performance. This step helps you identify areas for data cleansing and improvement.
Capture data distribution.
Analyze the distribution of values within each column to determine data patterns. Identify frequent and rare values, outliers, and data skews. To optimize query performance, choose appropriate indexing strategies and query optimization techniques based on the distribution.
Monitor data performance
Data performance monitoring is the practice of consistently tracking the efficiency of data stores, partitions, and indexes in real-time. It involves collecting and analyzing performance metrics specific to data operations, using tools tailored for system-level, database-specific, or third-party monitoring solutions. Effective data performance monitoring allows you to proactively identify and mitigate potential bottlenecks, ensuring that data-related processes and tasks are efficient. To monitor data performance, consider the following strategies:
Collect data-specific metrics.
Gather key metrics that directly relate to data performance. These metrics include query response times, data throughput, disk I/O related to data access, and the load times of specific data partitions.
Set up data alerts.
Set up alerts specifically for data metrics. Use predefined thresholds or anomalies in these metrics to trigger alerts. Alerts enable you to receive notifications when performance metrics exceed acceptable ranges or show abnormal behavior. For instance, if a database query takes longer than expected or if data throughput drops significantly, it would trigger an alert. You can set up these alerts using specialized monitoring tools or custom scripts.
Diagnose data performance issues.
Regularly review the collected data metrics to pinpoint potential performance bottlenecks or degradation in data operations. Visualization tools or dashboards can be invaluable in this process, helping to highlight trends, bottlenecks, and outliers in data performance. Once identified, delve into the root causes of these issues and strategize appropriate remediation steps.
Partition data
Partitioning involves dividing large datasets or high-volume workloads into smaller, manageable subsets. Partitioning enhances data performance efficiency by distributing the workload and improving parallel processing. It also ensures more effective data access based on specific needs and query patterns. You can partition data vertically or horizontally (also called sharding).
Strategy
Definition
Example
Use cases
Vertical partitioning
Divide a table into smaller tables by selecting specific columns or fields for each partition. Each partition represents a subset of the complete data.
If you have a table with columns A, B, C, and D, you could create one table with columns A and B and another with columns C and D.
- A table contains many columns, but queries don't access all columns together.
- Some columns are larger than others and separating them can boost I/O performance.
- Different data parts have diverse access patterns.
Horizontal partitioning
Split data based on rows or ranges of values (also known as sharding). Each partition contains a subset of rows with similar characteristics.
If you have a table with rows 1 to 1000, you might create one partition with rows 1 to 500 and another with rows 501 to 1000.
- A dataset is too large for a single location or server.
- Data is accessed based on specific ranges or filters.
- Need to distribute the workload across physical nodes or servers for enhanced performance.
To partition your data, consider the following steps:
Analyze data and queries.
Analyze data and query patterns to identify suitable partitioning or sharding strategies. Understand the nature of the data, access patterns, and distribution requirements.
Determine a key.
Choose a partitioning or sharding key to distribute data across partitions or shards. Carefully select the key based on data characteristics and query requirements.
Determine logic.
Determine a partitioning or sharding logic based on the chosen key. Consider dividing the data into ranges, applying hashing algorithms, or using other partitioning techniques.
Configure the infrastructure.
Configure the database system to support partitioning or sharding. Consider creating the necessary infrastructure, defining the partitions or shards, and configuring the data distribution.
For more information, see
Data partitioning guidance
.
Optimize database queries
Optimizing database queries refines queries using techniques such index hints and caching. These adjustments increase efficiency and speed of data retrieval. As a result, the database has a lighter workload, resources work more effectively, and users enjoy smoother interactions. To optimize database queries, consider the following strategies:
Rewrite queries.
Review and analyze complex queries to identify opportunities to rewrite them. Consider restructuring query logic, eliminating redundant operations, or simplifying query syntax.
Avoid the N+1 query problem.
Minimize the number of roundtrips to the database by using joins and batch fetching to retrieve related data efficiently.
Reorder joins.
Evaluate the query plan and consider rearranging the join order to minimize the number of rows in each join operation. The order in which you join tables can affect query performance.
Use index hints.
Use index hints so a database engine can specify the use of indexes when it runs a query. Index hints guide the optimizer to select the most appropriate indexes.
Cache queries.
Store the results of frequently run queries in memory. Query caching eliminates the need for repeatedly running the same query, and it reduces query processing overhead.
Optimize locking.
Avoid unnecessary or restrictive lock hints in queries. Efficient locking strategies can enhance query performance and concurrency. Apply optimized locking mechanisms that the database system provides. Analyze and adjust isolation levels to balance data consistency and query performance.
Monitor and tune.
Monitor query performance metrics, such as runtime, resource utilization, and query throughput. Use database profiling tools and monitoring functionalities to identify poorly performing queries. Evaluate and fine-tune query plans based on collected performance data. Analyze query plans and wait statistics to identify bottlenecks. Use that information to optimize query performance.
Optimize index performance
Indexes enhance data retrieval speed by allowing databases to swiftly find data using specific columns or fields. When you optimize these indexes, sorting and join operations become more efficient, leading to faster queries. Well-optimized indexes cut down on the disk I/O operations required for queries. Removing unneeded or redundant indexes also frees up valuable storage space. To optimize index performance, consider the following strategies:
Analyze query patterns.
Understand the query patterns that run on your database. Identify the queries that run frequently and might degrade performance. Analyze query patterns to determine which indexes are beneficial for optimizing performance.
Evaluate existing indexes.
Review the existing indexes in your database. Evaluate their usage, performance effects, and relevance to the query patterns. Identify redundant or unused indexes that you can remove to improve write performance and reduce storage overhead.
Identify columns for indexing.
Identify columns that are frequently used in the
where
,
join
, and
order by
clauses of your queries. These columns are potential candidates for indexing because they can enable fast data retrieval.
Choose an appropriate index type.
Select an appropriate index type based on your database system. Common options include b-tree indexes for equality and range queries, hash indexes for exact match queries, and full-text indexes for text search operations. Choose an index type that best matches your query requirements.
Consider index column order.
When you create composite indexes, or indexes with multiple columns, consider the order of the columns. Place the columns that are most frequently used in queries at the beginning of the index. Column order helps ensure that your workload is effectively using indexes for a wide range of queries.
Balance index size.
Avoid creating indexes on columns with low cardinality, or columns that have a low number of distinct values. Such indexes can be inefficient and increase the size of your database. Instead, index columns that have a high selectivity.
Maintain index usage.
Continuously monitor the usage and performance of your indexes. Look for opportunities to create new indexes or modify existing indexes based on changes in query patterns or performance requirements. Remove or update indexes that are no longer beneficial. Indexes have maintenance overhead. As data changes, indexes can fragment and affect performance. Regularly perform index maintenance tasks, such as rebuilding or reorganizing indexes, to ensure optimal performance.
Test and validate.
Before you revise indexes in a production environment, perform thorough testing and validation. Measure the performance effect of index revisions by using representative workloads. Verify the improvements against predefined benchmarks.
Tradeoff
: B-tree indexes might have high storage overhead, and exact-match queries might be slow. Hash indexes aren't suitable for range queries or comparison operators. Full-text indexes might have high storage requirements, and nontextual data queries might be slow.
Consider data compression
Data compression is the process of reducing the size of data to optimize storage space and improve workload performance efficiency. Compressed data requires less storage space and less bandwidth for transmitting, which results in fast data transfer. You would compress data to reduce your storage footprint and improve data access times. When you compress data, it reduces I/O operations and network bandwidth requirements.
Lossless compression and lossy compression are data compression algorithms. Lossless compression algorithms reduce the size of data without losing any information. Lossy compression algorithms achieve high compression ratios by removing less important or redundant information.
Tradeoff
: To compress and decompress data, you need computational resources, like CPU and memory. The more data that you compress, the more resources you need.
Archive and purge data
Archiving and purging are strategies that streamline data storage. Archiving relocates older, less-frequently accessed data to a more cost-effective storage. Purging data permanently removes redundant data. They contribute to performance efficiency by reducing data volume, increases data access speed, and reducing backup and recovery times:
Reducing data volume
: Less data means faster processing times, ensuring quick responses to user requests.
Increasing data access speed
: A trimmed dataset allows for swifter queries and data retrieval, optimizing system responsiveness.
Reducing backup and recovery times
: Smaller datasets expedite backup and restoration processes, minimizing downtime and ensuring consistent performance.
Archiving and purging are instrumental in maintaining peak performance efficiency in data-driven systems.
Optimize storage load
Optimizing storage load means streamlining requests to the storage system. It helps eliminate unnecessary requests. It also enhances data retrieval and prevents overwhelming the storage. Optimizing the storage load ensures the storage system remains responsive to legitimate requests and maintains peak performance. Implement strategies to reduce the processing burden on the data store. To optimize data store load, consider the following strategies:
Use caching
Caching stores commonly accessed data in a fast-access storage area, making data retrieval quicker than pulling it from the main source. This technique boosts data performance by cutting down on access times and avoiding repetitive data fetches. Caching improves read speeds and user response times, especially for frequently accessed data  This method is most effective on static data or data that rarely changes.
To ensure optimal caching efficiency, consider factors like expiration policies, eviction strategies, and managing cache size. Adjust settings, such as the time to live (TTL), for optimal performance. To use a cache to optimize storage load, consider the following strategies:
In-memory caching
: Perform in-memory caching to store frequently accessed data in memory for fast retrieval. You can use this technique for application data that's expensive to compute or retrieve from a database. In-memory caching is useful for data that you read frequently but don't change frequently.
Database query caching
: Use this technique to cache the results of database queries to avoid running the same query multiple times. Database query caching is useful for complex and time-consuming database queries. When you cache the results of a query, subsequent requests for the same query are returned quickly.
Content delivery network caching
: Use this technique to cache web content on distributed network servers to reduce latency and improve content delivery. Content delivery network caching is effective for static content, like images, CSS files, and JavaScript files. Content delivery networks store copies of content in multiple locations worldwide, so users can access the content from a server that's near them geographically.
Use read replicas
Many databases support multiple read replicas. Distribute read queries across replicas to minimize the demand on the write database. Each read replica can serve a subset of traffic, which can improve performance.
When you have a workload with multiple data replicas that you expect to stay in sync, it's helpful to model this distributed system by using the PACELC theorem. The PACELC theorem helps you understand latency versus constancy tradeoff choices in the nonpartitioned state of the system. Use this information to help you choose a database engine and data sync strategy that best addresses the system in a partitioned and nonpartitioned state. For more information, see
Command and Query Responsibility Segregation (CQRS) pattern
.
Optimize data consistency
In a distributed workload, where data resides across multiple nodes or locations, the level of consistency you select determines how quickly changes in one location reflect in others. Opting for stricter consistency consumes more compute resources and can negatively affect performance efficiency. On the other hand, a less strict consistency level, like eventual consistency introduces temporary inconsistencies among nodes but can boost performance efficiency.
Eventual consistency strikes a balance between data accuracy and workload performance. Changes spread gradually instead of instantly, boosting workload responsiveness and data processing speed. Although it introduces short-lived inconsistencies, the workload eventually presents consistent data across all nodes. Choosing eventual consistency can elevate a workload's performance and further enhance its availability and scalability.
Optimize data updates
You can use optimistic concurrency to handle concurrent updates to the same data. Instead of locking data and preventing other updates, optimistic concurrency allows multiple users or processes to work concurrently and assumes that conflicts are rare.
With optimistic concurrency, each update operation includes a version or timestamp that represents the state of the data at the time of the update. When a conflicting update is detected, the system resolves the conflict by rejecting the update or merging the changes.
Optimistic concurrency minimizes contention and allows concurrent updates to proceed without unnecessary locking. It reduces wait time for resources and provides high throughput.
Optimize data movement and processing
Optimizing data movement and processing involves improving the efficiency and performance of operations related to data extraction, transformation, loading, and processing. Consider the following key aspects of optimizing data movement and processing:
Extract, transform, and load (ETL) optimization
: Optimize ETL processes to minimize processing time. You can streamline the extraction process, implement efficient transformation algorithms, and optimize the loading process. When you make each step efficient, you can optimize the overall workflow.
Parallel processing
: Utilize parallel processing techniques to improve performance. When you distribute data processing tasks across multiple threads or nodes, you can divide and process the workload concurrently, which results in fast processing.
Batch processing
: Group similar tasks together to reduce overhead caused by repeated operations. Process multiple tasks in a batch to reduce overall processing time.
Optimize storage design
Optimizing storage design entails crafting a precise data storage architecture and selecting appropriate storage technologies. A streamlined storage design enhances data access, retrieval, and manipulation. Through strategic storage design, a workload achieves improved response times and overall functionality.
Design for data proximity
Data proximity refers to the strategic placement of data closer to the users or services that access it most frequently. By reducing the physical or logical distance between data and its users, data proximity ensures faster data access and improved responsiveness. To optimize design for close proximity, consider these strategies:
Evaluate data access patterns
: Assess your workload's access patterns and frequently accessed data. This analysis can help determine where to place data for maximum benefit.
Choose solutions that support data relocation
: Consider solutions that offer dynamic data relocation based on changing access patterns, ensuring optimal data positioning.
Choose solutions that support data synchronization
: If catering to a distributed user base, opt for solutions that facilitate data synchronization across various regions, ensuring that data replicas are available in proximity to users.
Tradeoff
: If underlying data changes frequently, implement a cache invalidation mechanism to ensure that the cached data remains up to date.
Use polyglot persistence
Polyglot persistence is the practice of using multiple data storage technologies to store and manage different types of data within an application or system. Different types of databases or storage solutions serve different data requirements.
Polyglot persistence takes advantage of the benefits of each data storage technology to ensure optimal performance and scalability for each type of data. For example, you might use a relational database to store structured, transactional data. And you might use a NoSQL database to store unstructured or semi-structured data.
Design a schema for each data storage technology based on the requirements of the data. For relational databases, you might create normalized tables with appropriate relationships. For NoSQL databases, you might define document structures or key-value pairs. Develop the necessary components to interact with each data storage technology, such as APIs, data access layers, or data integration pipelines. Ensure that the application can read and write data to the appropriate data stores.
Tradeoff
: A data structure that has low normalization can improve performance but introduce complexities.
Separate OLTP and OLAP systems
To separate
OLTP
and
OLAP
systems, design and deploy distinct systems for transactional processing and analytical processing tasks. This separation allows you to optimize each system for its specific workload and characteristics.
OLTP systems are used for real-time transactional processing. They efficiently and reliably handle individual transactions. OLTP systems are typically used to perform day-to-day operational tasks, such as online order processing, inventory management, and customer data management. OLTP systems prioritize responsiveness, consistency, and concurrency.
OLAP systems are used for complex analytical processing and reporting. They handle large volumes of data and perform intensive calculations and aggregations. OLAP systems are used for tasks such as business intelligence, data mining, and decision support. OLAP systems prioritize query performance, data aggregation, and multidimensional analysis.
When you separate OLTP and OLAP systems, you can allocate appropriate resources and optimize each system for its specific workload. Separation allows you to apply different data modeling techniques to each system. OLTP systems typically use normalized schemas for efficient transactional processing. OLAP systems might use denormalized schemas or data warehousing techniques to optimize query performance.
Azure facilitation
Profiling data
: Azure offers tools and services that you can use to profile data, such as
Azure Data Catalog
,
Azure Purview
, and
Azure Synapse Analytics
. These tools enable you to extract, transform, and load data from various sources, perform data quality checks, and gain insights into the data.
Monitoring data performance
: To monitor data performance, you can use Azure Monitor to collect and analyze infrastructure metrics, logs, and application data. You can integrate Monitor with other services like Application Insights. Application Insights provides application performance monitoring and supports many platforms.
Application Insights collects usage and performance data. You can use Log Analytics to correlate that data with configuration and performance data across Azure resources.
You can use the insights feature of
Azure SQL
and
Azure Cosmos DB
to monitor your database. This feature enables you to diagnose and tune database performance issues.
Partitioning data
: Azure offers various partitioning strategies for different data stores. Each data store might have different considerations and configuration options for data partitioning. For more information, see
Data partitioning strategies
.
Optimizing database queries and index performance
: Use the query performance insight feature of Azure SQL Database to optimize queries, tables, and databases. You can use this feature to identify and troubleshoot query performance issues.
For relational databases, you should follow the
index design guidelines
,
SQL Server index guidance
, and
Azure Cosmos DB index guidance
. Use SQL Database to perform
automatic tuning
for queries to improve their performance.
In SQL databases, you should regularly
reorganize or rebuild indexes
. Identify slow queries and tune them to improve performance. Many database engines have query-tuning features. For more information, see
Best practices for query performance
.
Azure Cosmos DB has a
default indexing policy
that indexes every property of every item and enforces range indexes for any string or number. This policy provides you with efficient query performance, and you don't have to manage indexes upfront.
Optimizing storage load
: Many Azure database services support read replicas. The availability and configuration of read replicas vary depending on the Azure database service. Refer to the official documentation for each service to understand the details and options.
Optimizing storage design
: Azure offers many different data stores to fit your workload needs.
Understand data store types
and
select an Azure data store for your application
.
Related links
Automatic tuning in SQL Database
Azure Cosmos DB
Azure Cosmos DB index guidance
Azure SQL
Best practices for query performance
CQRS pattern
Data partitioning guidance
Data partitioning strategies
Default indexing policy
Index design guidance
OLAP overview
OLTP overview
Partitioning best practices
Reorganize or rebuild indexes
Select an Azure data store for your application
SQL Server index guidance
Understand data store types
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for optimizing operational tasks - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for optimizing operational tasks
Article
2023-11-15
5 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:10
Optimize operational tasks. Monitor and minimize the effects of the software development lifecycle and other routine operations on workload performance. These operations include virus scans, secret rotations, backups, reindexing databases, and deployments.
This guide describes the recommendations for optimizing operational tasks. Optimizing operational tasks is the process of minimizing the effects of tasks that you perform as part of routing workload operations. Operations activities use the same compute resources as the workload itself. Failure to consider the effects of operations tasks can cause the workload to miss its performance targets. It can also negatively affect the performance of the workload for your customers.
Definitions
Term
Definition
Blue-green deployment
A deployment strategy that uses two identical environments and controls the direction of traffic to new deployments (green deployments).
Database index rebuilding
A maintenance activity that drops and recreates an index.
Database index reorganization
A maintenance activity that optimizes the current database index.
Database schema
The general structure of a database and its relationships to other data.
Deployment slot
A feature of Azure App Service that enables you to deploy live apps with their own host names.
In-place upgrades
The process of upgrading a component or an application without replacing it or migrating it to a new environment.
Infrastructure as code (IaC)
A descriptive model for defining and deploying infrastructure, including networks, virtual machines, load balancers, and connection topologies.
Key design strategies
You need to take measures to reduce the effects of the software development lifecycle and other routine operations on workload performance. The goal is to ensure that routine operations, like virus scans, secret rotations, backups, index optimization (reorganization or rebuilding), and deployments, don't significantly degrade the performance of the workload.
Account for operational tasks
It's important to consider operational tasks when you set performance targets. By incorporating routine, regular, and ad-hoc tasks into performance targets, you can ensure that the workload operates efficiently. To account for operational tasks in performance targets, here are some key points to consider:
Identify operational tasks.
Identify and include relevant operational tasks in performance targets. Examples of routine tasks can include virus scanning, database index reorganization, database index rebuilding, disk or database backups, certificate rotations, patching an operating system, rotating passwords, rotating API keys, penetration testing, and audit reviews in production.
Evaluate performance targets.
Evaluate current performance targets and adjust them to account for operational tasks that are specific to the workload. Doing so ensures that performance targets align with the workload's operational requirements.
Optimize deployments
Optimizing deployments refers to refining the process of releasing resources and code to guarantee seamless performance and minimal interruptions. It involves planning, effective resource distribution, and thorough testing of both the infrastructure-as-code (IaC) and the application code before they are introduced to a live environment. Deployment inadequacies can lead to reduced speed and efficiency of a workload, potential resource constraints, and a compromised user experience in the operational setting. To optimize deployments, consider these strategies:
Assess acceptable downtime.
If downtime is acceptable, you can implement deployment strategies that prioritize speed and efficiency. However, it's important to carefully assess the effect of downtime on business requirements before you make that decision. On the other hand, if downtime isn't acceptable, you need to implement deployment strategies that ensure continuous availability of the workload. Consider using techniques like blue-green deployments or canary deployments, where you gradually roll out new versions of the workload while you monitor for issues. These strategies help minimize the effect of downtime and ensure a seamless user experience.
Deploy at current instance count.
You should also avoid deployments that cause immediate scale operations. You shouldn't deploy resources into a live system with an instance count so low that it forces the system to immediately perform a scale operation. For example, your infrastructure-as-code (IaC) template might not match the number of instances that you need at the time of deployment. It might have an instance count of two, even though the current deployed environment is running eight instances. The deployment would remove six instances and negatively affect performance.
Use a blue-green deployment strategy.
Deployments can cause service interruptions and downtime. To mitigate these issues, select a deployment strategy that minimizes performance impact, like a blue-green deployment. These approaches allow for seamless transitions between environments and reduce the risk of service disruptions. When you use the blue-green deployment approach, you have two separate environments: the blue and green environments. If any issues or performance degradation is detected in the green environment, you can easily roll back to the stable blue environment. This strategy helps you ensure minimal downtime and allows you to maintain a high level of performance for your workload. To deploy by using the blue-green approach, follow these general steps:
Deploy the new environment.
Set up the new environment (green) alongside the existing environment (blue) with the updated version of your application.
Validate the new environment.
Deployments can introduce latency and increase response times. Consider prewarming instances before cutover. Prewarming involves preparing the new environment by simulating production-like traffic and workload to ensure that the environment is ready to handle the expected load. It helps minimize the effects on latency and response times. Thoroughly test and validate the new environment to ensure that it functions correctly and meets performance expectations. Testing helps warm up caches, establish database connections, and ensure that the environment is ready to handle the expected load.
Gradually shift traffic.
After the new environment is prewarmed and validated, gradually shift production traffic from the old environment (blue) to the new environment (green). Initially, direct a small percentage of traffic to the green environment and gradually increase it after verifying its stability and expected application health. You can use a global load balancer or traffic management mechanism. The controlled traffic shifting allows you to identify any performance issues early and take corrective actions before fully transitioning the workload to the new environment.
Monitor and optimize.
Deployments might use shared computing resources. Continuously monitor the performance and health of the new environment after you shift traffic. Make any necessary optimizations or adjustments to ensure the desired performance and user experience.
Remove the old environment.
After you successfully transition all traffic to the green environment, remove the blue environment from existing connections. This step helps optimize the cost of maintaining the old environment and ensures that new environments are free of configuration drift.
Repeat the process.
For future deployments, reverse the roles of the blue and green environments. Deploy changes to the new blue environment, validate them, orchestrate traffic transition, and decommission the old green environment.
Use multiple builds.
Different types of builds can help you optimize build times and ensure the quality of deployments. For example, you can have continuous integration (CI) builds that trigger with every code commit. You could have nightly builds that run automated tests regularly, and release builds that are used for deploying to production. Each type of build should have a specific purpose, like continuous integration, automated testing, or production deployment. Testing and validation of the workload before deployment help identify and address issues or bugs early in the development process.
Consider feature flags.
Feature flags are used in software development to control the visibility and behavior of certain features in an application. By using feature flags, developers can enable or disable specific features without needing to redeploy the application. Feature flags work by introducing conditional logic in the code that determines whether a feature should be enabled or disabled. This logic can be based on various factors, like user roles, user preferences, or specific conditions that are defined by the development team. By using feature flags, developers can gradually roll out new features to a subset of users or enable features for specific groups for testing (canary testing).
Optimize upgrades
An in-place upgrade is an upgrade to an existing resource or application. In-place  upgrades can temporarily slow down or interrupt a workload. It's important to ensure that upgrades are compatible with the workload. Before you apply an upgrade, we recommend that you test it in a separate environment to identify any potential issues. Provide a rollback plan in case any issues arise during the upgrade process. It's crucial to take a complete backup of critical data and configurations before you apply the upgrade. Monitor the upgraded system closely after the upgrade to ensure that everything functions as expected. The backup allows you to restore to a good state if you need to. You should prioritize scheduling the upgrade during off-peak hours to minimize the effect on users and workload performance. Notify users in advance about the planned upgrade, including the expected downtime and any necessary actions they need to take.
Tradeoff
: Waiting to perform operations activities during off-peak hours can affect operational efficiency. It might be less convenient to have the personnel with the right skill set work during off-peak hours.
Optimize tooling
Essential tools for file integrity monitoring, virus scanning, intrusion detection, and other operational tasks can affect workload performance. They consume compute resources and can add latency and performance overhead. You need to test and understand the effects your tools have on workload performance. Based on the test results, you should fine-tune tool configurations, adjust scan frequency, and reallocate compute resources. For virus scanning, you could create a relevant exclusion list to minimize the duration of scans.
Optimize database operations
Optimizing database operations refers to the process of refining and fine-tuning database tasks to ensure maximum efficiency and minimal resource utilization. These operations include tasks like backups, schema changes, performance tuning, and monitoring. Efficient database operations lead to faster query responses, reduced system overhead, and an overall smoother user experience.
Schema changes involve modifying the structure of a database, such as adding or altering tables, columns, or indexes. These changes might require extra processing and resource utilization during the deployment process, potentially affecting the overall performance of the workload. Schema changes can disrupt performance to active queries, indexes, or transactions or cause data to be unavailable.
To minimize these effects, you should plan and test schema changes in a nonproduction environment. You can use various deployment techniques to implement schema updates. You should also use available schema changing tools to optimize the process. Archiving data and partitioning can help reduce the effects of schema changes.
Optimize backups
Backups consume workload resources like processing power, network bandwidth, and disk I/O. You need to test and select a backup strategy that minimizes these effects. You should perform backups during off-peak hours when you can. Your strategy should include incremental backups instead of full backups each time. Snapshots can be less resource intensive than backups. You should consider built-in platform backup and restore features rather than building a custom solution. You need to test these options and use a combination that provides the best performance for your workload.
Optimize monitoring and debugging
Excessive or poorly implemented logging, telemetry, instrumentation, and distributed tracing capture and collection can affect performance. Likewise, convenience features like remote debugging can also affect performance. You need to measure and know their performance effects on the environment. You don't want these processes to degrade performance. You should configure or disable any processes whose performance effects outweigh their benefits.
Azure facilitation
Accounting for operational tasks
:
Azure DevOps
is a set of development tools and services that enable teams to plan, develop, test, and deliver software efficiently. It includes features like version control, continuous integration and delivery, project management, and more.
Azure provides service-to-service integration that minimizes the effects of many operational tasks. For example, services that integrate with Azure Key Vault often support seamless certificate rotation or secret rotation that minimizes effects on performance.
Optimizing deployments
: App Service provides
deployment slots
. You can use deployment slots to deploy code to a nonproduction environment. You can swap app content and configuration elements between two deployment slots. For example, you can switch app content from a nonproduction slot to the production slot.
Azure Front Door and Azure Traffic Manager enable you to implement a
blue-green deployment strategy
. Some Azure compute services also support advanced deployment strategies like blue-green deployments. You can combine those services with your traffic shifting or instance warming strategy to mitigate the performance effects of deployment.
Optimizing database operations
:
Azure SQL Database
automatically takes full backups, differential backups, and transaction log backups.
Azure Cosmos DB
automatically takes backups of your data at regular intervals. The automatic backups are taken without affecting the performance or availability of database operations. Azure Cosmos DB stores the backups in a separate storage service.
Optimizing backups
: Some Azure data services support low-to-no performance impact for point-in-time recovery and indexing. Azure Backup is a reliable and scalable cloud-based backup solution that enables you to protect your data and applications. It provides features like incremental backups, compression, and encryption to minimize the effects on performance during backup operations. Azure Site Recovery helps you protect your applications by replicating them to a secondary location. It provides continuous replication and automated failover capabilities to minimize the downtime and performance impacts during backup and disaster recovery operations.
Managing business continuity and disaster recovery
: You can also use
Azure Business Continuity Center
to streamline backup and disaster recovery management with a unified web interface for configuring backups, setting protection policies, monitoring operations, and reviewing configurations across diverse environments.
Related links
Deployment slots
Blue-green deployment strategy
Azure SQL Database
Azure Cosmos DB
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for defining performance targets - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for defining performance targets
Article
2023-11-15
4 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:01
Define performance targets. Performance targets should be numerical values that are tied to workload requirements. You should implement performance targets for all workload flows.
This guide describes the recommendations for establishing and exposing performance targets. Performance targets are metrics that define performance objectives. These metrics are expressed as a single numerical value or a numerical range. They're clear and specific metrics that drive continuous improvement. Performance targets are a numerical foundation for improvements, and they help teams align their efforts toward specific goals. Without clear performance targets, teams might lack focus and lack of accountability for performance issues. By setting performance targets, teams can work toward specific objectives and drive continuous improvement.
Definitions
Term
Definition
Data flow
The movement of data within a system or between systems.
Dependency
A component that a workload relies on.
Flow
In a workload, a sequence of operations that performs a specific function. It involves the movement of data and the running of processes between components of the workload.
Metrics
Numerical values that are collected at regular intervals. Metrics describe some aspects of a system at a particular time.
Performance targets
Metrics that define performance objectives. These metrics are expressed as a single numerical value or a numerical range.
User flow
The paths or sequences of actions that users take within an application or system.
Workflow
The sequence of steps that a workload runs to accomplish a task.
Key design strategies
Establishing performance targets is an essential step for achieving workload performance efficiency. Performance targets define the desired level of performance for your workload and help you gauge its effectiveness in meeting those objectives. Performance targets provide a benchmark to measure and compare a workload's efficiency. This benchmark can help you highlight improvement areas. The targets also align tasks with your organization's objectives and enhance business outcomes. Additionally, performance targets offer guidance in resource allocation, helping you ensure that workloads can adapt to varying demands while maintaining optimal performance.
Set performance targets early
Set performance targets before you deploy your workload. For a workload in a design, performance targets require research. Conduct market research, competitive analysis, and surveys to generate your performance target ranges. For a production workload that has no performance targets, use production data and customer feedback to establish performance targets.
Determine performance requirements
Determining performance requirements is about identifying essential performance metrics like response time, throughput, and latency that are critical for your application. Aligning these performance targets with your organization's business goals ensures the workload meets the desired standards, whether for a best-in-class or average product. For example, you might aim to reduce response times, increase throughput rates, or optimize resource use.
When setting performance goals, it's important to align the organization's objectives with the distinct needs of the user base. Users ultimately determine the success of performance, emphasizing the need to align performance targets with their expectations. This balance ensures that performance targets capture the intended user experience and the overall efficiency of the workload. To comprehensively gauge and optimize workload performance, you should consider setting performance targets for the following list:
Individual components
: Individual components are the separate units or segments of the workload, each potentially having distinct performance attributes and demands.
User flows
: These pathways chart how users maneuver through the workload, and ensuring their fluidity directly enhances user experience.
Workflows
: Workflows defined internal processes are crafted to achieve particular results and often dictate operational efficiency.
Data flows
: Data flows refer to the movement and interaction of data within the workload, helping identify potential inefficiencies or bottlenecks.
External dependencies
: External dependencies are elements outside the primary workload (integrated third-party services or tools) that can significantly affect performance.
Scale units
: Scale units relate to the workload's scalable segments. Ensuring robust performance under increased loads is pivotal, especially in growth scenarios.
Technology levels
: Technology levels are direct performance indicators such as the speed of API access, database operation latencies, and potential network delays.
Business transactions
: Business transactions represent end-to-end user operations like completing a purchase or booking a service, their seamless execution is directly tied to user satisfaction.
Workload all up
: This holistic metric gives an overview of the collective performance encompassing all components and aspects of the workload.
Identify key metrics
Identifying key performance metrics involves determining the essential measurements that track the progress towards achieving workload performance goals. This identification provides a quantifiable way to measure and improve performance efficiency. When you identify key metrics to focus on, consider metrics related to availability, capacity, and response time:
Availability
: Error rate is an availability performance metric. Error rate represents the percentage of failed requests over a period. A common target for error rate is 0.1% percent of requests.
Capacity
: Throughput and concurrency are sample capacity metrics. Throughput refers to the ability to handle a specific number of transactions within a given time period. For instance, an application might need to sustain 100 million transactions per month. Concurrency is a measure of simultaneous users or actions.
Response time:
Latency and load time are common response time metrics. Latency is the time it takes to respond to a request (200 milliseconds). Load time is the time it takes for an application or web page to be interactive. A common target is 99% of sign-in requests completing less than 1 second.
Set specific targets
After you identify the key metrics, you need to specify performance targets or thresholds for each metric. Performance targets should be measurable, realistic, and aligned with your workload objectives. For example, you might set a target response time of less than 500 milliseconds (ms) or a target error rate of less than 1 percent. Avoid qualitative assessments of performance like
fast
or
slow
. By using numerical targets, you can objectively assess performance over time. As you set specific performance targets, consider these recommendations:
Consider the customer
: When you set performance targets, adopt a customer-centric perspective. Recognizing the customer as the ultimate judge of performance helps ensure that performance targets align with customer expectations. This alignment involves considering both organizational objectives and the distinct requirements of the customer base. When you integrate these two aspects, you can tailor performance targets to reflect the desired customer experience and overall workload effectiveness. By defining performance objectives that consider customer expectations, you can strive to provide a high-quality customer experience and meet the needs of your customers.
Use percentiles
: Percentiles, such as P99, P95, and P50, are the industry standard to represent the result of performance assessments. Percentiles are measures that indicate how much data the number includes. For example, P99 covers 99% of the data. Use percentiles, rather than simple averages, to provide a more comprehensive understanding of workload performance. To measure percentiles, collect performance data over a period of time, typically using monitoring tools or logging mechanisms. Then analyze this data to determine the response time values at different percentiles.
Document and expose performance target
Documenting and exposing performance targets is about recording all performance targets in a centralized location. Meeting performance targets is a shared responsibility between development and operations teams. To ensure that the workload consistently meets or exceeds these targets, provide teams with the information and access to take action. To document and expose performance targets, consider these recommendations:
Document performance targets
: Document all performance targets. Ensure that all performance targets are documented in a centralized location, easily accessible by both development and operations teams. It promotes alignment and aids in real-time decision-making.
Expose performance targets
: All responsible teams should be able to review and create actionable tasks from the performance targets. Use information radiators, such as dashboards and reports, to make the performance targets accessible.
Make it actionable
: The documentation and information radiators should suggest clear next steps. For example, a rise in errors might prompt an immediate check, or meeting a target consistently might suggest a reevaluation of that benchmark.
Evaluate customer feedback
Evaluating customer feedback involves actively seeking out and analyzing the responses and suggestions of your customers. Actively collecting and analyzing customer feedback offers valuable insights into their needs and expectations. Regular communication helps in adjusting performance targets in line with changing preferences and tech trends. A focus on customer needs means that the workload not only aligns with technical benchmarks but also undergoes continuous refinement. This approach, emphasizing customer satisfaction, ensures that the workload remains relevant and successful in the long run.
Azure facilitation
Setting performance targets
: Azure Advisor provides
performance recommendations
that can inform your performance targets.
Azure Monitor
is a full-stack monitoring service that provides a complete set of features to monitor your Azure resources and measure performance targets. It collects platform metrics and provides ready-to-use dashboards. It allows you to configure alerts based on metrics. It also stores and correlates metrics to ensure a single source of truth.
Related links
Azure Advisor performance recommendations
Azure Monitor
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for performance testing - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for performance testing
Article
2023-11-15
4 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:06
Test performance. Perform regular testing in an environment that matches the production environment. Compare results against the performance targets and the performance benchmark.
This guide describes the recommendations for testing. Performance testing helps you evaluate the functionality of a workload in various scenarios. It involves testing the workload's response time, throughput, resource utilization, and stability to help ensure that the workload meets its performance requirements.
Testing helps to prevent performance issues. It also helps ensure that your workload meets its service-level agreements. Without performance testing, a workload can experience performance degradations that are often preventable. Workload performance can drift from performance targets and established baselines.
Definitions
Term
Definition
Chaos testing
A performance test that aims to test the resilience and stability of a system by deliberately introducing random and unpredictable failures or disruptions.
Load test
A performance test that measures system performance under typical and heavy load.
Performance baseline
A set of metrics that represent the behavior of a workload under normal conditions as validated by testing.
Stress test
A performance test that overloads a system until it breaks.
Synthetic test
A performance test that simulates user requests in an application.
Key design strategies
Performance testing helps you gather measurable data on a workload. When you run tests early enough, they also help you build workloads to the right specifications. You should conduct performance tests as early as possible in the software development lifecycle. Early testing allows you to catch and fix performance issues earlier in development. You can use a proof of concept (POC) if production code isn't ready.
Prepare the test
Preparing performance tests refers to setting up and arranging the resources, configurations, and test scenarios that you need to conduct performance testing effectively.
Define acceptance criteria
Acceptance criteria specify the performance requirements that a workload needs to meet to be considered acceptable or successful. Define criteria that align with the performance targets.
Review performance targets.
Performance targets define your desired level of performance for your workload. Review the performance targets that are established for the workload. Performance targets are metrics that can involve response time, throughput, resource utilization, or any other relevant performance indicators. For example, you might have a target for your response time to be under a certain threshold, such as less than 2 seconds.
Define acceptance criteria.
Translate the performance targets into specific acceptance criteria that you can use to evaluate the performance of your workload. For example, suppose your performance target for response time is 2 seconds or less. Your acceptance criterion could be
The average response time of the workload should be less than 2 seconds
. Use these acceptance criteria to determine whether the workload meets the desired level of performance.
When you define acceptance criteria, it's important to focus on users and their expectations. Acceptance criteria help ensure that the delivered work meets user needs and requirements. Keep in mind the following considerations for incorporating the user perspective into acceptance criteria:
User requirements
: Understand the user needs and goals for the workload. Consider how the workload should perform to satisfy these requirements.
User experience
: Define acceptance criteria that capture the desired user experience. Include factors such as response time, usability, accessibility, and overall satisfaction.
Functional requirements
: Address the specific functionality that the user expects to see in the workload. Define acceptance criteria around these functional requirements to help ensure that they're met.
Use cases
: Consider different scenarios or use cases that the user might encounter. Define acceptance criteria based on these use cases to validate the workload's performance in real-world situations.
Set acceptance thresholds.
Determine the thresholds within the acceptance criteria that indicate whether the workload meets the performance targets. These thresholds define the acceptable range of performance for each metric. For example, suppose the acceptance criterion for response time is less than 2 seconds. You can set the threshold at 2.5 seconds. This level indicates that any response time over 2.5 seconds is considered a performance issue.
Define passing criteria.
Establish the criteria for determining whether the workload passed or failed the performance test. You might define passing as meeting all the acceptance criteria or achieving a certain percentage of them.
Select the test type
To select the right type of performance test, it's important to align the test with your acceptance criteria. The acceptance criteria define the conditions that need to be met for a requirement or bug fix to be considered done. Performance tests should aim to verify whether a workload meets these acceptance criteria and performs as expected under specified conditions. Aligning the performance test type with the acceptance criteria helps ensure that the test focuses on meeting the performance expectations that the criteria define.
Understand acceptance criteria
. Review the acceptance criteria for the requirement or bug fix. The criteria outline the specific conditions and functionalities to be met.
Identify relevant performance metrics
. Based on the acceptance criteria, determine the performance metrics that are critical to achieving the desired outcomes. For example, if the acceptance criteria focus on response time, prioritizing load testing might be appropriate.
Select an appropriate test type
. Evaluate the available test types and choose the one that best aligns with the identified performance metrics and acceptance criteria.
The following table provides a sample of test types and their use cases.
Test type
Description
Use case
Load testing
Simulate realistic user loads to measure how your workload performs under expected peak workloads.
Determines load tolerance.
Stress testing
Push your workload beyond its normal limits to identify its breaking points and measure its ability to recover.
Determines resilience and robustness.
Soak testing (endurance testing)
Run your workload under sustained high loads for an extended period to identify performance degradation, memory leaks, or resource issues.
Evaluates stability and reliability over time.
Spike testing
Simulate sudden increases in user load to assess how your workload handles abrupt changes in demand.
Measures the ability to scale and maintain performance during peak periods.
Compatibility testing
Test your workload's performance across various platforms, browsers, or devices.
Helps ensure consistent performance across various environments.
Prioritize your selected test types based on the characteristics and requirements of your workload. Consider factors such as the criticality of performance metrics, user expectations, business priorities, and known issues or vulnerabilities.
Select testing tools
Choose appropriate tools based on the type of performance testing that you want to run. Evaluate the testing environment's infrastructure, resources, and constraints. Choose testing tools that support the desired test types and provide the necessary features for monitoring, measurement, analysis, and reporting.
An application performance monitoring (APM) tool provides deep insights into applications and is an essential testing tool. It helps you trace individual transactions and map their paths through various workload services. After testing, you should use the APM tool to analyze and compare testing data against your performance baseline.
Use profiling tools to identify performance bottlenecks in your code. Profiling helps identify areas of the code that consume the most resources and need optimization. It provides insights into the execution time and memory usage of different parts of the code.
The following steps can help you select the appropriate testing tools:
Identify testing requirements
. Begin by understanding the specific requirements of your performance testing. Consider various factors:
The type of workload
Performance metrics to measure, such as response time and throughput
The complexity of the workload architecture
The testing environment, such as cloud-based, on-premises, or hybrid
Research testing tools
. Conduct research to identify performance testing tools that align with your requirements. Consider commercial and open-source tools that are available in the market. Look for tools that support your desired types of performance testing, such as load testing or stress testing, and that provide features for measuring performance metrics.
Evaluate tool features
. Assess the features that each testing tool provides. Look for capabilities such as simulation of realistic user behavior and scalability to handle large user loads. Consider support for various protocols and technologies, integration with other testing tools or frameworks, and reporting and analysis capabilities.
Consider compatibility and integration
. Determine the compatibility of the testing tools with your existing infrastructure and technologies. Ensure that the tools can be easily integrated into your testing environment and can communicate with the necessary workload for monitoring and analysis.
Evaluate cost and licensing
. Assess the cost structure and licensing terms that are associated with the testing tools. Consider factors such as the initial investment, maintenance costs, and support costs. Also consider other licensing requirements that depend on the number of users or virtual users.
Conduct a POC
. Select a few tools that appear to be the most suitable based on your evaluation. Conduct a small-scale POC to validate the usability, features, and effectiveness of the tools in your specific testing scenario.
Consider support and training
. Evaluate the level of support and training that the tool's vendor or community provides. Determine the availability of documentation, tutorials, and technical support channels to assist with any challenges or issues that might arise during the testing process.
Create test scenarios
Creating test scenarios refers to the process of designing specific situations or conditions that are suitable for testing the performance of a workload. Test scenarios are created to emulate realistic user behavior and workload patterns. These scenarios provide a way for performance testers to evaluate how the workload performs under various conditions.
Test scenarios make it possible to replicate various workload patterns, such as concurrent user access, peak load periods, or specific transaction sequences. By testing the workload under different workload patterns, you can identify performance bottlenecks and optimize resource allocation.
Define user behavior
. Emulate realistic user behavior and workload patterns by identifying the steps and actions that users perform when they interact with the workload. Consider activities such as signing in, performing searches, submitting forms, or accessing specific features. Break down each scenario into specific steps and actions that represent the user's interaction with the workload. You can include navigating through pages, performing transactions, or interacting with various elements of the workload.
Determine data involvement
. Identify the test data required to run the test scenarios. You might include creating or generating realistic data sets that represent various scenarios, user profiles, or data volumes. Ensure that the test data is diverse and covers different use cases to provide a comprehensive performance evaluation.
Design test scripts
. Create test scripts that automate the execution of the defined test scenarios. Test scripts typically consist of a sequence of actions, HTTP requests, or interactions with workload APIs or user interfaces. Use performance testing tools or programming languages to write the scripts, considering factors such as parameterization, correlation, and dynamic data handling. Validate the test scripts for correctness and functionality. Debug any issues, such as script errors, missing or incorrect actions, or data-related problems. Test script validation is crucial to help ensure accurate and reliable performance test execution.
Configure test variables and parameters
. Configure variables and parameters within test scripts to introduce variability and simulate real-world scenarios. Include parameters such as user credentials, input data, or randomization to mimic different user behaviors and workload responses.
Iteratively refine scripts
. Continuously refine and enhance test scripts based on feedback, test results, or changing requirements. Consider optimizing script logic, parameterization, and error handling, or adding extra validation and checkpoints.
Configure the test environment
Configuring a test environment refers to the process of setting up the infrastructure, software, and network configurations that you need to create an environment that closely resembles your production environment.
To set up your testing environment in a way that boosts performance efficiency, include the following steps in your configuration process:
Mirror your production environment
. Set up your test environment to closely resemble your production environment. Consider factors such as infrastructure configuration, network settings, and software configurations. The goal is to ensure that the performance test results are representative of real-world conditions.
Provision sufficient resources
. Allocate adequate resources such as CPU, memory, and disk space to the test environment. Ensure that the available resources can handle the expected workload and provide accurate performance measurements.
Replicate network conditions
. Configure the network settings in the test environment to replicate the expected network conditions during the actual workload deployment. You need to include bandwidth, latency, and network protocols.
Install and configure dependencies
. Install the software, libraries, databases, and other dependencies that are required for the workload to run correctly. Configure these dependencies to match the expected production environment.
Tradeoff
: There are costs associated with maintaining separate test environments, storing data, using tooling, and running tests. Know the cost of performance testing, and find a way to optimize spending.
Risk
: Production data can contain sensitive information. Without a robust scrubbing and masking strategy, you risk leaking sensitive data when you use production data for testing.
Perform the tests
Run the performance tests by using the chosen testing tool. Testing involves measuring and recording performance metrics, monitoring health, and capturing any performance issues that arise.
Monitor and collect performance metrics such as response time, throughput, CPU and memory utilization, and other relevant indicators.
Use the defined test scenarios to put the workload under expected loads.  Conduct tests under these varying load conditions. For example, use levels, such as normal, peak, and stress levels, to analyze the behavior of the workload in various scenarios.
Analyze the results
Analyzing the test results involves examining the collected data and metrics from the performance tests to gain insights into the performance of the workload. The goal is to identify performance issues and use the feedback to adjust priorities in application development. The following actions are key steps for analyzing test results.
Review performance metrics.
Look at the performance metrics that you collect during performance testing, such as response times, throughput, error rates, CPU and memory utilization, and network latency. Analyze these metrics to understand the overall performance of the workload.
Identify bottlenecks
. Evaluate the performance metrics to identify any bottlenecks or areas of inefficient performance. The evaluation can include high response times, resource constraints, database issues, network latency, and scalability limitations. Pinpointing the root causes of these bottlenecks helps you prioritize performance improvements.
Correlate metrics
. Assess the relationships and correlations between various performance metrics. For example, analyze how increased load or resource utilization affects response times. Understanding these correlations can provide valuable insights into workload behavior under different conditions. Look for patterns and trends in the performance data over time. Analyze performance under different load levels or during specific periods. Detecting trends can help identify seasonal variations, peak usage times, or recurring performance issues.
Evaluate acceptance criteria.
Compare the retest results against the predefined acceptance criteria and performance goals. Assess whether the workload meets the desired performance standards. If the workload doesn't meet the acceptance criteria, further investigate and refine the optimizations.
Iterate and refine the analysis.
Make other adjustments and improvements as needed. Use the collected data and metrics to diagnose specific performance issues. The diagnosis might involve tracing through the workload components, examining log files, monitoring resource usage, or analyzing error messages. Dig deeper into the data to understand the underlying causes of performance problems.
Based on the analysis of the test results, prioritize identified performance issues and implement necessary improvements. The improvements can involve optimizing code, tuning database queries, improving caching mechanisms, and optimizing network configurations.
Establish baselines
Baselines provide a reference point for comparing performance results over time. Baselines should be meaningful snapshots of workload performance—you don't need to use every test as a baseline.
Consider the workload objectives, and document performance snapshots that allow you to learn over time and optimize. Use these baseline measurements as a benchmark for future performance tests, and use them to identify any degradation or improvement.
To establish baselines for performance testing and use them as a benchmark for future performance tests, follow these steps:
Identify performance metrics
. Determine the specific performance metrics that you want to measure and track. Examples include:
Response time, or how quickly the workload responds to requests.
Throughput, or the number of requests that are processed per unit of time.
Resource utilization, such as CPU, memory, and disk usage.
Record meaningful measurements
. Record the performance metrics that you obtain during the test as the baseline measurements. These measurements represent the starting point against which you compare future performance tests.
Compare future tests
. In subsequent performance tests, compare the performance metrics against the established baselines and thresholds. The comparison allows you to identify any improvements or degradation in performance.
Test continuously
Continuous testing involves the ongoing monitoring and refinement of your tests. Continuous testing helps you maintain consistent and acceptable levels of performance. A workload should provide a consistent and acceptable level of performance relative to the baseline. You should tune the workload over time to produce consistent performance that's within the acceptable limits of performance. Here are some key practices:
Set degradation limits
. Define numeric thresholds that specify the level of performance degradation that's acceptable over time. By setting these limits, you can monitor performance fluctuations and receive alerts when the performance falls below the defined threshold.
Include quality assurance
. Incorporate performance requirements, such as CPU utilization and maximum requests per second, into the quality assurance process. Treat performance requirements with the same level of importance as functional requirements. This process helps ensure that the workload meets the defined performance requirements before you deploy it to production.
Automate alerting
. In live environments, rapid detection and response are crucial. Set up automated alerting systems that use the performance baseline as their reference. If there's a significant deviation in performance, the necessary teams are alerted immediately to act.
Test changes
. Some performance issues might only manifest in a live setting. Apply thorough testing practices for proposed code and infrastructure changes. Use code instrumentation to gain insights into the application's performance characteristics, such as hot paths, memory allocations, and garbage collection. This testing ensures that any change introduced doesn't degrade performance beyond the acceptable limits.
Azure facilitation
Perform the tests
:
Azure Pipelines
makes it possible for you to integrate performance testing into your CI/CD pipeline. You can incorporate load testing as a step in your pipeline to validate the performance and scalability of your applications.
Azure Chaos Studio
provides a way for you to inject real-world faults into your application so that you can run controlled fault injection experiments. The experiments help you measure, understand, and improve your cloud application and service resilience.
Azure Load Testing
is a load testing service that generates high-scale load on any application. Load Testing provides capabilities for automating load tests and integrating them into your continuous integration and continuous delivery (CI/CD) workflow. You can define test criteria, such as average response time or error thresholds, and automatically stop load tests based on specific error conditions. Load Testing offers a dashboard that provides live updates and detailed resource metrics of Azure application components during a load test. You can analyze the test results, identify performance bottlenecks, and compare multiple test runs to understand performance regressions over time.
Analyzing the results
:
Azure Monitor
is a comprehensive monitoring solution for collecting, analyzing, and responding to telemetry from your cloud and on-premises environments.
Application Insights
is an extension of Monitor that provides APM features. You can use Application Insights to monitor applications during development and testing and also in production.
Tradeoff
: Testing takes time and skill to perform and can affect operational efficiency.
Related links
Recommendations for security testing
Recommendations for designing a reliability testing strategy
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Performance Efficiency design principles - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Performance Efficiency design principles
Article
2023-11-15
4 contributors
Feedback
In this article
Performance efficiency is the ability of your workload to adjust to changes in demands. A workload must be able to
handle an increase in load without compromising user experience
. Conversely,
when there's a decrease in load, the workload must conserve its resources
. Capacity, which indicates resource availability (CPU and memory), is a significant factor.
The workload design shouldn't just rely on pre-provisioned capacity, which guarantees performance up to a certain limit. If that limit is exceeded, the workload might have performance issues or even experience outages. When load is under that limit, resources continue to run unnecessarily, incurring costs.
You need a comprehensive strategy to sustain performance targets over time. Performance considerations shouldn't be an afterthought in the design process, only to be addressed when issues arise in production. Instead,
adopt a mindset where performance is a key consideration from the early stages of design
. Initially, build the system without any specific performance targets. But from there, test and measure performance at each stage of development to ensure progress and effectiveness. Continual optimization of these targets throughout the process and incorporating lessons learned from production can significantly mitigate potential issues in advance.
These design principles can help
build your strategy for managing capacity
of resources to sufficiently meet your business requirements for expected usage. Also, reduce waste during off-peak hours. After you've decided on a strategy, solidify your design by using the
Performance Efficiency checklist
.
Performance efficiency is about effective use of workload resources. Without a good strategy, you might not be able to anticipate and meet user demands. You might have to resort to an approach of long-term forecasting and pre-provisioned capacity, which doesn't let you take full advantage of your cloud platform.
Negotiate realistic performance targets
The intended user experience is defined, and there's a strategy to develop a benchmark and measure targets against the pre-established business requirements.
From a performance perspective, it's ideal to have well-defined performance targets to start your design process. To set those targets, you need to have a good understanding of the business requirements and the anticipated quality of service that the workload is expected to deliver. Define the expectations in collaboration with the business stakeholders. Instead of only focusing on technical metrics, determine the acceptable effects on the user experience for the key flows.
There's a circular dependency. You can't measure what you haven't defined, and you can't define without measurement. So, it's also important to
measure the workload performance until you achieve a satisfactory definition of acceptable threshold
with collective agreement.
There's a strong correlation between performance and reliability targets, which help determine the quality of service in terms of performance, availability, and resilience. Without a clear definition, it's challenging to measure, alert for, and test performance. After you establish the targets and identify actual numbers through testing over time, you can implement automation for continuous testing against these targets.
Adhere to best practices in defining targets at the macro level, even if they're approximate or within a range.
Approach
Benefits
Prepare for effective negotiation
by understanding technical concepts, exploring design possibilities with the available infrastructure, and using results from concrete experimentation, if available.
Use historical data
to get visibility into usage patterns and bottlenecks.
Bring insight from external factors, such as
input from market analysis, experts, and industry standards
.
You can make
informed decisions
based on practical insights.
The performance targets are focused on user experience that's based on what's feasible, industry best practices, and current market trends.
Collaborate with the business owners
to understand user promises, in terms of quality and regulatory compliance, if applicable.
Maintain a broad perspective
and avoid diving into granular details at this stage.
Be explicit about
what represents acceptable performance
, based on the investments.
Understand the business context and
anticipated growth
.
You'll
avoid making assumptions
that might not align with the business goals. It also drives clarity and motivation within the workload team.
Having a business context on functional and nonfunctional requirements might uncover design changes in other Azure Well-Architected pillars and
help you make informed tradeoffs
.
Defining parameters early on helps avoid costs associated with potential solution redesigns later.
It enables you to ensure that
performance targets cover future projections
, so you can align current efforts with long-term goals.
Identify the workload flows
and prioritize the flows in the architectural diagram.
Define each flow's performance tolerance
as a range from aspirational to unacceptable performance.
Evaluate the entry and exit points for each flow
, considering the path's criticality, usage frequency, and architectural intensity.
By prioritizing flows, you can
focus your resources on critical areas
that have the most effect on user and business outcomes.
By breaking down the system into its parts and dependencies, you understand each component's function and influence on performance. You also become aware of potential issues.
It helps establish a performance baseline and drive optimization.
Start building a performance model
Consider whether usage patterns show seasonal or daily variations. Factor in the cost, operations, and criticality to the business.
Use industry standards to quantify metrics
and aggregation methods, such as using percentiles.
Evaluate the demand and supply expectations
and limitations that business constraints impose.
Incorporate growth prospects.
A performance model provides
insight into optimal use of resources
and helps with strategic planning.
Industry standards help with benchmarking.
Future proofing ensures that the performance goals remain relevant and can adapt to changes.
Design to meet capacity requirements
Provide enough supply to address anticipated demand.
It's important to proactively measure performance. Measuring performance involves
measuring baselines
and having a preliminary understanding of which components of the system are likely to pose challenges. You can achieve it without conducting a full performance test or through granular optimization. By taking these initial steps, you establish a foundation for effective performance management early in the development lifecycle.
Examine the system as a whole, rather than focusing on individual components. Avoid fine-tuning at this stage. Making granular performance improvements results in tradeoffs in other areas. As you progress through the lifecycle and begin user acceptance testing or move toward production, you can quickly identify which areas require further optimization.
Approach
Benefit
Evaluate the elasticity demands
for the identified flows.
Explore design patterns
that can be implemented across the technology stack, considering the application and the underlying compute and data layers.
You're able to
define scalability requirements
on existing components that need more capacity and the areas where you need extra components to distribute load.
You're aware of potential bottlenecks in the system and
design compensating controls
, such as adding caching capabilities to decrease latency and system load.
Choose the right resources
across the technology stack, which enables you to meet performance goals and integrate with the system.
Consider features
that can fulfill the scalability requirements.
Find the right balance between resource allocation and system requirements
, to handle unexpected surges efficiently.
By analyzing the varying capabilities of the resources, you ensure that each component contributes to the
overall functionality and performance of the system
.
You can
take advantage of the built-in capabilities
that automatically trigger scaling operations.
Right-sizing resources can meet changes in demand without overprovisioning, which leads to cost savings.
Do capacity planning
based on demand and the capability of selected resources to enrich your performance model.
Use predictive modeling techniques
to forecast anticipated changes in capacity that can occur with predictable and unexpected changes.
Define performance targets
that can be translated into technical requirements.
You can
efficiently use resources
and meet the demand without overprovisioning, thereby avoiding unnecessary costs.
You understand how the design choices affect performance.
Implement a proof of concept
that validates the technical requirements and design choices.
A proof of concept is instrumental in
validating the design
to determine if the system can meet the performance targets and if those targets are realistic. Based on the anticipated load, you can validate whether anticipated capacity can meet the performance targets.
Also, verify the cost implications of the design choices.
Document your performance testing strategy
.
Include use cases, different methodologies, and cadence of your test plans.
Define a process for operation outlined by the performance test plan.
Triage and prioritize the test cases in the plan. Focus on cases that offer valuable insights into performance targets and align capacity planning.
You ensure that the
right aspects of the system are tested
.
You can allocate resources effectively and conduct tests in a manner that aligns with the business priorities and requirements.
Document your performance monitoring strategy
.
Assess metrics at different abstraction levels for each identified flow.
You can
track progress towards attainment
of performance targets throughout the development cycle.
Achieve and sustain performance
Protect against performance degradation while the system is in use and as it evolves.
Development isn't a one-time effort. It's an ongoing process. Expect changes in performance as features change. There's variance in user patterns and profiles, even changes from optimizations in other Azure Well-Architected pillars. Any change can strain workload resources.
Safeguard the system from changes
so that it doesn't slide back on performance targets.
Integrate testing and monitoring in the development process
. Test the system's performance in production with real load and simulate that load with automated testing prior to production. In both cases, you should have monitoring practices in place for verification purposes.
Throughout the development lifecycle,
conduct various types of tests at different stages
. In the initial stages, test the proof of concept to make sure performance results aren't entirely unexpected. As development progresses, conduct
manual, low-effort tests
to establish benchmarks. In the build stage, start developing
automated routine performance tests
that evaluate latency, stress levels, load capacity, and other characteristics defined in the test plans.
Monitoring must be an integral part of that effort, rather than being an isolated exercise. You can
see how the system and its resources perform over time. You can then fine-tune them to maximize their value
, and ensure they continue to meet performance standards.
Keep in mind that performance targets vary over time, in response to changes. Update the performance model based on tested and monitored metrics. Clearly indicate increased, reduced, or no effect on the performance of the flows.
Always be ready to renegotiate and reset expectations with business stakeholders.
Approach
Benefit
Integrate routine performance tests
in Azure Pipelines.
Choose pipelines that can integrate tests. Conversely, choose test tools that can be integrated into the pipelines.
Automated tests save time and provide consistency that makes it
easier to detect regressions or improvements
.
These artifacts allow for continuous monitoring of any deviations or drift over time, so you can maintain consistent performance and quality.
Formalize performance tests as quality gates
that can approve or deny release promotion and the final deployment to production.
These checkpoints ensure that
each stage of deployment meets the required performance standards
before you proceed to the next. The checkpoints help prevent unintended performance regression.
For instance, if the performance is significantly below expectations, you might block a release until improvements are made.
Set up a repeatable process for monitoring
real transactions in production and deviations against your performance targets.
Use synthetic transactions in production.
Set up monitoring alerts on performance regressions.
You want insight into the
actual performance of your system under real-world load
that couldn't be simulated through tests.
Then you can proactively identify issues and areas of improvement such as potential bottlenecks, underutilized resources, and other concerns.
Review performance test results and monitoring data
meticulously and optimize until you meet the performance targets.
Prioritize actions derived from those reviews and add them to the backlog for planned execution.
Based on test results,
you can capture and compare data
and start analyzing trends.
Your optimization efforts are data driven.
Build coding skills that focus on performance
.
Have coding standards that exemplify performance-driven coding patterns.
Code that doesn't have performance issues can make
testing cycles more efficient
because tests can be focused on more significant issues.
Coding patterns helps avoid rework and keeps your coding style consistent.
Address performance erosion
as usage increases, features change, and data accumulates over time to sustain performance.
Reset expectations and establish new targets, if fine-tuning brings only short-term benefits.
You can
preserve the performance state
before degradation develops into problems that negatively affect user experience beyond the acceptable range.
Changing targets resets the performance model, and you don't waste time in optimizing the system that has already reached its capacity.
Improve efficiency through optimization
Improve system efficiency within the defined performance targets to increase workload value.
The targets set during the initial phase are based on a reasonable level of user experience, considering various constraints. You should
reassess and adjust targets to further enhance the experience
. To further enhance the experience, it requires a clear understanding of how the system is used, how it has evolved, and how the platform or technology has changed over time. The cycle of monitoring, optimizing, testing, and deploying is a continuous process.
Efficiency optimization efforts allow a workload to work with lower resource consumption. They can cause the workload to be in an overprovisioned state with spare capacity. Use that capacity to improve reliability of the system. Eliminate capacity to improve the cost of the system. Or repurpose the capacity to support new product features on existing resources.
When the system gains efficiencies, take the opportunity to set and maintain new performance targets.
Approach
Benefit
Allocate dedicated cycles for performance optimization
to address nonfunctional requirements and optimizations in functional areas. Targets for this optimization are resources, code, data retention, database queries, and others.
You can
build a culture of performance-driven optimization
. You keep the team accountable for proactively monitoring performance patterns and also fine-tune the application.
Enhance the architecture with new design patterns and components
, which can boost performance, in ways that you previously didn't consider because of limited time or budget.
New design and components can optimize the system,
leading to better user experience
. For example, you can use caching or adding a content delivery network component.
It can also lead to long-term cost benefits.
Use monitoring tools to analyze historical trends
and to identify the flows and code implementation paths that would benefit the most from a performance optimization effort. We recommend application performance monitoring (APM) tools and profilers for this purpose.
Identify operation hot paths and other potential bottlenecks in the system.
When you identify the recurring problematic areas, the team can
focus where gains are the highest
.
Get current and stay current with technology innovations
that can improve performance.
Take advantage of the new versions released for the dependent frameworks and libraries.
Similarly, use the new features for platform resources as they're updated and patched.
Adopting new technology can often be the motivating factor to
look for opportunities to improve
.
Code that might have been slow in the past can become faster with these updates. You also want to be aware of how certain updates negatively affect performance.
Next steps
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for prioritizing the performance of critical flows - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for prioritizing the performance of critical flows
Article
2023-11-30
5 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:09
Prioritize the performance of critical flows. The allocation of workload resources and performance optimization efforts should prioritize the flows that support the most important business processes, users, and operations.
This guide describes the recommendations for prioritizing the performance of critical flows in a workload. Critical flows represent crucial business processes that generate revenue or drive high-priority operations. When you prioritize the performance of critical flows, you ensure the flows that have the most impact get the resources they need before lower priority flows. Failure to do this prioritization can have disproportionate negative effects on workload priorities and the user experience.
Definitions
Term
Definition
Flow
In a workload, the sequence of actions that performs a specific function. A flow involves the movement of data and the running of processes between components of the workload.
Priority queue processing
The act of processing high-priority tasks before low-priority tasks.
Rate limiting
The act of limiting how many requests can access a resource.
System flow
The flow of information and processes within a system. The system automatically follows this flow to enable user flows or workload functionality.
User flow
The sequence that a user follows to accomplish a task.
Key design strategies
Critical flows refer to the key user flows for customers or the system and data flows for operations that are crucial to the workload functionality. These flows can include actions such as user registrations, sign-ins, product purchases, accessing pages behind a paywall, or any other key path or process within your workload.
Critical flows significantly affect the user experience or business operations. Critical flows have higher performance targets and service-level agreements than noncritical flows. Where resources are limited, noncritical flows should yield resource usage to critical flows. You need to identify, monitor, and prioritize all flows before isolating and optimizing critical flows.
Identify all flows
The first step in prioritizing the performance of critical flows is identifying all the flows within your workload. Flow identification involves systematically mapping and understanding every user paths and component communication. The focus is on understanding the performance metrics and potential impact of flows on workload performance.
By dissecting the workload into discrete flows, you can find performance bottlenecks, inefficient resource utilization, and opportunities for performance optimization. This knowledge exposes areas of needed improvement and is the first step to identifying critical flows. For more information, see
Identify and rate user and system flows
.
Monitor flow performance metrics
After you identify all flows within your workload, you need to collect performance metrics on each flow and monitor those metrics. Flow metrics provide insights into response times, error rates, and throughput. The goal is to consistently observe and record performance-related metrics to further refine your understanding of each flow's impact on workload performance. To monitor flow metrics, you can use the following tools to collect data:
Analytic and tracking tools
: These tools provide insights into user behavior and interactions within your application. By analyzing user data, you can identify the most common flows, bottlenecks, or potential issues.
Application performance monitoring (APM) tools
: Use APM tools to monitor the performance of your application and track how flows run. These tools provide visibility into response times, errors, and other performance metrics, allowing you to identify critical flows and optimize their performance.
Logging and debugging tools
: Use these tools to capture and analyze logs and debug information while your application runs. Review logs and debugging information to trace how flows are running and identify issues or errors.
Identify critical flows
With the performance data available, you can begin ranking all the flows and identifying the critical flows. The identification of critical flows involves evaluating the performance impact and criticality of each flow. Effective flow prioritization ensures that the most important flows receive the resources needed before less critical flows. To prioritize flows in your application, consider these steps:
Identify business impact
: Start by assessing the importance of each flow within your operations. Focus on how each flow aligns with your business objectives, its impact on users, and the potential negative effects of poor performance. For instance, while a free service tier might attract more users, a paid tier could be more vital for your business goals.
Additionally, consider the performance impacts of a flow across one or more business processes. Multiple flows might support a single business process, but often, one flow has a significant effect on the performance of that process. You want to identify the flows that the greatest performance impact. Conversely, a single flow might underpin several processes. In such cases, the performance of this flow directly influences the efficacy of all related processes, and it's likely a critical flow.
Analyze performance data
: Analyze the performance metrics associated with each flow. Look for patterns, anomalies, or standout metrics that can provide insights into the flow's efficiency and importance. For example, system flows with significant usage are likely important flows.
Assign criticality rating
: Based on the business impact and performance indicators, you should prioritize the flows. Use criticality ratings of
High
,
Medium
, and
Low
. Flows with a significant business impact or high performance demand should receive a "High" criticality rating. These flows are your critical flows. Focus on flows with high user traffic or have a direct effect on revenue generation. The following table provides characteristics of critical (
High
) and noncritical flows (
Medium
to
Low
).
Critical flows
Noncritical flows
High usage
Low usage
Business critical
Not business critical
Expensive operations
Small operations
Time-sensitive
Not time-sensitive
Production
Preproduction
Real-time processing
Batch processing
Latency sensitive
Not latency sensitive
Paying user
Nonpaying user
Premium tier
Basic tier
Important tasks
Nonessential tasks
High-revenue accounts
Low-revenue accounts
Isolate critical flows
The process of isolating critical flows is about providing dedicated resources or capacity to support critical flows. You want to allocate resources and attention to those flows that are essential for optimal user experience or significant business outcomes. The goal is to ensure critical flows receive enough computing power, network bandwidth, and resources to operate efficiently and effectively. By isolating critical flows, you can more easily manage the resources that support critical flows. Here are recommendations for isolating critical flows:
Resource segmentation
: Create separate resources for critical flows, allowing them to operate independently without interference from other processes. For example, you can isolate critical flows on dedicated network segments or by using dedicated servers to handle the processing needs of these flows. This approach helps minimize how noncritical flows can negatively affect critical flows.
Logical segmentation
: Use virtualization and containerization tools like Docker or Kubernetes to isolate flows at the software level. You can separate critical flows into virtual machines (VMs). By doing so, you create an isolated environment, reducing dependencies and potential interference from other flows.
Capacity allocation
: For critical flows, explicitly allocate a fixed set of capacity such as CPU, memory, and disk I/O. This allocation ensures that critical flows always have enough resources to operate efficiently. Set resource quotas or limits by using orchestration platforms. By explicitly allocating resources to critical flows, you prevent resource contention and prioritize how they run.
Tradeoff
: Resource segmentation affects costs. When you dedicate resources to a flow, you often increase the cost and leave some resources underutilized. To justify the performance enhancements to critical flows, the increase in business impact must outweigh the increase in cost.
Optimize capacity allocation
When you can't isolate critical flows, the next best option is to prioritize critical flows in accessing available capacity. The optimization of capacity allocation is about strategically distributing available capacity to different flows based on their criticality. Capacity includes CPU, memory, storage, and network bandwidth. The goal is to ensure that the most critical flows (highest priority) receive the necessary capacity to operate effectively. To decide how to allocate capacity, consider these strategies:
Assess resource capacity
: Evaluate how much resource capacity can be allocated to the flows. Capacity might include resources such as CPU, memory, storage, and network bandwidth. Understand the limitations and constraints of your infrastructure or environment.
Analyze flow requirements
: Analyze the resource requirements of each flow. Understand the resources the flow needs to operate efficiently. For each flow, identify the resource demands, such as CPU utilization, memory requirements, and network bandwidth.
Prioritize allocations
: Match the available resource capacity to the resource requirements of the flows. Allocate resources based on flow priorities, ensuring that higher-priority flows receive the necessary resources to meet their requirements. Understand where your tightest constraints are and optimize capacity allocations where they're needed. For example, queues can process only some messages per minute, but some storage limits are hard to reach.
Use rate limiting
: To ensure that critical flows can consume the resources they need to meet their performance targets, apply rate limits to noncritical flows and tasks. Rate limits cap the number of requests lower-priority flows and users can make to constrained resources. For example, you might rate-limit nonpriority requests to an API. For more information, see the
Rate Limiting pattern
and
Rate limiting an HTTP handler in .NET
.
Use priority queue processing
: Priority queue processing gives high priority to certain requests. Queues usually have a first in, first out (FIFO) structure, but you can update your application to assign a priority to messages it adds to the queue. Use this capability to prioritize critical flows and users. For more information, see the
Priority Queue pattern
.
Risk
: It can be a challenge to balance the needs of critical flows with the overall performance of a workload. Although you should prioritize critical flows, you shouldn't neglect noncritical flows. The overall performance efficiency of a workload depends on all flows. Neglected noncritical flows could create issues that affect all users. Too much noise from nonessential items steals attention from critical items. But too little noise could harm the entire workload. The amount of data and the number of alerts should reflect these balanced priorities.
Azure facilitation
Identifying and monitoring flows
: Azure provides different solutions to help you monitor the performance of critical flows in your workload. Azure Monitor, Azure Monitor Logs, and Azure Application Insights are some of the services that offer comprehensive monitoring capabilities for several types of applications and workloads.
Optimizing capacity allocations
: Some Azure services support resource segmentation, logical segmentation, and capacity allocation techniques to allocate capacity and resources to critical flows. You can isolate critical flows through techniques such as creating separate resources, increasing density, using virtualization and containerization, and explicitly allocating resources to critical flows.
Some Azure services, such as
Azure API Management
, provide built-in policies for rate limiting. Azure provides detailed guidance and a sample implementation of the
Rate Limiting design pattern
.
Azure supports priority queue processing. Azure Functions provides event-driven functions that you can trigger in various ways, including by a new message in a queue or topic. Combine Azure Functions with Azure Queue Storage or
Azure Service Bus
to process messages based on their priority.
Related links
Priority Queue pattern
Rate Limiting pattern
Rate limiting an HTTP handler in .NET
Azure API Management
Azure Service Bus
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for responding to live performance issues - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for responding to live performance issues
Article
2023-11-15
5 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:11
Respond to live performance issues. Plan how to address performance problems by incorporating clear lines of communication and responsibilities. When a problematic situation occurs, use what you learn to identify preventive measures and incorporate them in your workload. Implement methods to return to normal operations faster when similar situations occur.
This guide describes the best practices for responding to live performance issues. Live performance issues refer to real-time challenges and bottlenecks that can hinder the optimal functioning of a workload. Addressing these issues promptly not only facilitates the immediate detection and rectification of performance hiccups but also ensures that the workload consistently meets its performance benchmarks. Failing to address them can lead to complications, including slowdowns, crashes, and system unresponsiveness, and degrade the user experience. They can also prevent users from completing their tasks efficiently, and, in turn, tarnish the reputation of the organization.
Definitions
Term
Definition
Data correlation
Aligning logs, metrics, and events from various parts of your workload to pinpoint underlying causes.
Root cause analysis
A process for identifying the underlying factors that are responsible for a problem.
Self-healing
The ability to automatically repair issues without human intervention.
Self-prevention
Implementations within a workload to prevent potential issues and failures.
Key design strategies
When you experience a live performance issue, you need to be prepared with the right data and a plan to respond to the issue. This plan should include clear lines of communication and responsibilities. The primary objective is to implement solutions that facilitate a quick return to regular operations and provide insights from the incident. Integrating preventive measures into your workflow is a pivotal strategy. The goal is to either prevent the same issue from happening again or lessen its effects on performance if it isn't preventable.
Prepare for issues
The ideal response to live-site performance issues is precise and fast. Precision and speed in performance remediation require preparation. To effectively respond to live performance issues, it's crucial to monitor key performance metrics, identify the root cause of the issues, and implement appropriate solutions or optimizations. To take these steps, you might need to analyze workload logs, conduct performance testing, optimize code or configurations, and scale resources. The following examples outline a few critical areas of preparation:
Have accurate architecture diagrams.
Your architecture diagrams should include all components and show how they interact. Visual representation can help identify bottlenecks and single points of failure that can lead to performance degradation or unavailability. Ideally, you catch and remove these issues before they cause problems, but having an up-to-date diagram can help you pinpoint issues in high-stress moments.
Check data access.
Data and logs from monitoring processes are critical for responding to performance issues in real time and conducting root cause analyses. But it's important to maintain the integrity and confidentiality of the data. Responding to live-site performance issues often requires access to underlying data that might not be normally accessible. You need to ensure that personnel have access to the data that they need when issues arise. But you should only grant time-restricted, least-privilege access, and you should limit that access to authorized personnel.
Set automatic alerts.
Alerts can help you identify and address issues as soon as they occur. Alerts should generate notifications when workload performance deviates from performance baselines. Over time, you should tweak alert configurations to avoid generating too many or too few notifications. The monitoring solutions that you use need to collect enough data to generate alerts. These alerts should align with performance targets and established baselines. You should avoid generating alerts on issues that are relevant to your goals. Examples of alerts include degradations in CPU usage, memory, response times, and database performance.
Create a triage plan
Creating a triage plan involves devising a structured approach to identify, escalate, analyze, prioritize, and communicate live-site performance issues. A triage plan is a strategy for responding to live performance issues. It ensures that performance disruptions are addressed promptly and effectively, with clear roles and procedures. Most performance issues don't merit disaster recovery protocols, but they can affect workload functionality enough to require triage planning. A well-documented triage plan ensures all team members are aligned and can act swiftly, minimizing the impact on users and workloads. A triage plan should include the following components:
Identification and monitoring
: Implement a system to identify and monitor performance issues in real time. You should have a list of the contact information of people who are capable of making decisions or escalating issues to higher levels. The plan should also identify roles and responsibilities. It needs to document which accounts gain access to protected information and for how long.
Escalation process
: Define a clear escalation process to ensure that performance issues are escalated to the appropriate teams or individuals in a timely manner. The process definition should include contact information and guidelines for escalating issues.
Root cause analysis
: Develop a process for conducting a root cause analysis to identify the underlying cause of each performance issue. The process should involve analyzing logs and performance metrics and conducting diagnostic tests to pinpoint the source of each problem.
Prioritization
: Establish a prioritization framework to determine the severity of performance issues and prioritize them based on their effect on the workload and users.
Communication
: Create a communication plan to keep stakeholders informed about the status of performance issues and the progress of their resolution. Consider regular updates, status reports, and clear communication channels.
Documentation
: Document the triage plan, including all its steps, processes, and best practices. This documentation should be easily accessible to the team members who are involved in responding to performance issues.
Develop methods to identify and resolve issues
Resolving live performance issues involves identifying and addressing any factors that can cause performance degradation or inefficiencies in a live workload. Data that you collect during monitoring is invaluable when you investigate and resolve performance-related incidents. This data provides a historical record of performance metrics. When you have monitoring data available, you can analyze root causes and identify contributing factors. You should use all relevant monitoring data to understand and fix each performance issue.
Use root cause analysis
Root cause analysis requires hypothesis testing. After you review monitoring data, you should list potential causes of the performance issue and test them. To conduct a root cause analysis on a live performance issue, you can follow these steps:
Gather information.
Collect as much information as possible about the performance issue. Examples include error messages, logs, performance metrics, and any other relevant data.
Define the problem.
Clearly define the problem by identifying the symptoms and the effect that the problem has on the workload or users.
Investigate potential causes.
Narrow down the scope of the analysis by identifying the specific component or area of the workload where the performance issue is occurring. Identify potential causes of the performance issue based on the gathered information. This process can involve analyzing code, configuration settings, infrastructure, or external dependencies.
Correlate data.
Dive deeper into the collected data to identify patterns, anomalies, or correlations that might contribute to the performance issue. Data correlation is key to identifying performance issues and causes. It can involve reviewing logs, analyzing performance metrics, and conducting tests.
Test hypotheses.
Formulate hypotheses based on the potential causes that you identify. Conduct tests to validate or refute your hypotheses. You should use a test environment to see whether you can replicate the error.
Implement solutions.
Once you identify a root cause, develop and implement solutions to address the performance issue.
Monitor and validate.
After you implement the solutions, continuously monitor the workload to ensure that the performance issue is resolved. Validate the effectiveness of the solutions by monitoring performance metrics and user feedback.
Tradeoff
: The steps of a root cause analysis, such as identifying possible causes, testing hypotheses, and documenting the analysis, can be time consuming. To correlate performance issues, you also need to collect and store data. The required time and infrastructure can add significant work to the operations teams and cost to the workload.
Risk
: If you perform a root cause analysis without proper security guardrails, there's a risk that you expose sensitive information when you provide access to logs and data.
Engage vendor support
Vendor support can be an essential step when you deal with ongoing performance issues. Vendors have the expertise, tools, resources, and experience to help fix issues with their products. Your support agreement with your supplier determines the level of support a vendor provides.
It's often best to work in parallel with vendors. You should create a plan to have some team members collaborate with vendor support while others continue to triage and fix performance issues. Vendor support teams can also make suggestions on how to help prevent and automate responses to similar events.
You need to have contact information available for your personnel. Vendors might also need access to data to effectively engage in problem-solving. You need to have a plan in place for authenticating and authorizing external or guest accounts to access monitoring data.
Learn from findings
After you fix a live-site performance issue, you need to review what happened. The goal is to learn from performance issues, not just identify problems. The best way to learn is through documentation. Document each issue and explain how to fix it. If a vendor helped, work with the vendor to enhance your documentation, train your team, and modify your workload accordingly.
The documentation should indicate how to prevent each problem from happening again. One way to avoid recurring problems is to introduce automation to respond to common issues. Automation should add self-healing and self-prevention qualities to a workload. Along with the automation, you can create refined alerts that help you respond early to performance issue indicators.
Azure facilitation
Developing methods to identify and resolve issues
: Azure provides several tools to help you respond to live performance issues:
Azure Monitor is a comprehensive monitoring solution that provides insights into the performance and health of your applications and infrastructure. Monitor offers features such as metrics, logs, alerts, and dashboards to help you monitor and diagnose performance issues.
Application Insights is an application performance management (APM) service that helps developers and DevOps professionals monitor live applications. It automatically detects performance anomalies, collects application-level logs and events, and provides analytics tools to diagnose issues.
Log Analytics is a service that collects and analyzes log data from various sources, including applications, virtual machines, and Azure resources. When you use Log Analytics, you can query and analyze log data to gain insights into the performance and behavior of your applications.
Related links
Recommendations for self-healing and self-preservation
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for optimizing scaling and partitioning - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for optimizing scaling and partitioning
Article
2023-11-15
3 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:05
Optimize scaling and partitioning. Incorporate reliable and controlled scaling and partitioning. The scale unit design of the workload is the basis of the scaling and partitioning strategy.
This guide describes the recommendations for scaling and partitioning a workload. Scaling is the ability to increase or decrease the resources allocated to a workload based on demand. Partitioning involves dividing the workload into smaller, manageable units to distribute data and processing across multiple resources. A workload that doesn't scale or partition might experience poor performance in high-demand periods and underutilized capacity in low-demand periods.
Definitions
Term
Definition
Autoscale
A feature that automatically adjusts the capacity limits of a service based on predefined configurations, allowing it to scale up or down as needed.
Capacity
The upper limit or maximum capacity of a given service or feature.
Client affinity (session affinity)
The intentional routing of requests from a single client to a single server instance to help ensure consistent session management.
Consistency (distributed database)
The uniformity of data across multiple nodes in a distributed database, ensuring that all replicas have the same data at a given point in time.
Consistency (relational database)
The property of a transaction bringing a database from one valid state to another, maintaining data integrity.
Consistency level
A configuration that defines how and when data is replicated in a distributed database system, determining the tradeoff between consistency and performance.
Data locking
A mechanism used to prevent simultaneous updates to the same data.
Horizontal scaling
A scaling approach that adds instances of a given type of resource.
Optimistic concurrency
An approach for updating databases that uses snapshots to make updates instead of traditional locking mechanisms.
Partitioning
The process of physically dividing data into separate data stores.
Scalability
The ability of a workload to dynamically change its capacity limits to accommodate varying levels of demand.
Scale unit
A group of resources that scale proportionately together.
State affinity
The storage of client session data on a single server so that the same server handles subsequent requests from the same client.
Vertical scaling
A scaling approach that adds compute capacity to existing resources.
Key design strategies
Both scaling and partitioning contribute to performance efficiency by ensuring that resources are used effectively and the workload can handle varying loads. These practices are especially important in cloud environments where applications need to be flexible and adaptable to changing demands. Scaling ensures you can expand workload capacity to meet increasing demands. Partitioning allows you to divide tasks or data efficiently to handle these growing needs. The foundation of both these processes is the scale unit design of the workload. It dictates how your workload should grow and distribute tasks. By incorporating a reliable and controlled approach to scaling and partitioning, you can sidestep potential workload inefficiencies.
Optimize scaling
Optimize scaling is the process of adjusting the number of servers, instances, or resources to meet the fluctuating demands of a workload. It ensures that the workload can handle increased traffic or demands without experiencing performance degradation or downtime.
Choose a scaling strategy
Choosing a scaling strategy involves deciding between vertical or horizontal methods to enhance the capacity of a workload based on its specific requirements. Selecting the right strategy ensures that resources are adjusted efficiently to meet workload demands without overuse or waste. To choose the right scaling strategy, you need to understand the uses cases for vertical and horizontal scaling and how they meet the needs of your workload.
Understand vertical scaling.
Using vertical scaling, you can increase the capacity of a single resource, such as upgrading to a larger server or instance size. Vertical scaling is useful when the workload can benefit from increased processing power, memory, or other resources within a single instance. Vertical scaling is appropriate for workloads that aren't easily divided into smaller parts or when the application architecture doesn't support horizontal scaling.
Understand horizontal scaling.
Using horizontal scaling, you can add more instances or resources to distribute the workload across multiple servers. Horizontal scaling offers benefits such as improved resiliency, increased capacity, and the ability to handle increased traffic or workload demands. It's effective for cloud-native applications designed to run on multiple nodes. Horizontal scaling is appropriate for workloads that can be divided into smaller parts that run independently.
Understand the workload.
The suitability of vertical or horizontal scaling depends on the specific characteristics and requirements of the workload. Regular performance monitoring and testing in the following areas can help optimize the scaling strategy over time:
Requirements
: Understand the specific requirements of the workload by considering factors such as resource demands, scalability needs, and the limitations of the workload.
Scale units
: Create a scale unit design for components expected to be scaled together. For example, 100 virtual machines might require two queues and three storage accounts to handle the extra workload. The scale unit would be 100 virtual machines, two queues, and three storage accounts. You should independently scale all the components that experience capacity-use fluctuation.
Architecture
: Assess the design of the application architecture. Some applications might be inherently designed to scale horizontally, with stateless components that can be easily distributed across multiple instances. Other applications might have stateful components or dependencies that make vertical scaling more appropriate. Evaluate the scalability and elasticity requirements of the workload.
Design infrastructure to scale
Designing infrastructure to scale is the process of creating an architecture that can handle increasing demands and workload by adding or adjusting resources as needed. It involves planning and implementing solutions that can scale horizontally or vertically to accommodate growth. Strategies include avoiding singletons that can become bottlenecks and decoupling application components to ensure independent scalability. When you design a workload to be scalable, it can effectively distribute the workload across multiple resources, which prevents bottlenecks and maximizes resource utilization.
Avoid singletons.
You should avoid the use of a single, centralized resource for the entire workload. Instead, distribute your workload across multiple resources for better scalability, fault tolerance, and performance. Explore some specific examples and design considerations to avoid singletons in workload resources:
Queue-based load leveling
: Instead of relying on a single queue to process messages, consider partitioning the workload across multiple queues to distribute the processing load. It provides better scalability and parallel processing.
Data processing
: Singleton patterns often appear in data processing scenarios where the processing doesn't fan out. Break long-running tasks into smaller tasks that can scale better to distribute the workload across multiple resources and take advantage of parallelism.
Design patterns
: Design patterns such as
Fan-out/Fan-in
or
Pipes and Filters
can help avoid singletons in workflows. These patterns enable the distribution of processing tasks across multiple resources and promote scalability and flexibility.
Decouple components.
Decoupling application components is an important aspect of designing for scalability. It involves breaking down the application into smaller, independent components that can operate and scale independently based on specific workload requirements. For example, if one component requires more resources due to increased demand, you can scale that component without affecting the others. This flexibility ensures efficient resource allocation and prevents bottlenecks. By decoupling components, you can isolate failures and minimize the effect on the overall application. If one component fails, the other components can continue to function independently.
Decoupled components are easier to maintain and update. Changes or updates to one component can be made without affecting the others because they're independent. Follow these guidelines to decouple application components for scalability:
Separation of concerns
: Identify the responsibilities and functionalities of your application. Divide the responsibilities into separate components based on their specific tasks. For example, you might have separate components for user authentication, data processing, and UI.
Loose coupling
: Design the components to communicate with each other through well-defined interfaces and protocols. This design reduces dependencies between components and allows for easier replacement or scaling of individual components.
Asynchronous communication
: Use asynchronous communication patterns such as message queues or event-driven architectures to decouple components further. These patterns allow components to process tasks independently at their own pace, improving overall scalability.
Microservices
: Consider implementing microservices, which are small, independent services that focus on specific business functionalities. Each microservice can be developed, deployed, and scaled independently, providing greater flexibility and scalability.
Design application to scale
As you scale a workload, you should design the application to distribute the load. Just because you can add more replicas at the infrastructure level doesn't mean your application can use the replicas. Designing an application to scale is about structuring an application so it can handle increased demands by distributing its workload across resources. Avoid solutions that require client affinity, data locking, or state affinity for a single instance if possible. You want to route a client or process to a resource that has available capacity. To design for application scalability, consider the following strategies:
Eliminate server-side session state.
You should design applications to be stateless where possible. For stateful applications, you should use a state store that's external to your server. Externalizing session state is the practice of storing session data outside of the application server or container. You can externalize session state to distribute session data across multiple servers or services, enabling seamless session management in a distributed environment. Consider the following when externalizing session state:
Evaluate your session requirements.
Understand the session data that needs to be stored and managed. Consider session attributes, session timeouts, and any specific requirements for session replication or persistence. Determine the size of your session state and the frequency of read and write operations.
Choose a solution.
Select a storage solution that aligns with your performance and scalability needs. Options include using a distributed cache, a database, or a session state service. Consider factors such as data consistency, latency, and scalability when making your choice.
Set up your application.
Update your application to use the chosen session state storage solution. You might need to change your application's configuration files or code to connect to the external storage service.
Update your logic.
Change your application's session management logic to store and retrieve session data from the external storage solution. You might need to use APIs or libraries provided by the storage solution to manage session state.
Eliminate client affinity.
Client affinity is also known as session affinity or sticky sessions. When you eliminate client affinity, you distribute client requests evenly across multiple replicas or servers, without routing all requests from a client to the same replica. This configuration can improve the scalability and performance of applications by allowing any available replica to process the requests.
Review your load balancing algorithm.
A load balancing algorithm can cause unintentional and artificial client pinning where too many requests are sent to one back-end instance. Pinning can happen if the algorithm is set up to always send requests from the same user to the same instance. It can also happen if the requests are too similar to each other.
Eliminate data locking.
Data locking ensures consistency but has performance disadvantages. It can cause lock escalations and negatively affect concurrency, latency, and availability. To eliminate data locking, you should implement
optimistic concurrency
. Nonrelational databases should use
optimistic concurrency control
and have the right
consistency level
. Your data partitioning strategy should also support your concurrency needs.
Use dynamic service discovery.
Dynamic service discovery is the process of automatically detecting and registering services in a distributed system. It allows clients to discover available services without being tightly coupled to specific instances. Clients shouldn't be able to take a direct dependency on a specific instance in the workload. To avoid these dependencies, you should use a proxy to distribute and redistribute client connections. The proxy acts as an intermediary between clients and services, providing a layer of abstraction that allows services to be added or removed without affecting clients.
Use background tasks.
When an application is scaled, it can handle an increasing workload or a higher number of concurrent requests. Offloading intensive tasks as background tasks allows the main application to handle user requests without resource-intensive operations overwhelming it. Follow these steps to offload tasks as background tasks:
Find the CPU-intensive and I/O-intensive tasks in your application that you can offload. These tasks typically involve heavy computations or interactions with external resources such as databases or network operations.
Design your application to support background tasks. Decouple the intensive tasks from the main application logic and provide a mechanism to start and manage background tasks.
Implement background task processing with appropriate technologies or frameworks. Include features provided by your programming language or platform, such as asynchronous programming, threading, or task queues. Contain intensive operations in separate tasks or threads, these tasks can be run concurrently or scheduled to run at specific intervals.
Distribute background tasks if there are many of them, or if the tasks require substantial time or resources. For one possible solution, see the
Competing Consumers pattern
.
Configure scaling
Configuring scaling is the process of setting up and adjusting parameters to dynamically allocate resources based on workload demands. It encompasses strategies such as using autoscaling features, understanding service scaling boundaries, and implementing meaningful load metrics. Proper configuration ensures that an application can respond to varying demands while maximizing efficiency. When you configure scaling, consider the following strategies:
Use services with autoscaling.
The autoscale feature automatically scales infrastructure to meet demand. Use platform as a service (PaaS) offerings with built-in autoscale features. The ease of scaling on PaaS is a major advantage. For example, scaling out virtual machines requires a separate load balancer, client-request handling, and externally stored state. PaaS offerings handle most of these tasks.
Constrain autoscaling.
Set automatic scaling limits to minimize over-scaling that could result in unnecessary costs. Sometimes you can't set scaling limits. In these cases, you should set alerts to notify you when the component reaches the maximum scale limit and over-scaled.
Understand service scaling boundaries.
When you understand service scaling limits, increments, and restrictions, you can make informed decisions when selecting a service. Scaling boundaries determine whether or not your chosen service can handle the expected workload, scale efficiently, and meet the performance requirements of your application. Scaling boundaries to consider include:
Scaling limits
: Scaling limits are the maximum capacity that a location or service can handle. It's important to know these limits to help ensure that the service can accommodate the expected workload and handle peak usage without performance degradation. Every resource has an upper scale limit. If you need to go beyond scale limits, you should partition your workload.
Scaling increments
: Services scale at defined increments. For example, compute services might scale by instances and pods while databases might scale by instances, transaction units, and virtual cores. It's important to understand these increments to optimize resource allocation and prevent resource flapping.
Scaling restrictions
: Some services allow you to scale up or out but limit your ability to automatically reverse scaling. You're forced to scale in manually, or you might have to redeploy a new resource. These limitations are often to protect the workload. Scaling down or scaling in can have implications on the availability and performance of the workload. A service might enforce certain limitations or constraints to help ensure that the workload has sufficient resources to operate effectively. These limitations can affect data consistency and synchronization, especially in distributed systems. The service might have mechanisms in place to handle data replication and consistency during scaling up or out but might not provide the same level of support for scaling down or in.
Use meaningful load metrics.
Scaling should use meaningful load metrics as scaling triggers. Meaningful load metrics include simple metrics, like CPU or memory. They also include more advanced metrics, such as queue depth, SQL queries, custom metrics queries, and HTTP queue length. Consider using a combination of simple and advanced load metrics as your scaling trigger.
Use a buffer.
A buffer is unused capacity that can be used to handle spikes in demand. A well-designed workload plans for unexpected spikes in workload. You should add a buffer to handle spikes for horizontal and vertical scaling.
Prevent flapping.
Flapping is a looping condition that occurs when one scale event triggers an opposite scale event, creating a continuous back-and-forth scaling action. For example, if scaling in reduces the number of instances, it might cause the CPU usage to rise in the remaining instances, triggering a scale-out event. The scale-out event, in turn, causes the CPU usage to drop, repeating the process.
It's important to choose an adequate margin between the scale-out and scale-in thresholds to avoid flapping. You can prevent frequent and unnecessary scale-in and scale-out actions by setting thresholds that provide a significant difference in CPU usage.
Use Deployment Stamps.
There are techniques that make it easier to scale a workload. You can use the
Deployment Stamps
pattern to easily scale a workload by adding one or more scale units.
Risk
: While scaling helps optimize costs by adjusting capacity to meet demand, it can result in overall increased cost during long periods of high demand.
Test scaling
Testing scaling involves simulating various workload scenarios in a controlled environment to evaluate how a workload responds to different levels of demand. It helps ensure the workload scales efficiently, maximizing performance efficiency during varied loads.
You need to ensure that your workload scales efficiently under real-world conditions. It's essential to perform load and stress tests in an environment that mirrors your production setup. These tests, conducted in nonproduction environments, enable you to evaluate both vertical and horizontal scaling strategies and determine which one optimizes performance most effectively. Here's a recommended approach to testing scaling:
Define workload scenarios.
Identify the key workload scenarios that you need to test, such as increasing user traffic, concurrent requests, data volume, or resource use.
Use production-like test environment.
Create a separate testing environment that closely resembles the production environment in terms of infrastructure, configuration, and data.
Set performance metrics.
Define the performance metrics to measure, such as response time, throughput, CPU and memory utilization, and error rates.
Develop test cases.
Develop test cases that simulate different workload scenarios, gradually increasing the load to assess the performance at various levels.
Execute and monitor tests.
Run the tests using the defined test cases and collect performance data at each load level. Monitor workload behavior, resource consumption, and performance degradation.
Analyze and optimize scaling.
Analyze the test results to identify performance bottlenecks, scalability limitations, or areas for improvement. Optimize the configuration, infrastructure, or code to enhance scalability and performance. It takes time for scaling to complete, so test the effects of scaling delays.
Address dependencies.
Find potential dependency issues. Scaling or partitioning in one area of a workload might cause performance issues on a dependency. The stateful parts of a workload, such as databases, are the most common cause of dependency performance issues. Databases require careful design to scale horizontally. You should consider measures, such as
optimistic concurrency
or data partitioning, to enable more throughput to the database.
Retest after adjustments.
Repeat the scalability tests after implementing optimizations to validate the improvements and help ensure the workload can handle the expected workloads efficiently.
Tradeoff
: Consider the budget constraints and cost-efficiency goals of your workload. Vertical scaling might involve higher costs due to the need for larger and more powerful resources. Horizontal scaling offers cost savings by using smaller instances that can be added or removed based on demand.
Partition workload
Partitioning is the process of dividing a large dataset or workload into smaller, more manageable parts called partitions. Each partition contains a subset of the data or workload and is typically stored or processed separately. Partitioning enables parallel processing and reduces contention. Dividing the workload into smaller units allows the application to process each unit independently. The result is better use of resources and faster processing times. Partitioning also helps distribute the data across multiple storage devices, reducing the load on individual devices and improving overall performance.
Understand partitioning
The specific partitioning approach you use depends on the type of data or workload you have and the technology you're using. Some common strategies for partitioning include:
Horizontal partitioning
: In this approach, the dataset or workload is divided based on specific criteria, such as ranges of values or specific attributes. Each partition contains a subset of the data that meets the defined criteria.
Vertical partitioning
: In this approach, the dataset or workload is divided based on specific attributes or columns. Each partition contains a subset of the columns or attributes, allowing for more efficient access to the required data.
Functional partitioning
: In this approach, the data or workload is divided based on the specific functions or operations that need to be performed. Each partition contains the data or components necessary for a specific function, enabling optimized processing and performance.
Plan partitioning
It's important to consider factors such as data distribution, query patterns, data growth, and workload requirements when partitioning. Proper planning and design are essential to help ensure the effectiveness of partitioning and maximize performance efficiency. If you address partitioning as an afterthought, it's more challenging because you already have a live workload to maintain. You might need to change data access logic, distribute large quantities of data across partitions, and support continued usage during data distribution.
Implement partitioning
It's important to analyze the characteristics of your data, access patterns, concurrency requirements, and scalability goals when deciding which type of partitioning to use. Each type of partitioning has its own advantages and considerations. Here are some factors to consider for each type of partitioning:
Horizontal partitioning
is appropriate when you want to distribute the data across multiple resources or servers for better scalability and performance. It's effective when the workload can be parallelized and processed independently on each partition. Consider horizontal partitioning when multiple users or processes need to be able to access or update the dataset concurrently.
Vertical partitioning
is appropriate when certain attributes or columns are frequently accessed, while others are accessed less frequently. Vertical partitioning allows for efficient access to the required data by minimizing unnecessary data retrieval.
Functional partitioning
is appropriate when different functions require different subsets of the data and can be processed independently. Functional partitioning can optimize performance by allowing each partition to focus specific operations.
Test and optimize partitioning
Test the partitioning scheme to verify the effectiveness and efficiency of the strategy so you can make adjustments to improve performance. Measure factors such as response time, throughput, and scalability. Compare the results against performance goals and identify any bottlenecks or issues. Based on the analysis, identify potential optimization opportunities. You might need to redistribute data across partitions, adjust partition sizes, or change the partitioning criteria.
Tradeoff
: Partitioning adds complexity to the design and development of a workload. Partitioning requires conversations and planning between developers and database administrators.
Risk
: Partitioning introduces some potential problems that need to be considered and addressed, including:
Data skew
: Partitioning can lead to data skew, where certain partitions receive a disproportionate amount of data or workload compared to others. Data skew can result in performance imbalances and increased contention on specific partitions.
Query performance
: Poorly designed partitioning schemes can negatively affect query performance. If queries need to access data across multiple partitions, it might require extra coordination and communication between partitions, leading to increased latency.
Azure facilitation
Optimizing scaling.
Azure has the infrastructure capacity to support vertical and horizontal scaling. Azure services have different performance tiers known as SKUs. SKUs allow you to scale vertically. Many of Azure's resources support automatic scaling or other in-place scale options. Some resources support advanced metrics or custom input to support fine-tuning scaling behavior. Most scaling implementations in Azure can set limits and support the necessary observability to be alerted to change.
Azure Monitor
allows you to monitor various metrics and conditions in your applications and infrastructure. You can use Monitor to trigger automated scaling actions based on predefined rules. For example, in Azure Kubernetes Service (AKS), you can use Monitor to enable horizontal pod automatic scaling (HPA) and cluster automatic scaling. Using Monitor's monitoring and alerting capabilities, you can effectively facilitate scaling in Azure and help ensure that your applications and infrastructure can dynamically adjust to meet demand.
You can also build custom automatic scaling in Azure. You can use alerts in Monitor for resources that don't have an autoscale feature. These alerts can be set up to be query-based or metric-based and can perform actions using
Azure Automation
. Automation provides a platform for hosting and running PowerShell and Python code across Azure, the cloud, and on-premises environments. It offers features such as deploying runbooks on demand or on a schedule, run history and logging, integrated secrets store, and source control integration.
Designing application to scale
: Here are some ways Azure facilitates application scaling design;
Eliminating data locking
: In Azure SQL Database, you can enable
optimized locking
to improve performance on databases that require strict consistency.
Using background tasks
: Azure offer services and guidance for implementing background jobs. For more information, see
Background jobs
.
Implementing load balancing
: Azure provides load balancers that don't require client affinity. These load balancers include
Azure Front Door
,
Azure Application Gateway
, and
Azure Load Balancer
.
Partitioning a workload
: Azure offers various partitioning strategies for different data stores. These strategies help improve performance and scalability by distributing the data across multiple partitions. For more information, see
Data partition strategies
.
Related links
Why partition data?
Best practices for automatic scaling
Overview of the autoscale feature in Azure
Horizontal, vertical, and functional data partitioning
Application design considerations
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Recommendations for selecting the right services - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Recommendations for selecting the right services
Article
2023-11-15
6 contributors
Feedback
In this article
Applies to this Azure Well-Architected Framework Performance Efficiency checklist recommendation:
PE:03
Select the right services. The services, infrastructure, and tier selections must support your ability to reach the workload's performance targets and accommodate expected capacity changes. The selections should also weigh the benefits of using platform features or building a custom implementation.
This guide describes the recommendations for selecting appropriate services for your workload. The following recommendations help you choose services that best meet the requirements and demands of your workload. When you use services that are designed to handle your workload's requirements, you can ensure that your workload meets your performance targets. If you choose inappropriate services for your workload, the services might not be capable of handling your workload's demands. Insufficient services can lead to slow response times, bottlenecks, or workload failures.
Definitions
Term
Definition
Availability zone
A separated group of datacenters within a region
. Each availability zone is independent of the others, with its own power, cooling, and networking infrastructure.
Many regions support availability zones
.
Compute service
A service that provides the infrastructure that you need to run an application.
Database service
A service that provides relational and nonrelational databases for your application.
Infrastructure
The physical components of cloud computing, and the geographic location of the components.
Infrastructure as a service (IaaS)
A service in which the customer is responsible for the operating system, identity, applications, and networking.
Platform as a service (PaaS)
A service in which the cloud service provider is responsible for the operating system. The cloud service provider shares responsibility with the customer for managing identity, applications, and networking.
Region
A geographic perimeter that contains a set of datacenters.
Resource
A single entity or component that you can create, configure, and utilize within a cloud service provider.
Service
A product or offering from a cloud service provider.
Stock keeping unit (SKU)
A service tier for an Azure service.
Storage service
A service that provides storage for objects, blocks, and files.
Key design strategies
The services you choose should align with your workload's performance targets and be adaptable to future capacity needs. As the workload expands or evolves, the services you use should match your performance standards without requiring major adjustments. Consider the balance between platform features and custom implementations. Platform features provide immediate solutions, but custom-built options offer precise tailoring. Your service selections should be both forward-thinking and tailored to your specific needs, taking into account the trade-offs between convenience and customization.
Understand workload requirements
Understanding workload requirements refers to grasping the technical and functional demands of a workload. This analysis helps determine the resources, storage, compute, network, and other specifications needed to run the workload. Aligning services with the specific needs of a workload helps prevent overprovisioning or underutilizing resources.
Evaluate the needs and characteristics of your workload to determine the requirements, and align your workload requirements to your performance targets at every tier. You must account for constraints or dependencies. When you understand your workload requirements, you can make informed decisions. You can determine the right infrastructure and implement strategies to handle peak loads or variations in demand.
Meet performance targets.
Select services that enable you to meet the performance targets for your workload. Ensure that a service can support the performance needs and that you can monitor its performance. Collect performance data for critical components.
Consider organizational restrictions.
Be familiar with restrictions that your organization might have on services that you deploy. Consider these restrictions when you design your solution.
Consider compliance and security requirements.
Compliance and security requirements can affect services and configurations that you select. Ensure that a service you choose meets the requirements that are related to storage, encryption, access controls, audit logs, and data locations.
Consider team skills.
Your team builds and maintains workloads. Different services require different skills. Choose services that your team knows how to use, or commit to training them before you choose a service. Ensure that team members possess the expertise and knowledge to effectively use services and to optimize their performance.
Tradeoff
: Specialized services offer specific functionalities but might limit customization. Flexible resources require more management and configuration compared to specialized services. Managed services offer ease of management, but you might have less control over the underlying infrastructure compared to self-managed resources.
Understand services
Understanding services is about knowing the capabilities, limits, and functionalities of a vendor's tools and offerings. An understanding of services helps you use built-in features, reducing the need for complex custom solutions and improving performance efficiency.
Consider various factors and gain a comprehensive understanding of a service before you choose it. Research and assess services and tools that the provider offers. Determine which services and tools best align with your workload requirements. Consider factors like managed services, serverless options, and specialized services.
Understand service limits
Service limits are the predefined thresholds or boundaries that service providers set. Service limits define the maximum usage of resources or capabilities within that service. When you're familiar with service limits, you can avoid issues such as resource contention, performance degradation, or unexpected service interruptions. You can plan and scale the infrastructure appropriately. Your planning takes into account factors such as data volume, processing capacity, and data residency requirements.
Prefer platform features
Preferring platform features is about using built-in functionalities provided by a provider to handle specific tasks without custom code. Vendors design platform features to handle specific tasks efficiently at scale, and they regularly maintain these features. Platform features allow you to better take advantage of cloud infrastructure capabilities. Choose services that allow you to offload functionality to the platform instead of writing and maintaining your own custom code. In many cases, platform-as-a-service (PaaS) solutions provide better performance efficiency than custom code. Custom code adds complexity and makes the workload prone to performance issues. Only develop custom code when service features aren't sufficient.
Tradeoff
: The best service for your workload might be a technology that your team isn't skilled at, can't afford, or it might require extra security layers. For example, a public load balancer might fit your performance needs. But if you don't have a web application firewall, you might have to deploy a firewall to secure the workload.
Evaluate infrastructure requirements
The performance efficiency of resources is tied to the infrastructure they reside on. It makes the selection of the right infrastructure critical to service performance efficiency. Evaluating infrastructure requirements means to identify the geographical region and availability zones best suited to support your workload. Key considerations in this decision-making include:
Understand regions and availability zones.
Every region corresponds to a distinct geographic location. Availability zones represent individual physical datacenters within a given region.
Single-region vs. multiple-region deployment model
. A single-region deployment model deploys all resources in a single region. A multiple-region deployment model deploys resources across multiple regions. A multiple-region deployment can reduce latency to end users and mitigate capacity constraints. However, it can also increase the cost and complexity of the workload. Choose the deployment model that best suits your workload needs.
Understand available features.
Different regions have different available features, such as the number of services and availability zones. Understand the features that are available in a region before you select it. Ensure that a region meets your workload performance needs.
Consider latency.
Latency, the time data takes to travel from source to destination, increases the further services are from each other. Services communicating across regions or availability zones can face increased latency. Identifying services that frequently communicate and positioning them within the same region is recommended. Additionally, selecting a region proximate to your primary user base can minimize latency, offering a better user experience.
Understand datacenter mapping.
Availability zones might not map consistently to the same datacenters across different subscriptions. For instance, 'Zone 1' in 'Subscription A' might be different from 'Zone 1' in 'Subscription B'. When operating with multiple subscriptions, you should know these mappings to select zones that bolster performance optimally.
Evaluate networking requirements
Assess your network needs to determine the appropriate workload services and configurations. Ensure that the network can support your workload. To evaluate networking requirements, consider:
Understand network traffic.
Assess the expected network traffic for the workload. Understand the data transfer needs and the frequency of network requests.
Understand bandwidth requirements.
Determine the bandwidth requirements for the workload. Consider the amount of data transmitted and received over the network.
Understand network Latency.
Evaluate the desired latency for the workload. Use private virtual networks and backbone networks instead of traversing the public internet. This technique decreases the latency of the workload.
Understand throughput.
Consider the required throughput for the workload. Throughput refers to the amount of data that can be transmitted over a network in a given time. Configure the network routing options to take advantage of network throughput benefits.
Tradeoff
: Private virtual networking limits public access and makes it difficult to deploy and manage resources.
Evaluate compute requirements
Evaluating compute requirements involves assessing the specific compute needs of a workload, including factors such as instance type, scalability, and containerization. Different compute services have varying capabilities and characteristics that can affect the performance of your workload. Select the optimal compute service to ensure that your workload runs efficiently. Consider the following strategies:
Understand instance types.
Different instance types are optimized for different workloads, such as CPU-optimized, memory-optimized, and GPU instances. Choose the instance type that aligns with your needs.
Consider automatic scaling.
If your workload has variable demand, consider a compute service with an autoscale feature that can automatically adjust the compute capacity based on demand. Automatically scaling helps ensure that you have enough resources during peak times and prevents overprovisioning during low demand periods.
Consider containerization.
Containers provide performance advantages compared to a noncontainerized workload. Consider using containerization if it suits your architectural needs. Containers improve compute performance through isolation, resource efficiency, fast startup time, and portability.
When you use containers, consider design factors such as containerizing all application components. Use Linux-based container runtimes for lightweight images. Give containers short lifecycles to make them immutable and replaceable. Gather relevant logs and metrics from containers, container hosts, and the underlying cluster. Use this data to monitor and analyze performance. Containers are just one component of an overall architecture. Choose an appropriate container orchestrator, like Kubernetes, to further enhance performance and scalability.
Container benefit
Description
Isolation
Containers provide isolated environments for applications. Containers ensure that application resources don't interfere with each other. This isolation ensures compute resources assigned to a container are dedicated to running a specific application, resulting in better performance.
Resource efficiency
Containers are lightweight and share the host operating system's kernel, which allows for efficient resource utilization. Multiple containers can run on the same virtualized infrastructure, which maximizes the use of compute resources.
Fast startup time
Container images are prebuilt and are quickly started when needed. This fast startup time enables rapid scalability. It allows applications to scale up or down based on demand and avoid performance bottlenecks.
Portability
Containers encapsulate all the required dependencies and libraries within the image. With containers, it's easier to move applications across different operating systems or environments. This portability enables flexibility in deploying applications and allows for easy migration between cloud providers or on-premises environments.
Choose the appropriate tier.
Within each compute service, you can set the compute capacity, select features, and enable capabilities. Based on your performance targets, choose the appropriate service tier for your compute service.
Determine the instance count.
Determine the minimum instance count that your workload requires. Some workloads, even at minimal load, might require more than one instance of a compute resource. Set the minimum instance count accordingly.
Evaluate load balancing requirements
Load balancing ensures that network traffic is distributed evenly and prevents any single server from being overwhelmed with requests. Load balancing helps prevent bottlenecks and reduce response times. Evaluate the different load balancing services that your cloud provider offers. Review the cloud provider's documentation and comparison tools to understand the features. Select the most suitable service for your workload. To select a load balancing service, consider:
Understand traffic type
: Determine whether the load balancing service needs to handle web traffic, like HTTP and HTTPS, or other protocols, such as Transmission Control Protocol (TCP) or User Datagram Protocol (UDP).
Know global or regional routing
: Determine whether your workload requires load balancing within a specific region or across multiple regions.
Know service-level objectives (SLOs)
: Consider the service-level agreement (SLA). Different load balancing services offer different levels of performance.
Understand features
: Consider load balancing services that provide site acceleration, optimal traffic distribution, and low-latency layer-4 load balancing.
Evaluate data store requirements
Evaluating data store requirements is about assessing the specific needs and conditions for storing, retrieving, and managing data. This assessment considers factors like data volume, access speed, consistency, and durability. A workload might require multiple types of data stores based on varying business and technical requirements. Identifying the right data store services and proper implementation helps prevent bottlenecks and ensures quick data access.
Evaluate database requirements
The database can affect factors such as data storage and retrieval, transaction processing, consistency guarantees, and handling of large or rapidly changing data. Assess the needs and criteria for your database. Select a database system that can meet those requirements. Evaluate the database requirements before you choose a database. To evaluate the database requirements and choose the appropriate database, follow these steps:
Identify the workload needs.
Understand the specific requirements of your workload, such as data volume, expected transaction rates, concurrency, data types, and expected growth. Evaluate different database systems based on your workload needs. For example, if your workload requires high-performance real-time data processing, you might choose a database system optimized for fast data ingestion and low latency.
Consider the data model.
Determine the data model that best suits your workload. Evaluate the database requirements to ensure that the chosen database supports the required data structures, relationships, and integrity constraints. For example, if your data has a highly relational structure, you might opt for a relational database management system (RDBMS) that provides robust support for transactions and referential integrity. The data model might be hierarchical, network, relational, object-oriented, or NoSQL. Assess the complexity of your data model. Ensure that the chosen database supports the required data structures and relationships.
Evaluate the capabilities.
Consider factors such as read/write patterns, query complexity, latency requirements, and scalability needs. Evaluate the performance capabilities of different database systems accordingly. Some databases excel in read-heavy workloads, while others are optimized for write-intensive or analytical workloads.
Assess the load.
Consider factors such as data volume, transaction rates, read/write ratios, and expected growth. Choose a database that can handle the anticipated workload to ensure smooth operation and prevent performance bottlenecks as your workload is scaled. Consider the scalability requirements of your workload. These requirements include anticipated data growth, concurrent user access, and the need for horizontal or vertical scaling. Evaluate the scalability options and availability features that different database systems provide.
Evaluate storage requirements
Choose storage services that align with your data access patterns, durability requirements, and performance needs. Most cloud workloads use a combination of storage technologies. This technique is known as the polyglot persistence approach. Determine the appropriate combination of storage services for your workload. You might also want to separate data to avoid contamination. For example, you might have separate storage accounts for monitoring data and business data. Choosing the right mix and correct implementation is important for optimizing application performance.
Evaluate cache requirements
A cache stores frequently accessed data. Caching reduces data access latency and lowers the load on data storage components. It allows the workload to handle more requests without scaling. It's common to cache workload data and static content. A Redis cache can store session data, database results, API responses, and reference data, such as configuration settings. A content delivery network or static web app can cache and serve static content. Consider caching data to improve your workload performance. Choose the right caching option for your workload, preferring the platform caching services, such as Azure Redis Cache, over custom or self-hosted ones.
Azure facilitation
Understanding requirements
: Use
Azure Monitor
to collect and analyze data from your workload. Monitor provides insights into the performance and health of your workloads, allowing you to identify and troubleshoot issues.
Understanding and evaluating services
: Review Azure
services and products
to determine if they meet your performance requirements. Azure offers several services that accomplish the same outcome. You have the flexibility to align your choice of service to your performance needs, team skill set, and cost requirements.
For a list of the most common Azure limits, see
Azure subscription and service limits, quotas, and constraints
.
The
Query limits and quotas sample
shows how to query the limits and quotas for commonly used resources.
Azure has many services that can accommodate any workload. Review the
selection guidance
for each service type to help you streamline your selection based on your requirements. See the following guides to choose:
A region
Compute services
Container services
Data store services
Load balancing services
Storage services
Related links
Azure regions with availability zone support
Recommendations for defining performance targets
Recommendations for using availability zones and regions
What are availability zones?
Select Azure regions
Performance Efficiency checklist
Refer to the complete set of recommendations.
Performance Efficiency checklist
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025
Performance Efficiency tradeoffs - Microsoft Azure Well-Architected Framework | Microsoft Learn
Skip to main content
Skip to Ask Learn chat experience
This browser is no longer supported.
Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.
Download Microsoft Edge
More info about Internet Explorer and Microsoft Edge
Table of contents
Exit focus mode
Ask Learn
Ask Learn
Read in English
Save
Table of contents
Read in English
Add
Add to plan
Edit
Share via
Facebook
x.com
LinkedIn
Email
Print
Note
Access to this page requires authorization. You can try
signing in
or
changing directories
.
Access to this page requires authorization. You can try
changing directories
.
Performance Efficiency tradeoffs
Article
2024-10-10
4 contributors
Feedback
In this article
A workload that meets its performance targets without overprovisioning is efficient. The goal of performance efficiency is to have just enough supply to handle demand at all times. Key strategies for performance efficiency include proper use of code optimizations, design patterns, capacity planning, and scaling. Clear performance targets and testing underpin this pillar.
During the process of negotiating a workload's performance targets and designing a workload for performance efficiency, it's important to be aware of how the
Performance Efficiency design principles
and the recommendations in the
Design review checklist for Performance Efficiency
might affect the optimization goals of other pillars. Certain performance efficiency decisions might benefit some pillars but constitute tradeoffs for others. This article lists example tradeoffs that a workload team might encounter when designing workload architecture and operations for performance efficiency.
Performance Efficiency tradeoffs with Reliability
Tradeoff: Reduced replication and increased density.
A cornerstone of reliability is ensuring resilience by using replication and limiting the blast radius of malfunctions.
A workload that achieves efficiency by delaying scaling until the last responsible moment closely meets demand but is vulnerable to unforeseen node failures and scaling delays.
Consolidating workload resources can use excess capacity and improve efficiency. However, it increases the blast radius of a malfunction in the co-located component or application platform.
Scaling in or scaling down to minimize surplus capacity can leave a workload underprovisioned during usage spikes, which leads to service disruptions due to insufficient supply.
Tradeoff: Increased complexity.
Reliability prioritizes simplicity.
Using autoscaling to balance workload supply against demand introduces variability in the workload's topology and adds a component that must work correctly for the system to be reliable. Autoscaling leads to triggering more application lifecycle events, like starting and stopping.
Data partitioning and sharding help avoid performance issues in large or frequently accessed datasets. However, the implementation of these patterns increases complexity because (eventual) consistency needs to be maintained across additional resources.
Denormalizing data for optimized access patterns can improve performance, but it introduces complexity because multiple representations of data need to be kept synchronized.
Performance-centric cloud design patterns sometimes necessitate the introduction of additional components. The use of these components increases the surface area of the workload. The components then must themselves be made reliable to keep the whole workload reliable. Examples include:
A message bus for load leveling, which introduces a critical, stateful component.
A load balancer for autoscaled replicas, which requires reliable operation and the enlistment of replicas.
Offloading data to caches, which requires reliable cache invalidation approaches.
Tradeoff: Testing and observation on active environments.
Avoiding the unnecessary use of production systems is a self-preservation and risk avoidance approach for reliability.
Performance testing in active environments, like the use of synthetic transactions, carries the risk of causing malfunctions due to the test actions or configurations.
Workloads should be instrumented with an application performance monitoring (APM) system that enables teams to learn from active environments. The APM tooling is installed and configured in application code or in the hosting environment. Improper use, exceeding limitations, or misconfiguration of the tool can compromise its functionality and maintenance, potentially undermining reliability.
Performance Efficiency tradeoffs with Security
Tradeoff: Reduction of security controls.
Security controls are established across multiple layers, sometimes redundantly, to provide defense in depth.
One performance optimization strategy is to remove or bypass components or processes that contribute to delays in a flow, especially when their processing time isn't justified. However, this strategy can compromise security and should be accompanied by a thorough risk analysis. Consider the following examples:
Removing encryption in transit or at rest to improve transfer speeds exposes the data to potential integrity or confidentiality breaches.
Removing or reducing security scanning or inspecting tools to reduce processing times can compromise the confidentiality, integrity, or availability that those tools protect.
Decreasing the frequency of security patching to limit the performance impact can leave a workload more vulnerable to emerging threats.
Removing firewall rules from network flows to improve network latency can allow undesirable communication.
Minimizing data validation or content safety checks for quicker data processing might compromise data integrity, especially if inputs are malicious.
Using less entropy in encryption or hashing algorithms, for example, on the initialization vector (IV), is more efficient but makes the encryption easier to crack.
Tradeoff: Increased workload surface area.
Security prioritizes a reduced and contained surface area to minimize attack vectors and reduce the management of security controls.
Performance-centric cloud design patterns sometimes necessitate the introduction of additional components. These components increase the surface area of the workload. The new components must be secured, possibly in ways that aren't already used in the system, and they often increase the compliance scope. Consider these commonly added components:
A message bus for load leveling
A load balancer for autoscaled replicas
Offloading data to caches, application delivery networks, or content delivery networks
Offloading processing to background jobs or even client compute
Tradeoff: Removing segmentation.
The Security pillar prioritizes strong segmentation to enable fine-grained security controls and reduce blast radius.
Sharing resources through increased density is an approach for improving efficiency. Examples include multitenancy scenarios or combining disparate applications in an architecture on a common application platform. The increased density can lead to the following security concerns:
Increased risk of unauthorized lateral movement from one tenant to another.
A shared workload identity that violates the principle of least privilege and obscures individual audit trails in access logs.
Perimeter security controls, for example network rules, that are reduced to cover all co-located components, giving individual components more access than necessary.
A compromise of the application platform host or an individual component due to a larger blast radius. This increase is caused by easier access to co-located components.
Co-locating disparate components leading to more components in scope for compliance because of their shared host.
Performance Efficiency tradeoffs with Cost Optimization
Tradeoff: Too much supply for demand.
Both Cost Optimization and Performance Efficiency prioritize having just enough supply to serve demand.
Overprovisioning is a risk when teams try to mitigate performance issues in a workload. Some common causes of overprovisioning include:
Initial capacity planning was misjudged because the team focused only on peak load estimates, neglecting strategies for peak smoothing in the workload design.
Scaling a resource up or out during a troubleshooting step of an incident response.
Autoscaling can be misconfigured. Some examples of misconfigured autoscaling include:
Scaling up with minimal changes in demand or an extended cooldown period can incur more cost than demand requires.
Using autoscaling without a set upper limit can lead to uncontrolled growth due to system malfunctions or abuse and exceed the expected workload requirements.
Expanding into multiple regions can enhance performance by bringing workloads closer to the user and can avoid temporary resource capacity constraints. However, that topology also adds complexity and resource duplication.
Tradeoff: More components.
One cost optimization technique is to consolidate with a smaller number of resources by increasing density, removing duplication, and co-locating functionality.
Performance-centric cloud design patterns sometimes necessitate the introduction of extra components. These extra components usually lead to an overall cost increase for the workload. For example, you might include a message bus for load leveling or offload tasks to an application or content delivery network for improved response times.
Resource segmentation allows different parts of a workload to have distinct performance characteristics, enabling independent tuning for each segment. However, it can increase the total ownership costs because it requires multiple optimized segments rather than a single, generalized component.
Tradeoff: Increased investment on items that aren't aligned with functional requirements.
One approach to cost optimization is evaluating the value provided by any solution that's deployed.
Premium services and SKUs can help a workload meet performance targets. These services usually cost more and can provide extra features. They might be underutilized if many of the premium features aren't used specifically for meeting performance targets.
A performant workload requires telemetry data for observability that must be transferred and stored. An increase in the performance telemetry being collected can increase the cost of telemetry data transfer and storage.
Performance testing activities add costs that aren't associated with the value of the production system. Examples of performance testing costs include:
Instantiating environments that are dedicated to performance-centric tests.
Using specialized performance tooling.
Spending time to run the tests.
Training team members for specialized performance optimization tasks or paying for performance tuning services adds to the cost of a workload.
Performance Efficiency tradeoffs with Operational Excellence
Tradeoff: Reduced observability.
Observability is necessary to provide a workload with meaningful alerting and help ensure successful incident response.
Reducing log and metric volume to reduce the processing time spent on collecting telemetry instead of other tasks reduces the overall observability of the system. Some examples of the resulting reduced observability include:
It limits the data points that are used to build meaningful alerts.
It leads to gaps in coverage for incident response activities.
It limits observability in security-sensitive or compliance-sensitive interactions and boundaries.
When performance design patterns are implemented, the complexity of the workload often increases. Components are added to critical flows. The workload monitoring strategy and performance monitoring must include those components. When a flow spans multiple components or application boundaries, the complexity of monitoring the performance of that flow increases. Flow performance needs to be correlated across all the interconnected components.
Tradeoff: Increased complexity in operations.
A complex environment has more complex interactions and a higher likelihood of a negative impact from routine, ad hoc, and emergency operations.
Improving performance efficiency by increasing density elevates the risk in operational tasks. An error in a single process can have a large blast radius.
As performance design patterns are implemented, they influence operational procedures like backups, key rotations, and recovery strategies. For example, data partitioning and sharding can complicate routine tasks when teams try to ensure that those tasks don't affect data consistency.
Tradeoff: Culture stress.
Operational Excellence is rooted in a culture of blamelessness, respect, and continuous improvement.
Conducting root cause analysis of performance issues identifies deficiencies in processes or implementations that require correction. The team should consider the exercise a learning opportunity. If team members are blamed for issues, morale can be affected.
Routine and ad hoc processes can affect workload performance. It's often considered preferable to perform these activities during off-peak hours. However, off-peak hours can be inconvenient or outside of regular hours for the team members who are responsible for or skilled in these tasks.
Related links
Explore the tradeoffs for the other pillars:
Reliability tradeoffs
Security tradeoffs
Cost Optimization tradeoffs
Operational Excellence tradeoffs
Feedback
Was this page helpful?
Yes
No
Additional resources
Additional resources
In this article
en-us
Your Privacy Choices
Theme
Light
Dark
High contrast
Previous Versions
Blog
Contribute
Privacy
Terms of Use
Trademarks
© Microsoft 2025