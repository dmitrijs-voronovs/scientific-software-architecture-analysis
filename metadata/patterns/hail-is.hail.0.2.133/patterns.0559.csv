filename,type,author,repo,version,wiki,url,patterns,purpose,summary
datasets/load/load.1000_Genomes_phase3_chrX.GRCh38.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.1000_Genomes_phase3_chrX.GRCh38.py,"[{'name': 'HaplotypeSplit', 'match_type': 'partial', 'implemented_parts': ['Sample selection', 'Variant annotation', 'Quality control', 'Sample metadata enrichment'], 'confidence': 0.95, 'evidence': ['GRCh37_POS column populated', 'AlleLE_TRANSFORM and REF_NEW_ALLELE columns filled', 'Sample metadata extension with annotations']}, {'name': 'MatrixTableConversion', 'match_type': 'full', 'implemented_parts': ['Matrix table conversion', 'Annotation merging'], 'confidence': 1.0, 'evidence': ['Data written to Google Cloud storage', 'Metadata embedded as annotations']}]",This script performs haplotype splitting on the 1000 Genomes phase3 chrX dataset.,"{'constants': ['Dataset path and reference genome'], 'types': ['Matrix table of variant calls'], 'classes': ['HaplotypeSplit object'], 'functions': ['Conversion of variant calls to a matrix table', 'Annotation enrichment of samples']}"
datasets/load/load.1000_Genomes_phase3_chrY.GRCh37.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.1000_Genomes_phase3_chrY.GRCh37.py,"[{'name': 'Sample QC', 'match_type': 'partial', 'implemented_parts': ['Sample filtering based on quality metrics', 'Variant quality control checks'], 'confidence': 0.95, 'evidence': ['sample_qc function', 'variant_qc function']}, {'name': 'Annotation', 'match_type': 'partial', 'implemented_parts': ['Adding global metadata to the dataset', 'Annotating variants with metadata from external sources'], 'confidence': 0.9, 'evidence': ['annotate_globals function', 'metadata field definition']}, {'name': 'Writing to GCS', 'match_type': 'partial', 'implemented_parts': ['Saving the dataset to Google Cloud Storage', 'Overwriting existing dataset if necessary'], 'confidence': 0.85, 'evidence': ['mt.write function', 'overwrite argument']}]",This script performs quality control and annotation of a 1000 Genomes dataset from phase 3 chrY.,"{'constants': ['Reference genome and metadata'], 'types': ['MatrixTable data structure'], 'classes': ['Hail dataset manipulation functions'], 'functions': ['sample_qc', 'variant_qc', 'annotate_globals', 'write']}"
datasets/load/load.1000_Genomes_phase3_chrY.GRCh38.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.1000_Genomes_phase3_chrY.GRCh38.py,"[{'name': 'sample_qc', 'match_type': 'partial', 'implemented_parts': ['sample quality control metrics'], 'confidence': 0.95, 'evidence': ['sample_qc function call']}, {'name': 'variant_qc', 'match_type': 'partial', 'implemented_parts': ['variant quality control metrics'], 'confidence': 0.98, 'evidence': ['variant_qc function call']}]",This file performs quality control on genetic variants identified in a Hail dataset.,"{'constants': ['Quality control thresholds'], 'types': ['Quality control metrics'], 'classes': ['Variant quality control functions'], 'functions': ['Sample quality control', 'Variant quality control']}"
datasets/load/load.1000_Genomes_phase3_European_autosomes_maf_gt_001_block_matrix.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.1000_Genomes_phase3_European_autosomes_maf_gt_001_block_matrix.py,"[{'name': 'MTFilter', 'match_type': 'partial', 'implemented_parts': ['ReadMatrixTable', 'FilterCols', 'VariantQC', 'FilterRows'], 'confidence': 0.95, 'evidence': ['use of hail.read_matrix_table', 'filter based on super_population', 'variant quality control checks', 'row filtering based on AF threshold']}, {'name': 'BlockMatrixWrite', 'match_type': 'partial', 'implemented_parts': ['EntryExpression', 'BlockMatrixWrite', 'MeanImpute', 'Normalize'], 'confidence': 0.9, 'evidence': ['use of entry_expr to count alternate alleles', 'creation of BlockMatrix from entry expression', 'mean imputation and normalization options']}]","This code reads genetic data from a cloud-stored dataset, filters variants, and writes them to a block matrix.","{'constants': ['Cloud storage paths for input data'], 'types': ['MatrixTable, BlockMatrix'], 'classes': ['BlockMatrix'], 'functions': ['ReadMatrixTable', 'BlockMatrixWrite']}"
datasets/load/load.1000_Genomes_phase3_European_autosomes_maf_gt_001_block_matrix_standardized.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.1000_Genomes_phase3_European_autosomes_maf_gt_001_block_matrix_standardized.py,"[{'name': 'DatasetImport', 'match_type': 'partial', 'implemented_parts': ['BlockMatrix read function'], 'confidence': 0.95, 'evidence': ['import hail.linalg.BlockMatrix', 'reading data from GCS']}, {'name': 'StatisticalNormalization', 'match_type': 'full', 'implemented_parts': ['Mean calculation', 'Standard deviation calculation'], 'confidence': 1.0, 'evidence': ['g.sum() function', 'sqrt() function']}, {'name': 'MetadataGeneration', 'match_type': 'partial', 'implemented_parts': ['Struct definition', 'write_expression function'], 'confidence': 0.9, 'evidence': ['hl.struct() function', 'writing metadata to GCS']}]","This file loads a genetic dataset, normalizes it statistically, and generates metadata.","{'constants': ['GCS paths for dataset and metadata'], 'types': ['BlockMatrix'], 'classes': ['BlockMatrix'], 'functions': ['read_expression function', 'write_expression function']}"
datasets/load/load.1000_Genomes_phase3_relationships.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.1000_Genomes_phase3_relationships.py,"[{'name': 'Unrelated', 'match_type': 'partial', 'implemented_parts': [""Relationship role is 'unrel'""], 'confidence': 1.0, 'evidence': [""Relationship role attribute explicitly set to 'unrel'""]}, {'name': 'Sibling Relationship', 'match_type': 'partial', 'implemented_parts': ['Sibling IDs are populated'], 'confidence': 0.9, 'evidence': [""Comma-separated list of IDs in 'sibling_ids' field""]}, {'name': 'Parental Relationship', 'match_type': 'partial', 'implemented_parts': ['Maternal ID and/or paternal ID is populated'], 'confidence': 0.8, 'evidence': [""Presence of values in 'maternal_id' or 'paternal_id' fields""]}, {'name': 'Children Relationship', 'match_type': 'partial', 'implemented_parts': ['Children IDs are populated'], 'confidence': 0.9, 'evidence': [""Comma-separated list of IDs in 'children_ids' field""]}, {'name': 'Second Order Relationship', 'match_type': 'partial', 'implemented_parts': ['Second order relationship IDs are populated'], 'confidence': 0.8, 'evidence': [""Comma-separated list of IDs in 'second_order_relationship_ids' field""]}, {'name': 'Third Order Relationship', 'match_type': 'partial', 'implemented_parts': ['Third order relationship IDs are populated'], 'confidence': 0.7, 'evidence': [""Comma-separated list of IDs in 'third_order_relationship_ids' field""]}]",This file contains relationship information for individuals in a family tree.,"{'constants': ['None'], 'types': ['Family-related IDs'], 'classes': ['None'], 'functions': ['Relationship identification and family tree construction']}"
datasets/load/load.1000_Genomes_phase3_samples.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.1000_Genomes_phase3_samples.py,"[{'name': 'DataImport', 'match_type': 'partial', 'implemented_parts': ['File import', 'Data transformation', 'Metadata annotation'], 'confidence': 0.95, 'evidence': ['gs URL for dataset', 'no_header flag', 'filter column', 'find_replace function', 'annotate with is_female column']}, {'name': 'DataManipulation', 'match_type': 'partial', 'implemented_parts': ['Column renaming', 'Selection of specific columns', 'Keying by a column', 'Global metadata annotation'], 'confidence': 0.9, 'evidence': ['rename function', 'select method', 'key_by function', 'annotate_globals with metadata']}]",This file loads and manipulates genomic data from a TSV file.,"{'constants': [], 'types': [], 'classes': [], 'functions': []}"
datasets/load/load.1000_Genomes_phase3_sites.GRCh37.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.1000_Genomes_phase3_sites.GRCh37.py,"[{'name': 'Memento', 'match_type': 'partial', 'implemented_parts': ['Originator creates Memento to capture state'], 'confidence': 0.7, 'evidence': ['create_memento method', 'get_memento method']}, {'name': 'Visitor', 'match_type': 'partial', 'implemented_parts': ['Visitor adds operations to Element without modifying it'], 'confidence': 0.8, 'evidence': ['visit method', 'Element class has accept method']}]",This file uses Hail library for loading and manipulating genomic datasets.,"{'constants': ['Genome reference genome'], 'types': ['Hail tables'], 'classes': ['Hail library functions'], 'functions': ['import_table loads data from external source']}"
datasets/load/load.CADD.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.CADD.py,"[{'name': 'Import Table', 'match_type': 'partial', 'implemented_parts': ['Importing data from a TSV file', 'Defining data types', 'Annotating with locus and alleles'], 'confidence': 0.9, 'evidence': ['import_table function', 'types definition', 'annotate function']}, {'name': 'Metadata Annotations', 'match_type': 'partial', 'implemented_parts': ['Adding dataset metadata', 'Storing information in a global struct'], 'confidence': 0.8, 'evidence': ['annotate_globals function', 'metadata struct definition']}, {'name': 'Output to HailDB', 'match_type': 'partial', 'implemented_parts': ['Writing data to HailDB format'], 'confidence': 0.7, 'evidence': ['ht.write function']}]","This code loads the CADD dataset from a TSV file, annotates it with genomic information and metadata, and stores the results in HailDB format.","{'constants': ['Data paths and HailDB root directory'], 'types': ['Representations for positions, scores, and loci'], 'classes': ['None'], 'functions': ['Import, annotate, and write functions']}"
datasets/load/load.DANN.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.DANN.py,"[{'name': 'TableImport', 'match_type': 'partial', 'implemented_parts': ['CSV import', 'column transformation'], 'confidence': 0.9, 'evidence': ['table import function', 'column type conversion']}, {'name': 'Annotation', 'match_type': 'partial', 'implemented_parts': ['Locus annotation', 'allele indexing'], 'confidence': 0.85, 'evidence': ['locus transformation function', 'allele array creation']}, {'name': 'ReferenceConversion', 'match_type': 'partial', 'implemented_parts': ['Genome version conversion', 'liftover procedure'], 'confidence': 0.75, 'evidence': ['reference genome mapping', 'liftover chain file']}]","This script imports a DANN dataset from a CSV file, annotates it with genomic locations and alleles, and converts it to a Hail table.","{'constants': ['Data root references'], 'types': ['CSV table, Hail table'], 'classes': ['Table import and annotation functions'], 'functions': ['Import data from CSV', 'Annotate with genomic data', 'Convert to Hail format']}"
datasets/load/load.dbSNP.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.dbSNP.py,"[{'name': 'Variant Annotation', 'match_type': 'partial', 'implemented_parts': ['Import variant data', 'Annotate variants with allele frequency and TOPMED scores', 'Drop redundant columns'], 'confidence': 0.95, 'evidence': [""Use of Hail's `import_vcf` function"", 'Annotation pipeline utilizing allele frequency and TOPMED scores', 'Explicit removal of unnecessary columns']}, {'name': 'Partitioning and Metadata', 'match_type': 'full', 'implemented_parts': ['Split variant data into partitions', 'Add metadata about dataset and processing parameters'], 'confidence': 1.0, 'evidence': ['Use of `hl.split_multi` function for partitioning', 'Annotation of metadata using `annotate_globals` method']}]","This code imports and annotates the dbSNP dataset from HailDB, then partitions the data and adds metadata.","{'constants': ['Reference genome used for annotation'], 'types': ['Variant data, metadata'], 'classes': ['Hail datasets and annotations'], 'functions': ['Importing, annotating, and partitioning variant data']}"
datasets/load/load.Ensembl.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.Ensembl.py,"[{'name': 'Dataset loading', 'match_type': 'partial', 'implemented_parts': ['Importing data from Hail datasets', 'Table manipulation'], 'confidence': 0.9, 'evidence': ['import_table function', 'table filtering', 'annotation creation']}, {'name': 'Sequence retrieval', 'match_type': 'full', 'implemented_parts': ['Peptide sequence retrieval', 'Table selection'], 'confidence': 1.0, 'evidence': ['annotation of peptide sequence', 'selection based on transcript type']}]",This code loads datasets from Hail and retrieves sequence information for transcripts.,"{'constants': ['Reference genome and Hail dataset paths'], 'types': ['Hail table types'], 'classes': ['Table manipulation methods'], 'functions': ['Importing and manipulating Hail datasets', 'Retrieving sequence data']}"
datasets/load/load.Ensembl_homo_sapiens_features.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.Ensembl_homo_sapiens_features.py,"[{'name': 'Visitor-Element Pattern', 'match_type': 'partial', 'implemented_parts': ['Visitor adds operations to Element without modifying it', 'Element accepts Visitors'], 'confidence': 0.9, 'evidence': ['Importing and modifying external data (ht variable)', 'Method call to describe() on Element object']}]",This file loads and analyzes genomic data from the Ensembl database.,"{'constants': ['Data roots for raw and hail datasets'], 'types': ['Hail table object'], 'classes': ['Argument parser for command-line options'], 'functions': ['Importing data from GFF3 files']}"
datasets/load/load.Ensembl_homo_sapiens_low_complexity_regions.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.Ensembl_homo_sapiens_low_complexity_regions.py,"[{'name': 'Import and Annotate', 'match_type': 'partial', 'implemented_parts': ['Importing dataset from GCS', 'Annotating with interval data', 'Keying by interval and selecting rows'], 'confidence': 0.9, 'evidence': ['import_table function', 'annotate method with interval data', 'key_by and select methods']}, {'name': 'Metadata and Output', 'match_type': 'full', 'implemented_parts': ['Adding metadata to global context', 'Writing data to Hail storage', 'Reading data from Hail storage'], 'confidence': 1.0, 'evidence': ['annotate_globals method with metadata', 'write method with overwrite option', 'read_table function']}]",This code imports a dataset from Ensembl and annotates it with intervals. It then adds metadata to the dataset and writes it to Hail storage.,"{'constants': ['Datasets paths'], 'types': ['Hail tables'], 'classes': ['None'], 'functions': ['Import, annotate, and write Hail tables']}"
datasets/load/load.Ensembl_homo_sapiens_reference_genome.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.Ensembl_homo_sapiens_reference_genome.py,"[{'name': 'Dataset Import', 'match_type': 'partial', 'implemented_parts': ['Table import from GCS'], 'confidence': 0.9, 'evidence': ['gs://hail-datasets-raw-data/Ensembl import statement']}, {'name': 'Locus Annotation', 'match_type': 'partial', 'implemented_parts': ['Chromosome and position conversion', 'Locus annotation'], 'confidence': 0.8, 'evidence': ['chromosome regex', 'position integer conversion']}, {'name': 'Reference Allele Extraction', 'match_type': 'partial', 'implemented_parts': ['Selection of reference allele'], 'confidence': 0.9, 'evidence': ['reference_allele column selection']}]",This code loads the Ensembl human reference genome dataset into Hail.,"{'constants': ['Data paths for raw and Hail datasets'], 'types': ['Hail Table object'], 'classes': ['Table', 'Locus'], 'functions': ['Import from GCS', 'Annotation', 'Selection']}"
datasets/load/load.gencode_v30_annotation_gff3.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.gencode_v30_annotation_gff3.py,"[{'name': 'Annotation import', 'match_type': 'partial', 'implemented_parts': ['File import', 'Column splitting', 'Data imputation'], 'confidence': 0.9, 'evidence': ['import statement for gencode annotation file', 'f8 column splitting based on semicolons', 'use of impute=True argument']}, {'name': 'Gene annotation', 'match_type': 'full', 'implemented_parts': ['Column extraction', 'Interval creation', 'Column conversion to dictionary'], 'confidence': 1.0, 'evidence': ['Extraction of gene-related columns', 'Creation of genomic intervals from locus information', 'Conversion of GFF_Columns to dictionary']}, {'name': 'Data transformation', 'match_type': 'partial', 'implemented_parts': ['Selection', 'Annotation', 'Column dropping'], 'confidence': 0.8, 'evidence': ['Filter rows based on gene type', 'Addition of annotations such as gene score, strand, and phase', 'Dropping of unnecessary columns']}]",This script imports and annotates gene data from a GFF3 file.,"{'constants': ['GRCh38 reference genome'], 'types': ['Hail tables representing gene data'], 'classes': ['Gene annotations'], 'functions': ['Column manipulation and annotation functions']}"
datasets/load/load.GERP++.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.GERP++.py,"[{'name': 'Locus-based elements', 'match_type': 'partial', 'implemented_parts': ['interval annotation', 'interval liftover', 'element selection'], 'confidence': 0.95, 'evidence': ['locus function call', 'interval type definition', 'element filtering based on interval']}, {'name': 'GERP++ elements statistics', 'match_type': 'full', 'implemented_parts': ['table import', 'metadata annotation'], 'confidence': 1.0, 'evidence': ['table import from Hail dataset', 'metadata addition with name, version, reference genome', 'row count and partition count retrieval']}]",This script imports GERP++ elements from a Hail dataset and annotates them with metadata.,"{'constants': ['Dataset and assembly locations'], 'types': ['intervals representing elements'], 'classes': ['Table representing elements'], 'functions': ['Element interval annotation', 'Liftover to different assemblies']}"
datasets/load/load.gnomad_v2.1.1_lof_metrics_by_gene.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.gnomad_v2.1.1_lof_metrics_by_gene.py,"[{'name': 'MementoPattern', 'match_type': 'partial', 'implemented_parts': ['Caretaker stores and restores Memento', 'save_state', 'restore_state'], 'confidence': 0.9, 'evidence': ['class Memento', 'save_state method', 'restore_state method']}, {'name': 'VisitorPattern', 'match_type': 'partial', 'implemented_parts': ['Visitor adds operations to Element without modifying it'], 'confidence': 0.8, 'evidence': ['accept method in Element class', 'visit method in Visitor class']}]",This file demonstrates the application of the Memento Pattern and the Visitor Pattern.,"{'constants': [], 'types': [], 'classes': ['Memento', 'Caretaker'], 'functions': ['save_state', 'restore_state', 'visit']}"
datasets/load/load.GTEx.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.GTEx.py,"[{'name': 'SMESTLBS Analysis', 'match_type': 'full', 'implemented_parts': ['Gene expression analysis', 'Variant annotation', 'Functional enrichment analysis'], 'confidence': 1.0, 'evidence': ['SMESTLBS gene set', 'SMMPPD expression data', 'SMRRNANM variant annotation', 'SMVQCFL enrichment analysis']}]",This code performs comprehensive analysis of gene expression and variant data from the SMESTLBS dataset.,"{'constants': ['SMESTLBS gene set', 'SMMPPD platform'], 'types': ['Gene expression data', 'Variant annotations'], 'classes': ['Analysis pipeline', 'Results summarizer'], 'functions': ['Gene expression analysis', 'Variant annotation', 'Functional enrichment analysis']}"
datasets/load/load.GTEx_v7.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.GTEx_v7.py,"[{'name': 'Junction analysis', 'match_type': 'partial', 'implemented_parts': ['Read counting', 'Junction interval annotation', 'Sample attribute retrieval'], 'confidence': 0.95, 'evidence': ['chr_start_end column extraction', 'junction_interval keying', 'read_count column calculation']}, {'name': 'GRCh38 reference genome support', 'match_type': 'conditional', 'implemented_parts': ['Liftover of junction intervals'], 'confidence': 0.9, 'evidence': ['GRCh38 reference addition', 'junction_interval liftover function']}]",This code performs junction analysis of sequencing data using Hail.,"{'constants': ['GRCh37 and GRCh38 reference genome identifiers'], 'types': ['Locus intervals'], 'classes': ['Junction analysis results'], 'functions': ['Locus interval annotation', 'Read counting', 'Liftover from GRCh37 to GRCh38']}"
