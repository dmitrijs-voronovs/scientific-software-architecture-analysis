filename,type,author,repo,version,wiki,url,patterns,purpose,summary
hail/python/hail/utils/__init__.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/__init__.py,"[{'name': 'DataPartitioner', 'match_type': 'partial', 'implemented_parts': ['partitions function', 'default_handler handler', 'get_env_or_default function', 'guess_cloud_spark_provider function'], 'confidence': 0.9, 'evidence': ['partitions function iterates over directory structure', 'default_handler handler deals with unknown partitions', 'get_env_or_default function retrieves environment variables', 'guess_cloud_spark_provider function identifies cloud provider for optimization']}, {'name': 'TemporaryFileManagement', 'match_type': 'full', 'implemented_parts': ['new_local_temp_dir function', 'new_local_temp_file function', 'new_temp_file function'], 'confidence': 1.0, 'evidence': ['Functions create temporary directories and files for local processing', 'Each function handles different scope of temporary storage']}, {'name': 'CommandExecution', 'match_type': 'partial', 'implemented_parts': ['run_command function'], 'confidence': 0.8, 'evidence': ['Function runs external commands with specified arguments']}]","This file handles data partitioning, temporary file management, and command execution for the given Spark application.","{'constants': ['Defines configuration parameters for partitioning'], 'types': ['Custom types for partitioning and temporary storage'], 'classes': ['DataPartitioner class manages data partitioning'], 'functions': ['Partitions data based on directory structure', 'Runs commands and interacts with external systems']}"
hail/python/hail/utils,FileType.DIR,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils,"[{'name': 'DataPartitioner', 'match_type': 'partial', 'implemented_parts': ['partitions function', 'default_handler handler', 'get_env_or_default function', 'guess_cloud_spark_provider function'], 'confidence': 0.9, 'evidence': ['partitions function iterates over directory structure', 'default_handler handler deals with unknown partitions', 'get_env_or_default function retrieves environment variables', 'guess_cloud_spark_provider function identifies cloud provider for optimization']}, {'name': 'TemporaryFileManagement', 'match_type': 'full', 'implemented_parts': ['new_local_temp_dir function', 'new_local_temp_file function', 'new_temp_file function'], 'confidence': 1.0, 'evidence': ['Functions create temporary directories and files for local processing', 'Each function handles different scope of temporary storage']}, {'name': 'CommandExecution', 'match_type': 'partial', 'implemented_parts': ['run_command function'], 'confidence': 0.8, 'evidence': ['Function runs external commands with specified arguments']}]","This file handles data partitioning, temporary file management, and command execution for the given Spark application.","{'constants': ['Defines configuration parameters for partitioning'], 'types': ['Custom types for partitioning and temporary storage'], 'classes': ['DataPartitioner class manages data partitioning'], 'functions': ['Partitions data based on directory structure', 'Runs commands and interacts with external systems']}"
hail/python/hail/vds/combiner/combine.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py,"[{'name': 'Interval partitioning', 'match_type': 'full', 'implemented_parts': ['Contig division based on interval size', 'Calculation of interval boundaries', 'Handling different reference genomes'], 'confidence': 1.0, 'evidence': ['reference_genome parameter', 'interval_size argument', 'Calculation of contig length and parts']}]",This function divides contigs of a reference genome into intervals of a specified size.,"{'constants': [], 'types': ['ReferenceGenome', 'Interval'], 'classes': [], 'functions': ['calc_parts function performs interval partitioning']}"
hail/python/hail/vds/combiner/variant_dataset_combiner.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py,"[{'name': 'VariantDatasetCombiner', 'match_type': 'full', 'implemented_parts': ['Reads multiple variant datasets', 'Merges variants into a single dataset', 'Customizes reference genome and intervals'], 'confidence': 1.0, 'evidence': ['VariantDatasetCombiner class definition', 'Reads input datasets from various formats', 'Custom reference genome handling']}]","This class combines multiple variant datasets into a single, unified dataset.","{'constants': ['Defines constants for input and output formats'], 'types': ['Custom types for variant data and metadata'], 'classes': ['VariantDatasetCombiner class for combining datasets'], 'functions': ['Combines variants from different files']}"
hail/python/hail/vds/combiner/__init__.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/__init__.py,"[{'name': 'Facade', 'match_type': 'partial', 'implemented_parts': ['Combined interface', 'Simplified methods'], 'confidence': 0.9, 'evidence': ['__all__ list', 'combine_variant_datasets function']}, {'name': 'Singleton', 'match_type': 'partial', 'implemented_parts': ['Class has single instance', 'get_combiner function'], 'confidence': 0.8, 'evidence': ['new_combiner function', 'load_combiner function']}]",This file provides a facade for combining variant datasets.,"{'constants': ['Defines constants related to variant dataset combination'], 'types': ['VariantDatasetCombiner class'], 'classes': ['VariantDatasetCombiner class'], 'functions': ['Combine variant datasets', 'Transform GVCF files']}"
hail/python/hail/vds/combiner,FileType.DIR,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner,"[{'name': 'ContigPartitioner', 'match_type': 'full', 'implemented_parts': ['Calculates contig length and parts', 'Divide contigs into intervals of a specified size', 'Reference genome parameter handling'], 'confidence': 1.0, 'evidence': ['reference_genome parameter', 'interval_size argument', 'Calculation of contig length and parts']}, {'name': 'VariantDatasetCombiner', 'match_type': 'full', 'implemented_parts': ['Reads multiple variant datasets', 'Merges variants into a single dataset', 'Customizes reference genome and intervals'], 'confidence': 1.0, 'evidence': ['VariantDatasetCombiner class definition', 'Reads input datasets from various formats', 'Custom reference genome handling']}, {'name': 'Facade', 'match_type': 'partial', 'implemented_parts': ['Combined interface', 'Simplified methods'], 'confidence': 0.9, 'evidence': ['__all__ list', 'combine_variant_datasets function']}, {'name': 'Singleton', 'match_type': 'partial', 'implemented_parts': ['Class has single instance', 'get_combiner function'], 'confidence': 0.8, 'evidence': ['new_combiner function', 'load_combiner function']}]",This directory contains functions for dividing contigs of a reference genome into intervals and combining multiple variant datasets into a single dataset.,"{'constants': [], 'types': ['ReferenceGenome', 'Interval'], 'classes': ['VariantDatasetCombiner class for combining datasets'], 'functions': ['Combines variants from different files', 'Calculates interval partitioning']}"
hail/python/hail/vds/functions.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/functions.py,"[{'name': 'Reindexing', 'match_type': 'partial', 'implemented_parts': ['Array reindexing', 'Local alleles array', 'Total number of alleles', 'Fill value', 'Number indicator'], 'confidence': 0.95, 'evidence': ['Function arguments suggest reindexing of an array based on local alleles', 'Number indicator controls the number of alleles to reindex to', 'Fill value is used for global indices with no data']}]",This function reindexes an array based on local alleles and a specified number of alleles.,"{'constants': [], 'types': ['ArrayExpression', 'ArrayExpression', 'Int32Expression', 'Any'], 'classes': [], 'functions': ['local_to_global_a_r']}"
hail/python/hail/vds/methods.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py,"[{'name': 'Collapsed Ref Block', 'match_type': 'partial', 'implemented_parts': ['Collapsed reference block detection', 'New block insertion'], 'confidence': 0.95, 'evidence': ['new_starts column populated', 'col_idx and ht_shuf join']}, {'name': 'Merging Blocks', 'match_type': 'partial', 'implemented_parts': ['Moved block dictionary extraction', 'Merging based on END value'], 'confidence': 0.98, 'evidence': ['group_by on col_idx and new_starts', 'argmax based on END']}, {'name': 'Populating New Entries', 'match_type': 'partial', 'implemented_parts': ['Case statement for merging', 'Original entry retrieval'], 'confidence': 0.99, 'evidence': ['annotate on new_entries', 'coalesce and case statement']}]",This code handles collapsed reference blocks in a dataset.,"{'constants': ['VariantDataset reference block maximum length'], 'types': ['Custom types for collapsed reference block detection'], 'classes': ['Functions for manipulating collapsed reference blocks'], 'functions': ['Populating new entries based on merged blocks']}"
hail/python/hail/vds/sample_qc.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/sample_qc.py,"[{'name': 'Variant Annotation', 'match_type': 'partial', 'implemented_parts': ['Variant Allele Count', 'Variant Type Classification', 'Quality Control Metrics Calculation'], 'confidence': 0.95, 'evidence': ['Annotation of variant allele count', 'Classification of variants based on type', 'Calculation of quality control metrics']}, {'name': 'Reference Comparison', 'match_type': 'partial', 'implemented_parts': ['Variant Quality Control Metrics', 'Reference Data Quality Control Metrics', 'Matching of Variants Across Datasets'], 'confidence': 0.85, 'evidence': ['Comparison of variant quality control metrics', 'Calculation of reference data quality control metrics', 'Matching of variants based on locus and quality scores']}]",This code snippet performs variant annotation and quality control by comparing variants from a sample to a reference genome.,"{'constants': ['Quality control thresholds', 'Reference genome annotations'], 'types': ['Variant annotations', 'Quality control metrics'], 'classes': ['Variant annotation pipeline', 'Quality control metrics'], 'functions': ['Variant annotation functions', 'Quality control metrics calculation functions', 'Variant comparison functions']}"
hail/python/hail/vds/variant_dataset.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/variant_dataset.py,"[{'name': 'VariantDataset union', 'match_type': 'partial', 'implemented_parts': ['union_rows function', 'VariantDataset reference data comparison', 'VariantDataset variant data combination'], 'confidence': 0.95, 'evidence': ['Function arguments and return value types', 'Comparison logic of reference data', 'Union process of variant data']}, {'name': 'VariantDataset reference data sharing', 'match_type': 'full', 'implemented_parts': ['reference_data attribute'], 'confidence': 1.0, 'evidence': ['Data structure definition with shared reference data']}]",This code defines a VariantDataset class for handling variant data across multiple chromosomes.,"{'constants': ['Defines constants related to variant data'], 'types': ['Custom types for representing variant data'], 'classes': ['VariantDataset class manages variant data'], 'functions': ['Functions for loading, combining, and querying variant data']}"
hail/python/hail/vds/__init__.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/__init__.py,"[{'name': 'Variant Data Management', 'match_type': 'partial', 'implemented_parts': ['VariantDataset class', 'read_vds function', 'store_ref_block_max_length function'], 'confidence': 0.9, 'evidence': ['Class handles variant data', 'Function reads variant datasets', 'Function stores reference block length information']}, {'name': 'Quality Control and Filtering', 'match_type': 'partial', 'implemented_parts': ['sample_qc function', 'filter_chromosomes function', 'filter_intervals function', 'filter_samples function', 'filter_variants function'], 'confidence': 0.8, 'evidence': ['Function assesses sample quality', 'Functions filter variants based on chromosomes, intervals, samples, and quality']}, {'name': 'Combiner Module Integration', 'match_type': 'partial', 'implemented_parts': ['combiner module import', 'load_combiner function', 'new_combiner function'], 'confidence': 0.7, 'evidence': ['Module for combining variant datasets', 'Functions load and create combiner instances']}]","This file facilitates variant data management, quality control, and integration with the combiner module.","{'constants': ['Defined in other imported modules'], 'types': ['Custom types for variant datasets and combiner instances'], 'classes': ['VariantDataset class'], 'functions': ['read_vds', 'store_ref_block_max_length', 'sample_qc', 'filter_chromosomes', 'filter_intervals', 'filter_samples', 'filter_variants', 'load_combiner', 'new_combiner']}"
hail/python/hail/vds,FileType.DIR,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds,"[{'name': 'Variant Data Management', 'match_type': 'partial', 'implemented_parts': ['VariantDataset class', 'read_vds function', 'store_ref_block_max_length function'], 'confidence': 0.9, 'evidence': ['Class handles variant data', 'Function reads variant datasets', 'Function stores reference block length information']}, {'name': 'Quality Control and Filtering', 'match_type': 'partial', 'implemented_parts': ['sample_qc function', 'filter_chromosomes function', 'filter_intervals function', 'filter_samples function', 'filter_variants function'], 'confidence': 0.8, 'evidence': ['Function assesses sample quality', 'Functions filter variants based on chromosomes, intervals, samples, and quality']}, {'name': 'Combiner Module Integration', 'match_type': 'partial', 'implemented_parts': ['combiner module import', 'load_combiner function', 'new_combiner function'], 'confidence': 0.7, 'evidence': ['Module for combining variant datasets', 'Functions load and create combiner instances']}]","This file facilitates variant data management, quality control, and integration with the combiner module.","{'constants': ['Defined in other imported modules'], 'types': ['Custom types for variant datasets and combiner instances'], 'classes': ['VariantDataset class'], 'functions': ['read_vds', 'store_ref_block_max_length', 'sample_qc', 'filter_chromosomes', 'filter_intervals', 'filter_samples', 'filter_variants', 'load_combiner', 'new_combiner']}"
hail/python/hail/builtin_references.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/builtin_references.py,"[{'name': 'Singleton', 'match_type': 'full', 'implemented_parts': ['Singleton instance', 'get_instance method'], 'confidence': 1.0, 'evidence': ['private constructor', 'static get_instance']}]",Defines built-in reference data and paths.,"{'constants': ['Mapping of reference resource names to paths'], 'functions': ['Retrieves reference resource path based on name']}"
hail/python/hail/conftest.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py,"[{'name': 'LD Score Regression', 'match_type': 'full', 'implemented_parts': ['LD score calculation', 'Regression analysis with phenotype data', 'Summaries of statistics for individual phenotypes'], 'confidence': 1.0, 'evidence': ['import of phenotype data', 'calculation of LD scores', 'fitting of linear models', 'summarization of results for each phenotype']}, {'name': 'Multi-Phenotype Analysis', 'match_type': 'partial', 'implemented_parts': ['Summary of LD scores across multiple phenotypes', 'Correlation analysis between phenotypes'], 'confidence': 0.9, 'evidence': ['reading of matrix table with phenotype data', 'calculation of correlations between phenotypes', 'annotation of rows with LD scores']}]",This file performs LD score regression analysis and summarizes the results for multiple phenotypes.,"{'constants': ['Defined in `doctest_namespace` variable'], 'types': ['Tables and matrices for phenotype and LD score data'], 'classes': [], 'functions': ['Functions for reading, summarizing, and analyzing data']}"
hail/python/hail/context.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py,"[{'name': 'Comparable', 'match_type': 'partial', 'implemented_parts': ['compare method'], 'confidence': 0.95, 'evidence': ['interface Comparable', 'equality check in compare']}, {'name': 'Iterator', 'match_type': 'full', 'implemented_parts': ['iterator method'], 'confidence': 1.0, 'evidence': ['generic type parameter T', 'next and peek methods']}]",This file defines utility functions for comparing and iterating over data structures.,"{'constants': ['Defines utility constants'], 'types': ['Custom types for iterating and comparing'], 'classes': ['None'], 'functions': ['compare', 'iterator']}"
hail/python/hail/hail_logging.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/hail_logging.py,"[{'name': 'Abstract Class', 'match_type': 'full', 'implemented_parts': ['Abstract method definitions'], 'confidence': 1.0, 'evidence': ['@abc.abstractmethod decorators']}, {'name': 'Concrete Implementation', 'match_type': 'partial', 'implemented_parts': ['Class inherits from abstract class', 'Concrete method implementations'], 'confidence': 0.9, 'evidence': ['inheritance from Logger', 'custom method implementations']}]",This file defines an abstract logging interface and provides a concrete implementation for Python.,"{'constants': [], 'types': ['Abstract class Logger'], 'classes': ['Concrete class PythonOnlyLogger'], 'functions': ['Defined methods for logging levels']}"
hail/python/hail/matrixtable.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py,"[{'name': 'Iterator', 'match_type': 'partial', 'implemented_parts': ['iterator object', 'next() method', '__iter__ method'], 'confidence': 0.9, 'evidence': ['class implements iterable protocol', 'next() fetches next element', '__iter__ returns iterator object']}, {'name': 'Factory Method', 'match_type': 'full', 'implemented_parts': ['factory function', 'creation of objects without using their constructors'], 'confidence': 1.0, 'evidence': ['factory function returns derived class instances', 'no explicit constructor in derived classes']}]",This file defines a generic iterator interface and concrete factory methods.,"{'constants': ['Defines constants related to iteration'], 'types': ['Iterator interface defines iteration behavior'], 'classes': ['AbstractIterator defines the iterator contract', 'Concrete factory methods create specific iterators'], 'functions': ['createIterator() function returns appropriate iterator', 'Iterator methods like next() and __iter__ enable iteration']}"
hail/python/hail/table.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py,"[{'name': 'Multi-Way Join', 'match_type': 'partial', 'implemented_parts': ['TableMultiWayZipJoin function', 'data_field_name and global_field_name arguments', 'ir.TableMultiWayZipJoin operator'], 'confidence': 0.95, 'evidence': ['iterates over multiple tables', 'uses join criteria from multiple fields', 'returns a joined table']}, {'name': 'Partitioning and Grouping', 'match_type': 'partial', 'implemented_parts': ['_group_within_partitions function', 'grouping based on a specified number of partitions', 'ir.TableMapPartitions operator'], 'confidence': 0.9, 'evidence': ['splits table data into partitions', 'groups rows within partitions based on a key', 'returns a transformed table']}]",This file implements functionalities for efficient data aggregation and joining.,"{'constants': [], 'types': ['Custom table type'], 'classes': ['Table class for representing data tables'], 'functions': ['Multi-way join function', 'Partitioning and grouping functions', 'Table manipulation methods']}"
hail/python/hail/__init__.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/__init__.py,"[{'name': 'Spark Context Creation', 'match_type': 'partial', 'implemented_parts': ['SparkContext object creation', 'tmp_dir usage'], 'confidence': 0.9, 'evidence': ['`spark_context` class', '`tmp_dir` directory manipulation']}, {'name': 'Data Access Functions', 'match_type': 'full', 'implemented_parts': ['`hadoop_open`', '`hadoop_copy`', '`hadoop_stat`'], 'confidence': 1.0, 'evidence': ['Functions for interacting with Hadoop files and directories']}, {'name': 'Data Manipulation and Analysis', 'match_type': 'partial', 'implemented_parts': ['`agg`', '`scan`'], 'confidence': 0.8, 'evidence': ['Aggregation and scanning data functionalities']}, {'name': 'ML and Statistics', 'match_type': 'full', 'implemented_parts': ['`genetics`', '`methods`', '`stats`'], 'confidence': 1.0, 'evidence': ['Machine learning and statistical analysis modules']}]",This file contains utilities for Spark-based data processing and analytics.,"{'constants': ['Defines constants related to Spark configuration'], 'types': ['Spark-related data structures'], 'classes': ['SparkContext class for interacting with Spark'], 'functions': ['Functions for data access, manipulation, analysis, and machine learning']}"
hail/python/hailtop/aiocloud/aioaws/fs.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py,"[{'name': 'Iterator pattern', 'match_type': 'partial', 'implemented_parts': ['__iter__ method', 'next method'], 'confidence': 0.95, 'evidence': ['class cons function', 'yields entries from iterator']}, {'name': 'URL parsing', 'match_type': 'full', 'implemented_parts': ['parse_url function'], 'confidence': 1.0, 'evidence': ['regex pattern for URL components']}, {'name': 'Concurrency utilities', 'match_type': 'partial', 'implemented_parts': ['blocking_to_async function', 'thread pool'], 'confidence': 0.85, 'evidence': ['async functions wrapped with blocking method', 'thread pool for parallel execution']}]","This file provides utilities for common tasks related to URL manipulation, concurrency, and asynchronous execution.","{'constants': ['Defines constants for URL components'], 'types': ['Custom types for URL parsing'], 'functions': ['parse_url', 'blocking_to_async'], 'classes': ['S3 class for interacting with Amazon S3 storage']}"
