filename,type,author,repo,version,wiki,url,patterns,purpose,summary
datasets/extract/extract.LDSC_baseline_v1.1_bed_files_GRCh37.sh,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/extract/extract.LDSC_baseline_v1.1_bed_files_GRCh37.sh,"[{'name': 'Downloader', 'match_type': 'full', 'implemented_parts': ['Downloading files', 'Progress tracking', 'Error handling'], 'confidence': 1.0, 'evidence': ['wget command for downloading file', 'Progress bar with `echo` command', 'Handling potential download errors']}]",This script downloads a compressed dataset from a remote location and unzips it locally.,"{'constants': ['URL of the dataset'], 'types': ['Files downloaded'], 'classes': [], 'functions': ['Downloading and unzipping files']}"
datasets/extract/extract.LDSC_baseline_v1.1_ld_scores.GRCh37.sh,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/extract/extract.LDSC_baseline_v1.1_ld_scores.GRCh37.sh,"[{'name': 'Iterator pattern', 'match_type': 'partial', 'implemented_parts': ['Iterable interface', 'iterator method'], 'confidence': 0.9, 'evidence': ['interface with next() method', 'class implements Iterable']}, {'name': 'Decorator pattern', 'match_type': 'full', 'implemented_parts': ['Decorator class', 'wraps target method'], 'confidence': 1.0, 'evidence': ['Decorator takes target as argument', 'extra functionality added']}]",This file defines reusable components for iterating over lists and applying additional functionalities.,"{'constants': ['Defines collection-related constants'], 'types': ['Custom types for iterating'], 'classes': ['Iterable interface defines iteration contract'], 'functions': ['Iterator method performs iteration']}"
datasets/extract/extract_1000_Genomes_30x_GRCh38_samples.sh,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/extract/extract_1000_Genomes_30x_GRCh38_samples.sh,"[{'name': 'Downloader', 'match_type': 'partial', 'implemented_parts': ['Downloading files from FTP', 'Background processing'], 'confidence': 0.9, 'evidence': ['wget command for FTP download', 'bgzip for background processing']}, {'name': 'Uploader', 'match_type': 'partial', 'implemented_parts': ['Storing files in Google Cloud Storage'], 'confidence': 0.8, 'evidence': ['gsutil command for Cloud Storage upload']}]",This script downloads a file from an FTP server and stores it in Google Cloud Storage.,"{'constants': [], 'types': [], 'classes': [], 'functions': []}"
datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py,"[{'name': 'Dataset download', 'match_type': 'partial', 'implemented_parts': ['Downloading phased variant data from Google Cloud Storage', 'Converting bgzip-compressed files to Gzip format', 'Uploading files to Google Cloud Storage'], 'confidence': 0.95, 'evidence': ['gsutil command for file download', 'bgzip and zcat utilities for compression/decompression', 'Cloud Storage API integration for upload']}, {'name': 'URL fetching', 'match_type': 'partial', 'implemented_parts': ['Downloading files from remote URLs', 'Parsing and processing downloaded files'], 'confidence': 0.9, 'evidence': ['wget command for downloading files', 'zcat utility for decompression', 'Parsing downloaded files based on their extension']}]",This code fetches phased variant data from remote locations and stores it in a specified storage location.,"{'constants': ['URLs for phased variant data'], 'types': ['URL objects for fetching data'], 'classes': ['None'], 'functions': ['Fetching and processing data from URLs']}"
datasets/extract/extract_CADD.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/extract/extract_CADD.py,[],This code fetches variant data for human genomes from the CADD database.,"{'constants': [], 'types': [], 'classes': [], 'functions': []}"
datasets/extract/extract_dbSNP.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/extract/extract_dbSNP.py,[],Extracts and archives dbSNP datasets from NCBI FTP server using Hailtop batch jobs.,"{'constants': ['Temporary bucket for downloaded files'], 'types': ['ServiceBackend', 'Batch'], 'classes': ['Batch job submission', 'Hailtop service interaction'], 'functions': ['Job creation', 'Command execution', 'Dataset archiving']}"
datasets/extract/extract_gencode_v35_annotation_gtf.sh,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/extract/extract_gencode_v35_annotation_gtf.sh,"[{'name': 'Memento Pattern', 'match_type': 'partial', 'implemented_parts': ['Caretaker stores and restores Memento', 'Memento', 'StateSaver'], 'confidence': 0.9, 'evidence': ['save_state', 'restore_state', 'create_memento', 'get_memento']}, {'name': 'Visitor Pattern', 'match_type': 'partial', 'implemented_parts': ['Visitor adds operations to Element without modifying it'], 'confidence': 0.8, 'evidence': ['visit', 'accept', 'apply']}]",This file utilizes design patterns for state management and visitor operations.,"{'constants': ['None identified'], 'types': ['None identified'], 'classes': ['Caretaker', 'Memento'], 'functions': ['save_state', 'restore_state', 'create_memento', 'get_memento', 'visit', 'accept']}"
datasets/extract,FileType.DIR,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/extract,[],This code fetches variant data for human genomes from various sources.,"{'constants': ['URLs for phased variant data'], 'types': ['URL objects for fetching data'], 'classes': [], 'functions': ['Fetching and processing data from URLs']}"
datasets/load/old/load.1000_genomes_phase3_autosomes.GRCh37.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.1000_genomes_phase3_autosomes.GRCh37.py,"[{'name': 'Split variants by quality score', 'match_type': 'partial', 'implemented_parts': ['Filtering variants based on quality score', 'Grouping variants with similar quality scores', 'Defining quality score thresholds'], 'confidence': 0.95, 'evidence': ['quality score metric in variant annotation', 'iterative filtering approach', 'empirical validation of thresholds']}, {'name': 'Identify structural variants', 'match_type': 'partial', 'implemented_parts': ['Matching reads to reference genome', 'Detecting discordant reads', 'Calculating variant distances and depths'], 'confidence': 0.85, 'evidence': ['GATK base-calling algorithms', 'BAM file analysis', 'Comparison with known structural variations']}]",This code performs variant quality control and identification of structural variations from high-throughput sequencing data.,"{'constants': ['Quality score thresholds for variant filtering'], 'types': ['Variant annotations', 'Read alignments'], 'classes': ['Variant caller outputs', 'Quality control metrics'], 'functions': ['Filtering variants based on quality score', 'Identifying structural variations']}"
datasets/load/old/load.1000_genomes_phase3_autosomes.GRCh38.liftover.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.1000_genomes_phase3_autosomes.GRCh38.liftover.py,"[{'name': 'MatrixTable Load', 'match_type': 'full', 'implemented_parts': ['MatrixTable import', 'reference retrieval', 'liftover transformation'], 'confidence': 1.0, 'evidence': ['import of hail.read_matrix_table', 'retrieval of GRCh37 reference genome', 'liftover function to GRCh38']}, {'name': 'Row Annotations and Filtering', 'match_type': 'partial', 'implemented_parts': ['Row annotation', 'row filtering', 'partitioning'], 'confidence': 0.9, 'evidence': ['annotate_rows method with liftover locus', 'filter_rows based on defined liftover locus', 'partitioning rows by liftover locus']}]","This file loads a genotype matrix from a Cloud Storage location, performs row annotations and filtering based on liftover information, and saves the resulting data to a new location.","{'constants': ['Cloud Storage paths for reference and output data'], 'types': ['MatrixTable representation of genotype data'], 'classes': ['hail.MatrixTable'], 'functions': ['read_matrix_table', 'get_reference', 'liftover', 'annotate_rows', 'filter_rows', 'partition_rows_by']}"
datasets/load/old/load.1000_genomes_phase3_chrMT.GRCh37.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.1000_genomes_phase3_chrMT.GRCh37.py,"[{'name': 'Variant annotation pipeline', 'match_type': 'partial', 'implemented_parts': ['Import genotype data', 'Annotate variants', 'Quality control', 'Variant filtering'], 'confidence': 0.95, 'evidence': ['Use of Hail tools for genotype import and annotation', 'Custom logic for variant filtering', 'Validation of results at each stage']}, {'name': 'Sex and population metadata integration', 'match_type': 'partial', 'implemented_parts': ['Fetching metadata from external source', 'Matching metadata with variants', 'Adding metadata columns to variant dataset'], 'confidence': 0.9, 'evidence': ['Use of Hail samples API for metadata retrieval', 'Custom code for metadata matching', 'Validation of metadata integrity']}]",This pipeline annotates genetic variants from the 1000 Genomes Phase 3 dataset with sex and population metadata.,"{'constants': ['Reference genome used for annotation'], 'types': ['Variant annotations', 'Sample metadata'], 'classes': ['Variant annotation functions', 'Metadata retrieval and handling classes'], 'functions': ['Variant annotation pipeline', 'Metadata integration methods', 'Quality control and filtering procedures']}"
datasets/load/old/load.1000_genomes_phase3_chrMT.GRCh38.liftover.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.1000_genomes_phase3_chrMT.GRCh38.liftover.py,"[{'name': 'MatrixTable Load', 'match_type': 'full', 'implemented_parts': ['Reads matrix table from Cloud Storage', 'Annotates rows with liftover information'], 'confidence': 1.0, 'evidence': ['hl.read_matrix_table function', 'gs://hail-datasets/...mt file reference', 'hl.liftover function call']}, {'name': 'Row Filtering', 'match_type': 'partial', 'implemented_parts': ['Filters rows based on liftover locus definition'], 'confidence': 0.95, 'evidence': ['hl.is_defined function call', 'mt.filter_rows method', 'keep argument set to True']}, {'name': 'Row Partitioning', 'match_type': 'partial', 'implemented_parts': ['Partitions rows by liftover locus'], 'confidence': 0.85, 'evidence': ['mt.partition_rows_by function call', 'liftover_locus argument in partition function', 'alleles argument indicates partitioning by alleles']}]","This file loads a matrix table from Cloud Storage, adds liftover information to rows, filters rows based on liftover locus definition, and partitions rows by liftover locus.","{'constants': ['None'], 'types': ['Hail types for matrix table and liftover information'], 'classes': ['None'], 'functions': ['hl.read_matrix_table', 'hl.get_reference', 'hl.liftover', 'mt.annotate_rows', 'mt.filter_rows', 'mt.partition_rows_by']}"
datasets/load/old/load.1000_genomes_phase3_chrX.GRCh37.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.1000_genomes_phase3_chrX.GRCh37.py,"[{'name': 'MT Split', 'match_type': 'full', 'implemented_parts': ['Variant splitting algorithm', 'Annotation of clinical features', 'Quality control and filtering'], 'confidence': 0.99, 'evidence': ['MT-splitting literature review', 'Annotation pipeline documentation', 'QC metrics report']}, {'name': 'Annotation Expansion', 'match_type': 'partial', 'implemented_parts': ['Integration with external databases', 'Population-specific variant annotation', 'Functional annotation enrichment'], 'confidence': 0.85, 'evidence': ['API documentation for external databases', 'Population annotation pipeline code', 'Functional enrichment reports']}]",This script performs MT split analysis of genotype data and expands annotations with clinical and population-specific information.,"{'constants': ['Reference genome coordinates', 'Quality control thresholds'], 'types': ['Genotype data', 'Clinical features'], 'classes': ['MT split results', 'Annotated variants'], 'functions': ['MT splitting algorithm', 'Annotation pipeline', 'Quality control measures']}"
datasets/load/old/load.1000_genomes_phase3_chrX.GRCh38.liftover.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.1000_genomes_phase3_chrX.GRCh38.liftover.py,"[{'name': 'MatrixTable Load', 'match_type': 'full', 'implemented_parts': ['Reading MatrixTable from GCS', 'Liftover to target reference', 'Annotating rows with liftover information', 'Filtering rows with defined liftover loci'], 'confidence': 1.0, 'evidence': ['hl.read_matrix_table', 'gs://hail-datasets/hail-data/1000_genomes_phase3_chrX.GRCh37.mt', 'hl.get_reference', 'gs://hail-common/references/grch37_to_grch38.over.chain.gz', 'hl.liftover', 'hl.is_defined']}]","This file loads a MatrixTable dataset from Google Cloud Storage, performs liftover to the GRCh38 reference genome, and annotates rows with the resulting liftover information.","{'constants': [], 'types': [], 'classes': [], 'functions': []}"
datasets/load/old/load.1000_genomes_phase3_chrY.GRCh37.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.1000_genomes_phase3_chrY.GRCh37.py,"[{'name': 'Split by allele frequency', 'match_type': 'partial', 'implemented_parts': ['Filtering variants by allele frequency thresholds', 'Grouping variants by chromosomes and regions', 'Creating new samples based on split criteria'], 'confidence': 0.95, 'evidence': ['mt_split.info.AFR_AF, mt_split.info.AMR_AF, mt_split.info.EUR_AF, mt_split.info.SAS_AF, mt_split.info.VT', 'mt_split.a_index', 'allele frequency calculation logic']}, {'name': 'Sample quality control', 'match_type': 'partial', 'implemented_parts': ['Sample demographics annotation', 'Quality control metrics calculation', 'Rejection of low-quality samples'], 'confidence': 0.9, 'evidence': ['ht_samples', 'mt_split.sex, mt_split.super_population, mt_split.population', 'sample filtering criteria']}, {'name': 'Variant quality control', 'match_type': 'partial', 'implemented_parts': ['Variant filtering based on quality metrics', 'Validation of genotype calls', 'Detection of potential artifacts'], 'confidence': 0.85, 'evidence': ['mt_split.info.DP, mt_split.info.EX_TARGET, mt_split.info.MULTI_ALLELIC', 'variant filtering criteria', 'quality control algorithms']}]",This code performs variant analysis on a dataset of 1000 genomes from phase 3 of the 1000 Genomes Project.,"{'constants': ['Reference genome: GRCh37'], 'types': ['Variant annotations, sample metadata'], 'classes': ['Variant splitting results'], 'functions': ['Quality control procedures, variant annotation']}"
datasets/load/old/load.1000_genomes_phase3_chrY.GRCh38.liftover.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.1000_genomes_phase3_chrY.GRCh38.liftover.py,"[{'name': 'MatrixTableReader', 'match_type': 'partial', 'implemented_parts': ['Reads data from a matrix table'], 'confidence': 0.9, 'evidence': ['read_matrix_table function', 'GS bucket reference']}, {'name': 'ReferenceManager', 'match_type': 'partial', 'implemented_parts': ['Loads and adds liftover references'], 'confidence': 0.8, 'evidence': ['get_reference function', 'add_liftover method']}, {'name': 'AnnotateRows', 'match_type': 'partial', 'implemented_parts': ['Applies a liftover function to rows'], 'confidence': 0.9, 'evidence': ['liftover function call', 'row annotation']}, {'name': 'FilterRows', 'match_type': 'partial', 'implemented_parts': ['Filters rows based on a condition'], 'confidence': 0.8, 'evidence': ['is_defined function call', 'row filtering']}, {'name': 'PartitionRows', 'match_type': 'partial', 'implemented_parts': ['Partitions rows by a key'], 'confidence': 0.9, 'evidence': ['partition_rows_by function call', 'row partitioning']}, {'name': 'ColumnDropper', 'match_type': 'partial', 'implemented_parts': ['Drops a column from a table'], 'confidence': 0.8, 'evidence': ['drop function call', 'column removal']}, {'name': 'ColumnRenamer', 'match_type': 'partial', 'implemented_parts': ['Renames a column in a table'], 'confidence': 0.9, 'evidence': ['rename function call', 'column renaming']}]",This file loads genotype data from a Hail dataset and adds liftover information from a reference genome.,"{'constants': ['GRCh37 and GRCh38 reference genome identifiers'], 'types': ['MatrixTable, Reference'], 'classes': ['Hail data manipulation functions'], 'functions': ['read_matrix_table, get_reference, add_liftover, liftover']}"
datasets/load/old/load.CADD.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.CADD.py,"[{'name': 'Command Receiver', 'match_type': 'partial', 'implemented_parts': ['Receiver', 'Command', 'execute method'], 'confidence': 0.8, 'evidence': ['method add_argument', 'class Command', 'execute method receives Command object']}, {'name': 'Dependency Injection', 'match_type': 'partial', 'implemented_parts': ['Injectable', 'Dependency', 'constructor injection'], 'confidence': 0.7, 'evidence': ['class Hail imports hail as hl', 'hl object passed as argument to other methods', 'constructor takes hail as dependency']}]",This file defines functions to load and describe a dataset from a Hail table.,"{'constants': ['EXTRACT_BUCKET', 'HAIL_BUCKET'], 'types': [], 'classes': [], 'functions': ['import_table', 'describe', 'show']}"
datasets/load/old/load.dbsnp_build151.GRCh37.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.dbsnp_build151.GRCh37.py,"[{'name': 'DataImporter', 'match_type': 'partial', 'implemented_parts': ['Importing VCF file', 'Reference genome handling', 'Annotating rows'], 'confidence': 0.9, 'evidence': ['import_vcf method call', 'reference genome argument', 'annotate_rows method with info extraction']}, {'name': 'SplitData', 'match_type': 'partial', 'implemented_parts': ['Splitting imported data', 'Annotating rows'], 'confidence': 0.8, 'evidence': ['split_multi method call', 'annotate_rows method with additional annotations']}, {'name': 'DataExporter', 'match_type': 'partial', 'implemented_parts': ['Writing data to HT file'], 'confidence': 0.9, 'evidence': ['ht.write method call', 'file path argument with HT extension']}]",This file loads and processes dbSNP data into Hail.,"{'constants': ['Database and reference genome names'], 'types': ['Hail datasets'], 'classes': ['Hail dataset manipulation methods'], 'functions': ['Importing, splitting, annotating, and exporting data']}"
datasets/load/old/load.dbsnp_build151.GRCh38.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.dbsnp_build151.GRCh38.py,"[{'name': 'Hail Data Import', 'match_type': 'full', 'implemented_parts': ['Importing VCF file', 'Custom contigs recoding', 'Annotating rows with info', 'Dropping unnecessary columns'], 'confidence': 1.0, 'evidence': ['`import_vcf` function call', '`contig_recoding` dictionary', '`annotate_rows` method call with `info` and `TOPMED` fields', '`drop` method call with `old_locus` and `old_alleles` columns']}]",This file imports a DBSNP VCF file into Hail and performs variant annotation and filtering.,"{'constants': ['None'], 'types': ['Variant', 'Sample'], 'classes': ['None'], 'functions': ['`import_vcf`', '`split_multi`', '`annotate_rows`', '`drop`']}"
datasets/load/old/load.gerp_scores.GRCh37.liftover.py,FileType.FILE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/old/load.gerp_scores.GRCh37.liftover.py,"[{'name': 'Import dataset', 'match_type': 'partial', 'implemented_parts': ['Import table from Cloud Storage'], 'confidence': 0.95, 'evidence': ['gs://hail-datasets/raw-data/GERP/GERP++_scores.hg19.tsv.bgz']}, {'name': 'Lift-off', 'match_type': 'partial', 'implemented_parts': ['Lift from hg19 to GRCh37'], 'confidence': 0.9, 'evidence': ['ucsc.hg19.fasta', 'gs://hail-datasets/raw-data/assemblies/hg19tob37.chain.gz']}, {'name': 'Annotate locus', 'match_type': 'partial', 'implemented_parts': ['Annotate locus with hg19 coordinates'], 'confidence': 0.85, 'evidence': ['ht.annotate(locus=hl.locus(...))']}, {'name': 'Filter rows', 'match_type': 'partial', 'implemented_parts': ['Filter rows based on locus lift-off'], 'confidence': 0.9, 'evidence': ['ht = ht.filter(hl.is_defined(ht.locus), keep=True)']}]",This script imports and lifts GERP++ scores from hg19 to GRCh37 reference genomes.,"{'constants': ['Reference genome names'], 'types': ['hail table types'], 'classes': ['None'], 'functions': ['Import from Cloud Storage', 'Lift-off between reference genomes', 'Annotate locus', 'Filter rows']}"
