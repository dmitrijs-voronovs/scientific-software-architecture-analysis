id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:1215,Safety,avoid,avoiding,1215,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:545,Usability,guid,guide,545,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:1491,Usability,guid,guide,1491,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:618,Availability,down,downstream,618,"Hi Avi,. Thanks for the detailed reply. I was able to run it (see logs below), but I had to use `ISR`, not `ISF` to get it to work. I also had to add these two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:3175,Performance,optimiz,optimizer,3175,alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 12:26:05.872] [alevinLog] [info] Total [32m95377[0m(has [32m101[0m low confidence) barcodes; > [2020-06-04 12:26:06.746] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 12:26:06.880] [alevinLog] [info] Total 1.2299% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 12:26:10.886] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 12:26:10.924] [alevinLog] [info] Total 118774 CB got sequence corrected; > [2020-06-04 12:26:10.936] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 12:26:10.936] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:26:10.936] [alevinLog] [info] Used Barcodes except Whitelist: 88156; > [2020-06-04 12:26:11.113] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:26:11.113] [alevinLog] [info] parsing read library format; > [2020-06-04 12:27:21.373] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:27:22.086] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.086] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 12:27:22.412] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:27:22.418] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:27:22.418] [alevinLog] [info] Finished optimizer. Run 2: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcod,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:3944,Performance,perform,performing,3944,"[alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:26:11.113] [alevinLog] [info] parsing read library format; > [2020-06-04 12:27:21.373] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:27:22.086] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.086] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 12:27:22.412] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:27:22.418] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:27:22.418] [alevinLog] [info] Finished optimizer. Run 2: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 200000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > [2020-06-04 12:40:45.455] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:40:45.456] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:40:45.461] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:42:01.202] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:42:01.202] [alevinLog] [info] # Barcodes Used: [32m52200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:4035,Performance,optimiz,optimizer,4035,"ng to Quantify; > ; > [2020-06-04 12:26:11.113] [alevinLog] [info] parsing read library format; > [2020-06-04 12:27:21.373] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:27:22.086] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.086] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 12:27:22.412] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:27:22.418] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:27:22.418] [alevinLog] [info] Finished optimizer. Run 2: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 200000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > [2020-06-04 12:40:45.455] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:40:45.456] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:40:45.461] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:42:01.202] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:42:01.202] [alevinLog] [info] # Barcodes Used: [32m52200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing to use 200000 cells; > [2020-06-04 12:42:02.037] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:6012,Performance,optimiz,optimizer,6012,200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing to use 200000 cells; > [2020-06-04 12:42:02.037] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 12:42:02.738] [alevinLog] [info] Total [32m197328[0m(has [32m101[0m low confidence) barcodes; > [2020-06-04 12:42:03.656] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 12:42:03.830] [alevinLog] [info] Total 0.780192% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:42:13.353] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-04 12:42:13.555] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:42:13.556] [alevinLog] [info] parsing read library format; > [2020-06-04 12:43:22.789] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:43:23.499] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.499] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 24009.00 UMI after deduplicating.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 89 BiDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [warning] Skipped 184123 barcodes due to No mapped read; > [2020-06-04 12:43:23.840] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:43:23.846] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:43:23.846] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:6782,Performance,perform,performing,6782,200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing to use 200000 cells; > [2020-06-04 12:42:02.037] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 12:42:02.738] [alevinLog] [info] Total [32m197328[0m(has [32m101[0m low confidence) barcodes; > [2020-06-04 12:42:03.656] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 12:42:03.830] [alevinLog] [info] Total 0.780192% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:42:13.353] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-04 12:42:13.555] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:42:13.556] [alevinLog] [info] parsing read library format; > [2020-06-04 12:43:22.789] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:43:23.499] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.499] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 24009.00 UMI after deduplicating.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 89 BiDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [warning] Skipped 184123 barcodes due to No mapped read; > [2020-06-04 12:43:23.840] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:43:23.846] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:43:23.846] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:6873,Performance,optimiz,optimizer,6873,200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing to use 200000 cells; > [2020-06-04 12:42:02.037] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 12:42:02.738] [alevinLog] [info] Total [32m197328[0m(has [32m101[0m low confidence) barcodes; > [2020-06-04 12:42:03.656] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 12:42:03.830] [alevinLog] [info] Total 0.780192% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:42:13.353] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-04 12:42:13.555] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:42:13.556] [alevinLog] [info] parsing read library format; > [2020-06-04 12:43:22.789] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:43:23.499] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.499] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 24009.00 UMI after deduplicating.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 89 BiDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [warning] Skipped 184123 barcodes due to No mapped read; > [2020-06-04 12:43:23.840] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:43:23.846] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:43:23.846] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:66,Testability,log,logs,66,"Hi Avi,. Thanks for the detailed reply. I was able to run it (see logs below), but I had to use `ISR`, not `ISF` to get it to work. I also had to add these two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:526,Testability,log,log,526,"Hi Avi,. Thanks for the detailed reply. I was able to run it (see logs below), but I had to use `ISR`, not `ISF` to get it to work. I also had to add these two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:1180,Testability,test,testing,1180,"two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alevinLog] [info] # Barcodes Used: [32m52200250[0m / [31m52200250[0m.; > [2020-06-04 12:26:04.435] [alevinLog] [info] Forcing to use 100000 cells; > [2020-",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:324,Usability,guid,guide,324,"Hi Avi,. Thanks for the detailed reply. I was able to run it (see logs below), but I had to use `ISR`, not `ISF` to get it to work. I also had to add these two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:3804,Usability,Clear,Clearing,3804,"] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:26:10.936] [alevinLog] [info] Used Barcodes except Whitelist: 88156; > [2020-06-04 12:26:11.113] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:26:11.113] [alevinLog] [info] parsing read library format; > [2020-06-04 12:27:21.373] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:27:22.086] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.086] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 12:27:22.412] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:27:22.418] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:27:22.418] [alevinLog] [info] Finished optimizer. Run 2: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 200000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > [2020-06-04 12:40:45.455] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:40:45.456] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:40:45.461] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:42:01.202] [alevinLog] [info] Done barcode density calculation.; ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:6642,Usability,Clear,Clearing,6642,200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing to use 200000 cells; > [2020-06-04 12:42:02.037] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 12:42:02.738] [alevinLog] [info] Total [32m197328[0m(has [32m101[0m low confidence) barcodes; > [2020-06-04 12:42:03.656] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 12:42:03.830] [alevinLog] [info] Total 0.780192% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:42:13.353] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-04 12:42:13.555] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:42:13.556] [alevinLog] [info] parsing read library format; > [2020-06-04 12:43:22.789] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:43:23.499] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.499] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 24009.00 UMI after deduplicating.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 89 BiDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [warning] Skipped 184123 barcodes due to No mapped read; > [2020-06-04 12:43:23.840] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:43:23.846] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:43:23.846] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220:857,Safety,detect,detect,857,"Hi Rob,. The joint distribution of the read and UMI counts can contain important information. The majority of observations (CB + guide combination) lie along a well defined experiment-specific mean trend whose slope is given by the coverage ( ratio of reads to UMIs). Also the same regularity can be observed when aggregating across the cell barcodes. See figure below. The points below the black horizontal line are cells with less than 100 reads. ![image](https://user-images.githubusercontent.com/9895004/83791774-30937080-a668-11ea-9b44-937ba8f69b34.png). At the guide level, it would look like this . ![image](https://user-images.githubusercontent.com/9895004/83792096-941d9e00-a668-11ea-9b26-976332f639fe.png). In general, I often find myself needing to work with read counts. For example, the read counts can be used to estimate the hopping rate and detect hopped reads in multiplexed scRNAseq data as we show in this recent paper https://www.nature.com/articles/s41467-020-16522-z . Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220:129,Usability,guid,guide,129,"Hi Rob,. The joint distribution of the read and UMI counts can contain important information. The majority of observations (CB + guide combination) lie along a well defined experiment-specific mean trend whose slope is given by the coverage ( ratio of reads to UMIs). Also the same regularity can be observed when aggregating across the cell barcodes. See figure below. The points below the black horizontal line are cells with less than 100 reads. ![image](https://user-images.githubusercontent.com/9895004/83791774-30937080-a668-11ea-9b44-937ba8f69b34.png). At the guide level, it would look like this . ![image](https://user-images.githubusercontent.com/9895004/83792096-941d9e00-a668-11ea-9b26-976332f639fe.png). In general, I often find myself needing to work with read counts. For example, the read counts can be used to estimate the hopping rate and detect hopped reads in multiplexed scRNAseq data as we show in this recent paper https://www.nature.com/articles/s41467-020-16522-z . Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220:567,Usability,guid,guide,567,"Hi Rob,. The joint distribution of the read and UMI counts can contain important information. The majority of observations (CB + guide combination) lie along a well defined experiment-specific mean trend whose slope is given by the coverage ( ratio of reads to UMIs). Also the same regularity can be observed when aggregating across the cell barcodes. See figure below. The points below the black horizontal line are cells with less than 100 reads. ![image](https://user-images.githubusercontent.com/9895004/83791774-30937080-a668-11ea-9b44-937ba8f69b34.png). At the guide level, it would look like this . ![image](https://user-images.githubusercontent.com/9895004/83792096-941d9e00-a668-11ea-9b26-976332f639fe.png). In general, I often find myself needing to work with read counts. For example, the read counts can be used to estimate the hopping rate and detect hopped reads in multiplexed scRNAseq data as we show in this recent paper https://www.nature.com/articles/s41467-020-16522-z . Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639020241:133,Security,access,access,133,"Thanks for the detailed answer, Rick! . I just saw that paper pop up yesterday and it was on my reading list :). Internally, we have access to the number of occurrences of each UMI, gene pair within each barcode, so I do not think it would be too difficult to to provide read counts (optionally) along with deduplicated counts (though @k3yavi would be best equipped to say how easy or difficult this would be from the implementation perspective). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639020241
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:1151,Availability,down,down,1151,"h the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read coun",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:1841,Availability,down,downstream,1841,"ingle-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the --dumpEq or --dumpBfh flags? Can tximport be used for this or do I need to use the Python parser first?. Congratulations on the awesome paper :). We were actually discussing yesterday about your paper and potentially modifying alevin to include model for correcting index-hoping, although it's still in discussion phase. To answer your question, thanks for the feature request, I can add that feature on the weekend if it's urgent. However, you can also generate that with the current version using the `--dumpBfh` flag and `bfh.txt` file. Unfortunately, since it's the first use case where we need the read-count matrix, we haven't included the support in `tximport` yet. However, like you said one can use the [read_bfh](https://g",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:764,Testability,log,log,764,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:1245,Testability,log,logs,1245," sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the --dumpEq or --dumpBfh flags? Can tximport be used for this or do I need to use the Python parser firs",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:1669,Testability,test,testing,1669,"ncreasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the --dumpEq or --dumpBfh flags? Can tximport be used for this or do I need to use the Python parser first?. Congratulations on the awesome paper :). We were actually discussing yesterday about your paper and potentially modifying alevin to include model for correcting index-hoping, although it's still in discussion phase. To answer your question, thanks for the feature request, I can add that feature on the weekend if it's urgent. However, you can also generate that with the current version using the `--dumpBfh` flag and `bfh",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:142,Usability,guid,guide,142,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:293,Usability,guid,guide,293,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:423,Usability,guid,guideRNA,423,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:562,Usability,guid,guide,562,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909:1184,Energy Efficiency,efficient,efficiently,1184,"Hi Avi,. Yes I just asked and the guide sequences were reverse complemented. I was looking through the results and comparing it with the output of another alignment software. I noticed that there are substantially fewer UMI per guide (in cell) throughout ( see figures for comparison). . ![image](https://user-images.githubusercontent.com/9895004/83803410-7eb16f80-a67a-11ea-832d-562c88dafef3.png) ; ![image](https://user-images.githubusercontent.com/9895004/83803427-8709aa80-a67a-11ea-9ea4-f66ca447a65c.png). Also, the number of UMIs per cell barcode is consistently lower and there is around 796 barcodes that are not found in the 10X whitelist, the majority of which tend to have 1 UMI count only. Here is tally, where the TRUE column indicates the barcode is found in the whitelist. The row names indicate the total number of UMIs; ; ![image](https://user-images.githubusercontent.com/9895004/83803984-7279e200-a67b-11ea-8578-fc863f94f714.png). It would be great if you can implement the index hopping correction in Alevin. The software we have works fine if the number of samples is not too large. If had known how to code in C++, I would have implemented part of the code more efficiently using Rcpp. Please let me know if you ever decide to add this feature to Salmon. I am more than happy to help. Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909:34,Usability,guid,guide,34,"Hi Avi,. Yes I just asked and the guide sequences were reverse complemented. I was looking through the results and comparing it with the output of another alignment software. I noticed that there are substantially fewer UMI per guide (in cell) throughout ( see figures for comparison). . ![image](https://user-images.githubusercontent.com/9895004/83803410-7eb16f80-a67a-11ea-832d-562c88dafef3.png) ; ![image](https://user-images.githubusercontent.com/9895004/83803427-8709aa80-a67a-11ea-9ea4-f66ca447a65c.png). Also, the number of UMIs per cell barcode is consistently lower and there is around 796 barcodes that are not found in the 10X whitelist, the majority of which tend to have 1 UMI count only. Here is tally, where the TRUE column indicates the barcode is found in the whitelist. The row names indicate the total number of UMIs; ; ![image](https://user-images.githubusercontent.com/9895004/83803984-7279e200-a67b-11ea-8578-fc863f94f714.png). It would be great if you can implement the index hopping correction in Alevin. The software we have works fine if the number of samples is not too large. If had known how to code in C++, I would have implemented part of the code more efficiently using Rcpp. Please let me know if you ever decide to add this feature to Salmon. I am more than happy to help. Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909:228,Usability,guid,guide,228,"Hi Avi,. Yes I just asked and the guide sequences were reverse complemented. I was looking through the results and comparing it with the output of another alignment software. I noticed that there are substantially fewer UMI per guide (in cell) throughout ( see figures for comparison). . ![image](https://user-images.githubusercontent.com/9895004/83803410-7eb16f80-a67a-11ea-832d-562c88dafef3.png) ; ![image](https://user-images.githubusercontent.com/9895004/83803427-8709aa80-a67a-11ea-9ea4-f66ca447a65c.png). Also, the number of UMIs per cell barcode is consistently lower and there is around 796 barcodes that are not found in the 10X whitelist, the majority of which tend to have 1 UMI count only. Here is tally, where the TRUE column indicates the barcode is found in the whitelist. The row names indicate the total number of UMIs; ; ![image](https://user-images.githubusercontent.com/9895004/83803984-7279e200-a67b-11ea-8578-fc863f94f714.png). It would be great if you can implement the index hopping correction in Alevin. The software we have works fine if the number of samples is not too large. If had known how to code in C++, I would have implemented part of the code more efficiently using Rcpp. Please let me know if you ever decide to add this feature to Salmon. I am more than happy to help. Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897:828,Deployability,integrat,integration,828,"Hi @rfarouni ,. Is it possible to visualize the above two plots on the same scale ? ; Regarding the few cells not from 10x whitelist, I should have been more clear last time. ; Basically, what I meant earlier when I said that 10x data is clean is that we do observe some cells from the non whitelist file _but_ they have very few UMI and we discard them anyway. I am guessing here your motivation is a bit different i.e. considering very low confidence (even with 1 UMI) barcodes, while generally we discard anything below 10 as noise. Thanks a lot for offering to help with index-hopping idea. I agree, it'd be great to include the model in the alevin framework. Currently I just got the gist of your paper, let us go through the paper in a bit more detail and we'll get back to you as soon as we have some free cycles for the integration.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897:828,Integrability,integrat,integration,828,"Hi @rfarouni ,. Is it possible to visualize the above two plots on the same scale ? ; Regarding the few cells not from 10x whitelist, I should have been more clear last time. ; Basically, what I meant earlier when I said that 10x data is clean is that we do observe some cells from the non whitelist file _but_ they have very few UMI and we discard them anyway. I am guessing here your motivation is a bit different i.e. considering very low confidence (even with 1 UMI) barcodes, while generally we discard anything below 10 as noise. Thanks a lot for offering to help with index-hopping idea. I agree, it'd be great to include the model in the alevin framework. Currently I just got the gist of your paper, let us go through the paper in a bit more detail and we'll get back to you as soon as we have some free cycles for the integration.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897:158,Usability,clear,clear,158,"Hi @rfarouni ,. Is it possible to visualize the above two plots on the same scale ? ; Regarding the few cells not from 10x whitelist, I should have been more clear last time. ; Basically, what I meant earlier when I said that 10x data is clean is that we do observe some cells from the non whitelist file _but_ they have very few UMI and we discard them anyway. I am guessing here your motivation is a bit different i.e. considering very low confidence (even with 1 UMI) barcodes, while generally we discard anything below 10 as noise. Thanks a lot for offering to help with index-hopping idea. I agree, it'd be great to include the model in the alevin framework. Currently I just got the gist of your paper, let us go through the paper in a bit more detail and we'll get back to you as soon as we have some free cycles for the integration.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:81,Testability,log,log,81,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:109,Testability,log,logs,109,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:201,Testability,log,log,201,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:221,Testability,log,logs,221,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:254,Testability,log,log,254,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:136,Usability,clear,clear,136,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:1796,Performance,optimiz,optimizer,1796,levinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 17:56:25.388] [alevinLog] [info] Total [32m95377[0m(has [32m11[0m low confidence) barcodes; > [2020-06-04 17:56:25.577] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 17:56:25.698] [alevinLog] [info] Total 1.2299% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 17:56:29.508] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 17:56:29.545] [alevinLog] [info] Total 118774 CB got sequence corrected; > [2020-06-04 17:56:29.557] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 17:56:29.557] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 17:56:29.557] [alevinLog] [info] Used Barcodes except Whitelist: 88156; > [2020-06-04 17:56:30.294] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 17:56:30.294] [alevinLog] [info] parsing read library format; > [2020-06-04 17:57:36.339] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-04 17:57:37.051] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.051] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 17:57:37.341] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 17:57:37.348] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.7,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:2568,Performance,perform,performing,2568,"evinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 17:56:30.294] [alevinLog] [info] parsing read library format; > [2020-06-04 17:57:36.339] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-04 17:57:37.051] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.051] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 17:57:37.341] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 17:57:37.348] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.700] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:2659,Performance,optimiz,optimizer,2659,"uantify; > ; > [2020-06-04 17:56:30.294] [alevinLog] [info] parsing read library format; > [2020-06-04 17:57:36.339] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-04 17:57:37.051] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.051] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 17:57:37.341] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 17:57:37.348] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.700] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:3884,Performance,Load,Loading,3884,"ompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; > [2020-06-04 17:55:11.700] [jointLog] [info] Using default value of 0.797619 for minScoreFraction in Alevin; > Using default value of 0.6 for consensusSlack in Alevin; > [2020-06-04 17:56:30.294] [jointLog] [info] There is 1 library.; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading pufferfish index; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading dense pufferfish index.; > [2020-06-04 17:56:30.355] [jointLog] [info] done; > [2020-06-04 17:56:30.355] [jointLog] [info] Index contained 64 targets; > [2020-06-04 17:56:30.355] [jointLog] [info] Number of decoys : 0; > [2020-06-04 17:57:36.305] [jointLog] [info] Computed 64 rich equivalence classes for further processing; > [2020-06-04 17:57:36.305] [jointLog] [info] Counted 39,818 total reads in the equivalence classes ; > [2020-06-04 17:57:36.305] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0; > [2020-06-04 17:57:36.305] [jointLog] [warning] Found 1354 reads with `N` in the UMI sequence and ignored the reads.; > Please report on github if this number is too large; > [2020-06-04 17:57:36.305] [jointLog] [info] Mapping rate = 0.0762793%; > ; > [2020-06-04 17:57:36.305] [jointLog] [info] finished quantifyLibrary()",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:3956,Performance,Load,Loading,3956,"ompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; > [2020-06-04 17:55:11.700] [jointLog] [info] Using default value of 0.797619 for minScoreFraction in Alevin; > Using default value of 0.6 for consensusSlack in Alevin; > [2020-06-04 17:56:30.294] [jointLog] [info] There is 1 library.; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading pufferfish index; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading dense pufferfish index.; > [2020-06-04 17:56:30.355] [jointLog] [info] done; > [2020-06-04 17:56:30.355] [jointLog] [info] Index contained 64 targets; > [2020-06-04 17:56:30.355] [jointLog] [info] Number of decoys : 0; > [2020-06-04 17:57:36.305] [jointLog] [info] Computed 64 rich equivalence classes for further processing; > [2020-06-04 17:57:36.305] [jointLog] [info] Counted 39,818 total reads in the equivalence classes ; > [2020-06-04 17:57:36.305] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0; > [2020-06-04 17:57:36.305] [jointLog] [warning] Found 1354 reads with `N` in the UMI sequence and ignored the reads.; > Please report on github if this number is too large; > [2020-06-04 17:57:36.305] [jointLog] [info] Mapping rate = 0.0762793%; > ; > [2020-06-04 17:57:36.305] [jointLog] [info] finished quantifyLibrary()",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:3028,Security,validat,validation,3028,"ovided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 17:57:37.341] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 17:57:37.348] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.700] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; > [2020-06-04 17:55:11.700] [jointLog] [info] Using default value of 0.797619 for minScoreFraction in Alevin; > Using default value of 0.6 for consensusSlack in Alevin; > [2020-06-04 17:56:30.294] [jointLog] [info] There is 1 library.; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading pufferfish index; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading dense pufferfish index.; > [2020-0",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:3042,Security,validat,validateMappings,3042,"ovided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 17:57:37.341] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 17:57:37.348] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.700] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; > [2020-06-04 17:55:11.700] [jointLog] [info] Using default value of 0.797619 for minScoreFraction in Alevin; > Using default value of 0.6 for consensusSlack in Alevin; > [2020-06-04 17:56:30.294] [jointLog] [info] There is 1 library.; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading pufferfish index; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading dense pufferfish index.; > [2020-0",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:3078,Security,validat,validation,3078," [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 17:57:37.341] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 17:57:37.348] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.700] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; > [2020-06-04 17:55:11.700] [jointLog] [info] Using default value of 0.797619 for minScoreFraction in Alevin; > Using default value of 0.6 for consensusSlack in Alevin; > [2020-06-04 17:56:30.294] [jointLog] [info] There is 1 library.; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading pufferfish index; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading dense pufferfish index.; > [2020-06-04 17:56:30.355] [jointLog] [info] done; > [2020-06-04 17:56:30.355] [joint",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:3148,Security,validat,validateMappings,3148,"37.338] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 17:57:37.341] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 17:57:37.348] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.700] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; > [2020-06-04 17:55:11.700] [jointLog] [info] Using default value of 0.797619 for minScoreFraction in Alevin; > Using default value of 0.6 for consensusSlack in Alevin; > [2020-06-04 17:56:30.294] [jointLog] [info] There is 1 library.; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading pufferfish index; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading dense pufferfish index.; > [2020-06-04 17:56:30.355] [jointLog] [info] done; > [2020-06-04 17:56:30.355] [jointLog] [info] Index contained 64 targets; > [2020-06-04 17:56:30.355] [jointLog] [i",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:3510,Security,validat,validateMappings,3510," Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.700] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; > [2020-06-04 17:55:11.700] [jointLog] [info] Using default value of 0.797619 for minScoreFraction in Alevin; > Using default value of 0.6 for consensusSlack in Alevin; > [2020-06-04 17:56:30.294] [jointLog] [info] There is 1 library.; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading pufferfish index; > [2020-06-04 17:56:30.355] [jointLog] [info] Loading dense pufferfish index.; > [2020-06-04 17:56:30.355] [jointLog] [info] done; > [2020-06-04 17:56:30.355] [jointLog] [info] Index contained 64 targets; > [2020-06-04 17:56:30.355] [jointLog] [info] Number of decoys : 0; > [2020-06-04 17:57:36.305] [jointLog] [info] Computed 64 rich equivalence classes for further processing; > [2020-06-04 17:57:36.305] [jointLog] [info] Counted 39,818 total reads in the equivalence classes ; > [2020-06-04 17:57:36.305] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0; > [2020-0",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:2691,Testability,log,log,2691,"alevinLog] [info] parsing read library format; > [2020-06-04 17:57:36.339] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-04 17:57:37.051] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.051] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 17:57:37.341] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 17:57:37.348] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.700] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; > [2020-06-04 17:55:11.700] [jointLog] [info] Using default value of 0.797619 for minScoreF",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415:2429,Usability,Clear,Clearing,2429,"nfo] Total Unique barcodes found: 604589; > [2020-06-04 17:56:29.557] [alevinLog] [info] Used Barcodes except Whitelist: 88156; > [2020-06-04 17:56:30.294] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 17:56:30.294] [alevinLog] [info] parsing read library format; > [2020-06-04 17:57:36.339] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-04 17:57:37.051] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.051] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 17:57:37.338] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 17:57:37.341] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 17:57:37.348] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 10.Can't performing whitelisting; Skipping; > [2020-06-04 17:57:37.348] [alevinLog] [info] Finished optimizer; > ; > . salmon_quant.log. > [2020-06-04 17:55:11.700] [jointLog] [info] setting maxHashResizeThreads to 7; > [2020-06-04 17:55:11.700] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > [2020-06-04 17:55:11.700] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; > [2020-06-04 17:55:11.700] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; > [2020-06-04 17:55:11.700] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equiva",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639196415
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639204641:69,Usability,guid,guide,69,"Lemme work with the reads you forwarded, is it possible to share the guide sequence as well ? Otherwise I won't be able to check the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639204641
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639235133:1035,Availability,error,error,1035,"I wonder if the max 1-edit distance restriction is too stringent for 21 length barcodes. One important flag to play with is the `--minScoreFraction`. The basic rule to set that is define [here](https://github.com/COMBINE-lab/salmon/blob/91091fc3650a3220f657a9f31616916513f0ad02/src/SalmonUtils.cpp#L3242-L3253). The gist being say if we wan't max k-edit we allow all the reads above the following threshold score (as in the log ):. ```; [2020-06-04 17:55:11.700] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; ```; i.e. we use the equation `(max_score + edit_cost) - 0.5) / max_score`; where `max_score` = 2 * length of barcode = 2 * 21 = 42,; and `edit_cost`= `min( k * (mismatch - match), k * (go + ge - match)`;; `mismatch` penalty = -4; `match` = 2; `go` gap open penalty = -4; `ge` gap extend penalty = -2. For k=1, we had `edit_cost = 8` leading to automatic setting of `minScoreFraction` of 0.797619.; we have looked at 15 length barcodes, but it's possible longer barcodes might have more sequencing error. Let's try allowing more edits i.e. k=2, by setting `--minScoreFraction 0.607` and see if it improves the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639235133
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639235133:818,Modifiability,extend,extend,818,"I wonder if the max 1-edit distance restriction is too stringent for 21 length barcodes. One important flag to play with is the `--minScoreFraction`. The basic rule to set that is define [here](https://github.com/COMBINE-lab/salmon/blob/91091fc3650a3220f657a9f31616916513f0ad02/src/SalmonUtils.cpp#L3242-L3253). The gist being say if we wan't max k-edit we allow all the reads above the following threshold score (as in the log ):. ```; [2020-06-04 17:55:11.700] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; ```; i.e. we use the equation `(max_score + edit_cost) - 0.5) / max_score`; where `max_score` = 2 * length of barcode = 2 * 21 = 42,; and `edit_cost`= `min( k * (mismatch - match), k * (go + ge - match)`;; `mismatch` penalty = -4; `match` = 2; `go` gap open penalty = -4; `ge` gap extend penalty = -2. For k=1, we had `edit_cost = 8` leading to automatic setting of `minScoreFraction` of 0.797619.; we have looked at 15 length barcodes, but it's possible longer barcodes might have more sequencing error. Let's try allowing more edits i.e. k=2, by setting `--minScoreFraction 0.607` and see if it improves the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639235133
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639235133:424,Testability,log,log,424,"I wonder if the max 1-edit distance restriction is too stringent for 21 length barcodes. One important flag to play with is the `--minScoreFraction`. The basic rule to set that is define [here](https://github.com/COMBINE-lab/salmon/blob/91091fc3650a3220f657a9f31616916513f0ad02/src/SalmonUtils.cpp#L3242-L3253). The gist being say if we wan't max k-edit we allow all the reads above the following threshold score (as in the log ):. ```; [2020-06-04 17:55:11.700] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; ```; i.e. we use the equation `(max_score + edit_cost) - 0.5) / max_score`; where `max_score` = 2 * length of barcode = 2 * 21 = 42,; and `edit_cost`= `min( k * (mismatch - match), k * (go + ge - match)`;; `mismatch` penalty = -4; `match` = 2; `go` gap open penalty = -4; `ge` gap extend penalty = -2. For k=1, we had `edit_cost = 8` leading to automatic setting of `minScoreFraction` of 0.797619.; we have looked at 15 length barcodes, but it's possible longer barcodes might have more sequencing error. Let's try allowing more edits i.e. k=2, by setting `--minScoreFraction 0.607` and see if it improves the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639235133
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639618956:124,Deployability,update,update,124,I will be trying your suggestion out. I might be able to share with you a toy dataset with a fewer number of guides. I will update you as soon as I get it.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639618956
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639618956:109,Usability,guid,guides,109,I will be trying your suggestion out. I might be able to share with you a toy dataset with a fewer number of guides. I will update you as soon as I get it.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639618956
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639622262:122,Testability,test,test,122,"Thanks @rfarouni ! A small dataset with few thousand reads would be great to have, the one I currently had was too few to test things on.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639622262
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639642373:1820,Performance,optimiz,optimizer,1820,31m52200250[0m.; > [2020-06-05 13:09:43.576] [alevinLog] [info] Forcing to use 100000 cells; > [2020-06-05 13:09:43.653] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-05 13:09:43.673] [alevinLog] [info] Total [32m95377[0m(has [32m11[0m low confidence) barcodes; > [2020-06-05 13:09:43.875] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-05 13:09:44.027] [alevinLog] [info] Total 1.2299% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-05 13:09:48.338] [alevinLog] [info] Done populating Z matrix; > [2020-06-05 13:09:48.376] [alevinLog] [info] Total 118774 CB got sequence corrected; > [2020-06-05 13:09:48.389] [alevinLog] [info] Done indexing Barcodes; > [2020-06-05 13:09:48.389] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-05 13:09:48.389] [alevinLog] [info] Used Barcodes except Whitelist: 88156; > [2020-06-05 13:09:49.130] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-05 13:09:49.132] [alevinLog] [info] parsing read library format; > [2020-06-05 13:11:01.670] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-05 13:11:02.377] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:11:02.377] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 322945.00 UMI after deduplicating.; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 15972 BiDirected Edges.; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 176951 UniDirected Edges.; > [2020-06-05 13:11:04.408] [alevinLog] [warning] Skipped 12046 barcodes due to No mapped read; > [2020-06-05 13:11:04.415] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-05 13:11:04.455] [alevinLog] [warning] Num Low confidence barcodes too less 8 < 10.Can't performing whitelisting; Skipping; > [2020-06-05 13:11:04.455] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639642373
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639642373:2600,Performance,perform,performing,2600,31m52200250[0m.; > [2020-06-05 13:09:43.576] [alevinLog] [info] Forcing to use 100000 cells; > [2020-06-05 13:09:43.653] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-05 13:09:43.673] [alevinLog] [info] Total [32m95377[0m(has [32m11[0m low confidence) barcodes; > [2020-06-05 13:09:43.875] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-05 13:09:44.027] [alevinLog] [info] Total 1.2299% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-05 13:09:48.338] [alevinLog] [info] Done populating Z matrix; > [2020-06-05 13:09:48.376] [alevinLog] [info] Total 118774 CB got sequence corrected; > [2020-06-05 13:09:48.389] [alevinLog] [info] Done indexing Barcodes; > [2020-06-05 13:09:48.389] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-05 13:09:48.389] [alevinLog] [info] Used Barcodes except Whitelist: 88156; > [2020-06-05 13:09:49.130] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-05 13:09:49.132] [alevinLog] [info] parsing read library format; > [2020-06-05 13:11:01.670] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-05 13:11:02.377] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:11:02.377] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 322945.00 UMI after deduplicating.; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 15972 BiDirected Edges.; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 176951 UniDirected Edges.; > [2020-06-05 13:11:04.408] [alevinLog] [warning] Skipped 12046 barcodes due to No mapped read; > [2020-06-05 13:11:04.415] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-05 13:11:04.455] [alevinLog] [warning] Num Low confidence barcodes too less 8 < 10.Can't performing whitelisting; Skipping; > [2020-06-05 13:11:04.455] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639642373
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639642373:2691,Performance,optimiz,optimizer,2691,31m52200250[0m.; > [2020-06-05 13:09:43.576] [alevinLog] [info] Forcing to use 100000 cells; > [2020-06-05 13:09:43.653] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-05 13:09:43.673] [alevinLog] [info] Total [32m95377[0m(has [32m11[0m low confidence) barcodes; > [2020-06-05 13:09:43.875] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-05 13:09:44.027] [alevinLog] [info] Total 1.2299% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-05 13:09:48.338] [alevinLog] [info] Done populating Z matrix; > [2020-06-05 13:09:48.376] [alevinLog] [info] Total 118774 CB got sequence corrected; > [2020-06-05 13:09:48.389] [alevinLog] [info] Done indexing Barcodes; > [2020-06-05 13:09:48.389] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-05 13:09:48.389] [alevinLog] [info] Used Barcodes except Whitelist: 88156; > [2020-06-05 13:09:49.130] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-05 13:09:49.132] [alevinLog] [info] parsing read library format; > [2020-06-05 13:11:01.670] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-05 13:11:02.377] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:11:02.377] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 322945.00 UMI after deduplicating.; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 15972 BiDirected Edges.; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 176951 UniDirected Edges.; > [2020-06-05 13:11:04.408] [alevinLog] [warning] Skipped 12046 barcodes due to No mapped read; > [2020-06-05 13:11:04.415] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-05 13:11:04.455] [alevinLog] [warning] Num Low confidence barcodes too less 8 < 10.Can't performing whitelisting; Skipping; > [2020-06-05 13:11:04.455] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639642373
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639642373:2461,Usability,Clear,Clearing,2461,31m52200250[0m.; > [2020-06-05 13:09:43.576] [alevinLog] [info] Forcing to use 100000 cells; > [2020-06-05 13:09:43.653] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-05 13:09:43.673] [alevinLog] [info] Total [32m95377[0m(has [32m11[0m low confidence) barcodes; > [2020-06-05 13:09:43.875] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-05 13:09:44.027] [alevinLog] [info] Total 1.2299% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-05 13:09:48.338] [alevinLog] [info] Done populating Z matrix; > [2020-06-05 13:09:48.376] [alevinLog] [info] Total 118774 CB got sequence corrected; > [2020-06-05 13:09:48.389] [alevinLog] [info] Done indexing Barcodes; > [2020-06-05 13:09:48.389] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-05 13:09:48.389] [alevinLog] [info] Used Barcodes except Whitelist: 88156; > [2020-06-05 13:09:49.130] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-05 13:09:49.132] [alevinLog] [info] parsing read library format; > [2020-06-05 13:11:01.670] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-05 13:11:02.377] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:11:02.377] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 322945.00 UMI after deduplicating.; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 15972 BiDirected Edges.; > [2020-06-05 13:11:04.408] [alevinLog] [info] Total 176951 UniDirected Edges.; > [2020-06-05 13:11:04.408] [alevinLog] [warning] Skipped 12046 barcodes due to No mapped read; > [2020-06-05 13:11:04.415] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-05 13:11:04.455] [alevinLog] [warning] Num Low confidence barcodes too less 8 < 10.Can't performing whitelisting; Skipping; > [2020-06-05 13:11:04.455] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639642373
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639663002:1935,Performance,optimiz,optimizer,1935,31m52200250[0m.; > [2020-06-05 13:39:18.623] [alevinLog] [info] Forcing to use 100000 cells; > [2020-06-05 13:39:19.364] [alevinLog] [info] Throwing 49909 barcodes with < 10 reads; > [2020-06-05 13:39:20.065] [alevinLog] [info] Total [32m50092[0m(has [32m201[0m low confidence) barcodes; > [2020-06-05 13:39:20.928] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-05 13:39:21.057] [alevinLog] [info] Total 1.70493% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-05 13:39:23.175] [alevinLog] [info] Done populating Z matrix; > [2020-06-05 13:39:23.175] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-05 13:39:23.175] [alevinLog] [info] Done indexing Barcodes; > [2020-06-05 13:39:23.175] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-05 13:39:23.175] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-05 13:39:23.278] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-05 13:39:23.278] [alevinLog] [info] parsing read library format; > [2020-06-05 13:40:35.769] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-05 13:40:36.476] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:40:36.476] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 227279.00 UMI after deduplicating.; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 14712 BiDirected Edges.; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 173086 UniDirected Edges.; > [2020-06-05 13:40:37.933] [alevinLog] [warning] Skipped 5326 barcodes due to No mapped read; > [2020-06-05 13:40:37.936] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-05 13:40:37.962] [alevinLog] [warning] Num Low confidence barcodes too less 165 < 200.Can't performing whitelisting; Skipping; > [2020-06-05 13:40:37.962] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639663002
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639663002:2717,Performance,perform,performing,2717,31m52200250[0m.; > [2020-06-05 13:39:18.623] [alevinLog] [info] Forcing to use 100000 cells; > [2020-06-05 13:39:19.364] [alevinLog] [info] Throwing 49909 barcodes with < 10 reads; > [2020-06-05 13:39:20.065] [alevinLog] [info] Total [32m50092[0m(has [32m201[0m low confidence) barcodes; > [2020-06-05 13:39:20.928] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-05 13:39:21.057] [alevinLog] [info] Total 1.70493% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-05 13:39:23.175] [alevinLog] [info] Done populating Z matrix; > [2020-06-05 13:39:23.175] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-05 13:39:23.175] [alevinLog] [info] Done indexing Barcodes; > [2020-06-05 13:39:23.175] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-05 13:39:23.175] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-05 13:39:23.278] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-05 13:39:23.278] [alevinLog] [info] parsing read library format; > [2020-06-05 13:40:35.769] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-05 13:40:36.476] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:40:36.476] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 227279.00 UMI after deduplicating.; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 14712 BiDirected Edges.; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 173086 UniDirected Edges.; > [2020-06-05 13:40:37.933] [alevinLog] [warning] Skipped 5326 barcodes due to No mapped read; > [2020-06-05 13:40:37.936] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-05 13:40:37.962] [alevinLog] [warning] Num Low confidence barcodes too less 165 < 200.Can't performing whitelisting; Skipping; > [2020-06-05 13:40:37.962] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639663002
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639663002:2808,Performance,optimiz,optimizer,2808,31m52200250[0m.; > [2020-06-05 13:39:18.623] [alevinLog] [info] Forcing to use 100000 cells; > [2020-06-05 13:39:19.364] [alevinLog] [info] Throwing 49909 barcodes with < 10 reads; > [2020-06-05 13:39:20.065] [alevinLog] [info] Total [32m50092[0m(has [32m201[0m low confidence) barcodes; > [2020-06-05 13:39:20.928] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-05 13:39:21.057] [alevinLog] [info] Total 1.70493% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-05 13:39:23.175] [alevinLog] [info] Done populating Z matrix; > [2020-06-05 13:39:23.175] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-05 13:39:23.175] [alevinLog] [info] Done indexing Barcodes; > [2020-06-05 13:39:23.175] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-05 13:39:23.175] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-05 13:39:23.278] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-05 13:39:23.278] [alevinLog] [info] parsing read library format; > [2020-06-05 13:40:35.769] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-05 13:40:36.476] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:40:36.476] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 227279.00 UMI after deduplicating.; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 14712 BiDirected Edges.; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 173086 UniDirected Edges.; > [2020-06-05 13:40:37.933] [alevinLog] [warning] Skipped 5326 barcodes due to No mapped read; > [2020-06-05 13:40:37.936] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-05 13:40:37.962] [alevinLog] [warning] Num Low confidence barcodes too less 165 < 200.Can't performing whitelisting; Skipping; > [2020-06-05 13:40:37.962] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639663002
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639663002:2575,Usability,Clear,Clearing,2575,31m52200250[0m.; > [2020-06-05 13:39:18.623] [alevinLog] [info] Forcing to use 100000 cells; > [2020-06-05 13:39:19.364] [alevinLog] [info] Throwing 49909 barcodes with < 10 reads; > [2020-06-05 13:39:20.065] [alevinLog] [info] Total [32m50092[0m(has [32m201[0m low confidence) barcodes; > [2020-06-05 13:39:20.928] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-05 13:39:21.057] [alevinLog] [info] Total 1.70493% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-05 13:39:23.175] [alevinLog] [info] Done populating Z matrix; > [2020-06-05 13:39:23.175] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-05 13:39:23.175] [alevinLog] [info] Done indexing Barcodes; > [2020-06-05 13:39:23.175] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-05 13:39:23.175] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-05 13:39:23.278] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-05 13:39:23.278] [alevinLog] [info] parsing read library format; > [2020-06-05 13:40:35.769] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-05 13:40:36.476] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:40:36.476] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 227279.00 UMI after deduplicating.; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 14712 BiDirected Edges.; > [2020-06-05 13:40:37.933] [alevinLog] [info] Total 173086 UniDirected Edges.; > [2020-06-05 13:40:37.933] [alevinLog] [warning] Skipped 5326 barcodes due to No mapped read; > [2020-06-05 13:40:37.936] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-05 13:40:37.962] [alevinLog] [warning] Num Low confidence barcodes too less 165 < 200.Can't performing whitelisting; Skipping; > [2020-06-05 13:40:37.962] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639663002
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397:25,Deployability,update,updates,25,"Thanks @rfarouni for the updates. > With --minScoreFraction 0.607 I get a way much better mapping rate. I wonder if there is way to determine the optimal value empirically?. Glad to hear that, may I ask what percent of the reads are mapping now ? It's not clear from the alevin logs you shared but I think the total number of deduplicated UMIs are similar to your baseline experiment. I think defining an optimal empirical threshold is a great idea but the issue is that 21 length barcodes are kind of in the middle i.e. a tad longer than the regular barcodes and somewhat smaller than a full read. The full read alignment process indeed allows more erroneous reads to map but 21 is a bit too short to work with. @rob-p might have more thoughts on this one. > But now there are a lot of barcodes that are not in the whitelist. Thanks again for checking this, it is indeed concerning. However, as I was mentioning earlier in a regular single-cell experiment we end up throwing away almost all of these very low frequency count cellular barcodes. I'd say even 45 reads CBs are most probably a noise and will be filtered away, because only a fraction of the reads will map and after deduplication it'll result in significantly low count in 1 cellular barcode. > Also with the default setting of --freqThreshold, no CB correction gets done. I can check why is this happening, let me know once you have a toy dataset to play with.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397:278,Testability,log,logs,278,"Thanks @rfarouni for the updates. > With --minScoreFraction 0.607 I get a way much better mapping rate. I wonder if there is way to determine the optimal value empirically?. Glad to hear that, may I ask what percent of the reads are mapping now ? It's not clear from the alevin logs you shared but I think the total number of deduplicated UMIs are similar to your baseline experiment. I think defining an optimal empirical threshold is a great idea but the issue is that 21 length barcodes are kind of in the middle i.e. a tad longer than the regular barcodes and somewhat smaller than a full read. The full read alignment process indeed allows more erroneous reads to map but 21 is a bit too short to work with. @rob-p might have more thoughts on this one. > But now there are a lot of barcodes that are not in the whitelist. Thanks again for checking this, it is indeed concerning. However, as I was mentioning earlier in a regular single-cell experiment we end up throwing away almost all of these very low frequency count cellular barcodes. I'd say even 45 reads CBs are most probably a noise and will be filtered away, because only a fraction of the reads will map and after deduplication it'll result in significantly low count in 1 cellular barcode. > Also with the default setting of --freqThreshold, no CB correction gets done. I can check why is this happening, let me know once you have a toy dataset to play with.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397:256,Usability,clear,clear,256,"Thanks @rfarouni for the updates. > With --minScoreFraction 0.607 I get a way much better mapping rate. I wonder if there is way to determine the optimal value empirically?. Glad to hear that, may I ask what percent of the reads are mapping now ? It's not clear from the alevin logs you shared but I think the total number of deduplicated UMIs are similar to your baseline experiment. I think defining an optimal empirical threshold is a great idea but the issue is that 21 length barcodes are kind of in the middle i.e. a tad longer than the regular barcodes and somewhat smaller than a full read. The full read alignment process indeed allows more erroneous reads to map but 21 is a bit too short to work with. @rob-p might have more thoughts on this one. > But now there are a lot of barcodes that are not in the whitelist. Thanks again for checking this, it is indeed concerning. However, as I was mentioning earlier in a regular single-cell experiment we end up throwing away almost all of these very low frequency count cellular barcodes. I'd say even 45 reads CBs are most probably a noise and will be filtered away, because only a fraction of the reads will map and after deduplication it'll result in significantly low count in 1 cellular barcode. > Also with the default setting of --freqThreshold, no CB correction gets done. I can check why is this happening, let me know once you have a toy dataset to play with.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640698401:80,Usability,clear,clear,80,"Yes, absolutely, above I meant in scRNA-seq context, my apologies if it was not clear.; Here, you are right we might have to think of ways to provide whitelist cellular barcode. One another thing you can try is providing the full 10x expected whitelisted cellular barcodes to alevin through `--whitelist` command.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640698401
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641276650:73,Availability,error,error,73,When I add the whitelist using `--whitelist` command I get the following error. `[2020-06-09 09:01:07.401] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; `,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641276650
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641276650:120,Availability,error,error,120,When I add the whitelist using `--whitelist` command I get the following error. `[2020-06-09 09:01:07.401] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; `,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641276650
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641296035:17,Safety,sanity check,sanity checks,17,"Oh man, too many sanity checks over the years, can you just remove one cellular barcode from the full list and try again?. Basically, many people have confused this flag by providing the full 10x whitelist without knowing the consequences, that's why the warning. Here our use case is specific and it should not matter.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641296035
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641426690:1990,Performance,optimiz,optimizer,1990,es in the index); > [2020-06-09 12:31:05.494] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-09 12:31:05.494] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-09 12:31:05.499] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > ; > [2020-06-09 12:32:20.000] [alevinLog] [info] Done barcode density calculation.; > [2020-06-09 12:32:20.000] [alevinLog] [info] # Barcodes Used: [32m52200250[0m / [31m52200250[0m.; > [2020-06-09 12:32:20.285] [alevinLog] [info] Done importing white-list Barcodes; > [2020-06-09 12:32:20.423] [alevinLog] [warning] Skipping 672237 Barcodes as no read was mapped; > [2020-06-09 12:32:20.578] [alevinLog] [info] Total 65042 white-listed Barcodes; > [2020-06-09 12:32:20.578] [alevinLog] [info] Sorting and dumping raw barcodes; > [2020-06-09 12:32:21.060] [alevinLog] [info] Total 5.06742% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-09 12:32:23.856] [alevinLog] [info] Done populating Z matrix; > [2020-06-09 12:32:23.882] [alevinLog] [info] Total 79207 CB got sequence corrected. > [2020-06-09 12:32:23.893] [alevinLog] [info] Done indexing Barcodes; > [2020-06-09 12:32:23.893] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-09 12:32:23.893] [alevinLog] [info] Used Barcodes except Whitelist: 71340; > [2020-06-09 12:32:24.004] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-09 12:32:24.004] [alevinLog] [info] parsing read library format; > [2020-06-09 12:33:33.719] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-09 12:33:35.712] [alevinLog] [info] Total 161852.00 UMI after deduplicating.; > [2020-06-09 12:33:35.712] [alevinLog] [info] Total 14936 BiDirected Edges.; > [2020-06-09 12:33:35.712] [alevinLog] [info] Total 177402 UniDirected Edges.; > [2020-06-09 12:33:35.712] [alevinLog] [warning] Skipped 12422 barcodes due to No mapped read; > [2020-06-09 12:33:35.719] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641426690
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641426690:2408,Performance,optimiz,optimizer,2408,es in the index); > [2020-06-09 12:31:05.494] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-09 12:31:05.494] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-09 12:31:05.499] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > ; > [2020-06-09 12:32:20.000] [alevinLog] [info] Done barcode density calculation.; > [2020-06-09 12:32:20.000] [alevinLog] [info] # Barcodes Used: [32m52200250[0m / [31m52200250[0m.; > [2020-06-09 12:32:20.285] [alevinLog] [info] Done importing white-list Barcodes; > [2020-06-09 12:32:20.423] [alevinLog] [warning] Skipping 672237 Barcodes as no read was mapped; > [2020-06-09 12:32:20.578] [alevinLog] [info] Total 65042 white-listed Barcodes; > [2020-06-09 12:32:20.578] [alevinLog] [info] Sorting and dumping raw barcodes; > [2020-06-09 12:32:21.060] [alevinLog] [info] Total 5.06742% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-09 12:32:23.856] [alevinLog] [info] Done populating Z matrix; > [2020-06-09 12:32:23.882] [alevinLog] [info] Total 79207 CB got sequence corrected. > [2020-06-09 12:32:23.893] [alevinLog] [info] Done indexing Barcodes; > [2020-06-09 12:32:23.893] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-09 12:32:23.893] [alevinLog] [info] Used Barcodes except Whitelist: 71340; > [2020-06-09 12:32:24.004] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-09 12:32:24.004] [alevinLog] [info] parsing read library format; > [2020-06-09 12:33:33.719] [alevinLog] [info] Starting optimizer; > ; > ; > [2020-06-09 12:33:35.712] [alevinLog] [info] Total 161852.00 UMI after deduplicating.; > [2020-06-09 12:33:35.712] [alevinLog] [info] Total 14936 BiDirected Edges.; > [2020-06-09 12:33:35.712] [alevinLog] [info] Total 177402 UniDirected Edges.; > [2020-06-09 12:33:35.712] [alevinLog] [warning] Skipped 12422 barcodes due to No mapped read; > [2020-06-09 12:33:35.719] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641426690
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641426690:51,Testability,log,log,51,"Thanks! Worked with a Mapping rate = 73.4157%. See log below. However, I only get half the number of mapped reads per cell-feature. I still need to examine the existing alignment to understand why. ![image](https://user-images.githubusercontent.com/9895004/84175848-8c863c80-aa4e-11ea-8d36-986e7d6f04b5.png). > [2020-06-09 12:31:05.494] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-09 12:31:05.494] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-09 12:31:05.494] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-09 12:31:05.499] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > ; > [2020-06-09 12:32:20.000] [alevinLog] [info] Done barcode density calculation.; > [2020-06-09 12:32:20.000] [alevinLog] [info] # Barcodes Used: [32m52200250[0m / [31m52200250[0m.; > [2020-06-09 12:32:20.285] [alevinLog] [info] Done importing white-list Barcodes; > [2020-06-09 12:32:20.423] [alevinLog] [warning] Skipping 672237 Barcodes as no read was mapped; > [2020-06-09 12:32:20.578] [alevinLog] [info] Total 65042 white-listed Barcodes; > [2020-06-09 12:32:20.578] [alevinLog] [info] Sorting and dumping raw barcodes; > [2020-06-09 12:32:21.060] [alevinLog] [info] Total 5.06742% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-09 12:32:23.856] [alevinLog] [info] Done populating Z matrix; > [2020-06-09 12:32:23.882] [alevinLog] [info] Total 79207 CB got sequence corrected. > [2020-06-09 12:32:23.893] [alevinLog] [info] Done indexing Barcodes; > [2020-06-09 12:32:23.893] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-09 12:32:23.893] [alevinLog] [info] Used Barcodes except Whitelist: 71340; > [2020-06-09 12:32:24.004] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-09 12:32:24.004] [alevinLog] [info] parsing read library format; > [2020-06-09 12:33:33.719] [alevinLog] [info] Starting optimizer; ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-641426690
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-638929976:1232,Availability,recover,recover,1232,"Hi @ShenTTT,. First, thank you for the detailed analysis (and for doing the work of looking up this issue in past posts). If the (trimmed) reads are of high quality and there aren't signs of contamination, my initial hunch would be that there are a considerable number of reads coming from outside of the annotated transcriptome (that is, I would suspect `2` to be the most likely culprit here). I don't think this is because of DNA `contamination` necessarily, but rather because of novel transcripts that don't appear in your annotation. Obviously, salmon can only quantify what it knows about in terms of annotated transcripts. If you are concerned about these mapping rates, one thing you might try is to do transcript assembly in these samples (using e.g. scallop or StringTie2) and then re-quantify using salmon under the expanded annotation. Also, while you are losing ~5M reads to low mapping scores (suggesting they are not a good match for the underlying annotated transcripts), this doesn't seem to be what is driving your overall mapping rate (i.e. this is only ~15% of fragments, so the rest of the difference between 85% and 56% comes from reads that don't fall into this category). Finally, while you may be able to ""recover"" more reads by lowering the mapping threshold `--minScoreFraction` from its default value of 0.65, that default is actually pretty liberal (especially for trimmed reads). So this would be letting in quite low-quality alignments. Note that, if you have unannotated isoforms that share sequence (exons) with annotated isoforms, this could partly describe what you are seeing with some of these reads, where part of the read aligns but part does not (because it comes from an unannotated splice form).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-638929976
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-638929976:1232,Safety,recover,recover,1232,"Hi @ShenTTT,. First, thank you for the detailed analysis (and for doing the work of looking up this issue in past posts). If the (trimmed) reads are of high quality and there aren't signs of contamination, my initial hunch would be that there are a considerable number of reads coming from outside of the annotated transcriptome (that is, I would suspect `2` to be the most likely culprit here). I don't think this is because of DNA `contamination` necessarily, but rather because of novel transcripts that don't appear in your annotation. Obviously, salmon can only quantify what it knows about in terms of annotated transcripts. If you are concerned about these mapping rates, one thing you might try is to do transcript assembly in these samples (using e.g. scallop or StringTie2) and then re-quantify using salmon under the expanded annotation. Also, while you are losing ~5M reads to low mapping scores (suggesting they are not a good match for the underlying annotated transcripts), this doesn't seem to be what is driving your overall mapping rate (i.e. this is only ~15% of fragments, so the rest of the difference between 85% and 56% comes from reads that don't fall into this category). Finally, while you may be able to ""recover"" more reads by lowering the mapping threshold `--minScoreFraction` from its default value of 0.65, that default is actually pretty liberal (especially for trimmed reads). So this would be letting in quite low-quality alignments. Note that, if you have unannotated isoforms that share sequence (exons) with annotated isoforms, this could partly describe what you are seeing with some of these reads, where part of the read aligns but part does not (because it comes from an unannotated splice form).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-638929976
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-638961902:58,Usability,clear,clear,58,"Hi @rob-p ,. Thank you for your explanation, that is very clear and helpful. Yes, the transcriptome annotation was not originally from my RNA-seq data and I may try using stringtie to discover new isoforms, probably that would cover more of my reads.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-638961902
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697107525:105,Deployability,pipeline,pipeline,105,"Hi @rob-p . I had a similar problem, however, I reassemblied the new isoforms using the Hisat2+Stringtie pipeline. The mapping rate from Hisat is 96.49% but it is 65.39% in Salmon. I also noticed a high number of mappings discarded because of alignment score. I also wonder why the number of mappings discarded can be larger than num of processed (57113760, the reads number in 1_1.fq.gz). Thanks. ```; Command: salmon-latest_linux_x86_64/bin/salmon quant -i transcript -l A -1 1_1.fq.gz -2 1_2.fq.gz -p 4 -o ${out}. [2020-09-23 10:09:32.992] [jointLog] [info] Index contained 153,995 targets; [2020-09-23 10:09:33.190] [jointLog] [info] Number of decoys : 0; [2020-09-23 10:09:40.178] [jointLog] [info] Automatically detected most likely library type as ISR; [2020-09-23 10:31:17.407] [jointLog] [info] Thread saw mini-batch with a maximum of 1.68% zero probability fragments; [2020-09-23 10:31:17.467] [jointLog] [info] Thread saw mini-batch with a maximum of 1.64% zero probability fragments; [2020-09-23 10:31:17.563] [jointLog] [info] Thread saw mini-batch with a maximum of 1.66% zero probability fragments; [2020-09-23 10:31:17.573] [jointLog] [info] Thread saw mini-batch with a maximum of 1.72% zero probability fragments; [2020-09-23 10:31:18.005] [jointLog] [info] Computed 329,858 rich equivalence classes for further processing; [2020-09-23 10:31:18.005] [jointLog] [info] Counted 37,348,440 total reads in the equivalence classes ; [2020-09-23 10:31:18.009] [jointLog] [info] Number of mappings discarded because of alignment score : 120,261,413; [2020-09-23 10:31:18.009] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 4,196,417; [2020-09-23 10:31:18.009] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0; [2020-09-23 10:31:18.009] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 569,393; [2020-09-23 10:31:18.009] [jointLog] [info]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697107525
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697107525:2152,Performance,optimiz,optimizer,2152,"32.992] [jointLog] [info] Index contained 153,995 targets; [2020-09-23 10:09:33.190] [jointLog] [info] Number of decoys : 0; [2020-09-23 10:09:40.178] [jointLog] [info] Automatically detected most likely library type as ISR; [2020-09-23 10:31:17.407] [jointLog] [info] Thread saw mini-batch with a maximum of 1.68% zero probability fragments; [2020-09-23 10:31:17.467] [jointLog] [info] Thread saw mini-batch with a maximum of 1.64% zero probability fragments; [2020-09-23 10:31:17.563] [jointLog] [info] Thread saw mini-batch with a maximum of 1.66% zero probability fragments; [2020-09-23 10:31:17.573] [jointLog] [info] Thread saw mini-batch with a maximum of 1.72% zero probability fragments; [2020-09-23 10:31:18.005] [jointLog] [info] Computed 329,858 rich equivalence classes for further processing; [2020-09-23 10:31:18.005] [jointLog] [info] Counted 37,348,440 total reads in the equivalence classes ; [2020-09-23 10:31:18.009] [jointLog] [info] Number of mappings discarded because of alignment score : 120,261,413; [2020-09-23 10:31:18.009] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 4,196,417; [2020-09-23 10:31:18.009] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0; [2020-09-23 10:31:18.009] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 569,393; [2020-09-23 10:31:18.009] [jointLog] [info] Mapping rate = 65.3931%. [2020-09-23 10:31:18.010] [jointLog] [info] finished quantifyLibrary(); [2020-09-23 10:31:18.097] [jointLog] [info] Starting optimizer; [2020-09-23 10:31:18.006] [fileLog] [info] . ""num_bootstraps"": 0,; ""num_processed"": 57113760,; ""num_mapped"": 37348440,; ""num_decoy_fragments"": 0,; ""num_dovetail_fragments"": 569393,; ""num_fragments_filtered_vm"": 4196417,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 120261413,; ""percent_mapped"": 65.39306815030214,; ""call"": ""quant"",; ```. Best,; Zheng Zhuqing",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697107525
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697107525:718,Safety,detect,detected,718,"Hi @rob-p . I had a similar problem, however, I reassemblied the new isoforms using the Hisat2+Stringtie pipeline. The mapping rate from Hisat is 96.49% but it is 65.39% in Salmon. I also noticed a high number of mappings discarded because of alignment score. I also wonder why the number of mappings discarded can be larger than num of processed (57113760, the reads number in 1_1.fq.gz). Thanks. ```; Command: salmon-latest_linux_x86_64/bin/salmon quant -i transcript -l A -1 1_1.fq.gz -2 1_2.fq.gz -p 4 -o ${out}. [2020-09-23 10:09:32.992] [jointLog] [info] Index contained 153,995 targets; [2020-09-23 10:09:33.190] [jointLog] [info] Number of decoys : 0; [2020-09-23 10:09:40.178] [jointLog] [info] Automatically detected most likely library type as ISR; [2020-09-23 10:31:17.407] [jointLog] [info] Thread saw mini-batch with a maximum of 1.68% zero probability fragments; [2020-09-23 10:31:17.467] [jointLog] [info] Thread saw mini-batch with a maximum of 1.64% zero probability fragments; [2020-09-23 10:31:17.563] [jointLog] [info] Thread saw mini-batch with a maximum of 1.66% zero probability fragments; [2020-09-23 10:31:17.573] [jointLog] [info] Thread saw mini-batch with a maximum of 1.72% zero probability fragments; [2020-09-23 10:31:18.005] [jointLog] [info] Computed 329,858 rich equivalence classes for further processing; [2020-09-23 10:31:18.005] [jointLog] [info] Counted 37,348,440 total reads in the equivalence classes ; [2020-09-23 10:31:18.009] [jointLog] [info] Number of mappings discarded because of alignment score : 120,261,413; [2020-09-23 10:31:18.009] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 4,196,417; [2020-09-23 10:31:18.009] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0; [2020-09-23 10:31:18.009] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 569,393; [2020-09-23 10:31:18.009] [jointLog] [info]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697107525
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697125235:179,Deployability,pipeline,pipeline,179,"Hi @biozzq,. Sure; let me try and explain the likely causes of these observations:. > I had a similar problem, however, I reassemblied the new isoforms using the Hisat2+Stringtie pipeline. The mapping rate from Hisat is 96.49% but it is 65.39% in Salmon. . The mapping rate of Hisat2 is the total number of reads assigned by Hisat2 both to the annotated transcriptome (what will be alignable to a transcript by salmon) as well as to the genome. It is quite common that the mapping rate to the genome is higher than that to the transcriptome. This is much more a result of what you are aligning _to_ than the aligner. If you were to take the transcriptome, and align to it using Hisat2 with `--no-spliced-alignment` and `--end-to-end` (since there won't be splice junctions when you align to the transcriptome), I'd expect you to get a similar mapping rate as you see in salmon. > I also noticed a high number of mappings discarded because of alignment score. I also wonder why the number of mappings discarded can be larger than num of processed (57113760, the reads number in 1_1.fq.gz). . Good question. The number you are looking at is the number of discarded _mappings_, not the number of discarded _fragments_. The difference is that every fragment can have many potential mappings. The number you are looking at is the total number of attempted _alignments_ that failed to achieve the threshold score. Luckily, salmon reports both numbers. The number of fragments for which _all_ alignments failed to reach the score threshold is `4,196,417`; given in `aux_info.json` by ` ""num_fragments_filtered_vm"": 4196417`. One point to note is that these are all fragments for which mapping is attempted (they had at least one k-mer match the reference), but no alignment was valid up to the threshold. You could try running the quantification again with `--softclip` to allow softclipping of the reads and see if any considerable fraction of these `4196417` failed to align because they overhang the annot",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697125235
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697125235:2029,Energy Efficiency,adapt,adapters,2029," transcript by salmon) as well as to the genome. It is quite common that the mapping rate to the genome is higher than that to the transcriptome. This is much more a result of what you are aligning _to_ than the aligner. If you were to take the transcriptome, and align to it using Hisat2 with `--no-spliced-alignment` and `--end-to-end` (since there won't be splice junctions when you align to the transcriptome), I'd expect you to get a similar mapping rate as you see in salmon. > I also noticed a high number of mappings discarded because of alignment score. I also wonder why the number of mappings discarded can be larger than num of processed (57113760, the reads number in 1_1.fq.gz). . Good question. The number you are looking at is the number of discarded _mappings_, not the number of discarded _fragments_. The difference is that every fragment can have many potential mappings. The number you are looking at is the total number of attempted _alignments_ that failed to achieve the threshold score. Luckily, salmon reports both numbers. The number of fragments for which _all_ alignments failed to reach the score threshold is `4,196,417`; given in `aux_info.json` by ` ""num_fragments_filtered_vm"": 4196417`. One point to note is that these are all fragments for which mapping is attempted (they had at least one k-mer match the reference), but no alignment was valid up to the threshold. You could try running the quantification again with `--softclip` to allow softclipping of the reads and see if any considerable fraction of these `4196417` failed to align because they overhang the annotated transcripts or contain adapters etc. Nonetheless, even if all of these mapped, the rate would still be ~72%. The remainder of the reads didn't even have a matching k-mer in common with the reference transcriptome, which means they are exceedingly unlikely to have come from the transcripts that were indexed. > Thanks. You're welcome! Please let me know if you have any follow-up questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697125235
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697125235:2029,Integrability,adapter,adapters,2029," transcript by salmon) as well as to the genome. It is quite common that the mapping rate to the genome is higher than that to the transcriptome. This is much more a result of what you are aligning _to_ than the aligner. If you were to take the transcriptome, and align to it using Hisat2 with `--no-spliced-alignment` and `--end-to-end` (since there won't be splice junctions when you align to the transcriptome), I'd expect you to get a similar mapping rate as you see in salmon. > I also noticed a high number of mappings discarded because of alignment score. I also wonder why the number of mappings discarded can be larger than num of processed (57113760, the reads number in 1_1.fq.gz). . Good question. The number you are looking at is the number of discarded _mappings_, not the number of discarded _fragments_. The difference is that every fragment can have many potential mappings. The number you are looking at is the total number of attempted _alignments_ that failed to achieve the threshold score. Luckily, salmon reports both numbers. The number of fragments for which _all_ alignments failed to reach the score threshold is `4,196,417`; given in `aux_info.json` by ` ""num_fragments_filtered_vm"": 4196417`. One point to note is that these are all fragments for which mapping is attempted (they had at least one k-mer match the reference), but no alignment was valid up to the threshold. You could try running the quantification again with `--softclip` to allow softclipping of the reads and see if any considerable fraction of these `4196417` failed to align because they overhang the annotated transcripts or contain adapters etc. Nonetheless, even if all of these mapped, the rate would still be ~72%. The remainder of the reads didn't even have a matching k-mer in common with the reference transcriptome, which means they are exceedingly unlikely to have come from the transcripts that were indexed. > Thanks. You're welcome! Please let me know if you have any follow-up questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697125235
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697125235:2029,Modifiability,adapt,adapters,2029," transcript by salmon) as well as to the genome. It is quite common that the mapping rate to the genome is higher than that to the transcriptome. This is much more a result of what you are aligning _to_ than the aligner. If you were to take the transcriptome, and align to it using Hisat2 with `--no-spliced-alignment` and `--end-to-end` (since there won't be splice junctions when you align to the transcriptome), I'd expect you to get a similar mapping rate as you see in salmon. > I also noticed a high number of mappings discarded because of alignment score. I also wonder why the number of mappings discarded can be larger than num of processed (57113760, the reads number in 1_1.fq.gz). . Good question. The number you are looking at is the number of discarded _mappings_, not the number of discarded _fragments_. The difference is that every fragment can have many potential mappings. The number you are looking at is the total number of attempted _alignments_ that failed to achieve the threshold score. Luckily, salmon reports both numbers. The number of fragments for which _all_ alignments failed to reach the score threshold is `4,196,417`; given in `aux_info.json` by ` ""num_fragments_filtered_vm"": 4196417`. One point to note is that these are all fragments for which mapping is attempted (they had at least one k-mer match the reference), but no alignment was valid up to the threshold. You could try running the quantification again with `--softclip` to allow softclipping of the reads and see if any considerable fraction of these `4196417` failed to align because they overhang the annotated transcripts or contain adapters etc. Nonetheless, even if all of these mapped, the rate would still be ~72%. The remainder of the reads didn't even have a matching k-mer in common with the reference transcriptome, which means they are exceedingly unlikely to have come from the transcripts that were indexed. > Thanks. You're welcome! Please let me know if you have any follow-up questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-697125235
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-1453502593:430,Usability,guid,guide,430,"Hello, @rob-p and first of all big thanks from me (and the community in general!) for being so reactive and helpful, and that's not even mentioning the tool you and your colleagues provided, which is very cool!. > do transcript assembly in these samples (using e.g. scallop or StringTie2) and then re-quantify using salmon under the expanded annotation. Could you please point me (and others who might be reading this topic) to a guide on how to convert this purported _de novo_ transcriptome to common gene/transcriptome names already known for the organism?? I am not even sure how that would work since many of us use well-known model organisms like mice or fruit flies, for which both the genome and transcriptome are well-described... What kind of genes/transcripts will I be looking at, exactly, after assembling this particular _de novo_ transcriptome which in theory came from mus musculus but in practice is only applicable to this particular single experiment???. Best regards,; Emile",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-1453502593
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644504783:184,Testability,log,logs,184,"Hi Rob, . Thanks for the quick response. The other computer was OSX, should I try a linux machine? . Here are some dropbox links to two of the files. I believe this is the set for the logs I posted. . https://www.dropbox.com/s/8c3p9hmgmadgj89/31_1.trimmed.fastq.gz?dl=0. https://www.dropbox.com/s/2y9jfvaphe9h21x/31_2.trimmed.fastq.gz?dl=0. Thanks, . Ryan . On Jun 15, 2020, at 9:32 PM, Rob Patro <notifications@github.com> wrote:. Thank you for the report. Can you share one of the samples where you see this issue? Also, out of curiosity, was the other machine you tried on also OSX, or was it a linux machine?; —; You are receiving this because you authored the thread.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644504783
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644505526:180,Testability,log,logs,180,"Thanks @shalercr,. I'll grab those files once they are finished uploading. I don't know if all samples show similar behavior, but these are called `31_1` and `31_2`, while in your logs you had `13_1` and `13_2`. Regarding OSX vs. linux, it should not really matter, obviously (salmon should work well under both, but I'm just curious since this is obviously atypical and unexpected behavior). --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644505526
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:81,Deployability,release,release,81,"I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.673]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:986,Performance,Load,Loading,986,"I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.673]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:1056,Performance,Load,Loading,1056,"develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments discarded; because t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:2449,Performance,optimiz,optimizer,2449,"56,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments discarded; because they are best-mapped to decoys : 0; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments discarded; because they have only dovetail (discordant) mappings to valid targets :; 1,360,397; [2020-06-16 00:00:59.673] [jointLog] [info] Mapping rate = 45.4405%. [2020-06-16 00:00:59.673] [jointLog] [info] finished quantifyLibrary(); [2020-06-16 00:00:59.673] [jointLog] [info] Starting optimizer; [2020-06-16 00:00:59.792] [jointLog] [info] Marked 0 weighted equivalence; classes as degenerate; [2020-06-16 00:00:59.819] [jointLog] [info] iteration = 0 | max rel diff. =; 8250.92; [2020-06-16 00:00:59.667] [fileLog] [info]; At end of round 0; ==================; Observed 28512328 total fragments (28512328 in most recent round). [2020-06-16 00:01:01.745] [jointLog] [info] iteration = 100 | max rel diff.; = 19.507; [2020-06-16 00:01:03.495] [jointLog] [info] iteration = 200 | max rel diff.; = 2.45489; [2020-06-16 00:01:05.225] [jointLog] [info] iteration = 300 | max rel diff.; = 3.459; [2020-06-16 00:01:06.968] [jointLog] [info] iteration = 400 | max rel diff.; = 4.38485; [2020-06-16 00:01:08.693] [jointLog] [info] iteration = 500 | max rel diff.; = 0.229724; [2020-06-16 00:01:10.426] [jointLog] [info] iteration = 600 | max rel diff.; = 0.242741; [2020-06-16 00:01:12.163] [jointLog] [info] iteration = 700 | max rel diff.; = 0.196958; [2020-06-16 00:01:14.015] [jointLog] [i",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:3732,Performance,optimiz,optimizer,3732,"ved 28512328 total fragments (28512328 in most recent round). [2020-06-16 00:01:01.745] [jointLog] [info] iteration = 100 | max rel diff.; = 19.507; [2020-06-16 00:01:03.495] [jointLog] [info] iteration = 200 | max rel diff.; = 2.45489; [2020-06-16 00:01:05.225] [jointLog] [info] iteration = 300 | max rel diff.; = 3.459; [2020-06-16 00:01:06.968] [jointLog] [info] iteration = 400 | max rel diff.; = 4.38485; [2020-06-16 00:01:08.693] [jointLog] [info] iteration = 500 | max rel diff.; = 0.229724; [2020-06-16 00:01:10.426] [jointLog] [info] iteration = 600 | max rel diff.; = 0.242741; [2020-06-16 00:01:12.163] [jointLog] [info] iteration = 700 | max rel diff.; = 0.196958; [2020-06-16 00:01:14.015] [jointLog] [info] iteration = 800 | max rel diff.; = 0.239461; [2020-06-16 00:01:15.803] [jointLog] [info] iteration = 900 | max rel diff.; = 0.197651; [2020-06-16 00:01:17.095] [jointLog] [info] iteration = 969 | max rel diff.; = 0.00714824; [2020-06-16 00:01:17.131] [jointLog] [info] Finished optimizer; [2020-06-16 00:01:17.131] [jointLog] [info] writing output; ```. So, you can note that the number of mappings discarded because of alignment; score is still quite big (134M) and, indeed, this took longer than I would; normally expect for; a sample with ~30M reads. However, it is _massively_ smaller (~116 times); than the 15.6B in your run. Thus, it may have been the same issue as in; the other GitHub issue. You can try the pre-compiled linux binary I linked; there if you have access to a linux machine to test on. By the way, the; issue there was a bunch of reads plagued by super-repetitive homopolymer; segments, I haven't looked at these reads yet for evidence of that though. On Mon, Jun 15, 2020 at 11:09 PM shalercr <notifications@github.com> wrote:. >; >; >; >; >; > Hi Rob,; >; >; > Thanks for the quick response. The other computer was OSX, should I try a; > linux machine?; >; >; > Here are some dropbox links to two of the files. I believe this is the set; > for the logs I",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:547,Security,validat,validateMappings,547,"I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.673]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:711,Security,validat,validateMappings,711,"I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.673]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:4224,Security,access,access,4224,"x rel diff.; = 0.229724; [2020-06-16 00:01:10.426] [jointLog] [info] iteration = 600 | max rel diff.; = 0.242741; [2020-06-16 00:01:12.163] [jointLog] [info] iteration = 700 | max rel diff.; = 0.196958; [2020-06-16 00:01:14.015] [jointLog] [info] iteration = 800 | max rel diff.; = 0.239461; [2020-06-16 00:01:15.803] [jointLog] [info] iteration = 900 | max rel diff.; = 0.197651; [2020-06-16 00:01:17.095] [jointLog] [info] iteration = 969 | max rel diff.; = 0.00714824; [2020-06-16 00:01:17.131] [jointLog] [info] Finished optimizer; [2020-06-16 00:01:17.131] [jointLog] [info] writing output; ```. So, you can note that the number of mappings discarded because of alignment; score is still quite big (134M) and, indeed, this took longer than I would; normally expect for; a sample with ~30M reads. However, it is _massively_ smaller (~116 times); than the 15.6B in your run. Thus, it may have been the same issue as in; the other GitHub issue. You can try the pre-compiled linux binary I linked; there if you have access to a linux machine to test on. By the way, the; issue there was a bunch of reads plagued by super-repetitive homopolymer; segments, I haven't looked at these reads yet for evidence of that though. On Mon, Jun 15, 2020 at 11:09 PM shalercr <notifications@github.com> wrote:. >; >; >; >; >; > Hi Rob,; >; >; > Thanks for the quick response. The other computer was OSX, should I try a; > linux machine?; >; >; > Here are some dropbox links to two of the files. I believe this is the set; > for the logs I posted.; >; >; > https://www.dropbox.com/s/8c3p9hmgmadgj89/31_1.trimmed.fastq.gz?dl=0; >; >; > https://www.dropbox.com/s/2y9jfvaphe9h21x/31_2.trimmed.fastq.gz?dl=0; >; >; >; >; > Thanks,; >; >; > Ryan; >; >; >; >; >; > On Jun 15, 2020, at 9:32 PM, Rob Patro <notifications@github.com> wrote:; >; >; >; >; > Thank you for the report. Can you share one of the samples where you see; > this issue? Also, out of curiosity, was the other machine you tried on also; > OSX, or was ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:265,Testability,log,log,265,"I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.673]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:4253,Testability,test,test,4253,"x rel diff.; = 0.229724; [2020-06-16 00:01:10.426] [jointLog] [info] iteration = 600 | max rel diff.; = 0.242741; [2020-06-16 00:01:12.163] [jointLog] [info] iteration = 700 | max rel diff.; = 0.196958; [2020-06-16 00:01:14.015] [jointLog] [info] iteration = 800 | max rel diff.; = 0.239461; [2020-06-16 00:01:15.803] [jointLog] [info] iteration = 900 | max rel diff.; = 0.197651; [2020-06-16 00:01:17.095] [jointLog] [info] iteration = 969 | max rel diff.; = 0.00714824; [2020-06-16 00:01:17.131] [jointLog] [info] Finished optimizer; [2020-06-16 00:01:17.131] [jointLog] [info] writing output; ```. So, you can note that the number of mappings discarded because of alignment; score is still quite big (134M) and, indeed, this took longer than I would; normally expect for; a sample with ~30M reads. However, it is _massively_ smaller (~116 times); than the 15.6B in your run. Thus, it may have been the same issue as in; the other GitHub issue. You can try the pre-compiled linux binary I linked; there if you have access to a linux machine to test on. By the way, the; issue there was a bunch of reads plagued by super-repetitive homopolymer; segments, I haven't looked at these reads yet for evidence of that though. On Mon, Jun 15, 2020 at 11:09 PM shalercr <notifications@github.com> wrote:. >; >; >; >; >; > Hi Rob,; >; >; > Thanks for the quick response. The other computer was OSX, should I try a; > linux machine?; >; >; > Here are some dropbox links to two of the files. I believe this is the set; > for the logs I posted.; >; >; > https://www.dropbox.com/s/8c3p9hmgmadgj89/31_1.trimmed.fastq.gz?dl=0; >; >; > https://www.dropbox.com/s/2y9jfvaphe9h21x/31_2.trimmed.fastq.gz?dl=0; >; >; >; >; > Thanks,; >; >; > Ryan; >; >; >; >; >; > On Jun 15, 2020, at 9:32 PM, Rob Patro <notifications@github.com> wrote:; >; >; >; >; > Thank you for the report. Can you share one of the samples where you see; > this issue? Also, out of curiosity, was the other machine you tried on also; > OSX, or was ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228:4726,Testability,log,logs,4726,"4; [2020-06-16 00:01:17.131] [jointLog] [info] Finished optimizer; [2020-06-16 00:01:17.131] [jointLog] [info] writing output; ```. So, you can note that the number of mappings discarded because of alignment; score is still quite big (134M) and, indeed, this took longer than I would; normally expect for; a sample with ~30M reads. However, it is _massively_ smaller (~116 times); than the 15.6B in your run. Thus, it may have been the same issue as in; the other GitHub issue. You can try the pre-compiled linux binary I linked; there if you have access to a linux machine to test on. By the way, the; issue there was a bunch of reads plagued by super-repetitive homopolymer; segments, I haven't looked at these reads yet for evidence of that though. On Mon, Jun 15, 2020 at 11:09 PM shalercr <notifications@github.com> wrote:. >; >; >; >; >; > Hi Rob,; >; >; > Thanks for the quick response. The other computer was OSX, should I try a; > linux machine?; >; >; > Here are some dropbox links to two of the files. I believe this is the set; > for the logs I posted.; >; >; > https://www.dropbox.com/s/8c3p9hmgmadgj89/31_1.trimmed.fastq.gz?dl=0; >; >; > https://www.dropbox.com/s/2y9jfvaphe9h21x/31_2.trimmed.fastq.gz?dl=0; >; >; >; >; > Thanks,; >; >; > Ryan; >; >; >; >; >; > On Jun 15, 2020, at 9:32 PM, Rob Patro <notifications@github.com> wrote:; >; >; >; >; > Thank you for the report. Can you share one of the samples where you see; > this issue? Also, out of curiosity, was the other machine you tried on also; > OSX, or was it a linux machine?; > —; > You are receiving this because you authored the thread.; > Reply to this email directly,; > view it on GitHub, or; > unsubscribe.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644504783>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AACYH7UHOYB7KYKDASFB5RDRW3O7HANCNFSM4N7EOYSQ>; > .; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644520228
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:24,Deployability,update,update,24,"Hi Rob,. Thanks for the update. I’ll see about setting up a Linux box in the morning and trying the v1.3.1. I expected some reads to be discharged as this is a mixed intestinal sample so there is likely a lot of bacterial rna as we used rRNA depletion not polyA; selection. We were hoping to align to both the mouse genome and one of the bacteria species of interest. . Given the several orders of magnitude difference in discarded alignments between mine on 1.2.1 and your test run on 1.3.1, would you recommend I redo the whole dataset alignment on 1.3.1? If it runs even close to what you saw it shouldn’t take too long; to rerun. . Thanks again,. Ryan. Sent from my iPhone. On Jun 16, 2020, at 12:13 AM, Rob Patro <notifications@github.com> wrote:. ﻿. I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:837,Deployability,release,release,837,"Hi Rob,. Thanks for the update. I’ll see about setting up a Linux box in the morning and trying the v1.3.1. I expected some reads to be discharged as this is a mixed intestinal sample so there is likely a lot of bacterial rna as we used rRNA depletion not polyA; selection. We were hoping to align to both the mouse genome and one of the bacteria species of interest. . Given the several orders of magnitude difference in discarded alignments between mine on 1.2.1 and your test run on 1.3.1, would you recommend I redo the whole dataset alignment on 1.3.1? If it runs even close to what you saw it shouldn’t take too long; to rerun. . Thanks again,. Ryan. Sent from my iPhone. On Jun 16, 2020, at 12:13 AM, Rob Patro <notifications@github.com> wrote:. ﻿. I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:1742,Performance,Load,Loading,1742,". I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.6",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:1812,Performance,Load,Loading,1812,"develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments discarded; because t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:3205,Performance,optimiz,optimizer,3205,"56,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size found was 1. [2020-06-16 00:00:59.673] [jointLog] [info] Number of mappings discarded; because of alignment score : 134,091,887; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments entirely; discarded because of alignment score : 2,429,390; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments discarded; because they are best-mapped to decoys : 0; [2020-06-16 00:00:59.673] [jointLog] [info] Number of fragments discarded; because they have only dovetail (discordant) mappings to valid targets :; 1,360,397; [2020-06-16 00:00:59.673] [jointLog] [info] Mapping rate = 45.4405%. [2020-06-16 00:00:59.673] [jointLog] [info] finished quantifyLibrary(); [2020-06-16 00:00:59.673] [jointLog] [info] Starting optimizer; [2020-06-16 00:00:59.792] [jointLog] [info] Marked 0 weighted equivalence; classes as degenerate; [2020-06-16 00:00:59.819] [jointLog] [info] iteration = 0 | max rel diff. =; 8250.92; [2020-06-16 00:00:59.667] [fileLog] [info]; At end of round 0; ==================; Observed 28512328 total fragments (28512328 in most recent round). [2020-06-16 00:01:01.745] [jointLog] [info] iteration = 100 | max rel diff.; = 19.507; [2020-06-16 00:01:03.495] [jointLog] [info] iteration = 200 | max rel diff.; = 2.45489; [2020-06-16 00:01:05.225] [jointLog] [info] iteration = 300 | max rel diff.; = 3.459; [2020-06-16 00:01:06.968] [jointLog] [info] iteration = 400 | max rel diff.; = 4.38485; [2020-06-16 00:01:08.693] [jointLog] [info] iteration = 500 | max rel diff.; = 0.229724; [2020-06-16 00:01:10.426] [jointLog] [info] iteration = 600 | max rel diff.; = 0.242741; [2020-06-16 00:01:12.163] [jointLog] [info] iteration = 700 | max rel diff.; = 0.196958; [2020-06-16 00:01:14.015] [jointLog] [i",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:4488,Performance,optimiz,optimizer,4488,"ved 28512328 total fragments (28512328 in most recent round). [2020-06-16 00:01:01.745] [jointLog] [info] iteration = 100 | max rel diff.; = 19.507; [2020-06-16 00:01:03.495] [jointLog] [info] iteration = 200 | max rel diff.; = 2.45489; [2020-06-16 00:01:05.225] [jointLog] [info] iteration = 300 | max rel diff.; = 3.459; [2020-06-16 00:01:06.968] [jointLog] [info] iteration = 400 | max rel diff.; = 4.38485; [2020-06-16 00:01:08.693] [jointLog] [info] iteration = 500 | max rel diff.; = 0.229724; [2020-06-16 00:01:10.426] [jointLog] [info] iteration = 600 | max rel diff.; = 0.242741; [2020-06-16 00:01:12.163] [jointLog] [info] iteration = 700 | max rel diff.; = 0.196958; [2020-06-16 00:01:14.015] [jointLog] [info] iteration = 800 | max rel diff.; = 0.239461; [2020-06-16 00:01:15.803] [jointLog] [info] iteration = 900 | max rel diff.; = 0.197651; [2020-06-16 00:01:17.095] [jointLog] [info] iteration = 969 | max rel diff.; = 0.00714824; [2020-06-16 00:01:17.131] [jointLog] [info] Finished optimizer; [2020-06-16 00:01:17.131] [jointLog] [info] writing output. ```. So, you can note that the number of mappings discarded because of alignment; score is still quite big (134M) and, indeed, this took longer than I would; normally expect for; a sample with ~30M reads. However, it is _massively_ smaller (~116 times); than the 15.6B in your run. Thus, it may have been the same issue as in; the other GitHub issue. You can try the pre-compiled linux binary I linked; there if you have access to a linux machine to test on. By the way, the; issue there was a bunch of reads plagued by super-repetitive homopolymer; segments, I haven't looked at these reads yet for evidence of that though. On Mon, Jun 15, 2020 at 11:09 PM shalercr <notifications@github.com> wrote:. >; >; >; >; >; > Hi Rob,; >; >; > Thanks for the quick response. The other computer was OSX, should I try a; > linux machine?; >; >; > Here are some dropbox links to two of the files. I believe this is the set; > for the logs I",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:1303,Security,validat,validateMappings,1303,"ouse genome and one of the bacteria species of interest. . Given the several orders of magnitude difference in discarded alignments between mine on 1.2.1 and your test run on 1.3.1, would you recommend I redo the whole dataset alignment on 1.3.1? If it runs even close to what you saw it shouldn’t take too long; to rerun. . Thanks again,. Ryan. Sent from my iPhone. On Jun 16, 2020, at 12:13 AM, Rob Patro <notifications@github.com> wrote:. ﻿. I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:1467,Security,validat,validateMappings,1467," run on 1.3.1, would you recommend I redo the whole dataset alignment on 1.3.1? If it runs even close to what you saw it shouldn’t take too long; to rerun. . Thanks again,. Ryan. Sent from my iPhone. On Jun 16, 2020, at 12:13 AM, Rob Patro <notifications@github.com> wrote:. ﻿. I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Number of decoys : 0; [2020-06-16 00:00:59.666] [jointLog] [info] Computed 344,764 rich; equivalence classes for further processing; [2020-06-16 00:00:59.666] [jointLog] [info] Counted 12,956,134 total reads; in the equivalence classes; [2020-06-16 00:00:59.673] [jointLog] [warning] 0.0736383% of fragments were; shorter than the k used to build the index.; If this fraction is too large, consider re-building the index with a; smaller k.; The minimum read size ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:4980,Security,access,access,4980,"x rel diff.; = 0.229724; [2020-06-16 00:01:10.426] [jointLog] [info] iteration = 600 | max rel diff.; = 0.242741; [2020-06-16 00:01:12.163] [jointLog] [info] iteration = 700 | max rel diff.; = 0.196958; [2020-06-16 00:01:14.015] [jointLog] [info] iteration = 800 | max rel diff.; = 0.239461; [2020-06-16 00:01:15.803] [jointLog] [info] iteration = 900 | max rel diff.; = 0.197651; [2020-06-16 00:01:17.095] [jointLog] [info] iteration = 969 | max rel diff.; = 0.00714824; [2020-06-16 00:01:17.131] [jointLog] [info] Finished optimizer; [2020-06-16 00:01:17.131] [jointLog] [info] writing output. ```. So, you can note that the number of mappings discarded because of alignment; score is still quite big (134M) and, indeed, this took longer than I would; normally expect for; a sample with ~30M reads. However, it is _massively_ smaller (~116 times); than the 15.6B in your run. Thus, it may have been the same issue as in; the other GitHub issue. You can try the pre-compiled linux binary I linked; there if you have access to a linux machine to test on. By the way, the; issue there was a bunch of reads plagued by super-repetitive homopolymer; segments, I haven't looked at these reads yet for evidence of that though. On Mon, Jun 15, 2020 at 11:09 PM shalercr <notifications@github.com> wrote:. >; >; >; >; >; > Hi Rob,; >; >; > Thanks for the quick response. The other computer was OSX, should I try a; > linux machine?; >; >; > Here are some dropbox links to two of the files. I believe this is the set; > for the logs I posted.; >; >; > https://www.dropbox.com/s/8c3p9hmgmadgj89/31_1.trimmed.fastq.gz?dl=0; >; >; > https://www.dropbox.com/s/2y9jfvaphe9h21x/31_2.trimmed.fastq.gz?dl=0; >; >; >; >; > Thanks,; >; >; > Ryan; >; >; >; >; >; > On Jun 15, 2020, at 9:32 PM, Rob Patro <notifications@github.com> wrote:; >; >; >; >; > Thank you for the report. Can you share one of the samples where you see; > this issue? Also, out of curiosity, was the other machine you tried on also; > OSX, or was ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:474,Testability,test,test,474,"Hi Rob,. Thanks for the update. I’ll see about setting up a Linux box in the morning and trying the v1.3.1. I expected some reads to be discharged as this is a mixed intestinal sample so there is likely a lot of bacterial rna as we used rRNA depletion not polyA; selection. We were hoping to align to both the mouse genome and one of the bacteria species of interest. . Given the several orders of magnitude difference in discarded alignments between mine on 1.2.1 and your test run on 1.3.1, would you recommend I redo the whole dataset alignment on 1.3.1? If it runs even close to what you saw it shouldn’t take too long; to rerun. . Thanks again,. Ryan. Sent from my iPhone. On Jun 16, 2020, at 12:13 AM, Rob Patro <notifications@github.com> wrote:. ﻿. I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:1021,Testability,log,log,1021," the update. I’ll see about setting up a Linux box in the morning and trying the v1.3.1. I expected some reads to be discharged as this is a mixed intestinal sample so there is likely a lot of bacterial rna as we used rRNA depletion not polyA; selection. We were hoping to align to both the mouse genome and one of the bacteria species of interest. . Given the several orders of magnitude difference in discarded alignments between mine on 1.2.1 and your test run on 1.3.1, would you recommend I redo the whole dataset alignment on 1.3.1? If it runs even close to what you saw it shouldn’t take too long; to rerun. . Thanks again,. Ryan. Sent from my iPhone. On Jun 16, 2020, at 12:13 AM, Rob Patro <notifications@github.com> wrote:. ﻿. I processed the data above using the latest salmon from the develop branch; (the release candidate for 1.3.0), and I got the following time (used 8; threads, so timing is not directly comparable). ```; 4604.57user 43.64system 9:24.15elapsed 823%CPU; ```. The whole log is. ```; [2020-06-15 23:51:54.747] [jointLog] [info] setting maxHashResizeThreads to; 8; [2020-06-15 23:51:54.747] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2020-06-15 23:51:54.747] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-06-15 23:51:54.747] [jointLog] [info] parsing read library format; [2020-06-15 23:51:54.747] [jointLog] [info] There is 1 library.; [2020-06-15 23:51:54.814] [jointLog] [info] Loading pufferfish index; [2020-06-15 23:51:54.814] [jointLog] [info] Loading dense pufferfish index.; [2020-06-15 23:51:55.555] [jointLog] [info] done; [2020-06-15 23:51:55.555] [jointLog] [info] Index contained 116,248 targets; [2020-06-15 23:51:55.588] [jointLog] [info] Num",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:5009,Testability,test,test,5009,"x rel diff.; = 0.229724; [2020-06-16 00:01:10.426] [jointLog] [info] iteration = 600 | max rel diff.; = 0.242741; [2020-06-16 00:01:12.163] [jointLog] [info] iteration = 700 | max rel diff.; = 0.196958; [2020-06-16 00:01:14.015] [jointLog] [info] iteration = 800 | max rel diff.; = 0.239461; [2020-06-16 00:01:15.803] [jointLog] [info] iteration = 900 | max rel diff.; = 0.197651; [2020-06-16 00:01:17.095] [jointLog] [info] iteration = 969 | max rel diff.; = 0.00714824; [2020-06-16 00:01:17.131] [jointLog] [info] Finished optimizer; [2020-06-16 00:01:17.131] [jointLog] [info] writing output. ```. So, you can note that the number of mappings discarded because of alignment; score is still quite big (134M) and, indeed, this took longer than I would; normally expect for; a sample with ~30M reads. However, it is _massively_ smaller (~116 times); than the 15.6B in your run. Thus, it may have been the same issue as in; the other GitHub issue. You can try the pre-compiled linux binary I linked; there if you have access to a linux machine to test on. By the way, the; issue there was a bunch of reads plagued by super-repetitive homopolymer; segments, I haven't looked at these reads yet for evidence of that though. On Mon, Jun 15, 2020 at 11:09 PM shalercr <notifications@github.com> wrote:. >; >; >; >; >; > Hi Rob,; >; >; > Thanks for the quick response. The other computer was OSX, should I try a; > linux machine?; >; >; > Here are some dropbox links to two of the files. I believe this is the set; > for the logs I posted.; >; >; > https://www.dropbox.com/s/8c3p9hmgmadgj89/31_1.trimmed.fastq.gz?dl=0; >; >; > https://www.dropbox.com/s/2y9jfvaphe9h21x/31_2.trimmed.fastq.gz?dl=0; >; >; >; >; > Thanks,; >; >; > Ryan; >; >; >; >; >; > On Jun 15, 2020, at 9:32 PM, Rob Patro <notifications@github.com> wrote:; >; >; >; >; > Thank you for the report. Can you share one of the samples where you see; > this issue? Also, out of curiosity, was the other machine you tried on also; > OSX, or was ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727:5482,Testability,log,logs,5482,"] [info] Finished optimizer; [2020-06-16 00:01:17.131] [jointLog] [info] writing output. ```. So, you can note that the number of mappings discarded because of alignment; score is still quite big (134M) and, indeed, this took longer than I would; normally expect for; a sample with ~30M reads. However, it is _massively_ smaller (~116 times); than the 15.6B in your run. Thus, it may have been the same issue as in; the other GitHub issue. You can try the pre-compiled linux binary I linked; there if you have access to a linux machine to test on. By the way, the; issue there was a bunch of reads plagued by super-repetitive homopolymer; segments, I haven't looked at these reads yet for evidence of that though. On Mon, Jun 15, 2020 at 11:09 PM shalercr <notifications@github.com> wrote:. >; >; >; >; >; > Hi Rob,; >; >; > Thanks for the quick response. The other computer was OSX, should I try a; > linux machine?; >; >; > Here are some dropbox links to two of the files. I believe this is the set; > for the logs I posted.; >; >; > https://www.dropbox.com/s/8c3p9hmgmadgj89/31_1.trimmed.fastq.gz?dl=0; >; >; > https://www.dropbox.com/s/2y9jfvaphe9h21x/31_2.trimmed.fastq.gz?dl=0; >; >; >; >; > Thanks,; >; >; > Ryan; >; >; >; >; >; > On Jun 15, 2020, at 9:32 PM, Rob Patro <notifications@github.com> wrote:; >; >; >; >; > Thank you for the report. Can you share one of the samples where you see; > this issue? Also, out of curiosity, was the other machine you tried on also; > OSX, or was it a linux machine?; > —; > You are receiving this because you authored the thread.; > Reply to this email directly,; > view it on GitHub, or; > unsubscribe.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644504783>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AACYH7UHOYB7KYKDASFB5RDRW3O7HANCNFSM4N7EOYSQ>; > .; >; —; You are receiving this because yo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644527727
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608:884,Availability,avail,available,884,"Hi @shalercr,. I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded _fragments_ (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608:809,Deployability,release,release,809,"Hi @shalercr,. I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded _fragments_ (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608:978,Deployability,release,release,978,"Hi @shalercr,. I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded _fragments_ (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608:623,Performance,perform,performance,623,"Hi @shalercr,. I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded _fragments_ (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608:141,Safety,avoid,avoids,141,"Hi @shalercr,. I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded _fragments_ (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-644796608
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801:1294,Availability,avail,available,1294,"Hey Rob,. I did manage to test v1.3 this evening. Ran much faster. The same sample that took about 6 hours ran in 45mins. Still not great, but I think it might be intrinsic to some of these samples, also I was running it off my laptop and was running Linux off a; flash drive so not an ideal setup. Either way much more reasonable. Do you want me to attach any logs or anything?. Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 10:20 AM, Rob Patro <notifications@github.com> wrote:. ﻿. Hi ; @shalercr,; I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded; fragments (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything; with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary. here. Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801:1219,Deployability,release,release,1219,"Hey Rob,. I did manage to test v1.3 this evening. Ran much faster. The same sample that took about 6 hours ran in 45mins. Still not great, but I think it might be intrinsic to some of these samples, also I was running it off my laptop and was running Linux off a; flash drive so not an ideal setup. Either way much more reasonable. Do you want me to attach any logs or anything?. Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 10:20 AM, Rob Patro <notifications@github.com> wrote:. ﻿. Hi ; @shalercr,; I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded; fragments (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything; with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary. here. Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801:1388,Deployability,release,release,1388,"Hey Rob,. I did manage to test v1.3 this evening. Ran much faster. The same sample that took about 6 hours ran in 45mins. Still not great, but I think it might be intrinsic to some of these samples, also I was running it off my laptop and was running Linux off a; flash drive so not an ideal setup. Either way much more reasonable. Do you want me to attach any logs or anything?. Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 10:20 AM, Rob Patro <notifications@github.com> wrote:. ﻿. Hi ; @shalercr,; I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded; fragments (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything; with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary. here. Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801:1118,Performance,perform,performance,1118,"Hey Rob,. I did manage to test v1.3 this evening. Ran much faster. The same sample that took about 6 hours ran in 45mins. Still not great, but I think it might be intrinsic to some of these samples, also I was running it off my laptop and was running Linux off a; flash drive so not an ideal setup. Either way much more reasonable. Do you want me to attach any logs or anything?. Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 10:20 AM, Rob Patro <notifications@github.com> wrote:. ﻿. Hi ; @shalercr,; I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded; fragments (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything; with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary. here. Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801:636,Safety,avoid,avoids,636,"Hey Rob,. I did manage to test v1.3 this evening. Ran much faster. The same sample that took about 6 hours ran in 45mins. Still not great, but I think it might be intrinsic to some of these samples, also I was running it off my laptop and was running Linux off a; flash drive so not an ideal setup. Either way much more reasonable. Do you want me to attach any logs or anything?. Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 10:20 AM, Rob Patro <notifications@github.com> wrote:. ﻿. Hi ; @shalercr,; I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded; fragments (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything; with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary. here. Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801:26,Testability,test,test,26,"Hey Rob,. I did manage to test v1.3 this evening. Ran much faster. The same sample that took about 6 hours ran in 45mins. Still not great, but I think it might be intrinsic to some of these samples, also I was running it off my laptop and was running Linux off a; flash drive so not an ideal setup. Either way much more reasonable. Do you want me to attach any logs or anything?. Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 10:20 AM, Rob Patro <notifications@github.com> wrote:. ﻿. Hi ; @shalercr,; I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded; fragments (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything; with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary. here. Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801:361,Testability,log,logs,361,"Hey Rob,. I did manage to test v1.3 this evening. Ran much faster. The same sample that took about 6 hours ran in 45mins. Still not great, but I think it might be intrinsic to some of these samples, also I was running it off my laptop and was running Linux off a; flash drive so not an ideal setup. Either way much more reasonable. Do you want me to attach any logs or anything?. Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 10:20 AM, Rob Patro <notifications@github.com> wrote:. ﻿. Hi ; @shalercr,; I agree that if each sample runs as quickly as you see here, it's not a big deal to just re-run them all. Note that, while my avoids ~116 times more (eventually-discarded) alignments than did yours, the difference in the number of discarded; fragments (i.e. fragments that had no mapping) is very small — ~7,000, or 0.023% of the fragments. Thus, despite the huge speed-up, I wouldn't expect many differences in the quantifications. However, I couldn't argue against re-quantifying everything; with the new version just for good measure :). When you have a chance, please do let me know if you see this same performance difference using the pre-compiled (linux) binary. here. Of course, once we finalize this release (soon), we will push to bioconda so we will also have OSX binaries available. However, I just want to make sure this fixes things on your end as well before the release.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645119801
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645122746:246,Testability,log,logs,246,"Hi @shalercr,. Thanks for reporting back! I agree that there are some challenging reads in these samples that are likely at the root of the slightly-longer-than-normal runtime. If you could upload the quant dir for this sample (that contains the logs), that would be useful. We (specifically, my student @mohsenzakeri, who is one of the main developers of the new selective-alignment algorithm) can poke around a bit to see if there is anything strange going on that can be characterized, but it might just be an inherent property of samples with very repetitive reads. Regardless, we'll be happy to take a look. Thanks!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645122746
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645127364:99,Availability,down,down,99,"P.S. @shalercr,. I also note that layering `--hitFilterPolicy BOTH` on top of the new version cuts down the time by another factor of 2 for me . ```; 2163.65user 12.72system 4:21.57elapsed 832%CPU (0avgtext+0avgdata 1221856maxresident)k; ```. and the number of mappings discarded alignments due to score comes down by another factor of ~6X. It might be worth seeing what you get with that option as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645127364
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645127364:310,Availability,down,down,310,"P.S. @shalercr,. I also note that layering `--hitFilterPolicy BOTH` on top of the new version cuts down the time by another factor of 2 for me . ```; 2163.65user 12.72system 4:21.57elapsed 832%CPU (0avgtext+0avgdata 1221856maxresident)k; ```. and the number of mappings discarded alignments due to score comes down by another factor of ~6X. It might be worth seeing what you get with that option as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645127364
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645762627:450,Availability,down,down,450,"Thanks for the heads up. I gave it a test this evening and wow, it is wicked fast. I’ll send you those quant files tomorrow when I get a chance, but adding that flag and the new version fixed the problem. . Thank you for all your help. . Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 11:36 PM, Rob Patro <notifications@github.com> wrote:. ﻿. P.S. ; @shalercr,; I also note that layering --hitFilterPolicy BOTH on top of the new version cuts down the time by another factor of 2 for me; 2163.65user 12.72system 4:21.57elapsed 832%CPU (0avgtext+0avgdata 1221856maxresident)k. and the number of mappings discarded alignments due to score comes down by another factor of ~6X. It might be worth seeing what you get with that option as well.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645762627
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645762627:650,Availability,down,down,650,"Thanks for the heads up. I gave it a test this evening and wow, it is wicked fast. I’ll send you those quant files tomorrow when I get a chance, but adding that flag and the new version fixed the problem. . Thank you for all your help. . Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 11:36 PM, Rob Patro <notifications@github.com> wrote:. ﻿. P.S. ; @shalercr,; I also note that layering --hitFilterPolicy BOTH on top of the new version cuts down the time by another factor of 2 for me; 2163.65user 12.72system 4:21.57elapsed 832%CPU (0avgtext+0avgdata 1221856maxresident)k. and the number of mappings discarded alignments due to score comes down by another factor of ~6X. It might be worth seeing what you get with that option as well.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645762627
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645762627:37,Testability,test,test,37,"Thanks for the heads up. I gave it a test this evening and wow, it is wicked fast. I’ll send you those quant files tomorrow when I get a chance, but adding that flag and the new version fixed the problem. . Thank you for all your help. . Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 11:36 PM, Rob Patro <notifications@github.com> wrote:. ﻿. P.S. ; @shalercr,; I also note that layering --hitFilterPolicy BOTH on top of the new version cuts down the time by another factor of 2 for me; 2163.65user 12.72system 4:21.57elapsed 832%CPU (0avgtext+0avgdata 1221856maxresident)k. and the number of mappings discarded alignments due to score comes down by another factor of ~6X. It might be worth seeing what you get with that option as well.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-645762627
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-647273636:830,Availability,down,down,830,"Hey Rob, . Sorry for the delay, here is a link to the quants folder if you guys are still interested. Everything worked well, especially with the additional flag. Any idea on the timeline for the bioconda release?. Best, . Ryan . https://www.dropbox.com/sh/rmy4f6brxx5iczo/AACxbyZFxN0XGcP3YRGjGO-pa?dl=0 . On Jun 18, 2020, at 12:21 AM, Ryan, Shaler <shalercr@mcmaster.ca> wrote:. Thanks for the heads up. I gave it a test this evening and wow, it is wicked fast. I’ll send you those quant files tomorrow when I get a chance, but adding that flag and the new version fixed the problem. . Thank you for all your help. . Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 11:36 PM, Rob Patro <notifications@github.com> wrote:. ﻿. P.S. ; @shalercr,; I also note that layering --hitFilterPolicy BOTH on top of the new version cuts down the time by another factor of 2 for me; 2163.65user 12.72system 4:21.57elapsed 832%CPU (0avgtext+0avgdata 1221856maxresident)k. and the number of mappings discarded alignments due to score comes down by another factor of ~6X. It might be worth seeing what you get with that option as well.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-647273636
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-647273636:1030,Availability,down,down,1030,"Hey Rob, . Sorry for the delay, here is a link to the quants folder if you guys are still interested. Everything worked well, especially with the additional flag. Any idea on the timeline for the bioconda release?. Best, . Ryan . https://www.dropbox.com/sh/rmy4f6brxx5iczo/AACxbyZFxN0XGcP3YRGjGO-pa?dl=0 . On Jun 18, 2020, at 12:21 AM, Ryan, Shaler <shalercr@mcmaster.ca> wrote:. Thanks for the heads up. I gave it a test this evening and wow, it is wicked fast. I’ll send you those quant files tomorrow when I get a chance, but adding that flag and the new version fixed the problem. . Thank you for all your help. . Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 11:36 PM, Rob Patro <notifications@github.com> wrote:. ﻿. P.S. ; @shalercr,; I also note that layering --hitFilterPolicy BOTH on top of the new version cuts down the time by another factor of 2 for me; 2163.65user 12.72system 4:21.57elapsed 832%CPU (0avgtext+0avgdata 1221856maxresident)k. and the number of mappings discarded alignments due to score comes down by another factor of ~6X. It might be worth seeing what you get with that option as well.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-647273636
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-647273636:205,Deployability,release,release,205,"Hey Rob, . Sorry for the delay, here is a link to the quants folder if you guys are still interested. Everything worked well, especially with the additional flag. Any idea on the timeline for the bioconda release?. Best, . Ryan . https://www.dropbox.com/sh/rmy4f6brxx5iczo/AACxbyZFxN0XGcP3YRGjGO-pa?dl=0 . On Jun 18, 2020, at 12:21 AM, Ryan, Shaler <shalercr@mcmaster.ca> wrote:. Thanks for the heads up. I gave it a test this evening and wow, it is wicked fast. I’ll send you those quant files tomorrow when I get a chance, but adding that flag and the new version fixed the problem. . Thank you for all your help. . Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 11:36 PM, Rob Patro <notifications@github.com> wrote:. ﻿. P.S. ; @shalercr,; I also note that layering --hitFilterPolicy BOTH on top of the new version cuts down the time by another factor of 2 for me; 2163.65user 12.72system 4:21.57elapsed 832%CPU (0avgtext+0avgdata 1221856maxresident)k. and the number of mappings discarded alignments due to score comes down by another factor of ~6X. It might be worth seeing what you get with that option as well.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-647273636
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-647273636:417,Testability,test,test,417,"Hey Rob, . Sorry for the delay, here is a link to the quants folder if you guys are still interested. Everything worked well, especially with the additional flag. Any idea on the timeline for the bioconda release?. Best, . Ryan . https://www.dropbox.com/sh/rmy4f6brxx5iczo/AACxbyZFxN0XGcP3YRGjGO-pa?dl=0 . On Jun 18, 2020, at 12:21 AM, Ryan, Shaler <shalercr@mcmaster.ca> wrote:. Thanks for the heads up. I gave it a test this evening and wow, it is wicked fast. I’ll send you those quant files tomorrow when I get a chance, but adding that flag and the new version fixed the problem. . Thank you for all your help. . Best,. Ryan . Sent from my iPhone. On Jun 16, 2020, at 11:36 PM, Rob Patro <notifications@github.com> wrote:. ﻿. P.S. ; @shalercr,; I also note that layering --hitFilterPolicy BOTH on top of the new version cuts down the time by another factor of 2 for me; 2163.65user 12.72system 4:21.57elapsed 832%CPU (0avgtext+0avgdata 1221856maxresident)k. and the number of mappings discarded alignments due to score comes down by another factor of ~6X. It might be worth seeing what you get with that option as well.; —; You are receiving this because you were mentioned.; Reply to this email directly, ; view it on GitHub, or ; unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-647273636
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-653791377:15,Deployability,release,released,15,"1.3.0 had been released, with theses improvements and a few others you can read about in the release notes. Let is know if you have any further issues!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-653791377
https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-653791377:93,Deployability,release,release,93,"1.3.0 had been released, with theses improvements and a few others you can read about in the release notes. Let is know if you have any further issues!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/537#issuecomment-653791377
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646058370:251,Safety,detect,detection,251,"Hi @deevdevil88,. The challenges I faced with this issue made me switch over to `kallisto` which has some nice advantages as far as speed. I didn't see any obvious affects on quality for my samples although I did have to re-implement some of the auto-detection that `alevin` and `salmon` do for you. . I personally observed some strange behaviour with Soupx - visually apparent differences in gene expression between samples that at the time I felt were artefactual of the adjustment by Soupx. I eventually rolled-my-own strategy where I omitted ambient outlier genes from differential expression. Ambient outliers were defined by taking droplets with UMI counts <10 with using a boxplot in R to define outliers. The osca.bioconductor.org [recommendations](https://osca.bioconductor.org/multi-sample-comparisons.html#ambient-problems) ended up being very similar. They also describe some of the pitfalls of adjusting counts. Best of luck! Always appreciative of all the great work and responsiveness of @k3yavi and the team!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646058370
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646058370:985,Usability,responsiv,responsiveness,985,"Hi @deevdevil88,. The challenges I faced with this issue made me switch over to `kallisto` which has some nice advantages as far as speed. I didn't see any obvious affects on quality for my samples although I did have to re-implement some of the auto-detection that `alevin` and `salmon` do for you. . I personally observed some strange behaviour with Soupx - visually apparent differences in gene expression between samples that at the time I felt were artefactual of the adjustment by Soupx. I eventually rolled-my-own strategy where I omitted ambient outlier genes from differential expression. Ambient outliers were defined by taking droplets with UMI counts <10 with using a boxplot in R to define outliers. The osca.bioconductor.org [recommendations](https://osca.bioconductor.org/multi-sample-comparisons.html#ambient-problems) ended up being very similar. They also describe some of the pitfalls of adjusting counts. Best of luck! Always appreciative of all the great work and responsiveness of @k3yavi and the team!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646058370
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646077084:481,Availability,down,downstream,481,"Thanks for the replies @alexvpickering @k3yavi ; As we have very high Ambient RNA and Doublet rate, we find that some of our key cell type genes for different neurotransmitters and glial expression are everywhere and this is to some extent affecting our clustering. While removing the doublets with 3 methods and taking a union of atleast 2 to get rid of cells helped, we still need to do something about the background.; So far the results seem ok, but we havent finished all our downstream analyses on the adjusted data to be sure that SoupX isnt doing something to the data that is weird. Thanks for the suggestions Avi, as I am no longer worried that I need CBs with UMIs in the range of 1-10 for SoupX i am using the default FreqThreshold 1 and only playing with MaxNumBarcodes and using KeepCBfraction 1.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646077084
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825059035:275,Availability,recover,recover,275,"@deevdevil88,. As an update to this, you can now use the alevin -> [alevin-fry](https://alevin-fry.readthedocs.io/en/latest/) pipeline to quantify with different strategies for filtering. If you're using a technology with an external permit list (like 10x chromium), you can recover and quantify unfiltered cells as well as of version 0.2.0 using the `--unfiltered-pl` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825059035
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825059035:21,Deployability,update,update,21,"@deevdevil88,. As an update to this, you can now use the alevin -> [alevin-fry](https://alevin-fry.readthedocs.io/en/latest/) pipeline to quantify with different strategies for filtering. If you're using a technology with an external permit list (like 10x chromium), you can recover and quantify unfiltered cells as well as of version 0.2.0 using the `--unfiltered-pl` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825059035
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825059035:126,Deployability,pipeline,pipeline,126,"@deevdevil88,. As an update to this, you can now use the alevin -> [alevin-fry](https://alevin-fry.readthedocs.io/en/latest/) pipeline to quantify with different strategies for filtering. If you're using a technology with an external permit list (like 10x chromium), you can recover and quantify unfiltered cells as well as of version 0.2.0 using the `--unfiltered-pl` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825059035
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825059035:275,Safety,recover,recover,275,"@deevdevil88,. As an update to this, you can now use the alevin -> [alevin-fry](https://alevin-fry.readthedocs.io/en/latest/) pipeline to quantify with different strategies for filtering. If you're using a technology with an external permit list (like 10x chromium), you can recover and quantify unfiltered cells as well as of version 0.2.0 using the `--unfiltered-pl` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825059035
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825303038:98,Testability,test,testing,98,"@deevdevil88,. Great! Please do let us know if you have any questions or run into any issues when testing it out :).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-825303038
https://github.com/COMBINE-lab/salmon/issues/539#issuecomment-647038324:354,Integrability,protocol,protocols,354,"The response by genomax [here](https://www.biostars.org/p/444853/) is spot on too. There are many, independent, overlapping reads here. If you have some other reason to believe this is wrong, we’d be happy to investigate further. However, I’m going to close the issue for the time being. . P.S. Yes, salmon is the right tool for full-length, plate-based protocols. Alevin is for tagged end (mostly droplet-based) protocols.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/539#issuecomment-647038324
https://github.com/COMBINE-lab/salmon/issues/539#issuecomment-647038324:413,Integrability,protocol,protocols,413,"The response by genomax [here](https://www.biostars.org/p/444853/) is spot on too. There are many, independent, overlapping reads here. If you have some other reason to believe this is wrong, we’d be happy to investigate further. However, I’m going to close the issue for the time being. . P.S. Yes, salmon is the right tool for full-length, plate-based protocols. Alevin is for tagged end (mostly droplet-based) protocols.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/539#issuecomment-647038324
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-647894779:111,Availability,error,error,111,Does `PB.40054.21` appear in your `PacBio/single_cell_pipeline/sqanti3_output/Alin_neg_txp2gene.tsv` file? The error is saying it is missing-- is it?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-647894779
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648204069:35,Availability,error,error,35,"@moschmi,. @roryk's parsing of the error is exactly what is intended. Specifically, alevin is looking to map the transcript `PB.40054.21` to a gene, but it is not finding that transcript in the file `PacBio/single_cell_pipeline/sqanti3_output/Alin_neg_txp2gene.tsv`. The question is a bit trickier when you have your own long-read derived transcriptome, since I don't know the process by which you are grouping transcripts into genes. It is always possible to have a transcript be its own gene (by having it appear in both the first and second columns in the same line of this file) — however, then you will lose information about how this transcript and other transcripts belonging to the same gene are related, which plays into how alevin quantifies gene-level abundance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648204069
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648248120:269,Availability,error,error,269,"Thanks @roryk and @rob-p ; The transcript PB.40054.21 is in all of the files: The gff files from which I made Alin_neg_txp2gene.tsv, the gentrome.fasta. The gene is ""novelGene_36859"". All are unique.; It should be indexed, I have several samples and they all have this error with various transcripts/genes.; What about this error: `ERROR: Txp to Gene Map not found for 52191 transcripts`; The transcript numbers are 88793",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648248120
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648248120:324,Availability,error,error,324,"Thanks @roryk and @rob-p ; The transcript PB.40054.21 is in all of the files: The gff files from which I made Alin_neg_txp2gene.tsv, the gentrome.fasta. The gene is ""novelGene_36859"". All are unique.; It should be indexed, I have several samples and they all have this error with various transcripts/genes.; What about this error: `ERROR: Txp to Gene Map not found for 52191 transcripts`; The transcript numbers are 88793",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648248120
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648248120:332,Availability,ERROR,ERROR,332,"Thanks @roryk and @rob-p ; The transcript PB.40054.21 is in all of the files: The gff files from which I made Alin_neg_txp2gene.tsv, the gentrome.fasta. The gene is ""novelGene_36859"". All are unique.; It should be indexed, I have several samples and they all have this error with various transcripts/genes.; What about this error: `ERROR: Txp to Gene Map not found for 52191 transcripts`; The transcript numbers are 88793",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648248120
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648250382:80,Availability,fault,fault,80,"Yes, I wonder if there is some sort of simple formatting issue that could be at fault here. Having a look at the file(s) would make it much easier to diagnose.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648250382
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648250382:39,Usability,simpl,simple,39,"Yes, I wonder if there is some sort of simple formatting issue that could be at fault here. Having a look at the file(s) would make it much easier to diagnose.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648250382
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648319730:187,Availability,error,error,187,"Hi @moschmi ,. Thanks for sharing the file.; You are right the transcript id `PB.40054.21` is indeed present & has the gene mapping in the file you forwarded. Unfortunately, the specific error message was not useful here but the issue is the following: ; ![image](https://user-images.githubusercontent.com/8772521/85436584-45706f00-b557-11ea-9ba2-95ebf4e43bc4.png). If you check the file you forwarded from line number 12,133 - 12,137, it seems a bunch of transcript ids are blank and has no assigned gene-ids. In this case the file parser was not intelligent enough to ignore such empty mappings and end up using the next line (before) tab as the wrong mapped gene-mappings. Later, when alevin sanity checks for the mappings of all transcripts, alevin complaints about not being able to find it for a random transcript much lower in the order. In short, I know you used the bioawk script for making the transcript to gene mapping file, but the script was written with the gencode generated GTF in mind, it seems the one you have has some small difference which is creating the issue. Currently, the easiest fix is to parse the GFF file again and generate the mappings for all the transcript in the proper format. In the future, we will add a sanity check for these type of corner cases, thanks for reporting this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648319730
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648319730:193,Integrability,message,message,193,"Hi @moschmi ,. Thanks for sharing the file.; You are right the transcript id `PB.40054.21` is indeed present & has the gene mapping in the file you forwarded. Unfortunately, the specific error message was not useful here but the issue is the following: ; ![image](https://user-images.githubusercontent.com/8772521/85436584-45706f00-b557-11ea-9ba2-95ebf4e43bc4.png). If you check the file you forwarded from line number 12,133 - 12,137, it seems a bunch of transcript ids are blank and has no assigned gene-ids. In this case the file parser was not intelligent enough to ignore such empty mappings and end up using the next line (before) tab as the wrong mapped gene-mappings. Later, when alevin sanity checks for the mappings of all transcripts, alevin complaints about not being able to find it for a random transcript much lower in the order. In short, I know you used the bioawk script for making the transcript to gene mapping file, but the script was written with the gencode generated GTF in mind, it seems the one you have has some small difference which is creating the issue. Currently, the easiest fix is to parse the GFF file again and generate the mappings for all the transcript in the proper format. In the future, we will add a sanity check for these type of corner cases, thanks for reporting this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648319730
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648319730:695,Safety,sanity check,sanity checks,695,"Hi @moschmi ,. Thanks for sharing the file.; You are right the transcript id `PB.40054.21` is indeed present & has the gene mapping in the file you forwarded. Unfortunately, the specific error message was not useful here but the issue is the following: ; ![image](https://user-images.githubusercontent.com/8772521/85436584-45706f00-b557-11ea-9ba2-95ebf4e43bc4.png). If you check the file you forwarded from line number 12,133 - 12,137, it seems a bunch of transcript ids are blank and has no assigned gene-ids. In this case the file parser was not intelligent enough to ignore such empty mappings and end up using the next line (before) tab as the wrong mapped gene-mappings. Later, when alevin sanity checks for the mappings of all transcripts, alevin complaints about not being able to find it for a random transcript much lower in the order. In short, I know you used the bioawk script for making the transcript to gene mapping file, but the script was written with the gencode generated GTF in mind, it seems the one you have has some small difference which is creating the issue. Currently, the easiest fix is to parse the GFF file again and generate the mappings for all the transcript in the proper format. In the future, we will add a sanity check for these type of corner cases, thanks for reporting this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648319730
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648319730:1243,Safety,sanity check,sanity check,1243,"Hi @moschmi ,. Thanks for sharing the file.; You are right the transcript id `PB.40054.21` is indeed present & has the gene mapping in the file you forwarded. Unfortunately, the specific error message was not useful here but the issue is the following: ; ![image](https://user-images.githubusercontent.com/8772521/85436584-45706f00-b557-11ea-9ba2-95ebf4e43bc4.png). If you check the file you forwarded from line number 12,133 - 12,137, it seems a bunch of transcript ids are blank and has no assigned gene-ids. In this case the file parser was not intelligent enough to ignore such empty mappings and end up using the next line (before) tab as the wrong mapped gene-mappings. Later, when alevin sanity checks for the mappings of all transcripts, alevin complaints about not being able to find it for a random transcript much lower in the order. In short, I know you used the bioawk script for making the transcript to gene mapping file, but the script was written with the gencode generated GTF in mind, it seems the one you have has some small difference which is creating the issue. Currently, the easiest fix is to parse the GFF file again and generate the mappings for all the transcript in the proper format. In the future, we will add a sanity check for these type of corner cases, thanks for reporting this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648319730
https://github.com/COMBINE-lab/salmon/issues/541#issuecomment-650568147:156,Deployability,release,release,156,Thanks @mathog : I've chosen another color that is readable on both light and dark backgrounds. These changes are on develop and will make it into the next release.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/541#issuecomment-650568147
https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239:1497,Energy Efficiency,adapt,adapter,1497," v0.8 and 1.2.1. The one that seems most relevant here is the introduction of selective-alignment to replace the quasi-mapping procedure that was originally used in salmon. Selective-alignment actually scores the mappings found to the transcriptome, and rejects alignments whose quality is below a (user-specified) threshold. Here, you can see that, in 1.2.1. * 39 fragments are mapped within the score threshold; * 216 fragments are discarded because no mapping location has an alignment score above the threshold. all together, this means that the total number of ""mapped"" fragments in 1.2.1 is very similar to 0.8 (1.2.1 maps 39+216 = 255, while 0.8 maps 254). However, 1.2.1 discards 216 of the fragments because no mapping is sufficiently good. The default for ""sufficiently good"", by the way, is to have an alignment score of at least 65% of the maximum possible for a read of the given length. For typical RNA-seq data, this is actually quite liberal / generous, and is similar to the type of noise in alignment that Bowtie2 allows with the sensitive flag. In general, the heuristics used in 1.2.1 (selective-alignment) tend to be more sensitive than those used in 0.8 (quasi-mapping), since the mappings are then validated using alignment scoring. However, this does mean that the quality of the alignment along the whole read matters. Thus, it is more important to do quality / adapter trimming in the newer version compared to the older one. There is also a flag in 1.2.1 (`--softclip`) that will allow mismatches / gaps at the ends of reads to not detract from the alignment score. So, these are the main differences. However, looking at the output logs you provided, a couple of basic questions did come to mind. Why are there so few reads to begin with? Even in 0.8, only 254 reads mapped, which is obviously a very small number of reads. Is there something non-typical about this sample? Is it a full RNA-seq sample? Are these reads something atypical (like long reads — ONT or PacBio)?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239
https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239:1497,Integrability,adapter,adapter,1497," v0.8 and 1.2.1. The one that seems most relevant here is the introduction of selective-alignment to replace the quasi-mapping procedure that was originally used in salmon. Selective-alignment actually scores the mappings found to the transcriptome, and rejects alignments whose quality is below a (user-specified) threshold. Here, you can see that, in 1.2.1. * 39 fragments are mapped within the score threshold; * 216 fragments are discarded because no mapping location has an alignment score above the threshold. all together, this means that the total number of ""mapped"" fragments in 1.2.1 is very similar to 0.8 (1.2.1 maps 39+216 = 255, while 0.8 maps 254). However, 1.2.1 discards 216 of the fragments because no mapping is sufficiently good. The default for ""sufficiently good"", by the way, is to have an alignment score of at least 65% of the maximum possible for a read of the given length. For typical RNA-seq data, this is actually quite liberal / generous, and is similar to the type of noise in alignment that Bowtie2 allows with the sensitive flag. In general, the heuristics used in 1.2.1 (selective-alignment) tend to be more sensitive than those used in 0.8 (quasi-mapping), since the mappings are then validated using alignment scoring. However, this does mean that the quality of the alignment along the whole read matters. Thus, it is more important to do quality / adapter trimming in the newer version compared to the older one. There is also a flag in 1.2.1 (`--softclip`) that will allow mismatches / gaps at the ends of reads to not detract from the alignment score. So, these are the main differences. However, looking at the output logs you provided, a couple of basic questions did come to mind. Why are there so few reads to begin with? Even in 0.8, only 254 reads mapped, which is obviously a very small number of reads. Is there something non-typical about this sample? Is it a full RNA-seq sample? Are these reads something atypical (like long reads — ONT or PacBio)?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239
https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239:1497,Modifiability,adapt,adapter,1497," v0.8 and 1.2.1. The one that seems most relevant here is the introduction of selective-alignment to replace the quasi-mapping procedure that was originally used in salmon. Selective-alignment actually scores the mappings found to the transcriptome, and rejects alignments whose quality is below a (user-specified) threshold. Here, you can see that, in 1.2.1. * 39 fragments are mapped within the score threshold; * 216 fragments are discarded because no mapping location has an alignment score above the threshold. all together, this means that the total number of ""mapped"" fragments in 1.2.1 is very similar to 0.8 (1.2.1 maps 39+216 = 255, while 0.8 maps 254). However, 1.2.1 discards 216 of the fragments because no mapping is sufficiently good. The default for ""sufficiently good"", by the way, is to have an alignment score of at least 65% of the maximum possible for a read of the given length. For typical RNA-seq data, this is actually quite liberal / generous, and is similar to the type of noise in alignment that Bowtie2 allows with the sensitive flag. In general, the heuristics used in 1.2.1 (selective-alignment) tend to be more sensitive than those used in 0.8 (quasi-mapping), since the mappings are then validated using alignment scoring. However, this does mean that the quality of the alignment along the whole read matters. Thus, it is more important to do quality / adapter trimming in the newer version compared to the older one. There is also a flag in 1.2.1 (`--softclip`) that will allow mismatches / gaps at the ends of reads to not detract from the alignment score. So, these are the main differences. However, looking at the output logs you provided, a couple of basic questions did come to mind. Why are there so few reads to begin with? Even in 0.8, only 254 reads mapped, which is obviously a very small number of reads. Is there something non-typical about this sample? Is it a full RNA-seq sample? Are these reads something atypical (like long reads — ONT or PacBio)?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239
https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239:1331,Security,validat,validated,1331," v0.8 and 1.2.1. The one that seems most relevant here is the introduction of selective-alignment to replace the quasi-mapping procedure that was originally used in salmon. Selective-alignment actually scores the mappings found to the transcriptome, and rejects alignments whose quality is below a (user-specified) threshold. Here, you can see that, in 1.2.1. * 39 fragments are mapped within the score threshold; * 216 fragments are discarded because no mapping location has an alignment score above the threshold. all together, this means that the total number of ""mapped"" fragments in 1.2.1 is very similar to 0.8 (1.2.1 maps 39+216 = 255, while 0.8 maps 254). However, 1.2.1 discards 216 of the fragments because no mapping is sufficiently good. The default for ""sufficiently good"", by the way, is to have an alignment score of at least 65% of the maximum possible for a read of the given length. For typical RNA-seq data, this is actually quite liberal / generous, and is similar to the type of noise in alignment that Bowtie2 allows with the sensitive flag. In general, the heuristics used in 1.2.1 (selective-alignment) tend to be more sensitive than those used in 0.8 (quasi-mapping), since the mappings are then validated using alignment scoring. However, this does mean that the quality of the alignment along the whole read matters. Thus, it is more important to do quality / adapter trimming in the newer version compared to the older one. There is also a flag in 1.2.1 (`--softclip`) that will allow mismatches / gaps at the ends of reads to not detract from the alignment score. So, these are the main differences. However, looking at the output logs you provided, a couple of basic questions did come to mind. Why are there so few reads to begin with? Even in 0.8, only 254 reads mapped, which is obviously a very small number of reads. Is there something non-typical about this sample? Is it a full RNA-seq sample? Are these reads something atypical (like long reads — ONT or PacBio)?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239
https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239:1770,Testability,log,logs,1770," v0.8 and 1.2.1. The one that seems most relevant here is the introduction of selective-alignment to replace the quasi-mapping procedure that was originally used in salmon. Selective-alignment actually scores the mappings found to the transcriptome, and rejects alignments whose quality is below a (user-specified) threshold. Here, you can see that, in 1.2.1. * 39 fragments are mapped within the score threshold; * 216 fragments are discarded because no mapping location has an alignment score above the threshold. all together, this means that the total number of ""mapped"" fragments in 1.2.1 is very similar to 0.8 (1.2.1 maps 39+216 = 255, while 0.8 maps 254). However, 1.2.1 discards 216 of the fragments because no mapping is sufficiently good. The default for ""sufficiently good"", by the way, is to have an alignment score of at least 65% of the maximum possible for a read of the given length. For typical RNA-seq data, this is actually quite liberal / generous, and is similar to the type of noise in alignment that Bowtie2 allows with the sensitive flag. In general, the heuristics used in 1.2.1 (selective-alignment) tend to be more sensitive than those used in 0.8 (quasi-mapping), since the mappings are then validated using alignment scoring. However, this does mean that the quality of the alignment along the whole read matters. Thus, it is more important to do quality / adapter trimming in the newer version compared to the older one. There is also a flag in 1.2.1 (`--softclip`) that will allow mismatches / gaps at the ends of reads to not detract from the alignment score. So, these are the main differences. However, looking at the output logs you provided, a couple of basic questions did come to mind. Why are there so few reads to begin with? Even in 0.8, only 254 reads mapped, which is obviously a very small number of reads. Is there something non-typical about this sample? Is it a full RNA-seq sample? Are these reads something atypical (like long reads — ONT or PacBio)?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/542#issuecomment-651332239
https://github.com/COMBINE-lab/salmon/issues/549#issuecomment-660248736:81,Availability,error,error,81,"Hi, . Sorry for not mentioning it previously, can you please also attach/add the error log you are getting.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/549#issuecomment-660248736
https://github.com/COMBINE-lab/salmon/issues/549#issuecomment-660248736:87,Testability,log,log,87,"Hi, . Sorry for not mentioning it previously, can you please also attach/add the error log you are getting.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/549#issuecomment-660248736
https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577187733:28,Deployability,install,installing,28,"Hi, I have the same problem installing version 1.10.1, Do you find any solution?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577187733
https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577237260:338,Deployability,install,installed,338,"This *usually* means that the version of the boost library you have was not compiled with a C++11-compatible ABI. There is a incompatibility between pre C++11 and post C++11 `std::string` representations, and since salmon uses modern C++ (C++14 as of this writing), you need a version of boost compiled in a compatible way. How was boost installed on your system?. Of course, if you don't need to compile from source, it's *much* easier to install via conda, or to grab the pre-built executable (1.10.0 is feature and bugfix identical to 1.10.1). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577237260
https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577237260:440,Deployability,install,install,440,"This *usually* means that the version of the boost library you have was not compiled with a C++11-compatible ABI. There is a incompatibility between pre C++11 and post C++11 `std::string` representations, and since salmon uses modern C++ (C++14 as of this writing), you need a version of boost compiled in a compatible way. How was boost installed on your system?. Of course, if you don't need to compile from source, it's *much* easier to install via conda, or to grab the pre-built executable (1.10.0 is feature and bugfix identical to 1.10.1). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577237260
https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952:115,Deployability,Integrat,Integrative,115,"Hi, thank you for your reply, I prefer to use the pre-built version 1.10.0. * J. Eduardo Martinez-Hernandez*; *PhD Integrative Genomics*. El lun, 5 jun 2023 a las 14:02, Rob Patro ***@***.***>); escribió:. > This *usually* means that the version of the boost library you have was; > not compiled with a C++11-compatible ABI. There is a incompatibility; > between pre C++11 and post C++11 std::string representations, and since; > salmon uses modern C++ (C++14 as of this writing), you need a version of; > boost compiled in a compatible way. How was boost installed on your system?; >; > Of course, if you don't need to compile from source, it's *much* easier; > to install via conda, or to grab the pre-built executable (1.10.0 is; > feature and bugfix identical to 1.10.1).; >; > Best,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577237260>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AM6EVFLJX45SB67GSQ3RYO3XJYNLBANCNFSM4PG7JFPQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952
https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952:556,Deployability,install,installed,556,"Hi, thank you for your reply, I prefer to use the pre-built version 1.10.0. * J. Eduardo Martinez-Hernandez*; *PhD Integrative Genomics*. El lun, 5 jun 2023 a las 14:02, Rob Patro ***@***.***>); escribió:. > This *usually* means that the version of the boost library you have was; > not compiled with a C++11-compatible ABI. There is a incompatibility; > between pre C++11 and post C++11 std::string representations, and since; > salmon uses modern C++ (C++14 as of this writing), you need a version of; > boost compiled in a compatible way. How was boost installed on your system?; >; > Of course, if you don't need to compile from source, it's *much* easier; > to install via conda, or to grab the pre-built executable (1.10.0 is; > feature and bugfix identical to 1.10.1).; >; > Best,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577237260>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AM6EVFLJX45SB67GSQ3RYO3XJYNLBANCNFSM4PG7JFPQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952
https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952:666,Deployability,install,install,666,"Hi, thank you for your reply, I prefer to use the pre-built version 1.10.0. * J. Eduardo Martinez-Hernandez*; *PhD Integrative Genomics*. El lun, 5 jun 2023 a las 14:02, Rob Patro ***@***.***>); escribió:. > This *usually* means that the version of the boost library you have was; > not compiled with a C++11-compatible ABI. There is a incompatibility; > between pre C++11 and post C++11 std::string representations, and since; > salmon uses modern C++ (C++14 as of this writing), you need a version of; > boost compiled in a compatible way. How was boost installed on your system?; >; > Of course, if you don't need to compile from source, it's *much* easier; > to install via conda, or to grab the pre-built executable (1.10.0 is; > feature and bugfix identical to 1.10.1).; >; > Best,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577237260>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AM6EVFLJX45SB67GSQ3RYO3XJYNLBANCNFSM4PG7JFPQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952
https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952:115,Integrability,Integrat,Integrative,115,"Hi, thank you for your reply, I prefer to use the pre-built version 1.10.0. * J. Eduardo Martinez-Hernandez*; *PhD Integrative Genomics*. El lun, 5 jun 2023 a las 14:02, Rob Patro ***@***.***>); escribió:. > This *usually* means that the version of the boost library you have was; > not compiled with a C++11-compatible ABI. There is a incompatibility; > between pre C++11 and post C++11 std::string representations, and since; > salmon uses modern C++ (C++14 as of this writing), you need a version of; > boost compiled in a compatible way. How was boost installed on your system?; >; > Of course, if you don't need to compile from source, it's *much* easier; > to install via conda, or to grab the pre-built executable (1.10.0 is; > feature and bugfix identical to 1.10.1).; >; > Best,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577237260>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AM6EVFLJX45SB67GSQ3RYO3XJYNLBANCNFSM4PG7JFPQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952
https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952:1104,Integrability,Message,Message,1104,"Hi, thank you for your reply, I prefer to use the pre-built version 1.10.0. * J. Eduardo Martinez-Hernandez*; *PhD Integrative Genomics*. El lun, 5 jun 2023 a las 14:02, Rob Patro ***@***.***>); escribió:. > This *usually* means that the version of the boost library you have was; > not compiled with a C++11-compatible ABI. There is a incompatibility; > between pre C++11 and post C++11 std::string representations, and since; > salmon uses modern C++ (C++14 as of this writing), you need a version of; > boost compiled in a compatible way. How was boost installed on your system?; >; > Of course, if you don't need to compile from source, it's *much* easier; > to install via conda, or to grab the pre-built executable (1.10.0 is; > feature and bugfix identical to 1.10.1).; >; > Best,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577237260>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AM6EVFLJX45SB67GSQ3RYO3XJYNLBANCNFSM4PG7JFPQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/551#issuecomment-1577285952
https://github.com/COMBINE-lab/salmon/issues/554#issuecomment-668140567:303,Deployability,release,releases,303,"Thanks for the report @arrowandbead,. Can you say something about the machine that you're running the indexing command on, and the FASTA file you're trying to index? We've seen poor behavior before specifically on cluster nodes with slow (networked) disk, but this should be largely mitigated in recent releases. --Rob. Edit: I saw that you closed the issue and identified the problem. Thanks for closing and for changing the title!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/554#issuecomment-668140567
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:250,Availability,avail,available,250,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:676,Availability,down,download,676,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:40,Deployability,release,release,40,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:317,Deployability,release,releases,317,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:534,Deployability,install,install,534,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:667,Deployability,release,releases,667,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:970,Deployability,install,install,970,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:765,Modifiability,config,config,765,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:806,Modifiability,config,config,806,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:846,Modifiability,config,config,846,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:886,Modifiability,config,config,886,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247:926,Modifiability,config,config,926,"Hi @rob-p . Thanks for the thanksgiving release of salmon. We saw salmon 1.3.0 for July 4th, salmon 1.4.0 for Thanksgiving, hopefully we would see 1.5.0 for Christmas and 1.6.0 for New Year :) . Keep up the great work. I see that you have salmon now available for ARM machines - https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0. As myself and @patrick-douglas has requested here - https://github.com/COMBINE-lab/salmon/issues/424 - the ability to compile salmon on ARM is great. A question: Will this work if I do a bioconda install or do I need to compile using the tar.gz file. I am using the ARM based conda from https://github.com/Archiconda/build-tools/releases/download/0.2.3/Archiconda3-0.2.3-Linux-aarch64.sh and with that I could do ; ``` ; conda config --add channels c4aarch64 && conda config --add channels defaults && conda config --add channels anaconda && conda config --add channels bioconda && conda config --add channels conda-forge ; ; conda install -c bioconda -c conda-forge -y nextflow bbmap numpy fastqc. ```. I don't see arch64 for salmon in bioconda here - https://anaconda.org/bioconda/salmon",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734442247
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734954158:135,Deployability,release,release,135,"Hi @tamuanand,. Yes, we had just about finished up a nice set of features and I thought it would be fun to have another holiday-themed release :). I'm not sure if we'll hit all of the holidays but we definitely have more stuff in the pipeline over the next few months. Currently, the ARM build has to be compiled from source. I asked a leader of the bioconda team about automatically compiling ARM binaries through the mainstream bioconda and having them be part of the package management system. They told me this isn't currently possible, but that capability should exist early next year. Once it's feasible upstream in the main bioconda, we'll have ARM executables built as well. For the time being, building from source is probably the easiest way. We may also make a pre-compiled ARM binary (e.g. on an AWS machine), though that will have limited compatibility, so its not as preferred as building from source.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734954158
https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734954158:234,Deployability,pipeline,pipeline,234,"Hi @tamuanand,. Yes, we had just about finished up a nice set of features and I thought it would be fun to have another holiday-themed release :). I'm not sure if we'll hit all of the holidays but we definitely have more stuff in the pipeline over the next few months. Currently, the ARM build has to be compiled from source. I asked a leader of the bioconda team about automatically compiling ARM binaries through the mainstream bioconda and having them be part of the package management system. They told me this isn't currently possible, but that capability should exist early next year. Once it's feasible upstream in the main bioconda, we'll have ARM executables built as well. For the time being, building from source is probably the easiest way. We may also make a pre-compiled ARM binary (e.g. on an AWS machine), though that will have limited compatibility, so its not as preferred as building from source.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/556#issuecomment-734954158
https://github.com/COMBINE-lab/salmon/issues/558#issuecomment-673153902:60,Availability,down,download,60,"Thanks. The ref genie thing might be a little big for me to download to be honest. I talked to my PI and he said it was to be expected that the RNA might be low quality. I ran it against mouse DNA as well and the mouse was multiples worse so I guess that's a good sign. The decoy hits also outnumber the mapped hits by about 6:1. But they never exceed about 7%. Does this indicate something wrong with my indexing?. I also tried mapping indexing against human CDS and NCRNA files from ftp://ftp.ensembl.org/pub/release-100/fasta/homo_sapiens/ and the human genome from the same source. Those had even lower hit rates. That was odd because rRNA wasn't filtered out for this RNA seq, so I would have expected the rRNA parts of the NCRNA to have a lot of hits. But alas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/558#issuecomment-673153902
https://github.com/COMBINE-lab/salmon/issues/558#issuecomment-673153902:511,Deployability,release,release-,511,"Thanks. The ref genie thing might be a little big for me to download to be honest. I talked to my PI and he said it was to be expected that the RNA might be low quality. I ran it against mouse DNA as well and the mouse was multiples worse so I guess that's a good sign. The decoy hits also outnumber the mapped hits by about 6:1. But they never exceed about 7%. Does this indicate something wrong with my indexing?. I also tried mapping indexing against human CDS and NCRNA files from ftp://ftp.ensembl.org/pub/release-100/fasta/homo_sapiens/ and the human genome from the same source. Those had even lower hit rates. That was odd because rRNA wasn't filtered out for this RNA seq, so I would have expected the rRNA parts of the NCRNA to have a lot of hits. But alas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/558#issuecomment-673153902
https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-674765288:141,Usability,usab,usable,141,@hariiyer16 I have not! The index works in an older version of salmon but not the newer one even though it was built in a way that should be usable by the newer Salmon version.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-674765288
https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-674827706:191,Availability,down,down,191,"@kalanir,. Can you please direct me to the files you used to build the index? Is the docker image above the one from docker hub? I would be hopeful, if this happens in docker, we could track down what's going on. When you say it works with an older version, can you tell me whst version works for you with this index?. Thanks, ; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-674827706
https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-738480434:198,Availability,error,error,198,"I suspect you're running this on a Mac, correct? ; I had the same issue, Salmon would index for the better part of a day, use ~30 gb of swap space, then die (killed 9 message). The versionInfo.json error is slightly misleading, most likely what happened is that the indexing didn't finish and since that file is only created if it finishes correctly, that's the error you get.; Running the indexing with the same code, with similar resources completed on our AWS node so I wonder if this is a Mac specific problem...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-738480434
https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-738480434:362,Availability,error,error,362,"I suspect you're running this on a Mac, correct? ; I had the same issue, Salmon would index for the better part of a day, use ~30 gb of swap space, then die (killed 9 message). The versionInfo.json error is slightly misleading, most likely what happened is that the indexing didn't finish and since that file is only created if it finishes correctly, that's the error you get.; Running the indexing with the same code, with similar resources completed on our AWS node so I wonder if this is a Mac specific problem...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-738480434
https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-738480434:167,Integrability,message,message,167,"I suspect you're running this on a Mac, correct? ; I had the same issue, Salmon would index for the better part of a day, use ~30 gb of swap space, then die (killed 9 message). The versionInfo.json error is slightly misleading, most likely what happened is that the indexing didn't finish and since that file is only created if it finishes correctly, that's the error you get.; Running the indexing with the same code, with similar resources completed on our AWS node so I wonder if this is a Mac specific problem...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-738480434
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674423103:123,Testability,test,test,123,"They should be unnecessary to diagnose, but if you want to extract the first 100k reads or so, we can try and use them for test quantification.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674423103
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674454751:1855,Performance,perform,perform,1855,"en I build this index on one of our machines. Perhaps we could see if these match: . ```; $:salmon_index [j1] (develop ?) $ sha256sum *; 306e9d98b3460859f579059bf876aa3b6e264c8f38c04cde332b03632edc6dfb complete_ref_lens.bin; 28519aac34b84b4d0570c97340815e719511c204e04a240dd43e365d2872eed3 ctable.bin; 1c7501deaa4524f4700152713228cb03949775dce481384eac67bb45458508be ctg_offsets.bin; dbc575fed0d589b4671c26bd8cbcb4b3d52ef41c299a90de978ab940abb751fc duplicate_clusters.tsv; 987050914456cf247a24136429d8faaa293cf5617bfd57166c64976b2778d95b info.json; 0b7e8cb4ebed78513900831c047f0d66589068921c33bb15c49b3567c84e2edc mphf.bin; 117369928fde1bff4ca278246c331e079cc0860c3b415e34cd4b08f588063abc pos.bin; 297492e67d274b2ff8f026d2fbc8045f96e17793a58dd74c19b5ab1b7156df8a pre_indexing.log; 8e665e5fdee5af6fcedabc69fd04eda6e66055ef811ebde6de6f86a66521198a rank.bin; 793c79f5fd6046dfea07bbc9587d2835088e54c78197d652d1b1f205c6b16983 refAccumLengths.bin; 92acf575c90c6954ff75be1ea791f822eee05e486c6e86c52943d8bc1a0849ca ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 94cb79a2f4acd811d2164f2926c96869a8103b9118170d0688f57b46e695cd5c seq.bin; 89d56bb135f32c7b5fa337bc3c45814b80c2886a3cccc31ff0533c6324ca11fd versionInfo.json; ```. I'm also including a link [here](https://drive.google.com/file/d/1uxGUy8gaQ20dpEi7-D3ookFF4JYawsIR/view?usp=sharing) to a tarball containing the index I built. Could you see if you can perform quantification with this index? Finally, it might be worth checking that nothing strange / unexpected is going on with how libraries are being resolved in the linker path when you are running salmon. Could you share the output of running `ldd salmon`? If none of those point at anything obvious, I might also suggest seeing if it runs as expected inside a Docker container. You can grab a dockerfile for salmon [here](https://hub.docker.com/r/combinelab/salmon).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674454751
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674454751:1101,Testability,log,log,1101," mapping and quantifying reads. From the files you shared, it certainly _does_ seem like the index is being created correctly. I'm including here the sha256sum of the index files I get when I build this index on one of our machines. Perhaps we could see if these match: . ```; $:salmon_index [j1] (develop ?) $ sha256sum *; 306e9d98b3460859f579059bf876aa3b6e264c8f38c04cde332b03632edc6dfb complete_ref_lens.bin; 28519aac34b84b4d0570c97340815e719511c204e04a240dd43e365d2872eed3 ctable.bin; 1c7501deaa4524f4700152713228cb03949775dce481384eac67bb45458508be ctg_offsets.bin; dbc575fed0d589b4671c26bd8cbcb4b3d52ef41c299a90de978ab940abb751fc duplicate_clusters.tsv; 987050914456cf247a24136429d8faaa293cf5617bfd57166c64976b2778d95b info.json; 0b7e8cb4ebed78513900831c047f0d66589068921c33bb15c49b3567c84e2edc mphf.bin; 117369928fde1bff4ca278246c331e079cc0860c3b415e34cd4b08f588063abc pos.bin; 297492e67d274b2ff8f026d2fbc8045f96e17793a58dd74c19b5ab1b7156df8a pre_indexing.log; 8e665e5fdee5af6fcedabc69fd04eda6e66055ef811ebde6de6f86a66521198a rank.bin; 793c79f5fd6046dfea07bbc9587d2835088e54c78197d652d1b1f205c6b16983 refAccumLengths.bin; 92acf575c90c6954ff75be1ea791f822eee05e486c6e86c52943d8bc1a0849ca ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 94cb79a2f4acd811d2164f2926c96869a8103b9118170d0688f57b46e695cd5c seq.bin; 89d56bb135f32c7b5fa337bc3c45814b80c2886a3cccc31ff0533c6324ca11fd versionInfo.json; ```. I'm also including a link [here](https://drive.google.com/file/d/1uxGUy8gaQ20dpEi7-D3ookFF4JYawsIR/view?usp=sharing) to a tarball containing the index I built. Could you see if you can perform quantification with this index? Finally, it might be worth checking that nothing strange / unexpected is going on with how libraries are being resolved in the linker path when you are running salmon. Could you share the output of running `ldd salmon`? If none of those point a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674454751
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674454751:1345,Testability,log,log,1345,"en I build this index on one of our machines. Perhaps we could see if these match: . ```; $:salmon_index [j1] (develop ?) $ sha256sum *; 306e9d98b3460859f579059bf876aa3b6e264c8f38c04cde332b03632edc6dfb complete_ref_lens.bin; 28519aac34b84b4d0570c97340815e719511c204e04a240dd43e365d2872eed3 ctable.bin; 1c7501deaa4524f4700152713228cb03949775dce481384eac67bb45458508be ctg_offsets.bin; dbc575fed0d589b4671c26bd8cbcb4b3d52ef41c299a90de978ab940abb751fc duplicate_clusters.tsv; 987050914456cf247a24136429d8faaa293cf5617bfd57166c64976b2778d95b info.json; 0b7e8cb4ebed78513900831c047f0d66589068921c33bb15c49b3567c84e2edc mphf.bin; 117369928fde1bff4ca278246c331e079cc0860c3b415e34cd4b08f588063abc pos.bin; 297492e67d274b2ff8f026d2fbc8045f96e17793a58dd74c19b5ab1b7156df8a pre_indexing.log; 8e665e5fdee5af6fcedabc69fd04eda6e66055ef811ebde6de6f86a66521198a rank.bin; 793c79f5fd6046dfea07bbc9587d2835088e54c78197d652d1b1f205c6b16983 refAccumLengths.bin; 92acf575c90c6954ff75be1ea791f822eee05e486c6e86c52943d8bc1a0849ca ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 94cb79a2f4acd811d2164f2926c96869a8103b9118170d0688f57b46e695cd5c seq.bin; 89d56bb135f32c7b5fa337bc3c45814b80c2886a3cccc31ff0533c6324ca11fd versionInfo.json; ```. I'm also including a link [here](https://drive.google.com/file/d/1uxGUy8gaQ20dpEi7-D3ookFF4JYawsIR/view?usp=sharing) to a tarball containing the index I built. Could you see if you can perform quantification with this index? Finally, it might be worth checking that nothing strange / unexpected is going on with how libraries are being resolved in the linker path when you are running salmon. Could you share the output of running `ldd salmon`? If none of those point at anything obvious, I might also suggest seeing if it runs as expected inside a Docker container. You can grab a dockerfile for salmon [here](https://hub.docker.com/r/combinelab/salmon).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674454751
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480:227,Deployability,update,update,227,"Hi Rob, After posting yesterdays message, I generated the vM23 index, and the alignments/quants worked. I had to use mem_free=34G for building index. Is that expected?; I will try building the vM25 index again and and post the update.; In the meantime, sha256sum of my vM25 index that I had generated has some mismatches from the one you created. Below is my sha256sum on vM25 index:; `306e9d98b3460859f579059bf876aa3b6e264c8f38c04cde332b03632edc6dfb complete_ref_lens.bin; 636b3df7e097d58fa846bd85ce650ce5bf72c66dc5b2d7566fc9e3db087c5c9c ctable.bin; 1c7501deaa4524f4700152713228cb03949775dce481384eac67bb45458508be ctg_offsets.bin; dbc575fed0d589b4671c26bd8cbcb4b3d52ef41c299a90de978ab940abb751fc duplicate_clusters.tsv; c3ec09a30adc9d47bc95839157cb2ff66530353106a4fd8e75b167ac5db67820 info.json; 430be78bae99a4592fcedc5c800a42313f2b1252e3953f89f347779056c1ee5b mphf.bin; 2fb0b5151f9f2544c06a9f95d03075f7af0494d0fe31745504a5a7da43edc1b1 pos.bin; 15d3bb6a16bcd8c1a6814852bd3dcfa439b60ec84c706f868ee7ec2d5a90581d pre_indexing.log; 8e665e5fdee5af6fcedabc69fd04eda6e66055ef811ebde6de6f86a66521198a rank.bin; 793c79f5fd6046dfea07bbc9587d2835088e54c78197d652d1b1f205c6b16983 refAccumLengths.bin; c5ea8eccca3fdc299ad7c9d2f07a4ed14c8c830940e83c315e7eaad6905a40aa ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 46bf28001e00d491b68bf8758b99c1f304523c79bd94a97d7797888856594e84 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json; `; Following are different.; `ctable.bin; info.json; mphf.bin; pos.bin; pre_indexing.log; ref_indexing.log; seq.bin; versionInfo.json`. I will try creating the vM25 index with increased memory. Wonder if its not building. Just FYI, my sha256sum on vM23 index is:; `9788716f4ce42b049fe7e865108f45392bb8a5847cfcd47369512783dc918239 complete_ref_lens.bin; 9c2453a47ce1808f54733f049b8c4cf38634c9116eb55ed725b73219caa",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480:33,Integrability,message,message,33,"Hi Rob, After posting yesterdays message, I generated the vM23 index, and the alignments/quants worked. I had to use mem_free=34G for building index. Is that expected?; I will try building the vM25 index again and and post the update.; In the meantime, sha256sum of my vM25 index that I had generated has some mismatches from the one you created. Below is my sha256sum on vM25 index:; `306e9d98b3460859f579059bf876aa3b6e264c8f38c04cde332b03632edc6dfb complete_ref_lens.bin; 636b3df7e097d58fa846bd85ce650ce5bf72c66dc5b2d7566fc9e3db087c5c9c ctable.bin; 1c7501deaa4524f4700152713228cb03949775dce481384eac67bb45458508be ctg_offsets.bin; dbc575fed0d589b4671c26bd8cbcb4b3d52ef41c299a90de978ab940abb751fc duplicate_clusters.tsv; c3ec09a30adc9d47bc95839157cb2ff66530353106a4fd8e75b167ac5db67820 info.json; 430be78bae99a4592fcedc5c800a42313f2b1252e3953f89f347779056c1ee5b mphf.bin; 2fb0b5151f9f2544c06a9f95d03075f7af0494d0fe31745504a5a7da43edc1b1 pos.bin; 15d3bb6a16bcd8c1a6814852bd3dcfa439b60ec84c706f868ee7ec2d5a90581d pre_indexing.log; 8e665e5fdee5af6fcedabc69fd04eda6e66055ef811ebde6de6f86a66521198a rank.bin; 793c79f5fd6046dfea07bbc9587d2835088e54c78197d652d1b1f205c6b16983 refAccumLengths.bin; c5ea8eccca3fdc299ad7c9d2f07a4ed14c8c830940e83c315e7eaad6905a40aa ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 46bf28001e00d491b68bf8758b99c1f304523c79bd94a97d7797888856594e84 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json; `; Following are different.; `ctable.bin; info.json; mphf.bin; pos.bin; pre_indexing.log; ref_indexing.log; seq.bin; versionInfo.json`. I will try creating the vM25 index with increased memory. Wonder if its not building. Just FYI, my sha256sum on vM23 index is:; `9788716f4ce42b049fe7e865108f45392bb8a5847cfcd47369512783dc918239 complete_ref_lens.bin; 9c2453a47ce1808f54733f049b8c4cf38634c9116eb55ed725b73219caa",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480:1025,Testability,log,log,1025," index, and the alignments/quants worked. I had to use mem_free=34G for building index. Is that expected?; I will try building the vM25 index again and and post the update.; In the meantime, sha256sum of my vM25 index that I had generated has some mismatches from the one you created. Below is my sha256sum on vM25 index:; `306e9d98b3460859f579059bf876aa3b6e264c8f38c04cde332b03632edc6dfb complete_ref_lens.bin; 636b3df7e097d58fa846bd85ce650ce5bf72c66dc5b2d7566fc9e3db087c5c9c ctable.bin; 1c7501deaa4524f4700152713228cb03949775dce481384eac67bb45458508be ctg_offsets.bin; dbc575fed0d589b4671c26bd8cbcb4b3d52ef41c299a90de978ab940abb751fc duplicate_clusters.tsv; c3ec09a30adc9d47bc95839157cb2ff66530353106a4fd8e75b167ac5db67820 info.json; 430be78bae99a4592fcedc5c800a42313f2b1252e3953f89f347779056c1ee5b mphf.bin; 2fb0b5151f9f2544c06a9f95d03075f7af0494d0fe31745504a5a7da43edc1b1 pos.bin; 15d3bb6a16bcd8c1a6814852bd3dcfa439b60ec84c706f868ee7ec2d5a90581d pre_indexing.log; 8e665e5fdee5af6fcedabc69fd04eda6e66055ef811ebde6de6f86a66521198a rank.bin; 793c79f5fd6046dfea07bbc9587d2835088e54c78197d652d1b1f205c6b16983 refAccumLengths.bin; c5ea8eccca3fdc299ad7c9d2f07a4ed14c8c830940e83c315e7eaad6905a40aa ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 46bf28001e00d491b68bf8758b99c1f304523c79bd94a97d7797888856594e84 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json; `; Following are different.; `ctable.bin; info.json; mphf.bin; pos.bin; pre_indexing.log; ref_indexing.log; seq.bin; versionInfo.json`. I will try creating the vM25 index with increased memory. Wonder if its not building. Just FYI, my sha256sum on vM23 index is:; `9788716f4ce42b049fe7e865108f45392bb8a5847cfcd47369512783dc918239 complete_ref_lens.bin; 9c2453a47ce1808f54733f049b8c4cf38634c9116eb55ed725b73219caa101c5 ctable.bin; 928ba619dc5388ccab6d5c4f8ce162e07a5b5c79028b",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480:1269,Testability,log,log,1269," mismatches from the one you created. Below is my sha256sum on vM25 index:; `306e9d98b3460859f579059bf876aa3b6e264c8f38c04cde332b03632edc6dfb complete_ref_lens.bin; 636b3df7e097d58fa846bd85ce650ce5bf72c66dc5b2d7566fc9e3db087c5c9c ctable.bin; 1c7501deaa4524f4700152713228cb03949775dce481384eac67bb45458508be ctg_offsets.bin; dbc575fed0d589b4671c26bd8cbcb4b3d52ef41c299a90de978ab940abb751fc duplicate_clusters.tsv; c3ec09a30adc9d47bc95839157cb2ff66530353106a4fd8e75b167ac5db67820 info.json; 430be78bae99a4592fcedc5c800a42313f2b1252e3953f89f347779056c1ee5b mphf.bin; 2fb0b5151f9f2544c06a9f95d03075f7af0494d0fe31745504a5a7da43edc1b1 pos.bin; 15d3bb6a16bcd8c1a6814852bd3dcfa439b60ec84c706f868ee7ec2d5a90581d pre_indexing.log; 8e665e5fdee5af6fcedabc69fd04eda6e66055ef811ebde6de6f86a66521198a rank.bin; 793c79f5fd6046dfea07bbc9587d2835088e54c78197d652d1b1f205c6b16983 refAccumLengths.bin; c5ea8eccca3fdc299ad7c9d2f07a4ed14c8c830940e83c315e7eaad6905a40aa ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 46bf28001e00d491b68bf8758b99c1f304523c79bd94a97d7797888856594e84 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json; `; Following are different.; `ctable.bin; info.json; mphf.bin; pos.bin; pre_indexing.log; ref_indexing.log; seq.bin; versionInfo.json`. I will try creating the vM25 index with increased memory. Wonder if its not building. Just FYI, my sha256sum on vM23 index is:; `9788716f4ce42b049fe7e865108f45392bb8a5847cfcd47369512783dc918239 complete_ref_lens.bin; 9c2453a47ce1808f54733f049b8c4cf38634c9116eb55ed725b73219caa101c5 ctable.bin; 928ba619dc5388ccab6d5c4f8ce162e07a5b5c79028be4aee4d838f43a3b9d92 ctg_offsets.bin; 0814d0e7dd8a4b126709c42728816995aefdf5a5bb6337c2d3c048cb0f56094d duplicate_clusters.tsv; dcbf8e140627b3c99d4dbcdaa585447a691fddb620f137811b669e73800f9b3b info.json; 5959abf5969a26481c6aa20fecbdddf19fa558e949cf",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480:1674,Testability,log,log,1674,"8ab940abb751fc duplicate_clusters.tsv; c3ec09a30adc9d47bc95839157cb2ff66530353106a4fd8e75b167ac5db67820 info.json; 430be78bae99a4592fcedc5c800a42313f2b1252e3953f89f347779056c1ee5b mphf.bin; 2fb0b5151f9f2544c06a9f95d03075f7af0494d0fe31745504a5a7da43edc1b1 pos.bin; 15d3bb6a16bcd8c1a6814852bd3dcfa439b60ec84c706f868ee7ec2d5a90581d pre_indexing.log; 8e665e5fdee5af6fcedabc69fd04eda6e66055ef811ebde6de6f86a66521198a rank.bin; 793c79f5fd6046dfea07bbc9587d2835088e54c78197d652d1b1f205c6b16983 refAccumLengths.bin; c5ea8eccca3fdc299ad7c9d2f07a4ed14c8c830940e83c315e7eaad6905a40aa ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 46bf28001e00d491b68bf8758b99c1f304523c79bd94a97d7797888856594e84 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json; `; Following are different.; `ctable.bin; info.json; mphf.bin; pos.bin; pre_indexing.log; ref_indexing.log; seq.bin; versionInfo.json`. I will try creating the vM25 index with increased memory. Wonder if its not building. Just FYI, my sha256sum on vM23 index is:; `9788716f4ce42b049fe7e865108f45392bb8a5847cfcd47369512783dc918239 complete_ref_lens.bin; 9c2453a47ce1808f54733f049b8c4cf38634c9116eb55ed725b73219caa101c5 ctable.bin; 928ba619dc5388ccab6d5c4f8ce162e07a5b5c79028be4aee4d838f43a3b9d92 ctg_offsets.bin; 0814d0e7dd8a4b126709c42728816995aefdf5a5bb6337c2d3c048cb0f56094d duplicate_clusters.tsv; dcbf8e140627b3c99d4dbcdaa585447a691fddb620f137811b669e73800f9b3b info.json; 5959abf5969a26481c6aa20fecbdddf19fa558e949cfbda5760205f38bb907b9 mphf.bin; 28460131b85c74ffb7627761a291614757e72b4e3b82971dcc048a50cc8d9e7f pos.bin; b5eb5e3fb0d03509d9fc90f6b5461c6aecc44423068f3303553cc07fffc7c1b9 pre_indexing.log; eca518136526233f3dc28d9684926793cb84327242d54c1a8a20c66aa1928fad rank.bin; a990247ba2b351fd0921de6470bf0c3505472d8f463e6f8b9ec7c221b6b56af8 refAccumLengths.bin; 436199afbb35045a70fdc7b9",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480:1692,Testability,log,log,1692,"c duplicate_clusters.tsv; c3ec09a30adc9d47bc95839157cb2ff66530353106a4fd8e75b167ac5db67820 info.json; 430be78bae99a4592fcedc5c800a42313f2b1252e3953f89f347779056c1ee5b mphf.bin; 2fb0b5151f9f2544c06a9f95d03075f7af0494d0fe31745504a5a7da43edc1b1 pos.bin; 15d3bb6a16bcd8c1a6814852bd3dcfa439b60ec84c706f868ee7ec2d5a90581d pre_indexing.log; 8e665e5fdee5af6fcedabc69fd04eda6e66055ef811ebde6de6f86a66521198a rank.bin; 793c79f5fd6046dfea07bbc9587d2835088e54c78197d652d1b1f205c6b16983 refAccumLengths.bin; c5ea8eccca3fdc299ad7c9d2f07a4ed14c8c830940e83c315e7eaad6905a40aa ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 46bf28001e00d491b68bf8758b99c1f304523c79bd94a97d7797888856594e84 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json; `; Following are different.; `ctable.bin; info.json; mphf.bin; pos.bin; pre_indexing.log; ref_indexing.log; seq.bin; versionInfo.json`. I will try creating the vM25 index with increased memory. Wonder if its not building. Just FYI, my sha256sum on vM23 index is:; `9788716f4ce42b049fe7e865108f45392bb8a5847cfcd47369512783dc918239 complete_ref_lens.bin; 9c2453a47ce1808f54733f049b8c4cf38634c9116eb55ed725b73219caa101c5 ctable.bin; 928ba619dc5388ccab6d5c4f8ce162e07a5b5c79028be4aee4d838f43a3b9d92 ctg_offsets.bin; 0814d0e7dd8a4b126709c42728816995aefdf5a5bb6337c2d3c048cb0f56094d duplicate_clusters.tsv; dcbf8e140627b3c99d4dbcdaa585447a691fddb620f137811b669e73800f9b3b info.json; 5959abf5969a26481c6aa20fecbdddf19fa558e949cfbda5760205f38bb907b9 mphf.bin; 28460131b85c74ffb7627761a291614757e72b4e3b82971dcc048a50cc8d9e7f pos.bin; b5eb5e3fb0d03509d9fc90f6b5461c6aecc44423068f3303553cc07fffc7c1b9 pre_indexing.log; eca518136526233f3dc28d9684926793cb84327242d54c1a8a20c66aa1928fad rank.bin; a990247ba2b351fd0921de6470bf0c3505472d8f463e6f8b9ec7c221b6b56af8 refAccumLengths.bin; 436199afbb35045a70fdc7b9e542ef805b5717",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480:2493,Testability,log,log,2493,"8c830940e83c315e7eaad6905a40aa ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 46bf28001e00d491b68bf8758b99c1f304523c79bd94a97d7797888856594e84 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json; `; Following are different.; `ctable.bin; info.json; mphf.bin; pos.bin; pre_indexing.log; ref_indexing.log; seq.bin; versionInfo.json`. I will try creating the vM25 index with increased memory. Wonder if its not building. Just FYI, my sha256sum on vM23 index is:; `9788716f4ce42b049fe7e865108f45392bb8a5847cfcd47369512783dc918239 complete_ref_lens.bin; 9c2453a47ce1808f54733f049b8c4cf38634c9116eb55ed725b73219caa101c5 ctable.bin; 928ba619dc5388ccab6d5c4f8ce162e07a5b5c79028be4aee4d838f43a3b9d92 ctg_offsets.bin; 0814d0e7dd8a4b126709c42728816995aefdf5a5bb6337c2d3c048cb0f56094d duplicate_clusters.tsv; dcbf8e140627b3c99d4dbcdaa585447a691fddb620f137811b669e73800f9b3b info.json; 5959abf5969a26481c6aa20fecbdddf19fa558e949cfbda5760205f38bb907b9 mphf.bin; 28460131b85c74ffb7627761a291614757e72b4e3b82971dcc048a50cc8d9e7f pos.bin; b5eb5e3fb0d03509d9fc90f6b5461c6aecc44423068f3303553cc07fffc7c1b9 pre_indexing.log; eca518136526233f3dc28d9684926793cb84327242d54c1a8a20c66aa1928fad rank.bin; a990247ba2b351fd0921de6470bf0c3505472d8f463e6f8b9ec7c221b6b56af8 refAccumLengths.bin; 436199afbb35045a70fdc7b9e542ef805b57170f41d6bc6a0ba4d88a8ca267fc ref_indexing.log; 65ce60d16b43f9e739cf68edb194daa63562c6d064a6e6bf441f612baec66983 reflengths.bin; 4f3fc9b3785f8cd0e1355e31d61df87226eb7e14e4438c0afc68706937df94a3 refseq.bin; 075122d399bd2c5cfd2e9e7405b2f2778c45178e9bf3a4a93f17750c808df7e0 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json`. Summary: vM23 is working and I will proceed with it. In the meantime I will troubleshot vM25 as well as try your tarball. Thank you very much for your quick help. Hari",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480:2737,Testability,log,log,2737,"8c830940e83c315e7eaad6905a40aa ref_indexing.log; b580b9c6257254a018a9ae22291a64892c1a3715c69272637f5c504fc5545a70 reflengths.bin; 89679603ac0b28042275e5ff04b222bad3fd431cab573f0c2b61e7455aec43e7 refseq.bin; 46bf28001e00d491b68bf8758b99c1f304523c79bd94a97d7797888856594e84 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json; `; Following are different.; `ctable.bin; info.json; mphf.bin; pos.bin; pre_indexing.log; ref_indexing.log; seq.bin; versionInfo.json`. I will try creating the vM25 index with increased memory. Wonder if its not building. Just FYI, my sha256sum on vM23 index is:; `9788716f4ce42b049fe7e865108f45392bb8a5847cfcd47369512783dc918239 complete_ref_lens.bin; 9c2453a47ce1808f54733f049b8c4cf38634c9116eb55ed725b73219caa101c5 ctable.bin; 928ba619dc5388ccab6d5c4f8ce162e07a5b5c79028be4aee4d838f43a3b9d92 ctg_offsets.bin; 0814d0e7dd8a4b126709c42728816995aefdf5a5bb6337c2d3c048cb0f56094d duplicate_clusters.tsv; dcbf8e140627b3c99d4dbcdaa585447a691fddb620f137811b669e73800f9b3b info.json; 5959abf5969a26481c6aa20fecbdddf19fa558e949cfbda5760205f38bb907b9 mphf.bin; 28460131b85c74ffb7627761a291614757e72b4e3b82971dcc048a50cc8d9e7f pos.bin; b5eb5e3fb0d03509d9fc90f6b5461c6aecc44423068f3303553cc07fffc7c1b9 pre_indexing.log; eca518136526233f3dc28d9684926793cb84327242d54c1a8a20c66aa1928fad rank.bin; a990247ba2b351fd0921de6470bf0c3505472d8f463e6f8b9ec7c221b6b56af8 refAccumLengths.bin; 436199afbb35045a70fdc7b9e542ef805b57170f41d6bc6a0ba4d88a8ca267fc ref_indexing.log; 65ce60d16b43f9e739cf68edb194daa63562c6d064a6e6bf441f612baec66983 reflengths.bin; 4f3fc9b3785f8cd0e1355e31d61df87226eb7e14e4438c0afc68706937df94a3 refseq.bin; 075122d399bd2c5cfd2e9e7405b2f2778c45178e9bf3a4a93f17750c808df7e0 seq.bin; 4c7e56ba28383774e786826099ef412761326fe18ce69f29033ad2886542985d versionInfo.json`. Summary: vM23 is working and I will proceed with it. In the meantime I will troubleshot vM25 as well as try your tarball. Thank you very much for your quick help. Hari",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674527480
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674528707:1442,Deployability,configurat,configuration,1442,"Hi Hari,. Some thoughts on your questions:. > I had to use mem_free=34G for building index. Is that expected?. Certainly, it is _not_ the case that index creation should require 34G of physical memory. When indexing the genome and transcriptome in dense mode, we typically expect it to require <20G of physical RAM (and <4 for just the transcriptome). However, we have noticed some strange behavior in the past about how compute clusters manage process allocation — sometimes, it seems, one must overcommit. Given the diversity of different software on which different compute clusters run, as well as the manifold way sysadmins may configure these things, we've not found a universal explanation / conclusion yet, but it does seem that the resources actually required (e.g. if you run salmon index under `/usr/bin/time` and look at the resources) are less than what should be requested. The differences in the sha256 sums are a bit strange and I don't have a great explanation for them. One difference is that I built with the head of the develop branch (which has version tag 1.3.1). That describes a difference in `versionInfo.json` but nothing upstream in the index building should have changed, so I am not sure why the other files would have different sha256 sums. I can try with the pre-compiled binary and see if my results match yours. In the meantime, please keep me posted. If index building ends up worth for you with a different configuration, it would be good to check this off of our list of TODOs, and if not, it would be good to get to the bottom of it. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674528707
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674528707:633,Modifiability,config,configure,633,"Hi Hari,. Some thoughts on your questions:. > I had to use mem_free=34G for building index. Is that expected?. Certainly, it is _not_ the case that index creation should require 34G of physical memory. When indexing the genome and transcriptome in dense mode, we typically expect it to require <20G of physical RAM (and <4 for just the transcriptome). However, we have noticed some strange behavior in the past about how compute clusters manage process allocation — sometimes, it seems, one must overcommit. Given the diversity of different software on which different compute clusters run, as well as the manifold way sysadmins may configure these things, we've not found a universal explanation / conclusion yet, but it does seem that the resources actually required (e.g. if you run salmon index under `/usr/bin/time` and look at the resources) are less than what should be requested. The differences in the sha256 sums are a bit strange and I don't have a great explanation for them. One difference is that I built with the head of the develop branch (which has version tag 1.3.1). That describes a difference in `versionInfo.json` but nothing upstream in the index building should have changed, so I am not sure why the other files would have different sha256 sums. I can try with the pre-compiled binary and see if my results match yours. In the meantime, please keep me posted. If index building ends up worth for you with a different configuration, it would be good to check this off of our list of TODOs, and if not, it would be good to get to the bottom of it. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674528707
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674528707:1442,Modifiability,config,configuration,1442,"Hi Hari,. Some thoughts on your questions:. > I had to use mem_free=34G for building index. Is that expected?. Certainly, it is _not_ the case that index creation should require 34G of physical memory. When indexing the genome and transcriptome in dense mode, we typically expect it to require <20G of physical RAM (and <4 for just the transcriptome). However, we have noticed some strange behavior in the past about how compute clusters manage process allocation — sometimes, it seems, one must overcommit. Given the diversity of different software on which different compute clusters run, as well as the manifold way sysadmins may configure these things, we've not found a universal explanation / conclusion yet, but it does seem that the resources actually required (e.g. if you run salmon index under `/usr/bin/time` and look at the resources) are less than what should be requested. The differences in the sha256 sums are a bit strange and I don't have a great explanation for them. One difference is that I built with the head of the develop branch (which has version tag 1.3.1). That describes a difference in `versionInfo.json` but nothing upstream in the index building should have changed, so I am not sure why the other files would have different sha256 sums. I can try with the pre-compiled binary and see if my results match yours. In the meantime, please keep me posted. If index building ends up worth for you with a different configuration, it would be good to check this off of our list of TODOs, and if not, it would be good to get to the bottom of it. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674528707
https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674870074:30,Performance,load,load,30,"Hi Rob,; The cluster behavior/load might explain the indexing behaviour. ; Will keep you posted as I redo with vM25. Thank you again.; Hari",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/561#issuecomment-674870074
https://github.com/COMBINE-lab/salmon/issues/562#issuecomment-674855490:1826,Performance,optimiz,optimize,1826,"Hi @cfischer1991,. Thanks for the report. There have certainly been _a lot_ of improvements and changes to salmon between v0.8.1 and v1.3.0. The built-in mapping functionality has been largely overhauled. However, I can see that you're not using that here (you're quantifying from alignments). There have been a number of improvements in the alignment-based codepath as well. However, I'd guess that one of the biggest differences in the results you're seeing is due to a changes in the variational Bayes prior that happened between these versions. Specifically, the prior was adjusted to be smaller, and the default was changed from a `per-nucleotide` prior to a `per-transcript` prior. You can try and achieve the newer functionality in 0.8.1 by setting `--perTranscriptPrior` and `--vbPrior 0.01` and seeing, under those settings, how differently things look between 0.8.1 and 1.3.0. *Also*, another important change is in the handling of _incompatible_ alignments — alignments that do not match the prescribed library type. The incompatibility prior used to be set to a small but non-zero value by default `9.9999999999999995e-21`, but has since been changed to `0` by default. Both of these changes in the default have been results of a lot of internal testing suggesting these settings improve quantification results _in general_ (of course, given the complexity in of the quantification problem, there is likely no universal set of parameters that are optimal with respect to every experiment). I'd suggest trying to set these parameters to be the same between versions and to see how much of the variance is controlled by these changes in default values. Then you can determine which settings you believe make more sense in your context, with the understanding that the newer settings have been chosen, in general, to optimize quantification accuracy. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/562#issuecomment-674855490
https://github.com/COMBINE-lab/salmon/issues/562#issuecomment-674855490:1258,Testability,test,testing,1258,"Hi @cfischer1991,. Thanks for the report. There have certainly been _a lot_ of improvements and changes to salmon between v0.8.1 and v1.3.0. The built-in mapping functionality has been largely overhauled. However, I can see that you're not using that here (you're quantifying from alignments). There have been a number of improvements in the alignment-based codepath as well. However, I'd guess that one of the biggest differences in the results you're seeing is due to a changes in the variational Bayes prior that happened between these versions. Specifically, the prior was adjusted to be smaller, and the default was changed from a `per-nucleotide` prior to a `per-transcript` prior. You can try and achieve the newer functionality in 0.8.1 by setting `--perTranscriptPrior` and `--vbPrior 0.01` and seeing, under those settings, how differently things look between 0.8.1 and 1.3.0. *Also*, another important change is in the handling of _incompatible_ alignments — alignments that do not match the prescribed library type. The incompatibility prior used to be set to a small but non-zero value by default `9.9999999999999995e-21`, but has since been changed to `0` by default. Both of these changes in the default have been results of a lot of internal testing suggesting these settings improve quantification results _in general_ (of course, given the complexity in of the quantification problem, there is likely no universal set of parameters that are optimal with respect to every experiment). I'd suggest trying to set these parameters to be the same between versions and to see how much of the variance is controlled by these changes in default values. Then you can determine which settings you believe make more sense in your context, with the understanding that the newer settings have been chosen, in general, to optimize quantification accuracy. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/562#issuecomment-674855490
https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718:423,Availability,error,error,423,"Hi @Cold7,. Thanks for the report. So, could you provide the full output that you get on the terminal when you run this? Your command line looks fine to me. Since version 1.0.0, `--validateMappings` has become the default behavior and so this flag technically has no effect (it is marked as ""deprecated""). However, the argument parser should _absolutely_ accept it, and it's not clear to me why it might be giving you this error. The full output from the terminal may help to diagnose this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718
https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718:181,Security,validat,validateMappings,181,"Hi @Cold7,. Thanks for the report. So, could you provide the full output that you get on the terminal when you run this? Your command line looks fine to me. Since version 1.0.0, `--validateMappings` has become the default behavior and so this flag technically has no effect (it is marked as ""deprecated""). However, the argument parser should _absolutely_ accept it, and it's not clear to me why it might be giving you this error. The full output from the terminal may help to diagnose this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718
https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718:379,Usability,clear,clear,379,"Hi @Cold7,. Thanks for the report. So, could you provide the full output that you get on the terminal when you run this? Your command line looks fine to me. Since version 1.0.0, `--validateMappings` has become the default behavior and so this flag technically has no effect (it is marked as ""deprecated""). However, the argument parser should _absolutely_ accept it, and it's not clear to me why it might be giving you this error. The full output from the terminal may help to diagnose this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718
https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680255521:24,Deployability,upgrade,upgraded,24,"my bad about that, just upgraded the alias to change salmon's path (from 0.7.2 to 1.3.0), but changes have not been affected. I'm so sorry about that",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680255521
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484:15,Availability,error,error,15,"I got the same error but under Ubuntu 20.04.; After Installing boost 1.60.0 manually it initially seemed to work, but then I got the following error:. ```; Error, cannot determine salmon version installed from (salmon: symbol lookup error: salmon: undefined symbol: _ZN5boost15program_options3argE; ) at /usr/local/bin/trinityrnaseq-v2.11.0/Trinity line 3969.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484:143,Availability,error,error,143,"I got the same error but under Ubuntu 20.04.; After Installing boost 1.60.0 manually it initially seemed to work, but then I got the following error:. ```; Error, cannot determine salmon version installed from (salmon: symbol lookup error: salmon: undefined symbol: _ZN5boost15program_options3argE; ) at /usr/local/bin/trinityrnaseq-v2.11.0/Trinity line 3969.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484:156,Availability,Error,Error,156,"I got the same error but under Ubuntu 20.04.; After Installing boost 1.60.0 manually it initially seemed to work, but then I got the following error:. ```; Error, cannot determine salmon version installed from (salmon: symbol lookup error: salmon: undefined symbol: _ZN5boost15program_options3argE; ) at /usr/local/bin/trinityrnaseq-v2.11.0/Trinity line 3969.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484:233,Availability,error,error,233,"I got the same error but under Ubuntu 20.04.; After Installing boost 1.60.0 manually it initially seemed to work, but then I got the following error:. ```; Error, cannot determine salmon version installed from (salmon: symbol lookup error: salmon: undefined symbol: _ZN5boost15program_options3argE; ) at /usr/local/bin/trinityrnaseq-v2.11.0/Trinity line 3969.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484:52,Deployability,Install,Installing,52,"I got the same error but under Ubuntu 20.04.; After Installing boost 1.60.0 manually it initially seemed to work, but then I got the following error:. ```; Error, cannot determine salmon version installed from (salmon: symbol lookup error: salmon: undefined symbol: _ZN5boost15program_options3argE; ) at /usr/local/bin/trinityrnaseq-v2.11.0/Trinity line 3969.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484:195,Deployability,install,installed,195,"I got the same error but under Ubuntu 20.04.; After Installing boost 1.60.0 manually it initially seemed to work, but then I got the following error:. ```; Error, cannot determine salmon version installed from (salmon: symbol lookup error: salmon: undefined symbol: _ZN5boost15program_options3argE; ) at /usr/local/bin/trinityrnaseq-v2.11.0/Trinity line 3969.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696784484
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696812977:26,Availability,error,error,26,"Hi @Rhinogradentia,. This error occurs when there is a binary mismatch between the library used to compile salmon versus that used to run it. Specifically, this occurs when the boost library is _not_ compiled with a modern ABI (Application Binary Interface) — when boost was not compiled in a way compatible with C++11/14/17/20. Are you using the version installed via bioconda, or the pre-compiled binary from github, or have you compiled this yourself? You can [use the `LD_LIBRARY_PATH`](https://stackoverflow.com/questions/13428910/how-to-set-the-environmental-variable-ld-library-path-in-linux) to set things so that the appropriate version of the library is discovered first. You want the version of boost that is found first (the one appearing earliest in the `LD_LIBRARY_PATH`) to be matched to the one with which salmon was compiled.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696812977
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696812977:355,Deployability,install,installed,355,"Hi @Rhinogradentia,. This error occurs when there is a binary mismatch between the library used to compile salmon versus that used to run it. Specifically, this occurs when the boost library is _not_ compiled with a modern ABI (Application Binary Interface) — when boost was not compiled in a way compatible with C++11/14/17/20. Are you using the version installed via bioconda, or the pre-compiled binary from github, or have you compiled this yourself? You can [use the `LD_LIBRARY_PATH`](https://stackoverflow.com/questions/13428910/how-to-set-the-environmental-variable-ld-library-path-in-linux) to set things so that the appropriate version of the library is discovered first. You want the version of boost that is found first (the one appearing earliest in the `LD_LIBRARY_PATH`) to be matched to the one with which salmon was compiled.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696812977
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696812977:247,Integrability,Interface,Interface,247,"Hi @Rhinogradentia,. This error occurs when there is a binary mismatch between the library used to compile salmon versus that used to run it. Specifically, this occurs when the boost library is _not_ compiled with a modern ABI (Application Binary Interface) — when boost was not compiled in a way compatible with C++11/14/17/20. Are you using the version installed via bioconda, or the pre-compiled binary from github, or have you compiled this yourself? You can [use the `LD_LIBRARY_PATH`](https://stackoverflow.com/questions/13428910/how-to-set-the-environmental-variable-ld-library-path-in-linux) to set things so that the appropriate version of the library is discovered first. You want the version of boost that is found first (the one appearing earliest in the `LD_LIBRARY_PATH`) to be matched to the one with which salmon was compiled.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696812977
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696812977:565,Modifiability,variab,variable-ld-library-path-in-linux,565,"Hi @Rhinogradentia,. This error occurs when there is a binary mismatch between the library used to compile salmon versus that used to run it. Specifically, this occurs when the boost library is _not_ compiled with a modern ABI (Application Binary Interface) — when boost was not compiled in a way compatible with C++11/14/17/20. Are you using the version installed via bioconda, or the pre-compiled binary from github, or have you compiled this yourself? You can [use the `LD_LIBRARY_PATH`](https://stackoverflow.com/questions/13428910/how-to-set-the-environmental-variable-ld-library-path-in-linux) to set things so that the appropriate version of the library is discovered first. You want the version of boost that is found first (the one appearing earliest in the `LD_LIBRARY_PATH`) to be matched to the one with which salmon was compiled.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-696812977
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-735962605:134,Availability,down,downgraded,134,"I recently had a similar problem using bioconda. . When installing salmon alongside [RSEM](https://anaconda.org/bioconda/rsem), conda downgraded salmon to version 0.6. As soon as I installed salmon in a dedicated environment, everything worked fine.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-735962605
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-735962605:56,Deployability,install,installing,56,"I recently had a similar problem using bioconda. . When installing salmon alongside [RSEM](https://anaconda.org/bioconda/rsem), conda downgraded salmon to version 0.6. As soon as I installed salmon in a dedicated environment, everything worked fine.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-735962605
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-735962605:181,Deployability,install,installed,181,"I recently had a similar problem using bioconda. . When installing salmon alongside [RSEM](https://anaconda.org/bioconda/rsem), conda downgraded salmon to version 0.6. As soon as I installed salmon in a dedicated environment, everything worked fine.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-735962605
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-780036521:88,Deployability,install,install,88,I bumped into a similar problem. Fixed by making an new enviroment and forcing conda to install the most recent version. ; conda install -c conda-forge -c bioconda salmon=1.3.0,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-780036521
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-780036521:129,Deployability,install,install,129,I bumped into a similar problem. Fixed by making an new enviroment and forcing conda to install the most recent version. ; conda install -c conda-forge -c bioconda salmon=1.3.0,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-780036521
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802737580:242,Availability,error,error,242,> I have the same issue. None of the conda version works on my Linux Centos.; > Is there any library to add in the conda recipe to fix the issue ?. Similar issue here too with salmon 1.4.0 installed via conda in a clean environment; `salmon: error while loading shared libraries: libtbb.so.2: cannot open shared object file: No such file or directory`. Edit: apparently solved by downgrading `tbb` as suggested in https://www.biostars.org/p/494922/,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802737580
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802737580:380,Availability,down,downgrading,380,> I have the same issue. None of the conda version works on my Linux Centos.; > Is there any library to add in the conda recipe to fix the issue ?. Similar issue here too with salmon 1.4.0 installed via conda in a clean environment; `salmon: error while loading shared libraries: libtbb.so.2: cannot open shared object file: No such file or directory`. Edit: apparently solved by downgrading `tbb` as suggested in https://www.biostars.org/p/494922/,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802737580
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802737580:189,Deployability,install,installed,189,> I have the same issue. None of the conda version works on my Linux Centos.; > Is there any library to add in the conda recipe to fix the issue ?. Similar issue here too with salmon 1.4.0 installed via conda in a clean environment; `salmon: error while loading shared libraries: libtbb.so.2: cannot open shared object file: No such file or directory`. Edit: apparently solved by downgrading `tbb` as suggested in https://www.biostars.org/p/494922/,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802737580
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802737580:254,Performance,load,loading,254,> I have the same issue. None of the conda version works on my Linux Centos.; > Is there any library to add in the conda recipe to fix the issue ?. Similar issue here too with salmon 1.4.0 installed via conda in a clean environment; `salmon: error while loading shared libraries: libtbb.so.2: cannot open shared object file: No such file or directory`. Edit: apparently solved by downgrading `tbb` as suggested in https://www.biostars.org/p/494922/,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802737580
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802751914:166,Integrability,depend,dependencies,166,We fix it by adding the tbb package (I also added the libgcc but I think it is not mandatory) :. ```; name: salmon; channels:; - bioconda; - conda-forge; - defaults; dependencies:; - libgcc-ng=9.1.0=hdf63c60_0; - tbb=2020.2=hc9558a2_0; - salmon=1.4.0=hf69c8f4_0; ```,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-802751914
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173:43,Availability,error,error,43,"I just conda installed salmon fresh. I got error when I ran it. . ```; $ conda create -n salmon salmon; Collecting package metadata (current_repodata.json): done; Solving environment: done. ## Package Plan ##. environment location:~/miniconda3/envs/salmon. added / updated specs:; - salmon. The following NEW packages will be INSTALLED:. _libgcc_mutex conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge; _openmp_mutex conda-forge/linux-64::_openmp_mutex-4.5-1_gnu; bzip2 conda-forge/linux-64::bzip2-1.0.8-h7f98852_4; icu conda-forge/linux-64::icu-64.2-he1b5a44_1; jemalloc conda-forge/linux-64::jemalloc-5.2.1-h9c3ff4c_5; libgcc-ng conda-forge/linux-64::libgcc-ng-9.3.0-h2828fa1_18; libgomp conda-forge/linux-64::libgomp-9.3.0-h2828fa1_18; libstdcxx-ng conda-forge/linux-64::libstdcxx-ng-9.3.0-h6de172a_18; salmon bioconda/linux-64::salmon-1.4.0-hf69c8f4_0; tbb conda-forge/linux-64::tbb-2021.1.1-h4bd325d_1; zlib conda-forge/linux-64::zlib-1.2.11-h516909a_1010. Proceed ([y]/n)? y. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; #; # To activate this environment, use; #; # $ conda activate salmon; #; # To deactivate an active environment, use; #; # $ conda deactivate. $ conda activate salmon; $ salmon; salmon: symbol lookup error: salmon: undefined symbol: _ZN3tbb8internal24concurrent_queue_base_v818internal_push_moveEPKv; ```. I think this is related with rob-p commented on Sep 22, 2020. ; I have a naive question: Is it possible to fix this in the recipe by adding the problematic dependencies?; I am using centOS 8.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173:1270,Availability,error,error,1270,"I just conda installed salmon fresh. I got error when I ran it. . ```; $ conda create -n salmon salmon; Collecting package metadata (current_repodata.json): done; Solving environment: done. ## Package Plan ##. environment location:~/miniconda3/envs/salmon. added / updated specs:; - salmon. The following NEW packages will be INSTALLED:. _libgcc_mutex conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge; _openmp_mutex conda-forge/linux-64::_openmp_mutex-4.5-1_gnu; bzip2 conda-forge/linux-64::bzip2-1.0.8-h7f98852_4; icu conda-forge/linux-64::icu-64.2-he1b5a44_1; jemalloc conda-forge/linux-64::jemalloc-5.2.1-h9c3ff4c_5; libgcc-ng conda-forge/linux-64::libgcc-ng-9.3.0-h2828fa1_18; libgomp conda-forge/linux-64::libgomp-9.3.0-h2828fa1_18; libstdcxx-ng conda-forge/linux-64::libstdcxx-ng-9.3.0-h6de172a_18; salmon bioconda/linux-64::salmon-1.4.0-hf69c8f4_0; tbb conda-forge/linux-64::tbb-2021.1.1-h4bd325d_1; zlib conda-forge/linux-64::zlib-1.2.11-h516909a_1010. Proceed ([y]/n)? y. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; #; # To activate this environment, use; #; # $ conda activate salmon; #; # To deactivate an active environment, use; #; # $ conda deactivate. $ conda activate salmon; $ salmon; salmon: symbol lookup error: salmon: undefined symbol: _ZN3tbb8internal24concurrent_queue_base_v818internal_push_moveEPKv; ```. I think this is related with rob-p commented on Sep 22, 2020. ; I have a naive question: Is it possible to fix this in the recipe by adding the problematic dependencies?; I am using centOS 8.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173:13,Deployability,install,installed,13,"I just conda installed salmon fresh. I got error when I ran it. . ```; $ conda create -n salmon salmon; Collecting package metadata (current_repodata.json): done; Solving environment: done. ## Package Plan ##. environment location:~/miniconda3/envs/salmon. added / updated specs:; - salmon. The following NEW packages will be INSTALLED:. _libgcc_mutex conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge; _openmp_mutex conda-forge/linux-64::_openmp_mutex-4.5-1_gnu; bzip2 conda-forge/linux-64::bzip2-1.0.8-h7f98852_4; icu conda-forge/linux-64::icu-64.2-he1b5a44_1; jemalloc conda-forge/linux-64::jemalloc-5.2.1-h9c3ff4c_5; libgcc-ng conda-forge/linux-64::libgcc-ng-9.3.0-h2828fa1_18; libgomp conda-forge/linux-64::libgomp-9.3.0-h2828fa1_18; libstdcxx-ng conda-forge/linux-64::libstdcxx-ng-9.3.0-h6de172a_18; salmon bioconda/linux-64::salmon-1.4.0-hf69c8f4_0; tbb conda-forge/linux-64::tbb-2021.1.1-h4bd325d_1; zlib conda-forge/linux-64::zlib-1.2.11-h516909a_1010. Proceed ([y]/n)? y. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; #; # To activate this environment, use; #; # $ conda activate salmon; #; # To deactivate an active environment, use; #; # $ conda deactivate. $ conda activate salmon; $ salmon; salmon: symbol lookup error: salmon: undefined symbol: _ZN3tbb8internal24concurrent_queue_base_v818internal_push_moveEPKv; ```. I think this is related with rob-p commented on Sep 22, 2020. ; I have a naive question: Is it possible to fix this in the recipe by adding the problematic dependencies?; I am using centOS 8.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173:265,Deployability,update,updated,265,"I just conda installed salmon fresh. I got error when I ran it. . ```; $ conda create -n salmon salmon; Collecting package metadata (current_repodata.json): done; Solving environment: done. ## Package Plan ##. environment location:~/miniconda3/envs/salmon. added / updated specs:; - salmon. The following NEW packages will be INSTALLED:. _libgcc_mutex conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge; _openmp_mutex conda-forge/linux-64::_openmp_mutex-4.5-1_gnu; bzip2 conda-forge/linux-64::bzip2-1.0.8-h7f98852_4; icu conda-forge/linux-64::icu-64.2-he1b5a44_1; jemalloc conda-forge/linux-64::jemalloc-5.2.1-h9c3ff4c_5; libgcc-ng conda-forge/linux-64::libgcc-ng-9.3.0-h2828fa1_18; libgomp conda-forge/linux-64::libgomp-9.3.0-h2828fa1_18; libstdcxx-ng conda-forge/linux-64::libstdcxx-ng-9.3.0-h6de172a_18; salmon bioconda/linux-64::salmon-1.4.0-hf69c8f4_0; tbb conda-forge/linux-64::tbb-2021.1.1-h4bd325d_1; zlib conda-forge/linux-64::zlib-1.2.11-h516909a_1010. Proceed ([y]/n)? y. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; #; # To activate this environment, use; #; # $ conda activate salmon; #; # To deactivate an active environment, use; #; # $ conda deactivate. $ conda activate salmon; $ salmon; salmon: symbol lookup error: salmon: undefined symbol: _ZN3tbb8internal24concurrent_queue_base_v818internal_push_moveEPKv; ```. I think this is related with rob-p commented on Sep 22, 2020. ; I have a naive question: Is it possible to fix this in the recipe by adding the problematic dependencies?; I am using centOS 8.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173:326,Deployability,INSTALL,INSTALLED,326,"I just conda installed salmon fresh. I got error when I ran it. . ```; $ conda create -n salmon salmon; Collecting package metadata (current_repodata.json): done; Solving environment: done. ## Package Plan ##. environment location:~/miniconda3/envs/salmon. added / updated specs:; - salmon. The following NEW packages will be INSTALLED:. _libgcc_mutex conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge; _openmp_mutex conda-forge/linux-64::_openmp_mutex-4.5-1_gnu; bzip2 conda-forge/linux-64::bzip2-1.0.8-h7f98852_4; icu conda-forge/linux-64::icu-64.2-he1b5a44_1; jemalloc conda-forge/linux-64::jemalloc-5.2.1-h9c3ff4c_5; libgcc-ng conda-forge/linux-64::libgcc-ng-9.3.0-h2828fa1_18; libgomp conda-forge/linux-64::libgomp-9.3.0-h2828fa1_18; libstdcxx-ng conda-forge/linux-64::libstdcxx-ng-9.3.0-h6de172a_18; salmon bioconda/linux-64::salmon-1.4.0-hf69c8f4_0; tbb conda-forge/linux-64::tbb-2021.1.1-h4bd325d_1; zlib conda-forge/linux-64::zlib-1.2.11-h516909a_1010. Proceed ([y]/n)? y. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; #; # To activate this environment, use; #; # $ conda activate salmon; #; # To deactivate an active environment, use; #; # $ conda deactivate. $ conda activate salmon; $ salmon; salmon: symbol lookup error: salmon: undefined symbol: _ZN3tbb8internal24concurrent_queue_base_v818internal_push_moveEPKv; ```. I think this is related with rob-p commented on Sep 22, 2020. ; I have a naive question: Is it possible to fix this in the recipe by adding the problematic dependencies?; I am using centOS 8.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173
https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173:1532,Integrability,depend,dependencies,1532,"I just conda installed salmon fresh. I got error when I ran it. . ```; $ conda create -n salmon salmon; Collecting package metadata (current_repodata.json): done; Solving environment: done. ## Package Plan ##. environment location:~/miniconda3/envs/salmon. added / updated specs:; - salmon. The following NEW packages will be INSTALLED:. _libgcc_mutex conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge; _openmp_mutex conda-forge/linux-64::_openmp_mutex-4.5-1_gnu; bzip2 conda-forge/linux-64::bzip2-1.0.8-h7f98852_4; icu conda-forge/linux-64::icu-64.2-he1b5a44_1; jemalloc conda-forge/linux-64::jemalloc-5.2.1-h9c3ff4c_5; libgcc-ng conda-forge/linux-64::libgcc-ng-9.3.0-h2828fa1_18; libgomp conda-forge/linux-64::libgomp-9.3.0-h2828fa1_18; libstdcxx-ng conda-forge/linux-64::libstdcxx-ng-9.3.0-h6de172a_18; salmon bioconda/linux-64::salmon-1.4.0-hf69c8f4_0; tbb conda-forge/linux-64::tbb-2021.1.1-h4bd325d_1; zlib conda-forge/linux-64::zlib-1.2.11-h516909a_1010. Proceed ([y]/n)? y. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; #; # To activate this environment, use; #; # $ conda activate salmon; #; # To deactivate an active environment, use; #; # $ conda deactivate. $ conda activate salmon; $ salmon; salmon: symbol lookup error: salmon: undefined symbol: _ZN3tbb8internal24concurrent_queue_base_v818internal_push_moveEPKv; ```. I think this is related with rob-p commented on Sep 22, 2020. ; I have a naive question: Is it possible to fix this in the recipe by adding the problematic dependencies?; I am using centOS 8.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/565#issuecomment-805608173
https://github.com/COMBINE-lab/salmon/issues/566#issuecomment-865076744:32,Availability,error,error,32,"Hello,; I found there may be an error in the example code that posted by @uros-sipetic. I believe from my calculation, the correct equation for converting between FPKM and TPM should be: ; `FPKMi = 10^3 * TPMi / (sum_to_i(TPMi * effective_lengthi))`. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/566#issuecomment-865076744
https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695004138:144,Deployability,release,release,144,"cc : @k3yavi . Hi @xuesoso, I know that Avi (tagged above) has implemented a flag for this, but I'm not certain if it is exposed in the current release. I'm tagging him here to chime in.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695004138
https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695004138:121,Security,expose,exposed,121,"cc : @k3yavi . Hi @xuesoso, I know that Avi (tagged above) has implemented a flag for this, but I'm not certain if it is exposed in the current release. I'm tagging him here to chime in.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695004138
https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695157454:976,Availability,reliab,reliable,976,"@k3yavi @rob-p ; Thank you for the prompt responses! I executed the following but seems to get no ""quant.sf"" as output (in fact no other output except for the log file). I don't find ""AlignmentLibrary"" object even though the log states that it did. Do you know what went wrong? Thank you!. Command:; ```bash; salmon quant -e ./aux_info/eq_classes.txt.gz --libType IU -o ./; ```. Output log:; [2020-09-18 20:01:55.879] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-09-18 20:01:55.879] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-09-18 20:01:55.879] [jointLog] [info] numQuantThreads = 4; [2020-09-18 20:02:50.408] [jointLog] [warning] Missing effective lens for 47121 transcripts; setting to 100.0.; [2020-09-18 20:02:50.408] [jointLog] [warning] NOTE: Since effective lengths are not provided, please do not rely on the TPM field ; in the ouput quantifications. Only the NumReads field will be reliable.; [2020-09-18 20:02:50.410] [jointLog] [info] Found total 187671 eqclasses and 47121 transcripts; [2020-09-18 20:02:50.682] [jointLog] [info] Created AlignmentLibrary object. My Salmon version is v1.3.0.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695157454
https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695157454:159,Testability,log,log,159,"@k3yavi @rob-p ; Thank you for the prompt responses! I executed the following but seems to get no ""quant.sf"" as output (in fact no other output except for the log file). I don't find ""AlignmentLibrary"" object even though the log states that it did. Do you know what went wrong? Thank you!. Command:; ```bash; salmon quant -e ./aux_info/eq_classes.txt.gz --libType IU -o ./; ```. Output log:; [2020-09-18 20:01:55.879] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-09-18 20:01:55.879] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-09-18 20:01:55.879] [jointLog] [info] numQuantThreads = 4; [2020-09-18 20:02:50.408] [jointLog] [warning] Missing effective lens for 47121 transcripts; setting to 100.0.; [2020-09-18 20:02:50.408] [jointLog] [warning] NOTE: Since effective lengths are not provided, please do not rely on the TPM field ; in the ouput quantifications. Only the NumReads field will be reliable.; [2020-09-18 20:02:50.410] [jointLog] [info] Found total 187671 eqclasses and 47121 transcripts; [2020-09-18 20:02:50.682] [jointLog] [info] Created AlignmentLibrary object. My Salmon version is v1.3.0.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695157454
https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695157454:225,Testability,log,log,225,"@k3yavi @rob-p ; Thank you for the prompt responses! I executed the following but seems to get no ""quant.sf"" as output (in fact no other output except for the log file). I don't find ""AlignmentLibrary"" object even though the log states that it did. Do you know what went wrong? Thank you!. Command:; ```bash; salmon quant -e ./aux_info/eq_classes.txt.gz --libType IU -o ./; ```. Output log:; [2020-09-18 20:01:55.879] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-09-18 20:01:55.879] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-09-18 20:01:55.879] [jointLog] [info] numQuantThreads = 4; [2020-09-18 20:02:50.408] [jointLog] [warning] Missing effective lens for 47121 transcripts; setting to 100.0.; [2020-09-18 20:02:50.408] [jointLog] [warning] NOTE: Since effective lengths are not provided, please do not rely on the TPM field ; in the ouput quantifications. Only the NumReads field will be reliable.; [2020-09-18 20:02:50.410] [jointLog] [info] Found total 187671 eqclasses and 47121 transcripts; [2020-09-18 20:02:50.682] [jointLog] [info] Created AlignmentLibrary object. My Salmon version is v1.3.0.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695157454
https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695157454:386,Testability,log,log,386,"@k3yavi @rob-p ; Thank you for the prompt responses! I executed the following but seems to get no ""quant.sf"" as output (in fact no other output except for the log file). I don't find ""AlignmentLibrary"" object even though the log states that it did. Do you know what went wrong? Thank you!. Command:; ```bash; salmon quant -e ./aux_info/eq_classes.txt.gz --libType IU -o ./; ```. Output log:; [2020-09-18 20:01:55.879] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-09-18 20:01:55.879] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-09-18 20:01:55.879] [jointLog] [info] numQuantThreads = 4; [2020-09-18 20:02:50.408] [jointLog] [warning] Missing effective lens for 47121 transcripts; setting to 100.0.; [2020-09-18 20:02:50.408] [jointLog] [warning] NOTE: Since effective lengths are not provided, please do not rely on the TPM field ; in the ouput quantifications. Only the NumReads field will be reliable.; [2020-09-18 20:02:50.410] [jointLog] [info] Found total 187671 eqclasses and 47121 transcripts; [2020-09-18 20:02:50.682] [jointLog] [info] Created AlignmentLibrary object. My Salmon version is v1.3.0.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/567#issuecomment-695157454
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661:683,Availability,recover,recoverOrphans,683,"I have a similar problem.; Attached are:; 1. gtf file, where clearly, the gene_ id and transcript_id are provided; 2. quant files are as followed for gene and transcripts; 3. my command as followed:; ---. /gpfsdata/apps/salmon-latest_linux_x86_64/bin/salmon quant \; -i /gpfshome/hockchuan/SALMON/GCF_900626175.2_cs10_index \; -l ISR \; -1 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_1.fastq.gz \; -2 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_2.fastq.gz \; --seqBias \; --gcBias \; --posBias \; --incompatPrior 0.0 \; --geneMap /gpfsdata/JangiLab/hockchuan/cs10_reference_genome/GCF_900626175.2_cs10_genomic.gtf \; --recoverOrphans \; --allowDovetail \; --threads $NSLOTS \; --dumpEq \; --minScoreFraction 0.65 \; --writeMappings /gpfshome/hockchuan/SALMON/MAP/HEADBANDSTEM \; --fldMean 250 \; --fldSD 25 \; --writeOrphanLinks \; --writeUnmappedNames \; --quiet \; -o /gpfshome/hockchuan/SALMON/HEADBANDSTEM_quant; ---. [fewLines.gtf.txt](https://github.com/COMBINE-lab/salmon/files/5383013/fewLines.gtf.txt); [quant.genes.txt](https://github.com/COMBINE-lab/salmon/files/5382998/quant.genes.txt); [quant.txt](https://github.com/COMBINE-lab/salmon/files/5382999/quant.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661:683,Safety,recover,recoverOrphans,683,"I have a similar problem.; Attached are:; 1. gtf file, where clearly, the gene_ id and transcript_id are provided; 2. quant files are as followed for gene and transcripts; 3. my command as followed:; ---. /gpfsdata/apps/salmon-latest_linux_x86_64/bin/salmon quant \; -i /gpfshome/hockchuan/SALMON/GCF_900626175.2_cs10_index \; -l ISR \; -1 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_1.fastq.gz \; -2 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_2.fastq.gz \; --seqBias \; --gcBias \; --posBias \; --incompatPrior 0.0 \; --geneMap /gpfsdata/JangiLab/hockchuan/cs10_reference_genome/GCF_900626175.2_cs10_genomic.gtf \; --recoverOrphans \; --allowDovetail \; --threads $NSLOTS \; --dumpEq \; --minScoreFraction 0.65 \; --writeMappings /gpfshome/hockchuan/SALMON/MAP/HEADBANDSTEM \; --fldMean 250 \; --fldSD 25 \; --writeOrphanLinks \; --writeUnmappedNames \; --quiet \; -o /gpfshome/hockchuan/SALMON/HEADBANDSTEM_quant; ---. [fewLines.gtf.txt](https://github.com/COMBINE-lab/salmon/files/5383013/fewLines.gtf.txt); [quant.genes.txt](https://github.com/COMBINE-lab/salmon/files/5382998/quant.genes.txt); [quant.txt](https://github.com/COMBINE-lab/salmon/files/5382999/quant.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661:61,Usability,clear,clearly,61,"I have a similar problem.; Attached are:; 1. gtf file, where clearly, the gene_ id and transcript_id are provided; 2. quant files are as followed for gene and transcripts; 3. my command as followed:; ---. /gpfsdata/apps/salmon-latest_linux_x86_64/bin/salmon quant \; -i /gpfshome/hockchuan/SALMON/GCF_900626175.2_cs10_index \; -l ISR \; -1 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_1.fastq.gz \; -2 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_2.fastq.gz \; --seqBias \; --gcBias \; --posBias \; --incompatPrior 0.0 \; --geneMap /gpfsdata/JangiLab/hockchuan/cs10_reference_genome/GCF_900626175.2_cs10_genomic.gtf \; --recoverOrphans \; --allowDovetail \; --threads $NSLOTS \; --dumpEq \; --minScoreFraction 0.65 \; --writeMappings /gpfshome/hockchuan/SALMON/MAP/HEADBANDSTEM \; --fldMean 250 \; --fldSD 25 \; --writeOrphanLinks \; --writeUnmappedNames \; --quiet \; -o /gpfshome/hockchuan/SALMON/HEADBANDSTEM_quant; ---. [fewLines.gtf.txt](https://github.com/COMBINE-lab/salmon/files/5383013/fewLines.gtf.txt); [quant.genes.txt](https://github.com/COMBINE-lab/salmon/files/5382998/quant.genes.txt); [quant.txt](https://github.com/COMBINE-lab/salmon/files/5382999/quant.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709325079:143,Testability,log,log,143,Is there any output to the terminal when salmon is running that would suggest it couldn't interpret the GTF properly? Can you share the salmon log file?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709325079
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709536328:87,Deployability,upgrade,upgraded,87,"> Any idea what went wrong?. I had that issue while using Salmon version 0.9.1. Once I upgraded the salmon version, I could index the genome properly and I obtained gene-level TPM for the samples.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709536328
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:1514,Availability,Error,Error,1514,"------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory it",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:189,Performance,Load,Loading,189,"This is the initial output log, where it reports an inccorrect gene annotation:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 13.512 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 382.03 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:334,Performance,Load,Loading,334,"This is the initial output log, where it reports an inccorrect gene annotation:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 13.512 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 382.03 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:465,Performance,Load,Loading,465,"This is the initial output log, where it reports an inccorrect gene annotation:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 13.512 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 382.03 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:599,Performance,Load,Loading,599,"This is the initial output log, where it reports an inccorrect gene annotation:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 13.512 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 382.03 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:843,Performance,Load,Loading,843,"This is the initial output log, where it reports an inccorrect gene annotation:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 13.512 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 382.03 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:994,Performance,Load,Loading,994,"This is the initial output log, where it reports an inccorrect gene annotation:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 13.512 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 382.03 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:1135,Performance,Load,Loading,1135," Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 13.512 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 382.03 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; --------------------------------------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:1278,Performance,Load,Loading,1278,"------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 382.03 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading r",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:1412,Performance,Load,Loading,1412,"-----------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Tim",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:1956,Performance,Load,Loading,1956,"-----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.881 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.7554 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 13.626 s; ---------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:2101,Performance,Load,Loading,2101,"-------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.881 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.7554 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 13.626 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:2232,Performance,Load,Loading,2232,"----------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.881 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.7554 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 13.626 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.5082 s; -----------------------------------------; -----------------------------------------; | Loading reference accumu",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:2366,Performance,Load,Loading,2366,"--------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.881 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.7554 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 13.626 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.5082 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 12.272 ms; -----------------------------------------; ---. However, the *.sf files are the same as previo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:2610,Performance,Load,Loading,2610,"---------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.881 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.7554 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 13.626 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.5082 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 12.272 ms; -----------------------------------------; ---. However, the *.sf files are the same as previous ones, i.e. no gene level results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:2761,Performance,Load,Loading,2761,"---------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.881 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.7554 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 13.626 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.5082 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 12.272 ms; -----------------------------------------; ---. However, the *.sf files are the same as previous ones, i.e. no gene level results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:2903,Performance,Load,Loading,2903,"---------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.881 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.7554 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 13.626 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.5082 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 12.272 ms; -----------------------------------------; ---. However, the *.sf files are the same as previous ones, i.e. no gene level results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:3046,Performance,Load,Loading,3046,"---------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.881 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.7554 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 13.626 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.5082 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 12.272 ms; -----------------------------------------; ---. However, the *.sf files are the same as previous ones, i.e. no gene level results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:3180,Performance,Load,Loading,3180,"---------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 336.77 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 10.195 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.3113 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.881 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.7554 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 13.626 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.5082 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 12.272 ms; -----------------------------------------; ---. However, the *.sf files are the same as previous ones, i.e. no gene level results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746:27,Testability,log,log,27,"This is the initial output log, where it reports an inccorrect gene annotation:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 13.512 s; -----------------------------------------; size = 16145665; -----------------------------------------; | Loading contig offsets | Time = 382.03 ms; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 9.4861 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 2.4236 s; -----------------------------------------; size = 1057188904; Number of ones: 16145664; Number of ones per inventory item: 512; Inventory entries filled: 31535; -----------------------------------------; | Loading contig boundaries | Time = 4.031 s; -----------------------------------------; size = 1057188904; -----------------------------------------; | Loading sequence | Time = 1.983 s; -----------------------------------------; size = 572818984; -----------------------------------------; | Loading positions | Time = 14.658 s; -----------------------------------------; size = 942318702; -----------------------------------------; | Loading reference sequence | Time = 1.4932 s; -----------------------------------------; -----------------------------------------; | Loading reference accumulative lengths | Time = 10.959 ms; -----------------------------------------; Error: invalid feature coordinates (end<start!) at line:; NC_029855.1	RefSeq	gene	406748	107842	.	+	.	gene_id ""A5N79_gp28""; db_xref ""GeneID:27215502""; exception ""trans-splicing""; gbkey ""Gene""; gene ""nad2""; gene_biotype ""protein_coding""; locus_tag ""A5N79_gp28""; ; --- . After I remove the erroneous entry, there is no more complaint:. ---; Version Info: This is the most recent version of salmon.; -----------------------------------------; | Loading contig table | Time = 14.648 s; -----",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-709699746
https://github.com/COMBINE-lab/salmon/issues/571#issuecomment-704947451:702,Integrability,depend,depend,702,"Hi @avocado851,. No worries, and welcome to the RNA-seq analysis world! Thanks for choosing salmon :). There's a detailed discussion over in [this](https://github.com/COMBINE-lab/salmon/issues/533) issue describing some reasons you might be seeing a lower than expected mapping rate. The TLDR is, make sure you trim your reads (and / or try mapping with the `--softclip` option) and see if your mapping rate improves to your satisfaction. Even then, it's not uncommon when aligning RNA-seq data to see the kind of mapping rate you're describing *to the annotated transcriptome*. In any given sample, you might see a non-trivial number of reads from outside of the annotation, and the level of this can depend on your tissue type, condition, annotation completeness etc. If there are no issues in your quality report (e.g. are you running this through FastQC / MultiQC etc.?), then this mapping rate alone need not be conclusive evidence of a bad sample.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/571#issuecomment-704947451
https://github.com/COMBINE-lab/salmon/issues/572#issuecomment-708550538:309,Availability,error,error,309,"Hi @gambardella,. Thank you for the detailed issue report. The warnings are noisy, but not themselves problematic (there's a reasonable discussion to be had if we should make the process less noisy by default, and have the user ask for verbosity to get all of the warnings etc.). However, the last line is an error, and this is the cause of the problem. It seems to suggest that `chr20`, which appeared in the `gentrome.fa.gz` file did not have its name appear in `decoys.txt`, and that it occurred after some records that _were_ marked as decoys. I'm assuming that `chr20` _should_ appear as a decoy here. Can you `grep` for `chr20` in the `decoys.txt` file and see if it shows up? If not, the question would be why it's not placed there by your first command. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/572#issuecomment-708550538
https://github.com/COMBINE-lab/salmon/issues/572#issuecomment-708555196:70,Availability,down,down,70,"Hi Rob,. I made such a stupid mistake, it is embarrassing to write it down, and I am ashamed of having wasted your time. Last time I used Salmon was to process a murine Chromium dataset... Copy/paste fossil ... :-( ; Time to call it a day.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/572#issuecomment-708555196
https://github.com/COMBINE-lab/salmon/issues/573#issuecomment-709323564:868,Availability,recover,recoverOrphans,868,"Hi @yeodynasty,. This is because of the unfortunate conventions regarding the parsing of command line options in the presence of a flag that has an _implicit_ option. Specifically, the `--writeMappings` flag has an implicit option. Therefore, to provide an explicit option to the long-form argument flag, you must use the syntax `--writeMappings=<outputdir>`. So, your command would look something like:. ```; /gpfsdata/apps/salmon-latest_linux_x86_64/bin/salmon quant; -i /gpfshome/hockchuan/SALMON/GCF_900626175.2_cs10_index; -l ISR; -1 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_1.fastq.gz; -2 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_2.fastq.gz; --seqBias; --gcBias; --posBias; --incompatPrior 0.0; --geneMap /gpfsdata/JangiLab/hockchuan/cs10_reference_genome/GCF_900626175.2_cs10_genomic.gtf; --recoverOrphans; --allowDovetail; --threads $NSLOTS; --dumpEq; --minScoreFraction 0.65; --writeMappings=/gpfshome/hockchuan/SALMON/MAP/HEADBANDSTEM; --fldMean 250; --fldSD 25; --writeOrphanLinks; --writeUnmappedNames; --quiet; -o /gpfshome/hockchuan/SALMON/HEADBANDSTEM_quant; ```. let me know if this resolves your issue. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/573#issuecomment-709323564
https://github.com/COMBINE-lab/salmon/issues/573#issuecomment-709323564:868,Safety,recover,recoverOrphans,868,"Hi @yeodynasty,. This is because of the unfortunate conventions regarding the parsing of command line options in the presence of a flag that has an _implicit_ option. Specifically, the `--writeMappings` flag has an implicit option. Therefore, to provide an explicit option to the long-form argument flag, you must use the syntax `--writeMappings=<outputdir>`. So, your command would look something like:. ```; /gpfsdata/apps/salmon-latest_linux_x86_64/bin/salmon quant; -i /gpfshome/hockchuan/SALMON/GCF_900626175.2_cs10_index; -l ISR; -1 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_1.fastq.gz; -2 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_2.fastq.gz; --seqBias; --gcBias; --posBias; --incompatPrior 0.0; --geneMap /gpfsdata/JangiLab/hockchuan/cs10_reference_genome/GCF_900626175.2_cs10_genomic.gtf; --recoverOrphans; --allowDovetail; --threads $NSLOTS; --dumpEq; --minScoreFraction 0.65; --writeMappings=/gpfshome/hockchuan/SALMON/MAP/HEADBANDSTEM; --fldMean 250; --fldSD 25; --writeOrphanLinks; --writeUnmappedNames; --quiet; -o /gpfshome/hockchuan/SALMON/HEADBANDSTEM_quant; ```. let me know if this resolves your issue. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/573#issuecomment-709323564
https://github.com/COMBINE-lab/salmon/issues/574#issuecomment-710738799:326,Usability,clear,clear,326,"Hi @Vivianstats ,. Thanks for reaching out. You can certainly dump the CB and UMI tagged Bam file, however, the problem is we can't mark a subset of reads as deduplicated. Alevin's algorithm does not deduplicate UMI at the read level instead we deduplicate at the level of an arboreacence. Basically, the problems is it's not clear which alignment / read should be marked as primary because of following reasons:. 1.) Alevin does fractional assignment of ambiguous reads.; 2.) If there are multiple equally good alignment of a read which alignment should be marked primary for the deduplication ? ; 3.) Even in the UMI tools world, a UMI from a single gene can come from a range of genomic loci, because of the random process of sequence fragmentation. I am not quite sure what UMI tools does, I can check with the developers, but I feel randomly marking one among multiple equally good choice can hide the full picture. Hope it helps. @rob-p feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/574#issuecomment-710738799
https://github.com/COMBINE-lab/salmon/issues/574#issuecomment-713083907:546,Integrability,protocol,protocols,546,"Hi,; Thank you vey much for explaining the reasons in detail. So, the main challenge in deduplication at the read level is that some reads are aligned to multiple positions, right?. I also have a question regrading the third point. My understanding is that reads that map to the same genomic location and have the same UMI and CB will be treated as PCR duplicates, and only one of such reads will be retained in the deduplication process. Can you elaborate on why ""a UMI from a single gene can come from a range of genomic loci""? Do 3' scRNA-seq protocols also involve CDNA fragmentation?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/574#issuecomment-713083907
https://github.com/COMBINE-lab/salmon/issues/574#issuecomment-713246157:58,Integrability,protocol,protocol,58,"yep, at least in my understanding, the 3' single cell 10x protocol goes through the process of cDNA fragmentation post amplification. I think you can more information from [here](https://assets.ctfassets.net/an68im79xiti/4tjk4KvXzTWgTs8f3tvUjq/2259891d68c53693e753e1b45e42de2d/CG000183_ChromiumSingleCell3__v3_UG_Rev_C.pdf)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/574#issuecomment-713246157
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:541,Deployability,integrat,integrate,541,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:541,Integrability,integrat,integrate,541,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:1409,Testability,test,testing,1409,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:697,Usability,simpl,simplicity,697,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:1020,Usability,Intuit,Intuitively,1020,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:1239,Usability,simpl,simply,1239,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531:388,Availability,down,down,388,"Hi @yeodynasty,. There are two different ways to tackle this question. The first relies on the fact that the correction employed by salmon for GC bias is done via the adjustment of transcript effective lengths. Here, you could compare the effective length in the quant.sf file to the effective length you would get ignoring GC-fragment (or other bias). Granted, the latter is not written down in the file here, but it is straightforward to calculate since salmon also writes out the fragment length distribution. ; The effective length discarding bias estimates is simply the transcript length, minus the mean of the conditional fragment length distribution (the fragment length distribution from 0 up to the transcript length, re-normalized to be an appropriate probability distribution). If you look at the differences between these values, you can infer how much bias correction was applied. Specifically, when the bias-corrected length is longer than the non bias-corrected length, then these transcripts are over-represented in sequencing and the bias correction aims to reduce their estimated abundance. On the other hand, when the bias-corrected length is shorter than the non bias-corrected length, then these transcripts are under-represented in sequencing and the bias correction aims to increase their estimated abundance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531
https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531:1076,Energy Efficiency,reduce,reduce,1076,"Hi @yeodynasty,. There are two different ways to tackle this question. The first relies on the fact that the correction employed by salmon for GC bias is done via the adjustment of transcript effective lengths. Here, you could compare the effective length in the quant.sf file to the effective length you would get ignoring GC-fragment (or other bias). Granted, the latter is not written down in the file here, but it is straightforward to calculate since salmon also writes out the fragment length distribution. ; The effective length discarding bias estimates is simply the transcript length, minus the mean of the conditional fragment length distribution (the fragment length distribution from 0 up to the transcript length, re-normalized to be an appropriate probability distribution). If you look at the differences between these values, you can infer how much bias correction was applied. Specifically, when the bias-corrected length is longer than the non bias-corrected length, then these transcripts are over-represented in sequencing and the bias correction aims to reduce their estimated abundance. On the other hand, when the bias-corrected length is shorter than the non bias-corrected length, then these transcripts are under-represented in sequencing and the bias correction aims to increase their estimated abundance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531
https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531:565,Usability,simpl,simply,565,"Hi @yeodynasty,. There are two different ways to tackle this question. The first relies on the fact that the correction employed by salmon for GC bias is done via the adjustment of transcript effective lengths. Here, you could compare the effective length in the quant.sf file to the effective length you would get ignoring GC-fragment (or other bias). Granted, the latter is not written down in the file here, but it is straightforward to calculate since salmon also writes out the fragment length distribution. ; The effective length discarding bias estimates is simply the transcript length, minus the mean of the conditional fragment length distribution (the fragment length distribution from 0 up to the transcript length, re-normalized to be an appropriate probability distribution). If you look at the differences between these values, you can infer how much bias correction was applied. Specifically, when the bias-corrected length is longer than the non bias-corrected length, then these transcripts are over-represented in sequencing and the bias correction aims to reduce their estimated abundance. On the other hand, when the bias-corrected length is shorter than the non bias-corrected length, then these transcripts are under-represented in sequencing and the bias correction aims to increase their estimated abundance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531
https://github.com/COMBINE-lab/salmon/issues/579#issuecomment-717279405:2657,Integrability,depend,depends,2657,"x (which is the colored De Bruijn graph). I understand that this index (along with all the equivalence class) remains the same even when gene counts of different RNA-Seq samples are estimated. But I am a bit confused about the alignment-based method. In this case, salmon does not require an index since it has the actual alignments. If we have multiple samples, which are mapped to the same transcriptome will Salmon return the same set of equivalence classes? Since the samples are different the weights will change and so will the reads mapped to each equivalence class but will the set of eq. classes change?. The index remains the same when different samples are processed, just as with a traditional alignment tool like STAR or HISAT2. However, the set of equivalence classes are _not_ fixed between samples. The equivalence classes are induced by the specific set of aligned or mapped reads. Further, salmon adopts a notion of [range-factorized equivalence classes](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977) in which the equivalence relation depends not only on the transcripts to which a read aligns or maps, but also on the conditional probabilities of the fragment being generated from these transcripts, which itself depends on experiment-specific parameters like the fragment length distribution. Thus, it is not the case under either its own builtin lightweight (selective) alignment, nor when operating with an external BAM file, that the set of equivalence classes produced by salmon will be the same across samples. The equivalence classes are _based_ on the underlying reference sequence, but are sample specific and induced both by the specific patterns of multimapping as well as by the sample-specific parameters (like the fragment length distribution). Thus, if you wish to perform some type of equivalence-class type analysis over multiple samples, you'll need to take the union over the equivalence classes observed in each of them. I hope this helps!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/579#issuecomment-717279405
https://github.com/COMBINE-lab/salmon/issues/579#issuecomment-717279405:2836,Integrability,depend,depends,2836,"x (which is the colored De Bruijn graph). I understand that this index (along with all the equivalence class) remains the same even when gene counts of different RNA-Seq samples are estimated. But I am a bit confused about the alignment-based method. In this case, salmon does not require an index since it has the actual alignments. If we have multiple samples, which are mapped to the same transcriptome will Salmon return the same set of equivalence classes? Since the samples are different the weights will change and so will the reads mapped to each equivalence class but will the set of eq. classes change?. The index remains the same when different samples are processed, just as with a traditional alignment tool like STAR or HISAT2. However, the set of equivalence classes are _not_ fixed between samples. The equivalence classes are induced by the specific set of aligned or mapped reads. Further, salmon adopts a notion of [range-factorized equivalence classes](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977) in which the equivalence relation depends not only on the transcripts to which a read aligns or maps, but also on the conditional probabilities of the fragment being generated from these transcripts, which itself depends on experiment-specific parameters like the fragment length distribution. Thus, it is not the case under either its own builtin lightweight (selective) alignment, nor when operating with an external BAM file, that the set of equivalence classes produced by salmon will be the same across samples. The equivalence classes are _based_ on the underlying reference sequence, but are sample specific and induced both by the specific patterns of multimapping as well as by the sample-specific parameters (like the fragment length distribution). Thus, if you wish to perform some type of equivalence-class type analysis over multiple samples, you'll need to take the union over the equivalence classes observed in each of them. I hope this helps!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/579#issuecomment-717279405
https://github.com/COMBINE-lab/salmon/issues/579#issuecomment-717279405:161,Performance,perform,perform,161,"Hi @sagnikbanerjee15,. > We are using Salmon to quantify gene counts for samples in RNA-Seq experiment. We will be using the weights of the equivalence class to perform a calculation for which we require the effective length of the equivalence classes. I checked in the eq_class.txt file under the aux_info directory but I was unable to find it. We are using alignment-based method where we supply a bamfile to salmon. Could you please help us to obtain the effective length of the equivalence classes?. An equivalence class is not an object that has a notion of a length or effective length. Specifically, an equivalence class represents a set of transcripts to which a set of fragments map or align. If you are using the range-factorized equivalence classes (which is what salmon uses by default internally), these represent a set of transcripts to which a set of fragments map or align with very similar conditional probability vectors. Since the equivalence class represents mappings or alignments to a collection of transcripts — where each transcript may have an arbitrarily different length — there is no such thing as the notion of the ""effective length"" of an equivalence class. Note that the sequence that induces an equivalence class need not even be a contiguous region of the underlying reference (see answer below), and thus the notion of an effective length (or a length in general) is not applicable here. > I have another question. Salmon operates in both lightweight alignment mode and in alignment mode. For the lightweight mode, one needs to first create an index (which is the colored De Bruijn graph). I understand that this index (along with all the equivalence class) remains the same even when gene counts of different RNA-Seq samples are estimated. But I am a bit confused about the alignment-based method. In this case, salmon does not require an index since it has the actual alignments. If we have multiple samples, which are mapped to the same transcriptome will Salmon r",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/579#issuecomment-717279405
https://github.com/COMBINE-lab/salmon/issues/579#issuecomment-717279405:3403,Performance,perform,perform,3403,"x (which is the colored De Bruijn graph). I understand that this index (along with all the equivalence class) remains the same even when gene counts of different RNA-Seq samples are estimated. But I am a bit confused about the alignment-based method. In this case, salmon does not require an index since it has the actual alignments. If we have multiple samples, which are mapped to the same transcriptome will Salmon return the same set of equivalence classes? Since the samples are different the weights will change and so will the reads mapped to each equivalence class but will the set of eq. classes change?. The index remains the same when different samples are processed, just as with a traditional alignment tool like STAR or HISAT2. However, the set of equivalence classes are _not_ fixed between samples. The equivalence classes are induced by the specific set of aligned or mapped reads. Further, salmon adopts a notion of [range-factorized equivalence classes](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977) in which the equivalence relation depends not only on the transcripts to which a read aligns or maps, but also on the conditional probabilities of the fragment being generated from these transcripts, which itself depends on experiment-specific parameters like the fragment length distribution. Thus, it is not the case under either its own builtin lightweight (selective) alignment, nor when operating with an external BAM file, that the set of equivalence classes produced by salmon will be the same across samples. The equivalence classes are _based_ on the underlying reference sequence, but are sample specific and induced both by the specific patterns of multimapping as well as by the sample-specific parameters (like the fragment length distribution). Thus, if you wish to perform some type of equivalence-class type analysis over multiple samples, you'll need to take the union over the equivalence classes observed in each of them. I hope this helps!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/579#issuecomment-717279405
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923:120,Availability,down,downstream,120,"Thanks @tamuanand for the (as always) detailed and clear question! Since this directly involves `tximport` and `DESeq2` downstream, let me also ping @mikelove here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923:144,Availability,ping,ping,144,"Thanks @tamuanand for the (as always) detailed and clear question! Since this directly involves `tximport` and `DESeq2` downstream, let me also ping @mikelove here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923:51,Usability,clear,clear,51,"Thanks @tamuanand for the (as always) detailed and clear question! Since this directly involves `tximport` and `DESeq2` downstream, let me also ping @mikelove here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719150287:271,Availability,down,downstream,271,"Thanks @rob-p and Thanks in advance @mikelove . The original question pertained to using salmon with say ILMN RNA-Seq followed by DGE with DESeq2. @rob-p - I will also use this opportunity to indulge myself on a related question (how to use salmon with QuantSeq and then downstream with DESeq2). I have asked many QuantSeq related questions on this GH forum and I am yet to find the correct recipe for using salmon with quantseq and downstream DGE; - https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848; - https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499732849; - and many others (do not want to get into a infinite loop here :) . @rob-p @mikelove - Here is my thought process (for salmon-QuantSeq-DESeq):; - I know salmon has the `--noLengthCorrection` feature and the help text says it is ""experimental"" for QuantSeq; - Probably, I should not use `--noLengthCorrection` feature when running salmon quant and just get the counts. ; - One might be wondeing why not to use `--noLengthCorrection` as it was introduced by @rob-p exclusively for QuantSeq -- that idea is based on what I see on [the tximport vignette for 3' tagged RNA-seq](https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html#3%E2%80%99_tagged_RNA-seq) which has this to state; ```; If you have 3’ tagged RNA-seq data, then correcting the counts for gene length will induce a bias in your analysis, ; because the counts do not have length bias. Instead of using the default full-transcript-length pipeline, ; we recommend to use the original counts, e.g. txi$counts as a counts matrix, e.g. ; providing to DESeqDataSetFromMatrix or to the edgeR or limma functions ; without calculating an offset and without using countsFromAbundance.; ```. Let me know if you would approach the salmon-QuantSeq-DESeq puzzle differently. Thanks in advance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719150287
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719150287:433,Availability,down,downstream,433,"Thanks @rob-p and Thanks in advance @mikelove . The original question pertained to using salmon with say ILMN RNA-Seq followed by DGE with DESeq2. @rob-p - I will also use this opportunity to indulge myself on a related question (how to use salmon with QuantSeq and then downstream with DESeq2). I have asked many QuantSeq related questions on this GH forum and I am yet to find the correct recipe for using salmon with quantseq and downstream DGE; - https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848; - https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499732849; - and many others (do not want to get into a infinite loop here :) . @rob-p @mikelove - Here is my thought process (for salmon-QuantSeq-DESeq):; - I know salmon has the `--noLengthCorrection` feature and the help text says it is ""experimental"" for QuantSeq; - Probably, I should not use `--noLengthCorrection` feature when running salmon quant and just get the counts. ; - One might be wondeing why not to use `--noLengthCorrection` as it was introduced by @rob-p exclusively for QuantSeq -- that idea is based on what I see on [the tximport vignette for 3' tagged RNA-seq](https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html#3%E2%80%99_tagged_RNA-seq) which has this to state; ```; If you have 3’ tagged RNA-seq data, then correcting the counts for gene length will induce a bias in your analysis, ; because the counts do not have length bias. Instead of using the default full-transcript-length pipeline, ; we recommend to use the original counts, e.g. txi$counts as a counts matrix, e.g. ; providing to DESeqDataSetFromMatrix or to the edgeR or limma functions ; without calculating an offset and without using countsFromAbundance.; ```. Let me know if you would approach the salmon-QuantSeq-DESeq puzzle differently. Thanks in advance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719150287
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719150287:1531,Deployability,pipeline,pipeline,1531,"Thanks @rob-p and Thanks in advance @mikelove . The original question pertained to using salmon with say ILMN RNA-Seq followed by DGE with DESeq2. @rob-p - I will also use this opportunity to indulge myself on a related question (how to use salmon with QuantSeq and then downstream with DESeq2). I have asked many QuantSeq related questions on this GH forum and I am yet to find the correct recipe for using salmon with quantseq and downstream DGE; - https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848; - https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499732849; - and many others (do not want to get into a infinite loop here :) . @rob-p @mikelove - Here is my thought process (for salmon-QuantSeq-DESeq):; - I know salmon has the `--noLengthCorrection` feature and the help text says it is ""experimental"" for QuantSeq; - Probably, I should not use `--noLengthCorrection` feature when running salmon quant and just get the counts. ; - One might be wondeing why not to use `--noLengthCorrection` as it was introduced by @rob-p exclusively for QuantSeq -- that idea is based on what I see on [the tximport vignette for 3' tagged RNA-seq](https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html#3%E2%80%99_tagged_RNA-seq) which has this to state; ```; If you have 3’ tagged RNA-seq data, then correcting the counts for gene length will induce a bias in your analysis, ; because the counts do not have length bias. Instead of using the default full-transcript-length pipeline, ; we recommend to use the original counts, e.g. txi$counts as a counts matrix, e.g. ; providing to DESeqDataSetFromMatrix or to the edgeR or limma functions ; without calculating an offset and without using countsFromAbundance.; ```. Let me know if you would approach the salmon-QuantSeq-DESeq puzzle differently. Thanks in advance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719150287
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719721141:629,Security,validat,validateMappings,629,"@mikelove - was this question with reference to salmon quant on QuantSeq data?. > Just to check: do you have length bias in your data (are counts roughly proportional to effective transcript length)?. @rob-p Is there a way to get the answer to Mike's question from the meta_info.json files. Also, aren't the counts in quant.sf file provided after taking into account length bias and effective transcript length?. This is the salmon quant command line being used for RNA-Seq quantification - still not figured out the right command line combination for QuantSeq data. ```; salmon --no-version-check quant --threads 16 --seqBias --validateMappings --numBootstraps 100 .......; ```. The original question in the post is ""what are the correct steps with tximport for running DESEQ after salmon quant""",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719721141
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719734272:359,Integrability,protocol,protocol,359,"The standard is the code chunk in the vignette:. ```; txi <- tximport(files, type = ""salmon"", tx2gene = tx2gene); # then below...; dds <- DESeqDataSetFromTximport(txi, sampleTable, ~condition); ```. Or even better, you can use tximeta:. ```; se <- tximeta(coldata); gse <- summarizeToGene(se); dds <- DESeqDataSet(gse, ~condition); ```. If you have a special protocol which does not involve fragmentation of a full length transcript, then you do something else. But if you are fragmenting molecules and sequencing from along the entire transcript, use those code chunks from the vignette.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719734272
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719784814:218,Availability,avail,available,218,"Thanks @mikelove . I believe tximeta can be used only for human/mouse? In my case, it is not human/mouse. @rob-p and @mikelove - Based on my reading of the salmon documentation, isn't it that the NumReads/TPM etc made available after lengthCorrection. Extending this, the NumReads in quant.sf corresponds to the estimated count value for each transcript and correlated by effective length. My idea is to therefore use the countsFromAbundance=“lengthScaledTPM” to compute counts that are on the same scale as original counts and not correlated with transcript length across samples. Given this - Is this below also valid (after salmon quant). ```; salmon_tx2gene_data = tximport(files, type=""salmon"", tx2gene=tx2gene,; countsFromAbundance = ""lengthScaledTPM""). # generate CSV for archival/use-for-other-purposes ; # then read in the csv and use with DESeq. write.csv(as.data.frame(salmon_tx2gene_data$counts),; file = ""lengthScaledTPM_tx2gene_counts.csv""). # other code for reading in csv, design_metadata etc. dds <- DESeqDataSetFromMatrix (countData = salmon_tx2gene_data$counts,; colData = coldata, ~ condition). ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719784814
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719784814:252,Modifiability,Extend,Extending,252,"Thanks @mikelove . I believe tximeta can be used only for human/mouse? In my case, it is not human/mouse. @rob-p and @mikelove - Based on my reading of the salmon documentation, isn't it that the NumReads/TPM etc made available after lengthCorrection. Extending this, the NumReads in quant.sf corresponds to the estimated count value for each transcript and correlated by effective length. My idea is to therefore use the countsFromAbundance=“lengthScaledTPM” to compute counts that are on the same scale as original counts and not correlated with transcript length across samples. Given this - Is this below also valid (after salmon quant). ```; salmon_tx2gene_data = tximport(files, type=""salmon"", tx2gene=tx2gene,; countsFromAbundance = ""lengthScaledTPM""). # generate CSV for archival/use-for-other-purposes ; # then read in the csv and use with DESeq. write.csv(as.data.frame(salmon_tx2gene_data$counts),; file = ""lengthScaledTPM_tx2gene_counts.csv""). # other code for reading in csv, design_metadata etc. dds <- DESeqDataSetFromMatrix (countData = salmon_tx2gene_data$counts,; colData = coldata, ~ condition). ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719784814
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719806601:17,Usability,simpl,simpler,17,"To keep the code simpler:. ```; txi <- tximport(files, type = ""salmon"", tx2gene = tx2gene, countsFromAbundance = ""lengthScaledTPM""); # then below...; dds <- DESeqDataSetFromTximport(txi, sampleTable, ~condition); ```. DESeq2 will do the right thing based on the value of `txi$countsFromAbundance`. This is the point of the importer functions. We also have them in tximeta for edgeR and limma. (You can still use tximeta with organisms other than human, mouse, or fly, you just have to run `makeLinkedTxome` and point to the GTF for your organism. It's just one step really.)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719806601
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719946865:1124,Availability,down,downstream,1124,"Thanks @mikelove . > we had some people using txi$counts alone and not using the countsFromAbundance argument. Based on the above, I assume that **_doing something like this is wrong_** as DESeqDataSetFromMatrix is being used after countsFromAbundance = ""no"". ```; txi = tximport(files, type=""salmon"", tx2gene=tx2gene,; countsFromAbundance = ""no""). dds <- DESeqDataSetFromMatrix (countData = txi$counts,; colData = coldata, ~ condition); ```. @rob-p and @mikelove -- While on this topic, how would you use salmon quant and DESeq2 for QuantSeq data (which would be 3' tagged RNA-seq)? Would you use `salmon quant without --noLengthCorrection` or would you use` salmon quant with ; --noLengthCorrection`. 1. call salmon quant as before (and **_do not use --noLengthCorrection_**) and then do as suggested/stated in these 2 links ; - https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html#Downstream_DGE_in_Bioconductor and https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html#3%E2%80%99_tagged_RNA-seq; - Do not manually pass the original gene-level counts to downstream methods without an offset. The only case where this would make sense is if there is no length bias to the counts, as happens in 3’ tagged RNA-seq data (see section below). The original gene-level counts are in txi$counts when tximport was run with countsFromAbundance=""no"". ; - If you have 3’ tagged RNA-seq data, then correcting the counts for gene length will induce a bias in your analysis, because the counts do not have length bias. Instead of using the default full-transcript-length pipeline, we recommend to use the original counts, e.g. txi$counts as a counts matrix , e.g. providing to DESeqDataSetFromMatrix",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719946865
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719946865:1625,Deployability,pipeline,pipeline,1625,"Thanks @mikelove . > we had some people using txi$counts alone and not using the countsFromAbundance argument. Based on the above, I assume that **_doing something like this is wrong_** as DESeqDataSetFromMatrix is being used after countsFromAbundance = ""no"". ```; txi = tximport(files, type=""salmon"", tx2gene=tx2gene,; countsFromAbundance = ""no""). dds <- DESeqDataSetFromMatrix (countData = txi$counts,; colData = coldata, ~ condition); ```. @rob-p and @mikelove -- While on this topic, how would you use salmon quant and DESeq2 for QuantSeq data (which would be 3' tagged RNA-seq)? Would you use `salmon quant without --noLengthCorrection` or would you use` salmon quant with ; --noLengthCorrection`. 1. call salmon quant as before (and **_do not use --noLengthCorrection_**) and then do as suggested/stated in these 2 links ; - https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html#Downstream_DGE_in_Bioconductor and https://bioconductor.org/packages/devel/bioc/vignettes/tximport/inst/doc/tximport.html#3%E2%80%99_tagged_RNA-seq; - Do not manually pass the original gene-level counts to downstream methods without an offset. The only case where this would make sense is if there is no length bias to the counts, as happens in 3’ tagged RNA-seq data (see section below). The original gene-level counts are in txi$counts when tximport was run with countsFromAbundance=""no"". ; - If you have 3’ tagged RNA-seq data, then correcting the counts for gene length will induce a bias in your analysis, because the counts do not have length bias. Instead of using the default full-transcript-length pipeline, we recommend to use the original counts, e.g. txi$counts as a counts matrix , e.g. providing to DESeqDataSetFromMatrix",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719946865
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-732397913:67,Deployability,upgrade,upgrade,67,"Any reasons why you want to still use 0.14.1 - probably you should upgrade to 1.3.0. This is not to suggest that upgrading will fix your issue - salmon has many new features since v1 (@rob-p can allude to it), but IMO you should be using salmon >= 1.2. > We have seen that others have had similar issue but haven't found a thread that mentions what the user did to remedy the problem. Any suggestions would be appreciated! We are using salmon V0.14.1.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-732397913
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:293,Availability,error,errors,293,"Hey, I'm having the same kind of problem. I aligned my PE reads against the transcriptome using BWA-mem and then sorted them by coordinates (as a regular procedure). I know Salmon assumes the alignments are not sorted, so I shuffled these bam files, and then run `salmon quant`.; Here are the errors I got in a number of trials:. ### Fresh installation of Salmon; ```; conda create --name salmon -c bioconda salmon; conda activate salmon; ```. ### 1. Shuffling a bam file with `samtools collate`; ```; samtools collate \; -@ 40 \; -o SRR3212847.Aligned.Shuffled.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.Shuffled.bam \; -o SRR3212847.Aligned.Shuffled ; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; # [ output ] => { SRR3212847.Aligned.Shuffled }; Logs will be written to SRR3212847.Aligned.Shuffled/logs; [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```. ### 2. Shuffling a headless bam file with `samtools collate`; (I think I saw something about the bam's header in another thread dealing with this issue); ```; samtools view \; -b \; -@ 40 \; -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:1734,Availability,fault,fault,1734,"led ; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; # [ output ] => { SRR3212847.Aligned.Shuffled }; Logs will be written to SRR3212847.Aligned.Shuffled/logs; [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```. ### 2. Shuffling a headless bam file with `samtools collate`; (I think I saw something about the bam's header in another thread dealing with this issue); ```; samtools view \; -b \; -@ 40 \; -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.bam. samtools collate \; -@ 40 \; -o SRR3212847.Aligned.Shuffled.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.NoHeader.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.Shuffled.NoHeader.bam \; -o SRR3212847.Aligned.Shuffled.NoHeader; ```. ```. .... [2021-01-08 12:42:10.700] [jointLog] [warning] . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.24133171; read2 : SRR3212847.33911054; The proper-pair statuses are inconsistent:; read1 [SRR3212847.24133171] : no proper-pair; not mapped; matenot mapped. read2 : [SRR3212847.24133171] : proper-pair; mapped; matemapped. [2021-01-08 12:42:10.700] [jointLog] [warning] . W",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:2859,Availability,fault,fault,2859,"eader in another thread dealing with this issue); ```; samtools view \; -b \; -@ 40 \; -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.bam. samtools collate \; -@ 40 \; -o SRR3212847.Aligned.Shuffled.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.NoHeader.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.Shuffled.NoHeader.bam \; -o SRR3212847.Aligned.Shuffled.NoHeader; ```. ```. .... [2021-01-08 12:42:10.700] [jointLog] [warning] . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.24133171; read2 : SRR3212847.33911054; The proper-pair statuses are inconsistent:; read1 [SRR3212847.24133171] : no proper-pair; not mapped; matenot mapped. read2 : [SRR3212847.24133171] : proper-pair; mapped; matemapped. [2021-01-08 12:42:10.700] [jointLog] [warning] . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.33911054; read2 : SRR3212847.30781941. Segmentation fault (core dumped); ```. ### 3. Sorting with `samtools sort -n`; ```; samtools sort \; -@ 40 \; -n \; -o SRR3212847.Aligned.SortedByName.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByName.bam \; -o SRR3212847.Aligned.SortedByName; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; # [ output ] => { SRR3212847.Aligned.SortedByName }; Logs will be written to SRR3212847.Aligned.SortedByName/logs; [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strande",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:4192,Availability,fault,fault,4192," . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.33911054; read2 : SRR3212847.30781941. Segmentation fault (core dumped); ```. ### 3. Sorting with `samtools sort -n`; ```; samtools sort \; -@ 40 \; -n \; -o SRR3212847.Aligned.SortedByName.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByName.bam \; -o SRR3212847.Aligned.SortedByName; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; # [ output ] => { SRR3212847.Aligned.SortedByName }; Logs will be written to SRR3212847.Aligned.SortedByName/logs; [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```; (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.). I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; ```; nohup salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByCoord.bam \; -o SRR3212847.Aligned.SortedByCoord \; > SRR3212847.Aligned.SortedByCoord.out &; ```; Even so, `SRR3212847.Aligned.SortedByCoord.out` contained ~3.5GB worth of the warnings above. Any help would be much appreciated.; Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:4248,Availability,error,error,4248," . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.33911054; read2 : SRR3212847.30781941. Segmentation fault (core dumped); ```. ### 3. Sorting with `samtools sort -n`; ```; samtools sort \; -@ 40 \; -n \; -o SRR3212847.Aligned.SortedByName.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByName.bam \; -o SRR3212847.Aligned.SortedByName; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; # [ output ] => { SRR3212847.Aligned.SortedByName }; Logs will be written to SRR3212847.Aligned.SortedByName/logs; [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```; (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.). I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; ```; nohup salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByCoord.bam \; -o SRR3212847.Aligned.SortedByCoord \; > SRR3212847.Aligned.SortedByCoord.out &; ```; Even so, `SRR3212847.Aligned.SortedByCoord.out` contained ~3.5GB worth of the warnings above. Any help would be much appreciated.; Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:4294,Availability,error,errors,4294," . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.33911054; read2 : SRR3212847.30781941. Segmentation fault (core dumped); ```. ### 3. Sorting with `samtools sort -n`; ```; samtools sort \; -@ 40 \; -n \; -o SRR3212847.Aligned.SortedByName.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByName.bam \; -o SRR3212847.Aligned.SortedByName; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; # [ output ] => { SRR3212847.Aligned.SortedByName }; Logs will be written to SRR3212847.Aligned.SortedByName/logs; [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```; (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.). I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; ```; nohup salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByCoord.bam \; -o SRR3212847.Aligned.SortedByCoord \; > SRR3212847.Aligned.SortedByCoord.out &; ```; Even so, `SRR3212847.Aligned.SortedByCoord.out` contained ~3.5GB worth of the warnings above. Any help would be much appreciated.; Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:340,Deployability,install,installation,340,"Hey, I'm having the same kind of problem. I aligned my PE reads against the transcriptome using BWA-mem and then sorted them by coordinates (as a regular procedure). I know Salmon assumes the alignments are not sorted, so I shuffled these bam files, and then run `salmon quant`.; Here are the errors I got in a number of trials:. ### Fresh installation of Salmon; ```; conda create --name salmon -c bioconda salmon; conda activate salmon; ```. ### 1. Shuffling a bam file with `samtools collate`; ```; samtools collate \; -@ 40 \; -o SRR3212847.Aligned.Shuffled.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.Shuffled.bam \; -o SRR3212847.Aligned.Shuffled ; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; # [ output ] => { SRR3212847.Aligned.Shuffled }; Logs will be written to SRR3212847.Aligned.Shuffled/logs; [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```. ### 2. Shuffling a headless bam file with `samtools collate`; (I think I saw something about the bam's header in another thread dealing with this issue); ```; samtools view \; -b \; -@ 40 \; -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:2375,Safety,Detect,Detected,2375,"ndedness:unstranded }; [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```. ### 2. Shuffling a headless bam file with `samtools collate`; (I think I saw something about the bam's header in another thread dealing with this issue); ```; samtools view \; -b \; -@ 40 \; -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.bam. samtools collate \; -@ 40 \; -o SRR3212847.Aligned.Shuffled.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.NoHeader.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.Shuffled.NoHeader.bam \; -o SRR3212847.Aligned.Shuffled.NoHeader; ```. ```. .... [2021-01-08 12:42:10.700] [jointLog] [warning] . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.24133171; read2 : SRR3212847.33911054; The proper-pair statuses are inconsistent:; read1 [SRR3212847.24133171] : no proper-pair; not mapped; matenot mapped. read2 : [SRR3212847.24133171] : proper-pair; mapped; matemapped. [2021-01-08 12:42:10.700] [jointLog] [warning] . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.33911054; read2 : SRR3212847.30781941. Segmentation fault (core dumped); ```. ### 3. Sorting with `samtools sort -n`; ```; samtools sort \; -@ 40 \; -n \; -o SRR3212847.Aligned.SortedByName.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByName.bam \; -o SRR3212847.Aligned.SortedByName; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:2731,Safety,Detect,Detected,2731,2. Shuffling a headless bam file with `samtools collate`; (I think I saw something about the bam's header in another thread dealing with this issue); ```; samtools view \; -b \; -@ 40 \; -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.bam. samtools collate \; -@ 40 \; -o SRR3212847.Aligned.Shuffled.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.NoHeader.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.Shuffled.NoHeader.bam \; -o SRR3212847.Aligned.Shuffled.NoHeader; ```. ```. .... [2021-01-08 12:42:10.700] [jointLog] [warning] . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.24133171; read2 : SRR3212847.33911054; The proper-pair statuses are inconsistent:; read1 [SRR3212847.24133171] : no proper-pair; not mapped; matenot mapped. read2 : [SRR3212847.24133171] : proper-pair; mapped; matemapped. [2021-01-08 12:42:10.700] [jointLog] [warning] . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.33911054; read2 : SRR3212847.30781941. Segmentation fault (core dumped); ```. ### 3. Sorting with `samtools sort -n`; ```; samtools sort \; -@ 40 \; -n \; -o SRR3212847.Aligned.SortedByName.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByName.bam \; -o SRR3212847.Aligned.SortedByName; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; # [ output ] => { SRR3212847.Aligned.SortedByName }; Logs will be written to SRR3212847.Aligned.SortedByName/logs; [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatib,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:1070,Testability,Log,Logs,1070,"criptome using BWA-mem and then sorted them by coordinates (as a regular procedure). I know Salmon assumes the alignments are not sorted, so I shuffled these bam files, and then run `salmon quant`.; Here are the errors I got in a number of trials:. ### Fresh installation of Salmon; ```; conda create --name salmon -c bioconda salmon; conda activate salmon; ```. ### 1. Shuffling a bam file with `samtools collate`; ```; samtools collate \; -@ 40 \; -o SRR3212847.Aligned.Shuffled.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.Shuffled.bam \; -o SRR3212847.Aligned.Shuffled ; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; # [ output ] => { SRR3212847.Aligned.Shuffled }; Logs will be written to SRR3212847.Aligned.Shuffled/logs; [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```. ### 2. Shuffling a headless bam file with `samtools collate`; (I think I saw something about the bam's header in another thread dealing with this issue); ```; samtools view \; -b \; -@ 40 \; -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.bam. samtools collate \; -@ 40 \; -o SRR321284",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:1122,Testability,log,logs,1122,"rdinates (as a regular procedure). I know Salmon assumes the alignments are not sorted, so I shuffled these bam files, and then run `salmon quant`.; Here are the errors I got in a number of trials:. ### Fresh installation of Salmon; ```; conda create --name salmon -c bioconda salmon; conda activate salmon; ```. ### 1. Shuffling a bam file with `samtools collate`; ```; samtools collate \; -@ 40 \; -o SRR3212847.Aligned.Shuffled.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.Shuffled.bam \; -o SRR3212847.Aligned.Shuffled ; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; # [ output ] => { SRR3212847.Aligned.Shuffled }; Logs will be written to SRR3212847.Aligned.Shuffled/logs; [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```. ### 2. Shuffling a headless bam file with `samtools collate`; (I think I saw something about the bam's header in another thread dealing with this issue); ```; samtools view \; -b \; -@ 40 \; -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; SRR3212847.Aligned.SortedByCoord.bam. samtools collate \; -@ 40 \; -o SRR3212847.Aligned.Shuffled.NoHeader.bam \; SRR3212847.Ali",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:3520,Testability,Log,Logs,3520,"nt:; read1 [SRR3212847.24133171] : no proper-pair; not mapped; matenot mapped. read2 : [SRR3212847.24133171] : proper-pair; mapped; matemapped. [2021-01-08 12:42:10.700] [jointLog] [warning] . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.33911054; read2 : SRR3212847.30781941. Segmentation fault (core dumped); ```. ### 3. Sorting with `samtools sort -n`; ```; samtools sort \; -@ 40 \; -n \; -o SRR3212847.Aligned.SortedByName.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByName.bam \; -o SRR3212847.Aligned.SortedByName; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; # [ output ] => { SRR3212847.Aligned.SortedByName }; Logs will be written to SRR3212847.Aligned.SortedByName/logs; [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```; (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.). I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; ```; nohup salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByCoord.bam \; -o SRR3212847.Aligned.SortedByCoord",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212:3576,Testability,log,logs,3576," mapped; matenot mapped. read2 : [SRR3212847.24133171] : proper-pair; mapped; matemapped. [2021-01-08 12:42:10.700] [jointLog] [warning] . WARNING: Detected suspicious pair --- ; The names are different:; read1 : SRR3212847.33911054; read2 : SRR3212847.30781941. Segmentation fault (core dumped); ```. ### 3. Sorting with `samtools sort -n`; ```; samtools sort \; -@ 40 \; -n \; -o SRR3212847.Aligned.SortedByName.bam \; SRR3212847.Aligned.SortedByCoord.bam. salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByName.bam \; -o SRR3212847.Aligned.SortedByName; ```. ```; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.4.0; # [ program ] => salmon ; # [ command ] => quant ; # [ targets ] => { mRNA.fasta }; # [ threads ] => { 20 }; # [ libType ] => { A }; # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; # [ output ] => { SRR3212847.Aligned.SortedByName }; Logs will be written to SRR3212847.Aligned.SortedByName/logs; [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; parseThreads = 6; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done. processed 0 reads in current roundSegmentation fault (core dumped); ```; (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.). I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; ```; nohup salmon quant \; -t mRNA.fasta \; -p 20 \; -l A \; -a SRR3212847.Aligned.SortedByCoord.bam \; -o SRR3212847.Aligned.SortedByCoord \; > SRR3212847.Aligned.SortedByCoord.out &; ```; Ev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-756727212
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:301,Availability,error,errors,301,"> Hey, I'm having the same kind of problem.; > ; > I aligned my PE reads against the transcriptome using BWA-mem and then sorted them by coordinates (as a regular procedure). I know Salmon assumes the alignments are not sorted, so I shuffled these bam files, and then run `salmon quant`. Here are the errors I got in a number of trials:; > ; > ### Fresh installation of Salmon; > ```; > conda create --name salmon -c bioconda salmon; > conda activate salmon; > ```; > ; > ### 1. Shuffling a bam file with `samtools collate`; > ```; > samtools collate \; > -@ 40 \; > -o SRR3212847.Aligned.Shuffled.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.Shuffled.bam \; > -o SRR3212847.Aligned.Shuffled ; > ```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; > # [ output ] => { SRR3212847.Aligned.Shuffled }; > Logs will be written to SRR3212847.Aligned.Shuffled/logs; > [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > ### 2. Shuffling a headless bam file with `samtools collate`; > (I think I saw something about the bam's header in another thread d",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:1836,Availability,fault,fault,1836,"e most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; > # [ output ] => { SRR3212847.Aligned.Shuffled }; > Logs will be written to SRR3212847.Aligned.Shuffled/logs; > [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > ### 2. Shuffling a headless bam file with `samtools collate`; > (I think I saw something about the bam's header in another thread dealing with this issue); > ; > ```; > samtools view \; > -b \; > -@ 40 \; > -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > samtools collate \; > -@ 40 \; > -o SRR3212847.Aligned.Shuffled.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord.NoHeader.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.Shuffled.NoHeader.bam \; > -o SRR3212847.Aligned.Shuffled.NoHeader; > ```; > ; > ```; > ; > ....; > ; > ; > [2021-01-08 12:42:10.700] [jointLog] [warning] ; > ; > WARNING: Detected suspicious pair --- ; > The names are different:; > read1 : SRR3212847.24133171; > read2 : SRR3212847.33911054; > The proper-pair statuses are inconsistent:; > read1 [SRR3212847.24133171] : no proper-pair; not mapped; matenot mapped; > ; > read2 : [SRR3212",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:3086,Availability,fault,fault,3086,Aligned.SortedByCoord.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > samtools collate \; > -@ 40 \; > -o SRR3212847.Aligned.Shuffled.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord.NoHeader.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.Shuffled.NoHeader.bam \; > -o SRR3212847.Aligned.Shuffled.NoHeader; > ```; > ; > ```; > ; > ....; > ; > ; > [2021-01-08 12:42:10.700] [jointLog] [warning] ; > ; > WARNING: Detected suspicious pair --- ; > The names are different:; > read1 : SRR3212847.24133171; > read2 : SRR3212847.33911054; > The proper-pair statuses are inconsistent:; > read1 [SRR3212847.24133171] : no proper-pair; not mapped; matenot mapped; > ; > read2 : [SRR3212847.24133171] : proper-pair; mapped; matemapped; > ; > [2021-01-08 12:42:10.700] [jointLog] [warning] ; > ; > WARNING: Detected suspicious pair --- ; > The names are different:; > read1 : SRR3212847.33911054; > read2 : SRR3212847.30781941; > ; > Segmentation fault (core dumped); > ```; > ; > ### 3. Sorting with `samtools sort -n`; > ```; > samtools sort \; > -@ 40 \; > -n \; > -o SRR3212847.Aligned.SortedByName.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByName.bam \; > -o SRR3212847.Aligned.SortedByName; > ```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; > # [ output ] => { SRR3212847.Aligned.SortedByName }; > Logs will be written to SRR3212847.Aligned.SortedByName/logs; > [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > ,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:4503,Availability,fault,fault,4503,"```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; > # [ output ] => { SRR3212847.Aligned.SortedByName }; > Logs will be written to SRR3212847.Aligned.SortedByName/logs; > [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.); > ; > I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; > ; > ```; > nohup salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByCoord.bam \; > -o SRR3212847.Aligned.SortedByCoord \; > > SRR3212847.Aligned.SortedByCoord.out &; > ```; > ; > Even so, `SRR3212847.Aligned.SortedByCoord.out` contained ~3.5GB worth of the warnings above.; > ; > Any help would be much appreciated. Thanks!. hello,i have the same problem,thanks for your answer. Your SRR3212847.Aligned.SortedByCoord.out contained ~3.5GB worth of the warnings above, What is the warning message? And in my log file,the warning as follow:. ![image](https://user-images.githubusercontent.com/45484925/206608510-b5cc88bd-18ac-42eb-bfa1-a5be862b0873.png); Can i ignore these warnings?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:4567,Availability,error,error,4567,"```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; > # [ output ] => { SRR3212847.Aligned.SortedByName }; > Logs will be written to SRR3212847.Aligned.SortedByName/logs; > [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.); > ; > I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; > ; > ```; > nohup salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByCoord.bam \; > -o SRR3212847.Aligned.SortedByCoord \; > > SRR3212847.Aligned.SortedByCoord.out &; > ```; > ; > Even so, `SRR3212847.Aligned.SortedByCoord.out` contained ~3.5GB worth of the warnings above.; > ; > Any help would be much appreciated. Thanks!. hello,i have the same problem,thanks for your answer. Your SRR3212847.Aligned.SortedByCoord.out contained ~3.5GB worth of the warnings above, What is the warning message? And in my log file,the warning as follow:. ![image](https://user-images.githubusercontent.com/45484925/206608510-b5cc88bd-18ac-42eb-bfa1-a5be862b0873.png); Can i ignore these warnings?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:4613,Availability,error,errors,4613,"```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; > # [ output ] => { SRR3212847.Aligned.SortedByName }; > Logs will be written to SRR3212847.Aligned.SortedByName/logs; > [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.); > ; > I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; > ; > ```; > nohup salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByCoord.bam \; > -o SRR3212847.Aligned.SortedByCoord \; > > SRR3212847.Aligned.SortedByCoord.out &; > ```; > ; > Even so, `SRR3212847.Aligned.SortedByCoord.out` contained ~3.5GB worth of the warnings above.; > ; > Any help would be much appreciated. Thanks!. hello,i have the same problem,thanks for your answer. Your SRR3212847.Aligned.SortedByCoord.out contained ~3.5GB worth of the warnings above, What is the warning message? And in my log file,the warning as follow:. ![image](https://user-images.githubusercontent.com/45484925/206608510-b5cc88bd-18ac-42eb-bfa1-a5be862b0873.png); Can i ignore these warnings?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:354,Deployability,install,installation,354,"> Hey, I'm having the same kind of problem.; > ; > I aligned my PE reads against the transcriptome using BWA-mem and then sorted them by coordinates (as a regular procedure). I know Salmon assumes the alignments are not sorted, so I shuffled these bam files, and then run `salmon quant`. Here are the errors I got in a number of trials:; > ; > ### Fresh installation of Salmon; > ```; > conda create --name salmon -c bioconda salmon; > conda activate salmon; > ```; > ; > ### 1. Shuffling a bam file with `samtools collate`; > ```; > samtools collate \; > -@ 40 \; > -o SRR3212847.Aligned.Shuffled.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.Shuffled.bam \; > -o SRR3212847.Aligned.Shuffled ; > ```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; > # [ output ] => { SRR3212847.Aligned.Shuffled }; > Logs will be written to SRR3212847.Aligned.Shuffled/logs; > [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > ### 2. Shuffling a headless bam file with `samtools collate`; > (I think I saw something about the bam's header in another thread d",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:5242,Integrability,message,message,5242,"```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; > # [ output ] => { SRR3212847.Aligned.SortedByName }; > Logs will be written to SRR3212847.Aligned.SortedByName/logs; > [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.); > ; > I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; > ; > ```; > nohup salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByCoord.bam \; > -o SRR3212847.Aligned.SortedByCoord \; > > SRR3212847.Aligned.SortedByCoord.out &; > ```; > ; > Even so, `SRR3212847.Aligned.SortedByCoord.out` contained ~3.5GB worth of the warnings above.; > ; > Any help would be much appreciated. Thanks!. hello,i have the same problem,thanks for your answer. Your SRR3212847.Aligned.SortedByCoord.out contained ~3.5GB worth of the warnings above, What is the warning message? And in my log file,the warning as follow:. ![image](https://user-images.githubusercontent.com/45484925/206608510-b5cc88bd-18ac-42eb-bfa1-a5be862b0873.png); Can i ignore these warnings?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:2562,Safety,Detect,Detected,2562," = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > ### 2. Shuffling a headless bam file with `samtools collate`; > (I think I saw something about the bam's header in another thread dealing with this issue); > ; > ```; > samtools view \; > -b \; > -@ 40 \; > -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > samtools collate \; > -@ 40 \; > -o SRR3212847.Aligned.Shuffled.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord.NoHeader.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.Shuffled.NoHeader.bam \; > -o SRR3212847.Aligned.Shuffled.NoHeader; > ```; > ; > ```; > ; > ....; > ; > ; > [2021-01-08 12:42:10.700] [jointLog] [warning] ; > ; > WARNING: Detected suspicious pair --- ; > The names are different:; > read1 : SRR3212847.24133171; > read2 : SRR3212847.33911054; > The proper-pair statuses are inconsistent:; > read1 [SRR3212847.24133171] : no proper-pair; not mapped; matenot mapped; > ; > read2 : [SRR3212847.24133171] : proper-pair; mapped; matemapped; > ; > [2021-01-08 12:42:10.700] [jointLog] [warning] ; > ; > WARNING: Detected suspicious pair --- ; > The names are different:; > read1 : SRR3212847.33911054; > read2 : SRR3212847.30781941; > ; > Segmentation fault (core dumped); > ```; > ; > ### 3. Sorting with `samtools sort -n`; > ```; > samtools sort \; > -@ 40 \; > -n \; > -o SRR3212847.Aligned.SortedByName.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByName.bam \; > -o SRR3212847.Aligned.SortedByName; > ```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:2946,Safety,Detect,Detected,2946, the bam's header in another thread dealing with this issue); > ; > ```; > samtools view \; > -b \; > -@ 40 \; > -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > samtools collate \; > -@ 40 \; > -o SRR3212847.Aligned.Shuffled.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord.NoHeader.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.Shuffled.NoHeader.bam \; > -o SRR3212847.Aligned.Shuffled.NoHeader; > ```; > ; > ```; > ; > ....; > ; > ; > [2021-01-08 12:42:10.700] [jointLog] [warning] ; > ; > WARNING: Detected suspicious pair --- ; > The names are different:; > read1 : SRR3212847.24133171; > read2 : SRR3212847.33911054; > The proper-pair statuses are inconsistent:; > read1 [SRR3212847.24133171] : no proper-pair; not mapped; matenot mapped; > ; > read2 : [SRR3212847.24133171] : proper-pair; mapped; matemapped; > ; > [2021-01-08 12:42:10.700] [jointLog] [warning] ; > ; > WARNING: Detected suspicious pair --- ; > The names are different:; > read1 : SRR3212847.33911054; > read2 : SRR3212847.30781941; > ; > Segmentation fault (core dumped); > ```; > ; > ### 3. Sorting with `samtools sort -n`; > ```; > samtools sort \; > -@ 40 \; > -n \; > -o SRR3212847.Aligned.SortedByName.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByName.bam \; > -o SRR3212847.Aligned.SortedByName; > ```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; > # [ output ] => { SRR3212847.Aligned.SortedByName }; > Logs will be written to SRR3212847.Aligned.SortedByName/logs; > [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:1152,Testability,Log,Logs,1152," procedure). I know Salmon assumes the alignments are not sorted, so I shuffled these bam files, and then run `salmon quant`. Here are the errors I got in a number of trials:; > ; > ### Fresh installation of Salmon; > ```; > conda create --name salmon -c bioconda salmon; > conda activate salmon; > ```; > ; > ### 1. Shuffling a bam file with `samtools collate`; > ```; > samtools collate \; > -@ 40 \; > -o SRR3212847.Aligned.Shuffled.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.Shuffled.bam \; > -o SRR3212847.Aligned.Shuffled ; > ```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; > # [ output ] => { SRR3212847.Aligned.Shuffled }; > Logs will be written to SRR3212847.Aligned.Shuffled/logs; > [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > ### 2. Shuffling a headless bam file with `samtools collate`; > (I think I saw something about the bam's header in another thread dealing with this issue); > ; > ```; > samtools view \; > -b \; > -@ 40 \; > -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:1204,Testability,log,logs,1204,"e not sorted, so I shuffled these bam files, and then run `salmon quant`. Here are the errors I got in a number of trials:; > ; > ### Fresh installation of Salmon; > ```; > conda create --name salmon -c bioconda salmon; > conda activate salmon; > ```; > ; > ### 1. Shuffling a bam file with `samtools collate`; > ```; > samtools collate \; > -@ 40 \; > -o SRR3212847.Aligned.Shuffled.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.Shuffled.bam \; > -o SRR3212847.Aligned.Shuffled ; > ```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.Shuffled.bam }; > # [ output ] => { SRR3212847.Aligned.Shuffled }; > Logs will be written to SRR3212847.Aligned.Shuffled/logs; > [2021-01-08 12:43:44.680] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 12:43:44.680] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 12:43:44.711] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.Shuffled.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > ### 2. Shuffling a headless bam file with `samtools collate`; > (I think I saw something about the bam's header in another thread dealing with this issue); > ; > ```; > samtools view \; > -b \; > -@ 40 \; > -o SRR3212847.Aligned.SortedByCoord.NoHeader.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > samtools collate \; > -@ 40 \; > -o SRR",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:3811,Testability,Log,Logs,3811,"[SRR3212847.24133171] : proper-pair; mapped; matemapped; > ; > [2021-01-08 12:42:10.700] [jointLog] [warning] ; > ; > WARNING: Detected suspicious pair --- ; > The names are different:; > read1 : SRR3212847.33911054; > read2 : SRR3212847.30781941; > ; > Segmentation fault (core dumped); > ```; > ; > ### 3. Sorting with `samtools sort -n`; > ```; > samtools sort \; > -@ 40 \; > -n \; > -o SRR3212847.Aligned.SortedByName.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByName.bam \; > -o SRR3212847.Aligned.SortedByName; > ```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; > # [ output ] => { SRR3212847.Aligned.SortedByName }; > Logs will be written to SRR3212847.Aligned.SortedByName/logs; > [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.); > ; > I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; > ; > ```; > nohup salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.Sorte",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:3867,Testability,log,logs,3867," > ; > [2021-01-08 12:42:10.700] [jointLog] [warning] ; > ; > WARNING: Detected suspicious pair --- ; > The names are different:; > read1 : SRR3212847.33911054; > read2 : SRR3212847.30781941; > ; > Segmentation fault (core dumped); > ```; > ; > ### 3. Sorting with `samtools sort -n`; > ```; > samtools sort \; > -@ 40 \; > -n \; > -o SRR3212847.Aligned.SortedByName.bam \; > SRR3212847.Aligned.SortedByCoord.bam; > ; > salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByName.bam \; > -o SRR3212847.Aligned.SortedByName; > ```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; > # [ output ] => { SRR3212847.Aligned.SortedByName }; > Logs will be written to SRR3212847.Aligned.SortedByName/logs; > [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.); > ; > I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; > ; > ```; > nohup salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByCoord.bam \; > -o SRR3212847.Aligned.SortedByCoord \",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456:5261,Testability,log,log,5261,"```; > ; > ```; > Version Info: This is the most recent version of salmon.; > # salmon (alignment-based) v1.4.0; > # [ program ] => salmon ; > # [ command ] => quant ; > # [ targets ] => { mRNA.fasta }; > # [ threads ] => { 20 }; > # [ libType ] => { A }; > # [ alignments ] => { SRR3212847.Aligned.SortedByName.bam }; > # [ output ] => { SRR3212847.Aligned.SortedByName }; > Logs will be written to SRR3212847.Aligned.SortedByName/logs; > [2021-01-08 13:02:04.845] [jointLog] [info] setting maxHashResizeThreads to 20; > [2021-01-08 13:02:04.845] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; > Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; > [2021-01-08 13:02:04.878] [jointLog] [info] numQuantThreads = 14; > parseThreads = 6; > Checking that provided alignment files have consistent headers . . . done; > Populating targets from aln = ""SRR3212847.Aligned.SortedByName.bam"", fasta = ""mRNA.fasta"" . . .done; > ; > processed 0 reads in current roundSegmentation fault (core dumped); > ```; > ; > (Which is the same as the 1st error. Actually, each time I re-run those two errors switched.); > ; > I tried running Salmon on the sorted-by-coordinates bam, and it didn't fail:; > ; > ```; > nohup salmon quant \; > -t mRNA.fasta \; > -p 20 \; > -l A \; > -a SRR3212847.Aligned.SortedByCoord.bam \; > -o SRR3212847.Aligned.SortedByCoord \; > > SRR3212847.Aligned.SortedByCoord.out &; > ```; > ; > Even so, `SRR3212847.Aligned.SortedByCoord.out` contained ~3.5GB worth of the warnings above.; > ; > Any help would be much appreciated. Thanks!. hello,i have the same problem,thanks for your answer. Your SRR3212847.Aligned.SortedByCoord.out contained ~3.5GB worth of the warnings above, What is the warning message? And in my log file,the warning as follow:. ![image](https://user-images.githubusercontent.com/45484925/206608510-b5cc88bd-18ac-42eb-bfa1-a5be862b0873.png); Can i ignore these warnings?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1343749456
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1979988398:41,Availability,error,error,41,Any update on this? I am having the same error,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1979988398
https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1979988398:4,Deployability,update,update,4,Any update on this? I am having the same error,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/582#issuecomment-1979988398
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:124,Deployability,update,updated,124,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:388,Deployability,update,update,388,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:732,Deployability,update,update,732,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:104,Integrability,message,message,104,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:400,Integrability,message,message,400,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:471,Integrability,message,messages,471,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:149,Security,validat,validateMappings,149,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:499,Testability,log,logger,499,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:704,Usability,clear,clear,704,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232:171,Availability,reliab,reliable,171,"Hi @keithgmitchell,. Alevin is designed for droplet-based, tagged-end protocols, and in the vast majority of these protocols, transcript-level quantification isn't really reliable enough to be useful. Since most tagged-end protocols sequence information from only the 3' end of the transcripts, there is a highly-biased coverage signal, and discerning UMI assignment at the transcript level is usually not possible. Therefore, I wouldn't generally recommend trying to obtain transcript-level counts from alevin and we haven't tested it in this context. If you have a particular reason you want to look at transcript counts and believe it may be reasonable in your specific use-case, you can alway pass in a gene-to-transcript map that just maps each transcript to itself, which will result in a transcript-level output matrix. However, I anticipate that the resolution problem will become more difficult in this case, and there will be much more uncertainty in the assignments. @k3yavi, please feel free to add anything you think I may have missed. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232
https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232:70,Integrability,protocol,protocols,70,"Hi @keithgmitchell,. Alevin is designed for droplet-based, tagged-end protocols, and in the vast majority of these protocols, transcript-level quantification isn't really reliable enough to be useful. Since most tagged-end protocols sequence information from only the 3' end of the transcripts, there is a highly-biased coverage signal, and discerning UMI assignment at the transcript level is usually not possible. Therefore, I wouldn't generally recommend trying to obtain transcript-level counts from alevin and we haven't tested it in this context. If you have a particular reason you want to look at transcript counts and believe it may be reasonable in your specific use-case, you can alway pass in a gene-to-transcript map that just maps each transcript to itself, which will result in a transcript-level output matrix. However, I anticipate that the resolution problem will become more difficult in this case, and there will be much more uncertainty in the assignments. @k3yavi, please feel free to add anything you think I may have missed. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232
https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232:115,Integrability,protocol,protocols,115,"Hi @keithgmitchell,. Alevin is designed for droplet-based, tagged-end protocols, and in the vast majority of these protocols, transcript-level quantification isn't really reliable enough to be useful. Since most tagged-end protocols sequence information from only the 3' end of the transcripts, there is a highly-biased coverage signal, and discerning UMI assignment at the transcript level is usually not possible. Therefore, I wouldn't generally recommend trying to obtain transcript-level counts from alevin and we haven't tested it in this context. If you have a particular reason you want to look at transcript counts and believe it may be reasonable in your specific use-case, you can alway pass in a gene-to-transcript map that just maps each transcript to itself, which will result in a transcript-level output matrix. However, I anticipate that the resolution problem will become more difficult in this case, and there will be much more uncertainty in the assignments. @k3yavi, please feel free to add anything you think I may have missed. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232
https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232:223,Integrability,protocol,protocols,223,"Hi @keithgmitchell,. Alevin is designed for droplet-based, tagged-end protocols, and in the vast majority of these protocols, transcript-level quantification isn't really reliable enough to be useful. Since most tagged-end protocols sequence information from only the 3' end of the transcripts, there is a highly-biased coverage signal, and discerning UMI assignment at the transcript level is usually not possible. Therefore, I wouldn't generally recommend trying to obtain transcript-level counts from alevin and we haven't tested it in this context. If you have a particular reason you want to look at transcript counts and believe it may be reasonable in your specific use-case, you can alway pass in a gene-to-transcript map that just maps each transcript to itself, which will result in a transcript-level output matrix. However, I anticipate that the resolution problem will become more difficult in this case, and there will be much more uncertainty in the assignments. @k3yavi, please feel free to add anything you think I may have missed. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232
https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232:526,Testability,test,tested,526,"Hi @keithgmitchell,. Alevin is designed for droplet-based, tagged-end protocols, and in the vast majority of these protocols, transcript-level quantification isn't really reliable enough to be useful. Since most tagged-end protocols sequence information from only the 3' end of the transcripts, there is a highly-biased coverage signal, and discerning UMI assignment at the transcript level is usually not possible. Therefore, I wouldn't generally recommend trying to obtain transcript-level counts from alevin and we haven't tested it in this context. If you have a particular reason you want to look at transcript counts and believe it may be reasonable in your specific use-case, you can alway pass in a gene-to-transcript map that just maps each transcript to itself, which will result in a transcript-level output matrix. However, I anticipate that the resolution problem will become more difficult in this case, and there will be much more uncertainty in the assignments. @k3yavi, please feel free to add anything you think I may have missed. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/588#issuecomment-729974232
https://github.com/COMBINE-lab/salmon/issues/589#issuecomment-733215735:693,Energy Efficiency,schedul,schedule,693,"Hi @AndrewSkelton,. There is currently no easy way to keep the index in RAM as STAR/Bowtie2 do. This is a feature we've been interested in for a _long_ time, but it's a feature that is very hard to justify spending a PhD student's time on since it's not going to contribute directly to any paper. But, this is a feature we'd like to add and maybe we can swing it with some of the CZI round-3 funding we just got. Nonetheless, the capability currently doesn't exist. Salmon can take multiple fastq files as input, but then it assumes they all derive from the same library, so you get one ""aggregate"" quant.sf, which isn't what you want here. So, I think the only approach currently would be to schedule a number of small jobs. I get why this isn't ideal. One small saving grace is that recent versions of salmon (>= 1.0.0) adopt the pufferfish index which is _much_ smaller than the previous RapMap index. Thus, the index loading time is quite small for a typical transcriptome. Also, this often allows operating system cache to keep the index around, even if it's not explicitly stored in shared memory. Thanks for both of the suggestions, and I'll be sure to keep you in the loop if we acquire either of the capabilities you mention above!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/589#issuecomment-733215735
https://github.com/COMBINE-lab/salmon/issues/589#issuecomment-733215735:921,Performance,load,loading,921,"Hi @AndrewSkelton,. There is currently no easy way to keep the index in RAM as STAR/Bowtie2 do. This is a feature we've been interested in for a _long_ time, but it's a feature that is very hard to justify spending a PhD student's time on since it's not going to contribute directly to any paper. But, this is a feature we'd like to add and maybe we can swing it with some of the CZI round-3 funding we just got. Nonetheless, the capability currently doesn't exist. Salmon can take multiple fastq files as input, but then it assumes they all derive from the same library, so you get one ""aggregate"" quant.sf, which isn't what you want here. So, I think the only approach currently would be to schedule a number of small jobs. I get why this isn't ideal. One small saving grace is that recent versions of salmon (>= 1.0.0) adopt the pufferfish index which is _much_ smaller than the previous RapMap index. Thus, the index loading time is quite small for a typical transcriptome. Also, this often allows operating system cache to keep the index around, even if it's not explicitly stored in shared memory. Thanks for both of the suggestions, and I'll be sure to keep you in the loop if we acquire either of the capabilities you mention above!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/589#issuecomment-733215735
https://github.com/COMBINE-lab/salmon/issues/589#issuecomment-733215735:1019,Performance,cache,cache,1019,"Hi @AndrewSkelton,. There is currently no easy way to keep the index in RAM as STAR/Bowtie2 do. This is a feature we've been interested in for a _long_ time, but it's a feature that is very hard to justify spending a PhD student's time on since it's not going to contribute directly to any paper. But, this is a feature we'd like to add and maybe we can swing it with some of the CZI round-3 funding we just got. Nonetheless, the capability currently doesn't exist. Salmon can take multiple fastq files as input, but then it assumes they all derive from the same library, so you get one ""aggregate"" quant.sf, which isn't what you want here. So, I think the only approach currently would be to schedule a number of small jobs. I get why this isn't ideal. One small saving grace is that recent versions of salmon (>= 1.0.0) adopt the pufferfish index which is _much_ smaller than the previous RapMap index. Thus, the index loading time is quite small for a typical transcriptome. Also, this often allows operating system cache to keep the index around, even if it's not explicitly stored in shared memory. Thanks for both of the suggestions, and I'll be sure to keep you in the loop if we acquire either of the capabilities you mention above!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/589#issuecomment-733215735
https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733278723:267,Deployability,pipeline,pipeline,267,"Hi @rob-p, thanks for you response! I think this answers my question, but let me ask a couple follow up questions just to be sure:. In `STAR` there is an option to use stranded alignment (`--readStrand`, which can take ""Unstranded"", ""Forward"", or ""Reverse""). For the pipeline I'm building it would be ideal if I didn't have to specify the strandedness of the library as I'm not the one preparing the samples and it's not always easy to get that information from the scientist in the lab. As such, it would be great if I can use the default strandedness argument to `STAR` (""Unstranded"") and let `salmon` ""do the right thing"" by letting it choose the `libType` for me. With that in mind, if I let `salmon` choose for me (`-l A`) am I risking throwing out any data?. In addition, if a transcript was aligned in a unstranded manner and ended up aligning to the wrong location due to ambiguity between the positive orientation of one transcript and the negative orientation of another, can `salmon` correct this by reassigning it to the right transcript based on the joint probability of all the other alignments (if you can't tell I'm at the edge of my BS zone)?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733278723
https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733278723:733,Safety,risk,risking,733,"Hi @rob-p, thanks for you response! I think this answers my question, but let me ask a couple follow up questions just to be sure:. In `STAR` there is an option to use stranded alignment (`--readStrand`, which can take ""Unstranded"", ""Forward"", or ""Reverse""). For the pipeline I'm building it would be ideal if I didn't have to specify the strandedness of the library as I'm not the one preparing the samples and it's not always easy to get that information from the scientist in the lab. As such, it would be great if I can use the default strandedness argument to `STAR` (""Unstranded"") and let `salmon` ""do the right thing"" by letting it choose the `libType` for me. With that in mind, if I let `salmon` choose for me (`-l A`) am I risking throwing out any data?. In addition, if a transcript was aligned in a unstranded manner and ended up aligning to the wrong location due to ambiguity between the positive orientation of one transcript and the negative orientation of another, can `salmon` correct this by reassigning it to the right transcript based on the joint probability of all the other alignments (if you can't tell I'm at the edge of my BS zone)?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733278723
https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813:2815,Availability,robust,robust,2815,"atible with the appropriate strand type (which may be unstranded if that is the protocol). Salmon is pretty conservative about reporting when there is any ambiguity. By default, if the strand bias is stronger than a few percent. In a stranded protocol, it will report and if it infers more than a few percent of fragments no having a valid alignment. So you can always double-check samples where the strandedness is at all ambiguous. > In addition, if a transcript was aligned in a unstranded manner and ended up aligning to the wrong location due to ambiguity between the positive orientation of one transcript and the negative orientation of another, can salmon correct this by reassigning it to the right transcript based on the joint probability of all the other alignments (if you can't tell I'm at the edge of my BS zone)?. If there is not an alignment to the correct location _in addition to_ the wrong location, then no. If you run salmon in alignment mode, it will assign each fragment probabilistically to the set of transcripts to which it aligns. There is, by definition, a probability of 0 for a fragment being assigned to a location where it doesn't align. That is, the reported alignment positions should contain the true alignment. STAR is pretty good at reporting all equally good alignments, but you could see some corner cases (e.g. if there is an alignment to a pseudogene location that looks _very_ similar to a gene, etc.). However, these are the issues that arise due to the inherent difficulty of spliced alignment. Salmon's built-in [selective alignment](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8) is quite sensitive, but if you're using the STAR alignments for other tasks apart from transcript quantification, it may not be worth it to align the reads twice. Overall, STAR unstranded (using the `--quantMode TranscriptomeSAM`) -> salmon with `-l A` and then checking samples where there are any warnings should be a pretty robust pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813
https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813:133,Deployability,pipeline,pipeline,133,"> In STAR there is an option to use stranded alignment (--readStrand, which can take ""Unstranded"", ""Forward"", or ""Reverse""). For the pipeline I'm building it would be ideal if I didn't have to specify the strandedness of the library as I'm not the one preparing the samples and it's not always easy to get that information from the scientist in the lab. As such, it would be great if I can use the default strandedness argument to STAR (""Unstranded"") and let salmon ""do the right thing"" by letting it choose the libType for me. With that in mind, if I let salmon choose for me (-l A) am I risking throwing out any data?. Right, so in this case, STAR should produce all highest-scoring valid alignments regardless of orientation. Then, when running salmon with `-l A` it will detect the strandedness and only discard alignments compatible with the appropriate strand type (which may be unstranded if that is the protocol). Salmon is pretty conservative about reporting when there is any ambiguity. By default, if the strand bias is stronger than a few percent. In a stranded protocol, it will report and if it infers more than a few percent of fragments no having a valid alignment. So you can always double-check samples where the strandedness is at all ambiguous. > In addition, if a transcript was aligned in a unstranded manner and ended up aligning to the wrong location due to ambiguity between the positive orientation of one transcript and the negative orientation of another, can salmon correct this by reassigning it to the right transcript based on the joint probability of all the other alignments (if you can't tell I'm at the edge of my BS zone)?. If there is not an alignment to the correct location _in addition to_ the wrong location, then no. If you run salmon in alignment mode, it will assign each fragment probabilistically to the set of transcripts to which it aligns. There is, by definition, a probability of 0 for a fragment being assigned to a location where it doesn't align.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813
https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813:2822,Deployability,pipeline,pipeline,2822,"atible with the appropriate strand type (which may be unstranded if that is the protocol). Salmon is pretty conservative about reporting when there is any ambiguity. By default, if the strand bias is stronger than a few percent. In a stranded protocol, it will report and if it infers more than a few percent of fragments no having a valid alignment. So you can always double-check samples where the strandedness is at all ambiguous. > In addition, if a transcript was aligned in a unstranded manner and ended up aligning to the wrong location due to ambiguity between the positive orientation of one transcript and the negative orientation of another, can salmon correct this by reassigning it to the right transcript based on the joint probability of all the other alignments (if you can't tell I'm at the edge of my BS zone)?. If there is not an alignment to the correct location _in addition to_ the wrong location, then no. If you run salmon in alignment mode, it will assign each fragment probabilistically to the set of transcripts to which it aligns. There is, by definition, a probability of 0 for a fragment being assigned to a location where it doesn't align. That is, the reported alignment positions should contain the true alignment. STAR is pretty good at reporting all equally good alignments, but you could see some corner cases (e.g. if there is an alignment to a pseudogene location that looks _very_ similar to a gene, etc.). However, these are the issues that arise due to the inherent difficulty of spliced alignment. Salmon's built-in [selective alignment](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8) is quite sensitive, but if you're using the STAR alignments for other tasks apart from transcript quantification, it may not be worth it to align the reads twice. Overall, STAR unstranded (using the `--quantMode TranscriptomeSAM`) -> salmon with `-l A` and then checking samples where there are any warnings should be a pretty robust pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813
https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813:911,Integrability,protocol,protocol,911,"> In STAR there is an option to use stranded alignment (--readStrand, which can take ""Unstranded"", ""Forward"", or ""Reverse""). For the pipeline I'm building it would be ideal if I didn't have to specify the strandedness of the library as I'm not the one preparing the samples and it's not always easy to get that information from the scientist in the lab. As such, it would be great if I can use the default strandedness argument to STAR (""Unstranded"") and let salmon ""do the right thing"" by letting it choose the libType for me. With that in mind, if I let salmon choose for me (-l A) am I risking throwing out any data?. Right, so in this case, STAR should produce all highest-scoring valid alignments regardless of orientation. Then, when running salmon with `-l A` it will detect the strandedness and only discard alignments compatible with the appropriate strand type (which may be unstranded if that is the protocol). Salmon is pretty conservative about reporting when there is any ambiguity. By default, if the strand bias is stronger than a few percent. In a stranded protocol, it will report and if it infers more than a few percent of fragments no having a valid alignment. So you can always double-check samples where the strandedness is at all ambiguous. > In addition, if a transcript was aligned in a unstranded manner and ended up aligning to the wrong location due to ambiguity between the positive orientation of one transcript and the negative orientation of another, can salmon correct this by reassigning it to the right transcript based on the joint probability of all the other alignments (if you can't tell I'm at the edge of my BS zone)?. If there is not an alignment to the correct location _in addition to_ the wrong location, then no. If you run salmon in alignment mode, it will assign each fragment probabilistically to the set of transcripts to which it aligns. There is, by definition, a probability of 0 for a fragment being assigned to a location where it doesn't align.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813
https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813:1074,Integrability,protocol,protocol,1074,"e""). For the pipeline I'm building it would be ideal if I didn't have to specify the strandedness of the library as I'm not the one preparing the samples and it's not always easy to get that information from the scientist in the lab. As such, it would be great if I can use the default strandedness argument to STAR (""Unstranded"") and let salmon ""do the right thing"" by letting it choose the libType for me. With that in mind, if I let salmon choose for me (-l A) am I risking throwing out any data?. Right, so in this case, STAR should produce all highest-scoring valid alignments regardless of orientation. Then, when running salmon with `-l A` it will detect the strandedness and only discard alignments compatible with the appropriate strand type (which may be unstranded if that is the protocol). Salmon is pretty conservative about reporting when there is any ambiguity. By default, if the strand bias is stronger than a few percent. In a stranded protocol, it will report and if it infers more than a few percent of fragments no having a valid alignment. So you can always double-check samples where the strandedness is at all ambiguous. > In addition, if a transcript was aligned in a unstranded manner and ended up aligning to the wrong location due to ambiguity between the positive orientation of one transcript and the negative orientation of another, can salmon correct this by reassigning it to the right transcript based on the joint probability of all the other alignments (if you can't tell I'm at the edge of my BS zone)?. If there is not an alignment to the correct location _in addition to_ the wrong location, then no. If you run salmon in alignment mode, it will assign each fragment probabilistically to the set of transcripts to which it aligns. There is, by definition, a probability of 0 for a fragment being assigned to a location where it doesn't align. That is, the reported alignment positions should contain the true alignment. STAR is pretty good at reporting all equa",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813
https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813:589,Safety,risk,risking,589,"> In STAR there is an option to use stranded alignment (--readStrand, which can take ""Unstranded"", ""Forward"", or ""Reverse""). For the pipeline I'm building it would be ideal if I didn't have to specify the strandedness of the library as I'm not the one preparing the samples and it's not always easy to get that information from the scientist in the lab. As such, it would be great if I can use the default strandedness argument to STAR (""Unstranded"") and let salmon ""do the right thing"" by letting it choose the libType for me. With that in mind, if I let salmon choose for me (-l A) am I risking throwing out any data?. Right, so in this case, STAR should produce all highest-scoring valid alignments regardless of orientation. Then, when running salmon with `-l A` it will detect the strandedness and only discard alignments compatible with the appropriate strand type (which may be unstranded if that is the protocol). Salmon is pretty conservative about reporting when there is any ambiguity. By default, if the strand bias is stronger than a few percent. In a stranded protocol, it will report and if it infers more than a few percent of fragments no having a valid alignment. So you can always double-check samples where the strandedness is at all ambiguous. > In addition, if a transcript was aligned in a unstranded manner and ended up aligning to the wrong location due to ambiguity between the positive orientation of one transcript and the negative orientation of another, can salmon correct this by reassigning it to the right transcript based on the joint probability of all the other alignments (if you can't tell I'm at the edge of my BS zone)?. If there is not an alignment to the correct location _in addition to_ the wrong location, then no. If you run salmon in alignment mode, it will assign each fragment probabilistically to the set of transcripts to which it aligns. There is, by definition, a probability of 0 for a fragment being assigned to a location where it doesn't align.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813
https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813:775,Safety,detect,detect,775,"> In STAR there is an option to use stranded alignment (--readStrand, which can take ""Unstranded"", ""Forward"", or ""Reverse""). For the pipeline I'm building it would be ideal if I didn't have to specify the strandedness of the library as I'm not the one preparing the samples and it's not always easy to get that information from the scientist in the lab. As such, it would be great if I can use the default strandedness argument to STAR (""Unstranded"") and let salmon ""do the right thing"" by letting it choose the libType for me. With that in mind, if I let salmon choose for me (-l A) am I risking throwing out any data?. Right, so in this case, STAR should produce all highest-scoring valid alignments regardless of orientation. Then, when running salmon with `-l A` it will detect the strandedness and only discard alignments compatible with the appropriate strand type (which may be unstranded if that is the protocol). Salmon is pretty conservative about reporting when there is any ambiguity. By default, if the strand bias is stronger than a few percent. In a stranded protocol, it will report and if it infers more than a few percent of fragments no having a valid alignment. So you can always double-check samples where the strandedness is at all ambiguous. > In addition, if a transcript was aligned in a unstranded manner and ended up aligning to the wrong location due to ambiguity between the positive orientation of one transcript and the negative orientation of another, can salmon correct this by reassigning it to the right transcript based on the joint probability of all the other alignments (if you can't tell I'm at the edge of my BS zone)?. If there is not an alignment to the correct location _in addition to_ the wrong location, then no. If you run salmon in alignment mode, it will assign each fragment probabilistically to the set of transcripts to which it aligns. There is, by definition, a probability of 0 for a fragment being assigned to a location where it doesn't align.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/590#issuecomment-733417813
https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733736492:353,Deployability,update,update,353,"Hi @guidohooiveld,. Yes, the original cutoff was set to accomodate TITIN, which, at the time, was the longest human transcript. Note, this warning doesn't prevent the transcript from being indexed, it just notifies the user its exceptionally long. I think updating the warning threshold makes sense so that gencode indexes cleanly in this regard. We'll update this. Thanks for the report!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733736492
https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733736492:4,Usability,guid,guidohooiveld,4,"Hi @guidohooiveld,. Yes, the original cutoff was set to accomodate TITIN, which, at the time, was the longest human transcript. Note, this warning doesn't prevent the transcript from being indexed, it just notifies the user its exceptionally long. I think updating the warning threshold makes sense so that gencode indexes cleanly in this regard. We'll update this. Thanks for the report!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733736492
https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733761227:156,Deployability,release,release,156,"This is fixed in https://github.com/COMBINE-lab/pufferfish/commit/e7fb924850e2a04793cdd2ace628afa8cf37885c, and should show up in the next (shortly coming) release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733761227
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:406,Availability,error,error,406,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:426,Availability,fault,fault,426,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:593,Availability,error,error,593,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:675,Availability,error,error,675,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:712,Availability,error,error,712,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:814,Availability,Error,Error,814,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:894,Availability,Error,Error,894,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:933,Availability,Error,Error,933,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:5,Deployability,install,install,5,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:164,Deployability,install,install,164,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:578,Integrability,wrap,wrapper,578,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424:686,Integrability,wrap,wrapper,686,"If I install Salmon Binary:; [100%] Building CXX object src/CMakeFiles/salmon.dir/BAMUtils.cpp.o; [100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf4e): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; Please submit a full bug report,; with preprocessed source if appropriate.; See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:421: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:659: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:163: all] Error 2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734935424
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734936468:28,Availability,error,error,28,"There is a (known) compiler error. To compile from source, you should pass `-DNO_IPO=TRUE` to the `cmake` step. That should fix your compile.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-734936468
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-736855646:108,Availability,down,downgrade,108,@lizhaozhi Do you have R 4.0.3 in your conda enviornment? 0.14 is the version of Salmon that conda tries to downgrade me to when I try to update from R 4.0.2 to R 4.0.3 see: https://github.com/COMBINE-lab/salmon/issues/594,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-736855646
https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-736855646:138,Deployability,update,update,138,@lizhaozhi Do you have R 4.0.3 in your conda enviornment? 0.14 is the version of Salmon that conda tries to downgrade me to when I try to update from R 4.0.2 to R 4.0.3 see: https://github.com/COMBINE-lab/salmon/issues/594,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/592#issuecomment-736855646
https://github.com/COMBINE-lab/salmon/issues/594#issuecomment-736155340:140,Integrability,depend,dependency,140,Thanks for the detailed report! I wonder if we can bump the max version on salmon 1.4.0. I seem to remember icu 67 conflicting with another dependency before. We'll have to look into that.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/594#issuecomment-736155340
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738166102:93,Deployability,update,update,93,"Thanks @ACastanza , I think it's a good idea. I have marked it as a feature request and we'd update you here once we have some progress into the next release. A bit tangential though, I find [refgenie](http://refgenomes.databio.org/) very useful as it has pre-built salmon indices with all the other relevant metadata (such as gtf to generate tgMap file) needed by salmon/alevin for quantifiation, but I agree saving the tgMap while indexing through GTF would be great for consistency.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738166102
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738166102:150,Deployability,release,release,150,"Thanks @ACastanza , I think it's a good idea. I have marked it as a feature request and we'd update you here once we have some progress into the next release. A bit tangential though, I find [refgenie](http://refgenomes.databio.org/) very useful as it has pre-built salmon indices with all the other relevant metadata (such as gtf to generate tgMap file) needed by salmon/alevin for quantifiation, but I agree saving the tgMap while indexing through GTF would be great for consistency.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738166102
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738180982:352,Deployability,pipeline,pipeline,352,"Yes, I'm aware of refgenie however, I was unable to identify for the hg38 salmon indices which specific transcriptome source (and additionally which version of said source) was used to build them.; Additionally, my use case here isn't entirely personal, I work for GSEA-MSigDB and GenePattern, we're in the process of improving the end-to-end analysis pipeline we offer to users, and one of the things we've been working on were wrapping the Salmon indexer, Salmon quant, and Alevin into GenePattern modules so that we can offer them to users who may want to run them on arbitrary transcriptomes in addition to the ones we offer specifically for GSEA compatibility. This issue was something we encountered when considering potential sources of inconsistency at different points in the pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738180982
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738180982:785,Deployability,pipeline,pipeline,785,"Yes, I'm aware of refgenie however, I was unable to identify for the hg38 salmon indices which specific transcriptome source (and additionally which version of said source) was used to build them.; Additionally, my use case here isn't entirely personal, I work for GSEA-MSigDB and GenePattern, we're in the process of improving the end-to-end analysis pipeline we offer to users, and one of the things we've been working on were wrapping the Salmon indexer, Salmon quant, and Alevin into GenePattern modules so that we can offer them to users who may want to run them on arbitrary transcriptomes in addition to the ones we offer specifically for GSEA compatibility. This issue was something we encountered when considering potential sources of inconsistency at different points in the pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738180982
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738180982:429,Integrability,wrap,wrapping,429,"Yes, I'm aware of refgenie however, I was unable to identify for the hg38 salmon indices which specific transcriptome source (and additionally which version of said source) was used to build them.; Additionally, my use case here isn't entirely personal, I work for GSEA-MSigDB and GenePattern, we're in the process of improving the end-to-end analysis pipeline we offer to users, and one of the things we've been working on were wrapping the Salmon indexer, Salmon quant, and Alevin into GenePattern modules so that we can offer them to users who may want to run them on arbitrary transcriptomes in addition to the ones we offer specifically for GSEA compatibility. This issue was something we encountered when considering potential sources of inconsistency at different points in the pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738180982
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842:358,Availability,avail,available,358,"Got it, thanks for the heads up. I'd probably reach out to the refgenie people about the hg38 specific versions. It makes sense to have the feature of having the gtf at the time of indexing. The only concern I have is that mandating to have the gtf might restrict the overall workflow by a bit. Specifically because a user might not always have the full GTF available for every use case, although we can always make having GTF as an optional requirement for indexing. Adding the support should not be too difficult but it will certainly add a new logic path which would need thorough testing. . We'll certainly keep you updated with the feature as we progress although it can take some time to get back. In terms of your pipeline one option would be to actually save the GTF explicitly in the salmon index folder post indexing. Although it's definitely not a very computer science friendly solution but it will help maintain the consistency while we work on the feature.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842:620,Deployability,update,updated,620,"Got it, thanks for the heads up. I'd probably reach out to the refgenie people about the hg38 specific versions. It makes sense to have the feature of having the gtf at the time of indexing. The only concern I have is that mandating to have the gtf might restrict the overall workflow by a bit. Specifically because a user might not always have the full GTF available for every use case, although we can always make having GTF as an optional requirement for indexing. Adding the support should not be too difficult but it will certainly add a new logic path which would need thorough testing. . We'll certainly keep you updated with the feature as we progress although it can take some time to get back. In terms of your pipeline one option would be to actually save the GTF explicitly in the salmon index folder post indexing. Although it's definitely not a very computer science friendly solution but it will help maintain the consistency while we work on the feature.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842:721,Deployability,pipeline,pipeline,721,"Got it, thanks for the heads up. I'd probably reach out to the refgenie people about the hg38 specific versions. It makes sense to have the feature of having the gtf at the time of indexing. The only concern I have is that mandating to have the gtf might restrict the overall workflow by a bit. Specifically because a user might not always have the full GTF available for every use case, although we can always make having GTF as an optional requirement for indexing. Adding the support should not be too difficult but it will certainly add a new logic path which would need thorough testing. . We'll certainly keep you updated with the feature as we progress although it can take some time to get back. In terms of your pipeline one option would be to actually save the GTF explicitly in the salmon index folder post indexing. Although it's definitely not a very computer science friendly solution but it will help maintain the consistency while we work on the feature.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842:547,Testability,log,logic,547,"Got it, thanks for the heads up. I'd probably reach out to the refgenie people about the hg38 specific versions. It makes sense to have the feature of having the gtf at the time of indexing. The only concern I have is that mandating to have the gtf might restrict the overall workflow by a bit. Specifically because a user might not always have the full GTF available for every use case, although we can always make having GTF as an optional requirement for indexing. Adding the support should not be too difficult but it will certainly add a new logic path which would need thorough testing. . We'll certainly keep you updated with the feature as we progress although it can take some time to get back. In terms of your pipeline one option would be to actually save the GTF explicitly in the salmon index folder post indexing. Although it's definitely not a very computer science friendly solution but it will help maintain the consistency while we work on the feature.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842
https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842:584,Testability,test,testing,584,"Got it, thanks for the heads up. I'd probably reach out to the refgenie people about the hg38 specific versions. It makes sense to have the feature of having the gtf at the time of indexing. The only concern I have is that mandating to have the gtf might restrict the overall workflow by a bit. Specifically because a user might not always have the full GTF available for every use case, although we can always make having GTF as an optional requirement for indexing. Adding the support should not be too difficult but it will certainly add a new logic path which would need thorough testing. . We'll certainly keep you updated with the feature as we progress although it can take some time to get back. In terms of your pipeline one option would be to actually save the GTF explicitly in the salmon index folder post indexing. Although it's definitely not a very computer science friendly solution but it will help maintain the consistency while we work on the feature.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/595#issuecomment-738200842
https://github.com/COMBINE-lab/salmon/issues/596#issuecomment-737335445:85,Deployability,release,released,85,"hey @kikegoni ,. I'd strongly advise updating to latest salmon. 0.12.0 is a bit old (released 16 cycles back) there were some bugs which we fixed over time. It'd be great if you can verify the issue persists with the latest release ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/596#issuecomment-737335445
https://github.com/COMBINE-lab/salmon/issues/596#issuecomment-737335445:224,Deployability,release,release,224,"hey @kikegoni ,. I'd strongly advise updating to latest salmon. 0.12.0 is a bit old (released 16 cycles back) there were some bugs which we fixed over time. It'd be great if you can verify the issue persists with the latest release ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/596#issuecomment-737335445
https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737603618:43,Deployability,pipeline,pipeline,43,"I looked into this, since I'm working on a pipeline that could encounter a similar issue, the problem is that in the ensembl .fq files the transcript IDs are represented as the standard ID.version format, e.g. ENST00000415118.1; but in the GTF, they're represented as separate fields:. gene_id ""ENSG00000223972""; gene_version ""5""; transcript_id ""ENST00000456328""; transcript_version ""2""; gene_name ""DDX11L1""; gene_source ""havana""; gene_biotype ""transcribed_unprocessed_pseudogene""; transcript_name ""DDX11L1-202""; transcript_source ""havana""; transcript_biotype ""processed_transcript""; tag ""basic""; transcript_support_level ""1"";. This is different than the Gencode GTFs where there is not a separate version field and the transcript ids match exactly between the GTF and Fq without parsing any additional fields:. gene_id ""ENSMUSG00000102693.1""; transcript_id ""ENSMUST00000193812.1""; gene_type ""TEC""; gene_name ""4933401J01Rik""; transcript_type ""TEC""; transcript_name ""4933401J01Rik-201""; level 2; transcript_support_level ""NA""; mgi_id ""MGI:1918292""; tag ""basic""; havana_gene ""OTTMUSG00000049935.1""; havana_transcript ""OTTMUST00000127109.1"";. This is also going to affect things like the feature request I made here: https://github.com/COMBINE-lab/salmon/issues/595. So it seems that any attempt at GTF parsing is going to need to check for those atypical gene_version and transcript_version fields and attempt to tack on that information to the base ID.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737603618
https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544:503,Availability,echo,echo,503,"The following bash code will detect and parse either format of gtf into an appropriately versioned two column tx2gene file. test=$(zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | head -n 1| cut -f9 | tr -s "";"" "" "" | awk '{print$3}' | sort | uniq | sed 's/\""//g'); if [[ $test == ""transcript_id"" ]]; then; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$4""\t""$2}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; elif [[ $test == ""gene_version"" ]]; then; echo ""Separate version field (ensembl, non-gencode transcriptome, eg. rat, etc)""; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$6 ""."" $8""\t""$2 ""."" $4}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; fi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544
https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544:29,Safety,detect,detect,29,"The following bash code will detect and parse either format of gtf into an appropriately versioned two column tx2gene file. test=$(zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | head -n 1| cut -f9 | tr -s "";"" "" "" | awk '{print$3}' | sort | uniq | sed 's/\""//g'); if [[ $test == ""transcript_id"" ]]; then; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$4""\t""$2}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; elif [[ $test == ""gene_version"" ]]; then; echo ""Separate version field (ensembl, non-gencode transcriptome, eg. rat, etc)""; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$6 ""."" $8""\t""$2 ""."" $4}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; fi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544
https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544:124,Testability,test,test,124,"The following bash code will detect and parse either format of gtf into an appropriately versioned two column tx2gene file. test=$(zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | head -n 1| cut -f9 | tr -s "";"" "" "" | awk '{print$3}' | sort | uniq | sed 's/\""//g'); if [[ $test == ""transcript_id"" ]]; then; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$4""\t""$2}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; elif [[ $test == ""gene_version"" ]]; then; echo ""Separate version field (ensembl, non-gencode transcriptome, eg. rat, etc)""; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$6 ""."" $8""\t""$2 ""."" $4}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; fi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544
https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544:278,Testability,test,test,278,"The following bash code will detect and parse either format of gtf into an appropriately versioned two column tx2gene file. test=$(zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | head -n 1| cut -f9 | tr -s "";"" "" "" | awk '{print$3}' | sort | uniq | sed 's/\""//g'); if [[ $test == ""transcript_id"" ]]; then; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$4""\t""$2}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; elif [[ $test == ""gene_version"" ]]; then; echo ""Separate version field (ensembl, non-gencode transcriptome, eg. rat, etc)""; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$6 ""."" $8""\t""$2 ""."" $4}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; fi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544
https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544:470,Testability,test,test,470,"The following bash code will detect and parse either format of gtf into an appropriately versioned two column tx2gene file. test=$(zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | head -n 1| cut -f9 | tr -s "";"" "" "" | awk '{print$3}' | sort | uniq | sed 's/\""//g'); if [[ $test == ""transcript_id"" ]]; then; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$4""\t""$2}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; elif [[ $test == ""gene_version"" ]]; then; echo ""Separate version field (ensembl, non-gencode transcriptome, eg. rat, etc)""; zless -S $gtf | grep -v ""#"" | awk '$3==""transcript""' | cut -f9 | tr -s "";"" "" "" | awk '{print$6 ""."" $8""\t""$2 ""."" $4}' | sort | uniq | sed 's/\""//g' > txp2gene.tsv; fi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-737617544
https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-988331751:337,Availability,avail,available,337,"hello! I am having a similar issues and was wondering if you had an answer for this. My salmon quant.gene.sf file have the ID.version format, and when trying to do DEG using BiomaRt the gene ID are not in the ID.version format, thus I am unable to match them. I did notice that the index used for salmon was GRCm38 but the one currently available in BiomaRt is GRCm39. Do you know if i could cut the .version from my quan.gene.sf file, or I have to run salmon again with the current genome?; thank you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/598#issuecomment-988331751
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:2032,Energy Efficiency,green,green,2032,"umGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both transcripts in each sample. The green line at the top is what you get if you sum the abundances of these two transcripts. The main point is that the inferential relative variance (adjusted ratio of the variance over the mean) is _much_ smaller for the sum of these transcripts than for either individually. This is strong evidence that they are _inherently_ uncertain given the read evidence and alignments used for quantification. The tool described in that paper, called [`terminus`](https://github.com/COMBINE-lab/terminus), is a tool for automatically finding such groups of transcripts. Anyway, once you have the Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you ar",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:4127,Energy Efficiency,reduce,reduce,4127,"r resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account the inferential uncertainty in the abundance estimates, using the Gibbs samples produced by salmon. This will allow it to avoid spurious DE calls that might otherwise occur when you have highly uncertain transcripts that, by chance, end up being assigned very different abundances in different samples / over different runs. Sorry for the information dump, but I wanted to lay out what might be going on, how to assess it, and what some potential solutions might be. If you dive in to start investigating this, feel free to reach out in this issue along the way if yo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:3323,Integrability,depend,depending,3323,"vidence that they are _inherently_ uncertain given the read evidence and alignments used for quantification. The tool described in that paper, called [`terminus`](https://github.com/COMBINE-lab/terminus), is a tool for automatically finding such groups of transcripts. Anyway, once you have the Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-un",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:201,Performance,optimiz,optimization,201,"Hi @reganhayward,. Thank you for the detailed report. It's interesting that this happens when running with STAR but not when running with selective alignment. However, salmon will attempt to solve the optimization problem with the alignments it is given, regardless of if those come from STAR or from it's built-in selective alignment. While I would generally expect these to be similar, the alignment algorithms are different; see [e.g. the differences between SA/SAF & STAR here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). Nonetheless, it is possible that for a small subset of transcripts, the probabilistic allocations are _so_ ambiguous, that you get large swings in the resulting quantification estimates based on tiny variations in where the optimization starts (which is, itself, stochastic due to the asynchronous nature of salmon's online inference phase). One way we can test this hypothesis is as follows. You can run salmon with `--numGibbsSamples 100` and `-d`. This will tell salmon to perform posterior Gibbs sampling (`--numGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:784,Performance,optimiz,optimization,784,"Hi @reganhayward,. Thank you for the detailed report. It's interesting that this happens when running with STAR but not when running with selective alignment. However, salmon will attempt to solve the optimization problem with the alignments it is given, regardless of if those come from STAR or from it's built-in selective alignment. While I would generally expect these to be similar, the alignment algorithms are different; see [e.g. the differences between SA/SAF & STAR here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). Nonetheless, it is possible that for a small subset of transcripts, the probabilistic allocations are _so_ ambiguous, that you get large swings in the resulting quantification estimates based on tiny variations in where the optimization starts (which is, itself, stochastic due to the asynchronous nature of salmon's online inference phase). One way we can test this hypothesis is as follows. You can run salmon with `--numGibbsSamples 100` and `-d`. This will tell salmon to perform posterior Gibbs sampling (`--numGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:1036,Performance,perform,perform,1036,"ing with STAR but not when running with selective alignment. However, salmon will attempt to solve the optimization problem with the alignments it is given, regardless of if those come from STAR or from it's built-in selective alignment. While I would generally expect these to be similar, the alignment algorithms are different; see [e.g. the differences between SA/SAF & STAR here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). Nonetheless, it is possible that for a small subset of transcripts, the probabilistic allocations are _so_ ambiguous, that you get large swings in the resulting quantification estimates based on tiny variations in where the optimization starts (which is, itself, stochastic due to the asynchronous nature of salmon's online inference phase). One way we can test this hypothesis is as follows. You can run salmon with `--numGibbsSamples 100` and `-d`. This will tell salmon to perform posterior Gibbs sampling (`--numGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both transcripts in each sample. The green line at the top is what you get if you sum the abundances of ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:4377,Performance,perform,performing,4377,"elective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account the inferential uncertainty in the abundance estimates, using the Gibbs samples produced by salmon. This will allow it to avoid spurious DE calls that might otherwise occur when you have highly uncertain transcripts that, by chance, end up being assigned very different abundances in different samples / over different runs. Sorry for the information dump, but I wanted to lay out what might be going on, how to assess it, and what some potential solutions might be. If you dive in to start investigating this, feel free to reach out in this issue along the way if you get stuck or have follow-up questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:4740,Safety,avoid,avoid,4740,"elective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account the inferential uncertainty in the abundance estimates, using the Gibbs samples produced by salmon. This will allow it to avoid spurious DE calls that might otherwise occur when you have highly uncertain transcripts that, by chance, end up being assigned very different abundances in different samples / over different runs. Sorry for the information dump, but I wanted to lay out what might be going on, how to assess it, and what some potential solutions might be. If you dive in to start investigating this, feel free to reach out in this issue along the way if you get stuck or have follow-up questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:2769,Security,access,access,2769,"485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both transcripts in each sample. The green line at the top is what you get if you sum the abundances of these two transcripts. The main point is that the inferential relative variance (adjusted ratio of the variance over the mean) is _much_ smaller for the sum of these transcripts than for either individually. This is strong evidence that they are _inherently_ uncertain given the read evidence and alignments used for quantification. The tool described in that paper, called [`terminus`](https://github.com/COMBINE-lab/terminus), is a tool for automatically finding such groups of transcripts. Anyway, once you have the Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:917,Testability,test,test,917,"Hi @reganhayward,. Thank you for the detailed report. It's interesting that this happens when running with STAR but not when running with selective alignment. However, salmon will attempt to solve the optimization problem with the alignments it is given, regardless of if those come from STAR or from it's built-in selective alignment. While I would generally expect these to be similar, the alignment algorithms are different; see [e.g. the differences between SA/SAF & STAR here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). Nonetheless, it is possible that for a small subset of transcripts, the probabilistic allocations are _so_ ambiguous, that you get large swings in the resulting quantification estimates based on tiny variations in where the optimization starts (which is, itself, stochastic due to the asynchronous nature of salmon's online inference phase). One way we can test this hypothesis is as follows. You can run salmon with `--numGibbsSamples 100` and `-d`. This will tell salmon to perform posterior Gibbs sampling (`--numGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:3599,Testability,test,test,3599," Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:3086,Usability,simpl,simply,3086,"top is what you get if you sum the abundances of these two transcripts. The main point is that the inferential relative variance (adjusted ratio of the variance over the mean) is _much_ smaller for the sum of these transcripts than for either individually. This is strong evidence that they are _inherently_ uncertain given the read evidence and alignments used for quantification. The tool described in that paper, called [`terminus`](https://github.com/COMBINE-lab/terminus), is a tool for automatically finding such groups of transcripts. Anyway, once you have the Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:3867,Usability,simpl,simply,3867," transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account the inferential uncertainty in the abundance estimates, using the Gibbs samples produced by salmon. This will allow it to avoid spurious DE calls that might otherwise occur when you have highly uncertain transcripts that, by chance,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-756948080:26,Availability,ping,pinging,26,"Thanks @reganhayward. I'm pinging @hiraksarkar here as well as he can help us dig into this. I do think it will be really useful to have the bootstrap (in this case Gibbs) folders for the runs, so we can load that data up and see what the posterior traces look like for these transcripts. If you can throw that up somewhere, then we can grab it as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-756948080
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-756948080:204,Performance,load,load,204,"Thanks @reganhayward. I'm pinging @hiraksarkar here as well as he can help us dig into this. I do think it will be really useful to have the bootstrap (in this case Gibbs) folders for the runs, so we can load that data up and see what the posterior traces look like for these transcripts. If you can throw that up somewhere, then we can grab it as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-756948080
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-757473486:1090,Availability,down,down,1090,"@rob-p thanks for tagging me. I missed the primary discussion and went over it. ; @reganhayward , thanks for posting this interesting phenomenon. Checking for ambiguity is data driven, there are many sources of ambiguity, ; 1. Exon sharing (when both the transcripts share a splicing even and thereby share an exon.) ; 2. They originate from genes that ae paralogs of each other. ; 3. They share sequence of read length without an obvious clue (might stem some evolutionary event in the past.). Looking into the ensemble database `ENST00000374675.7` originates from [ENSG00000112473](https://grch37.ensembl.org/Homo_sapiens/Gene/Summary?g=ENSG00000112473;r=6:33168222-33172216) and `ENST00000428189.5` belongs to [ENSG00000229470.5](https://grch37.ensembl.org/Homo_sapiens/Location/View?db=core;g=ENSG00000229470;r=HSCHR6_MHC_DBB:30499388-30515186). The second one comes from an alternate assembly according to ensemble from the same chromosome 6. There is a possibility that there is exon sharing. But I can not be sure before doing a blast etc. . There is a way from salmon to track this down. If you use `-d` option for dumping equivalence classes then, we can check for equivalence classes where these transcripts occur. If they co-occure in an equivalence class then we could be sure that it is due to read sharing. If you can provide me the equivalence class file (should be a smaller one), I can look into it. The gibbs/bootstraps would come handy once we know the equivalence classes, since it will reveal how the uncertain the estimates are for these two transcripts when compared to their equivalence class members. . Best; Hirak",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-757473486
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-757504634:2773,Availability,reliab,reliable,2773,".1',; 'ENST00000493496.5',; 'ENST00000460361.1',; 'ENST00000441953.6',; 'ENST00000431735.6',; 'ENST00000462670.1',; 'ENST00000491795.5',; 'ENST00000468378.1',; 'ENST00000383214.8',; 'ENST00000383213.8',; 'ENST00000486373.5',; 'ENST00000491702.1',; 'ENST00000478834.1',; 'ENST00000444757.5',; 'ENST00000429042.5',; 'ENST00000454420.5',; 'ENST00000425069.5',; 'ENST00000414757.5',; 'ENST00000414916.5']; ```; and, the other transcript is connected to ; ```; ['ENST00000376621.7',; 'ENST00000487166.1',; 'ENST00000383450.3',; 'ENST00000497332.1',; 'ENST00000441604.5',; 'ENST00000481420.1',; 'ENST00000487687.1',; 'ENST00000454829.5',; 'ENST00000490227.1',; 'ENST00000437917.5',; 'ENST00000481380.1',; 'ENST00000383596.6',; 'ENST00000488456.1',; 'ENST00000417834.5',; 'ENST00000486264.1',; 'ENST00000462708.1',; 'ENST00000478748.1',; 'ENST00000465483.1',; 'ENST00000478986.1',; 'ENST00000480572.1',; 'ENST00000497917.1',; 'ENST00000469494.1',; 'ENST00000489631.1',; 'ENST00000433809.1',; 'ENST00000456550.1',; 'ENST00000450423.1',; 'ENST00000435788.1',; 'ENST00000416639.1',; 'ENST00000443235.1',; 'ENST00000458592.1',; 'ENST00000430236.1',; 'ENST00000464231.1',; 'ENST00000496960.1',; 'ENST00000492408.1',; 'ENST00000479883.1',; 'ENST00000467241.1',; 'ENST00000483987.1',; 'ENST00000495838.1',; 'ENST00000467550.1']; ```; Clearly, in the alignment-based is connected to a lot of other transcripts connected, so their different behaviors is expected. ; I think this makes the solution so unstable that EM assigns all the reads to one rather than distributing them to other members. We need to look at the actual bootstrap/gibbs to have more insight. . I would also like to add, as @rob-p suggested previously, this is a classic example where EM algorithm is not that reliable, b/c of uncertainty and `terminus` might be the best answer. . Here is the [script](https://gist.github.com/hiraksarkar/30d8ce2d52035181e00be1479be50a57) for constructing the graph from the equivalence class file. . Best; Hirak",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-757504634
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-757504634:2329,Usability,Clear,Clearly,2329,".1',; 'ENST00000493496.5',; 'ENST00000460361.1',; 'ENST00000441953.6',; 'ENST00000431735.6',; 'ENST00000462670.1',; 'ENST00000491795.5',; 'ENST00000468378.1',; 'ENST00000383214.8',; 'ENST00000383213.8',; 'ENST00000486373.5',; 'ENST00000491702.1',; 'ENST00000478834.1',; 'ENST00000444757.5',; 'ENST00000429042.5',; 'ENST00000454420.5',; 'ENST00000425069.5',; 'ENST00000414757.5',; 'ENST00000414916.5']; ```; and, the other transcript is connected to ; ```; ['ENST00000376621.7',; 'ENST00000487166.1',; 'ENST00000383450.3',; 'ENST00000497332.1',; 'ENST00000441604.5',; 'ENST00000481420.1',; 'ENST00000487687.1',; 'ENST00000454829.5',; 'ENST00000490227.1',; 'ENST00000437917.5',; 'ENST00000481380.1',; 'ENST00000383596.6',; 'ENST00000488456.1',; 'ENST00000417834.5',; 'ENST00000486264.1',; 'ENST00000462708.1',; 'ENST00000478748.1',; 'ENST00000465483.1',; 'ENST00000478986.1',; 'ENST00000480572.1',; 'ENST00000497917.1',; 'ENST00000469494.1',; 'ENST00000489631.1',; 'ENST00000433809.1',; 'ENST00000456550.1',; 'ENST00000450423.1',; 'ENST00000435788.1',; 'ENST00000416639.1',; 'ENST00000443235.1',; 'ENST00000458592.1',; 'ENST00000430236.1',; 'ENST00000464231.1',; 'ENST00000496960.1',; 'ENST00000492408.1',; 'ENST00000479883.1',; 'ENST00000467241.1',; 'ENST00000483987.1',; 'ENST00000495838.1',; 'ENST00000467550.1']; ```; Clearly, in the alignment-based is connected to a lot of other transcripts connected, so their different behaviors is expected. ; I think this makes the solution so unstable that EM assigns all the reads to one rather than distributing them to other members. We need to look at the actual bootstrap/gibbs to have more insight. . I would also like to add, as @rob-p suggested previously, this is a classic example where EM algorithm is not that reliable, b/c of uncertainty and `terminus` might be the best answer. . Here is the [script](https://gist.github.com/hiraksarkar/30d8ce2d52035181e00be1479be50a57) for constructing the graph from the equivalence class file. . Best; Hirak",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-757504634
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-757553955:70,Testability,log,logical,70,Thanks for helping to look into this @hiraksarkar ; That seems like a logical outcome re the higher number of connected transcripts - interesting. As requested - I've added in the bootstrap folder - here are the new links:; [Selective alignment](https://drive.google.com/file/d/11TfZXuBZqMbtCL6BwZwI7ms3gzPNipav/view?usp=sharingl); [Alignment based - STAR](https://drive.google.com/file/d/12piPEagYNuDcPC861CKHYjQ1AVDu8QYP/view?usp=sharing),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-757553955
https://github.com/COMBINE-lab/salmon/issues/602#issuecomment-748729130:682,Availability,error,error,682,"Hi @shnmunk,. Yes, the `--noLengthCorrection` flag is recommended for nanopore based quantification. We expect (and generally observe) that there is not a fragmentation effect in ONT data. Therefore, it is recommended that you use the `--noLengthCorrection` flag with ONT data. You are correct that this make the setting of `--fldMean` and `--fldSD` irrelevant, since there is no length effect applied during the EM algorithm. The other point of note is that selective-alignment is not really designed for long reads, so folks typically use salmon with external alignment for quantifying long-read data (minimap2 is a popular choice). Currently, it's necessary to turn off the read error model `--noErrorModel` when processing long-read alignments, as the long read error profiles are very different form those of short reads. However, this is a temporary issue, as a long-read error model has been developed and is in the pipeline. For the time being though, `--noErrorModel --noLengthCorrection` is the relevant set of flags to get the best results with long-read RNA-seq (either cDNA or direct RNA). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/602#issuecomment-748729130
https://github.com/COMBINE-lab/salmon/issues/602#issuecomment-748729130:766,Availability,error,error,766,"Hi @shnmunk,. Yes, the `--noLengthCorrection` flag is recommended for nanopore based quantification. We expect (and generally observe) that there is not a fragmentation effect in ONT data. Therefore, it is recommended that you use the `--noLengthCorrection` flag with ONT data. You are correct that this make the setting of `--fldMean` and `--fldSD` irrelevant, since there is no length effect applied during the EM algorithm. The other point of note is that selective-alignment is not really designed for long reads, so folks typically use salmon with external alignment for quantifying long-read data (minimap2 is a popular choice). Currently, it's necessary to turn off the read error model `--noErrorModel` when processing long-read alignments, as the long read error profiles are very different form those of short reads. However, this is a temporary issue, as a long-read error model has been developed and is in the pipeline. For the time being though, `--noErrorModel --noLengthCorrection` is the relevant set of flags to get the best results with long-read RNA-seq (either cDNA or direct RNA). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/602#issuecomment-748729130
https://github.com/COMBINE-lab/salmon/issues/602#issuecomment-748729130:878,Availability,error,error,878,"Hi @shnmunk,. Yes, the `--noLengthCorrection` flag is recommended for nanopore based quantification. We expect (and generally observe) that there is not a fragmentation effect in ONT data. Therefore, it is recommended that you use the `--noLengthCorrection` flag with ONT data. You are correct that this make the setting of `--fldMean` and `--fldSD` irrelevant, since there is no length effect applied during the EM algorithm. The other point of note is that selective-alignment is not really designed for long reads, so folks typically use salmon with external alignment for quantifying long-read data (minimap2 is a popular choice). Currently, it's necessary to turn off the read error model `--noErrorModel` when processing long-read alignments, as the long read error profiles are very different form those of short reads. However, this is a temporary issue, as a long-read error model has been developed and is in the pipeline. For the time being though, `--noErrorModel --noLengthCorrection` is the relevant set of flags to get the best results with long-read RNA-seq (either cDNA or direct RNA). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/602#issuecomment-748729130
https://github.com/COMBINE-lab/salmon/issues/602#issuecomment-748729130:923,Deployability,pipeline,pipeline,923,"Hi @shnmunk,. Yes, the `--noLengthCorrection` flag is recommended for nanopore based quantification. We expect (and generally observe) that there is not a fragmentation effect in ONT data. Therefore, it is recommended that you use the `--noLengthCorrection` flag with ONT data. You are correct that this make the setting of `--fldMean` and `--fldSD` irrelevant, since there is no length effect applied during the EM algorithm. The other point of note is that selective-alignment is not really designed for long reads, so folks typically use salmon with external alignment for quantifying long-read data (minimap2 is a popular choice). Currently, it's necessary to turn off the read error model `--noErrorModel` when processing long-read alignments, as the long read error profiles are very different form those of short reads. However, this is a temporary issue, as a long-read error model has been developed and is in the pipeline. For the time being though, `--noErrorModel --noLengthCorrection` is the relevant set of flags to get the best results with long-read RNA-seq (either cDNA or direct RNA). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/602#issuecomment-748729130
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850:190,Availability,error,error,190,"Hi @Davidwei7,. Thank you for the very detailed bug report! So, I have two initial responses / thoughts about your issue. First, you asked if the issue may be related to a memory allocation error wherein the index didn't build successfully. This is quite possible (and the error you see during quantification is consistent with that). The *full* decoy index is substantially larger than the index on just the transcriptome (after all, it is indexing the entire human genome in addition to the transcriptome). One thing you might try to test this hypothesis, other than requesting to build on a node with more RAM, is to compute a hash (e.g. md5 or sha256 sum) on all of the files in the index, and also record their sizes. Then we can build the index on the same version of the files on our end and compare. Second — and perhaps more importantly for your end goal — the main purpose of the decoy-aware index is to improve specificity rather than sensitivity. That is, the decoys are designed to help avoid _spurious_ mapping of reads to an annotated transcript when a better explanation for the read exists elsewhere in the genome. However, the reads that are mapped to decoys are not otherwise used for quantification. Thus, using the decoy aware transcriptome index is unlikely to improve your mapping rate. I agree that your mapping rate does seem rather low. There are a few potential culprits here, and some diagnostics we can look at to see what might be going wrong. First, you can take a look at the file `aux_info/meta_info.json` in the salmon quantification directories to get a few more details about why reads were not mapped. If you share one of those files here I can describe the relevant fields. Also, I have two more rather common things to consider that might affect the mapping rate. One is to add the sequence for the ribosomal RNAs to your transcriptome before indexing and then quantifying. If your mapping rate increases considerably, this is evidence of rather inefficient depl",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850:273,Availability,error,error,273,"Hi @Davidwei7,. Thank you for the very detailed bug report! So, I have two initial responses / thoughts about your issue. First, you asked if the issue may be related to a memory allocation error wherein the index didn't build successfully. This is quite possible (and the error you see during quantification is consistent with that). The *full* decoy index is substantially larger than the index on just the transcriptome (after all, it is indexing the entire human genome in addition to the transcriptome). One thing you might try to test this hypothesis, other than requesting to build on a node with more RAM, is to compute a hash (e.g. md5 or sha256 sum) on all of the files in the index, and also record their sizes. Then we can build the index on the same version of the files on our end and compare. Second — and perhaps more importantly for your end goal — the main purpose of the decoy-aware index is to improve specificity rather than sensitivity. That is, the decoys are designed to help avoid _spurious_ mapping of reads to an annotated transcript when a better explanation for the read exists elsewhere in the genome. However, the reads that are mapped to decoys are not otherwise used for quantification. Thus, using the decoy aware transcriptome index is unlikely to improve your mapping rate. I agree that your mapping rate does seem rather low. There are a few potential culprits here, and some diagnostics we can look at to see what might be going wrong. First, you can take a look at the file `aux_info/meta_info.json` in the salmon quantification directories to get a few more details about why reads were not mapped. If you share one of those files here I can describe the relevant fields. Also, I have two more rather common things to consider that might affect the mapping rate. One is to add the sequence for the ribosomal RNAs to your transcriptome before indexing and then quantifying. If your mapping rate increases considerably, this is evidence of rather inefficient depl",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850:2071,Energy Efficiency,adapt,adapter,2071,"e *full* decoy index is substantially larger than the index on just the transcriptome (after all, it is indexing the entire human genome in addition to the transcriptome). One thing you might try to test this hypothesis, other than requesting to build on a node with more RAM, is to compute a hash (e.g. md5 or sha256 sum) on all of the files in the index, and also record their sizes. Then we can build the index on the same version of the files on our end and compare. Second — and perhaps more importantly for your end goal — the main purpose of the decoy-aware index is to improve specificity rather than sensitivity. That is, the decoys are designed to help avoid _spurious_ mapping of reads to an annotated transcript when a better explanation for the read exists elsewhere in the genome. However, the reads that are mapped to decoys are not otherwise used for quantification. Thus, using the decoy aware transcriptome index is unlikely to improve your mapping rate. I agree that your mapping rate does seem rather low. There are a few potential culprits here, and some diagnostics we can look at to see what might be going wrong. First, you can take a look at the file `aux_info/meta_info.json` in the salmon quantification directories to get a few more details about why reads were not mapped. If you share one of those files here I can describe the relevant fields. Also, I have two more rather common things to consider that might affect the mapping rate. One is to add the sequence for the ribosomal RNAs to your transcriptome before indexing and then quantifying. If your mapping rate increases considerably, this is evidence of rather inefficient depletion prior to sequencing. The other thing to consider is to do basic adapter / quality trimming on the reads to see if that affects your mapping rate at all. I hope these two different responses are useful, and I'll keep this issue open so feel free to reply here with any further questions or discoveries you make regarding the above.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850:2071,Integrability,adapter,adapter,2071,"e *full* decoy index is substantially larger than the index on just the transcriptome (after all, it is indexing the entire human genome in addition to the transcriptome). One thing you might try to test this hypothesis, other than requesting to build on a node with more RAM, is to compute a hash (e.g. md5 or sha256 sum) on all of the files in the index, and also record their sizes. Then we can build the index on the same version of the files on our end and compare. Second — and perhaps more importantly for your end goal — the main purpose of the decoy-aware index is to improve specificity rather than sensitivity. That is, the decoys are designed to help avoid _spurious_ mapping of reads to an annotated transcript when a better explanation for the read exists elsewhere in the genome. However, the reads that are mapped to decoys are not otherwise used for quantification. Thus, using the decoy aware transcriptome index is unlikely to improve your mapping rate. I agree that your mapping rate does seem rather low. There are a few potential culprits here, and some diagnostics we can look at to see what might be going wrong. First, you can take a look at the file `aux_info/meta_info.json` in the salmon quantification directories to get a few more details about why reads were not mapped. If you share one of those files here I can describe the relevant fields. Also, I have two more rather common things to consider that might affect the mapping rate. One is to add the sequence for the ribosomal RNAs to your transcriptome before indexing and then quantifying. If your mapping rate increases considerably, this is evidence of rather inefficient depletion prior to sequencing. The other thing to consider is to do basic adapter / quality trimming on the reads to see if that affects your mapping rate at all. I hope these two different responses are useful, and I'll keep this issue open so feel free to reply here with any further questions or discoveries you make regarding the above.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850:2071,Modifiability,adapt,adapter,2071,"e *full* decoy index is substantially larger than the index on just the transcriptome (after all, it is indexing the entire human genome in addition to the transcriptome). One thing you might try to test this hypothesis, other than requesting to build on a node with more RAM, is to compute a hash (e.g. md5 or sha256 sum) on all of the files in the index, and also record their sizes. Then we can build the index on the same version of the files on our end and compare. Second — and perhaps more importantly for your end goal — the main purpose of the decoy-aware index is to improve specificity rather than sensitivity. That is, the decoys are designed to help avoid _spurious_ mapping of reads to an annotated transcript when a better explanation for the read exists elsewhere in the genome. However, the reads that are mapped to decoys are not otherwise used for quantification. Thus, using the decoy aware transcriptome index is unlikely to improve your mapping rate. I agree that your mapping rate does seem rather low. There are a few potential culprits here, and some diagnostics we can look at to see what might be going wrong. First, you can take a look at the file `aux_info/meta_info.json` in the salmon quantification directories to get a few more details about why reads were not mapped. If you share one of those files here I can describe the relevant fields. Also, I have two more rather common things to consider that might affect the mapping rate. One is to add the sequence for the ribosomal RNAs to your transcriptome before indexing and then quantifying. If your mapping rate increases considerably, this is evidence of rather inefficient depletion prior to sequencing. The other thing to consider is to do basic adapter / quality trimming on the reads to see if that affects your mapping rate at all. I hope these two different responses are useful, and I'll keep this issue open so feel free to reply here with any further questions or discoveries you make regarding the above.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850:1000,Safety,avoid,avoid,1000,"led bug report! So, I have two initial responses / thoughts about your issue. First, you asked if the issue may be related to a memory allocation error wherein the index didn't build successfully. This is quite possible (and the error you see during quantification is consistent with that). The *full* decoy index is substantially larger than the index on just the transcriptome (after all, it is indexing the entire human genome in addition to the transcriptome). One thing you might try to test this hypothesis, other than requesting to build on a node with more RAM, is to compute a hash (e.g. md5 or sha256 sum) on all of the files in the index, and also record their sizes. Then we can build the index on the same version of the files on our end and compare. Second — and perhaps more importantly for your end goal — the main purpose of the decoy-aware index is to improve specificity rather than sensitivity. That is, the decoys are designed to help avoid _spurious_ mapping of reads to an annotated transcript when a better explanation for the read exists elsewhere in the genome. However, the reads that are mapped to decoys are not otherwise used for quantification. Thus, using the decoy aware transcriptome index is unlikely to improve your mapping rate. I agree that your mapping rate does seem rather low. There are a few potential culprits here, and some diagnostics we can look at to see what might be going wrong. First, you can take a look at the file `aux_info/meta_info.json` in the salmon quantification directories to get a few more details about why reads were not mapped. If you share one of those files here I can describe the relevant fields. Also, I have two more rather common things to consider that might affect the mapping rate. One is to add the sequence for the ribosomal RNAs to your transcriptome before indexing and then quantifying. If your mapping rate increases considerably, this is evidence of rather inefficient depletion prior to sequencing. The other thing t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850:630,Security,hash,hash,630,"Hi @Davidwei7,. Thank you for the very detailed bug report! So, I have two initial responses / thoughts about your issue. First, you asked if the issue may be related to a memory allocation error wherein the index didn't build successfully. This is quite possible (and the error you see during quantification is consistent with that). The *full* decoy index is substantially larger than the index on just the transcriptome (after all, it is indexing the entire human genome in addition to the transcriptome). One thing you might try to test this hypothesis, other than requesting to build on a node with more RAM, is to compute a hash (e.g. md5 or sha256 sum) on all of the files in the index, and also record their sizes. Then we can build the index on the same version of the files on our end and compare. Second — and perhaps more importantly for your end goal — the main purpose of the decoy-aware index is to improve specificity rather than sensitivity. That is, the decoys are designed to help avoid _spurious_ mapping of reads to an annotated transcript when a better explanation for the read exists elsewhere in the genome. However, the reads that are mapped to decoys are not otherwise used for quantification. Thus, using the decoy aware transcriptome index is unlikely to improve your mapping rate. I agree that your mapping rate does seem rather low. There are a few potential culprits here, and some diagnostics we can look at to see what might be going wrong. First, you can take a look at the file `aux_info/meta_info.json` in the salmon quantification directories to get a few more details about why reads were not mapped. If you share one of those files here I can describe the relevant fields. Also, I have two more rather common things to consider that might affect the mapping rate. One is to add the sequence for the ribosomal RNAs to your transcriptome before indexing and then quantifying. If your mapping rate increases considerably, this is evidence of rather inefficient depl",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850:536,Testability,test,test,536,"Hi @Davidwei7,. Thank you for the very detailed bug report! So, I have two initial responses / thoughts about your issue. First, you asked if the issue may be related to a memory allocation error wherein the index didn't build successfully. This is quite possible (and the error you see during quantification is consistent with that). The *full* decoy index is substantially larger than the index on just the transcriptome (after all, it is indexing the entire human genome in addition to the transcriptome). One thing you might try to test this hypothesis, other than requesting to build on a node with more RAM, is to compute a hash (e.g. md5 or sha256 sum) on all of the files in the index, and also record their sizes. Then we can build the index on the same version of the files on our end and compare. Second — and perhaps more importantly for your end goal — the main purpose of the decoy-aware index is to improve specificity rather than sensitivity. That is, the decoys are designed to help avoid _spurious_ mapping of reads to an annotated transcript when a better explanation for the read exists elsewhere in the genome. However, the reads that are mapped to decoys are not otherwise used for quantification. Thus, using the decoy aware transcriptome index is unlikely to improve your mapping rate. I agree that your mapping rate does seem rather low. There are a few potential culprits here, and some diagnostics we can look at to see what might be going wrong. First, you can take a look at the file `aux_info/meta_info.json` in the salmon quantification directories to get a few more details about why reads were not mapped. If you share one of those files here I can describe the relevant fields. Also, I have two more rather common things to consider that might affect the mapping rate. One is to add the sequence for the ribosomal RNAs to your transcriptome before indexing and then quantifying. If your mapping rate increases considerably, this is evidence of rather inefficient depl",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744162850
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744750168:139,Security,hash,hash,139,"Dear @rob-p,. Hope you are well, thank you so much for your response and input. . Following your suggestions, firstly I tried to compute a hash on all of the files in the index using this command:; find /scratch/scratch/skgtjzw/workspace/middle_aged_microglia/salmon_quantification_SAF/salmon_index -type f -exec md5sum {} \; | md5sum; c0959830010f005c7f2e041aac4829ef -. Secondly, I have attached the aux_info/meta_info.json on here ; ![SRR2557120](https://user-images.githubusercontent.com/50330051/102141871-71f7aa80-3e59-11eb-9d3f-5e824b7a1346.PNG); ![SRR2557121](https://user-images.githubusercontent.com/50330051/102141883-73c16e00-3e59-11eb-9c50-6cc61b8d34c1.PNG); ![SRR2557119](https://user-images.githubusercontent.com/50330051/102141894-76bc5e80-3e59-11eb-80e2-4cb86c4466b1.PNG). Thirdly, as I am quite new to the RNA-Seq analysis world and coding, I am not sure how can I add the sequence for the ribosomal RNA to my transcriptome. For example, where could I find such files with gencode? and with a file of ribosomal RNA, do I give both the gencode.v36.transcripts.fa.g and the ribosomal RNA together to the salmon index command? or there is certain parameter in the salmon index command that needs to be changed?. Finally, for these three data, it was from published paper, and I am trying to do re-analysis on them. And before salmon quantification, I have already ran fastp on them to do the trim and QC. . Hopefully, I am making sense here. Please let me know if there is anything incorrect here. Thank you for helping out in advance. . Best Wishes, . David",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-744750168
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992:3091,Availability,down,down,3091,"eads unmapped: too many mismatches | 0.00%; % of reads unmapped: too short | 8.07%; % of reads unmapped: other | 0.12%; CHIMERIC READS:; Number of chimeric reads | 0; % of chimeric reads | 0.00%; ```. *yet, only `9,310,303` reads* were determined by STAR to project properly to annotated transcripts (slightly _less_ than are mapped to the transcriptome by salmon, at least without the decoy sequence included). So, there is a very high fraction of the reads that align to the genome, but a much smaller fraction ~45%-50% that align to the transcriptome. There are many reasons something like this could happen, but it suggests that there are a lot of reads being generated from outside of annotated transcripts. This could be a mix of novel transcripts in this sample (both at entirely novel loci, as well as novel transcripts within annotated loci), as well as of noisy transcription, unannotated transcribed pseudogenes etc. I took a look at the bigwig generated by this pipeline, and STAR seems to be mapping quite a lot of reads to chr21 as well as to the mitochondrial genome (chrM). However, as evidenced by the fact that neither salmon using selective-alignment (and mapping to the transcriptome) nor the STAR->salmon pipeline see these reads mapping to annotated transcripts they must be arising from outside of these regions. There are a few option in this case. You could manually investigate where these reads are coming from by aligning them with e.g. STAR and inspecting the BAM files. Alternatively, you could attempt to assemble novel transcripts (e.g. using StringTie or Scallop) and then add them to the transcriptome for quantification. However, it does seem that getting down to the bottom of the relatively low mapping rate to the annotated transcriptome, in light of the relatively high mapping rate to the whole genome, but outside of annotated transcripts, may require a bit more digging. I'm happy to answer any other specific questions that might arise if you dig into this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992:153,Deployability,pipeline,pipeline,153,"Thanks for the details, David. I wanted to get a broader perspective of what was going on mapping-wise, so I ran the first sample through a STAR->salmon pipeline (the [brand new one in nf-core](https://github.com/nf-core/rnaseq/releases/tag/3.0)). This will also let us get a notion of what is happening in terms of mapping to the genome versus the annotated transcripts. Here's what I found:. The STAR mapping report shows:. ```; Started job on | Dec 15 18:01:41; Started mapping on | Dec 15 18:12:46; Finished on | Dec 15 18:25:37; Mapping speed, Million of reads per hour | 103.29. Number of input reads | 22120369; Average input read length | 290; UNIQUE READS:; Uniquely mapped reads number | 18500061; Uniquely mapped reads % | 83.63%; Average mapped length | 284.22; Number of splices: Total | 5999311; Number of splices: Annotated (sjdb) | 5961890; Number of splices: GT/AG | 5905895; Number of splices: GC/AG | 41312; Number of splices: AT/AC | 11584; Number of splices: Non-canonical | 40520; Mismatch rate per base, % | 0.36%; Deletion rate per base | 0.01%; Deletion average length | 1.46; Insertion rate per base | 0.01%; Insertion average length | 1.72; MULTI-MAPPING READS:; Number of reads mapped to multiple loci | 1805463; % of reads mapped to multiple loci | 8.16%; Number of reads mapped to too many loci | 3745; % of reads mapped to too many loci | 0.02%; UNMAPPED READS:; % of reads unmapped: too many mismatches | 0.00%; % of reads unmapped: too short | 8.07%; % of reads unmapped: other | 0.12%; CHIMERIC READS:; Number of chimeric reads | 0; % of chimeric reads | 0.00%; ```. *yet, only `9,310,303` reads* were determined by STAR to project properly to annotated transcripts (slightly _less_ than are mapped to the transcriptome by salmon, at least without the decoy sequence included). So, there is a very high fraction of the reads that align to the genome, but a much smaller fraction ~45%-50% that align to the transcriptome. There are many reasons something like this cou",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992:228,Deployability,release,releases,228,"Thanks for the details, David. I wanted to get a broader perspective of what was going on mapping-wise, so I ran the first sample through a STAR->salmon pipeline (the [brand new one in nf-core](https://github.com/nf-core/rnaseq/releases/tag/3.0)). This will also let us get a notion of what is happening in terms of mapping to the genome versus the annotated transcripts. Here's what I found:. The STAR mapping report shows:. ```; Started job on | Dec 15 18:01:41; Started mapping on | Dec 15 18:12:46; Finished on | Dec 15 18:25:37; Mapping speed, Million of reads per hour | 103.29. Number of input reads | 22120369; Average input read length | 290; UNIQUE READS:; Uniquely mapped reads number | 18500061; Uniquely mapped reads % | 83.63%; Average mapped length | 284.22; Number of splices: Total | 5999311; Number of splices: Annotated (sjdb) | 5961890; Number of splices: GT/AG | 5905895; Number of splices: GC/AG | 41312; Number of splices: AT/AC | 11584; Number of splices: Non-canonical | 40520; Mismatch rate per base, % | 0.36%; Deletion rate per base | 0.01%; Deletion average length | 1.46; Insertion rate per base | 0.01%; Insertion average length | 1.72; MULTI-MAPPING READS:; Number of reads mapped to multiple loci | 1805463; % of reads mapped to multiple loci | 8.16%; Number of reads mapped to too many loci | 3745; % of reads mapped to too many loci | 0.02%; UNMAPPED READS:; % of reads unmapped: too many mismatches | 0.00%; % of reads unmapped: too short | 8.07%; % of reads unmapped: other | 0.12%; CHIMERIC READS:; Number of chimeric reads | 0; % of chimeric reads | 0.00%; ```. *yet, only `9,310,303` reads* were determined by STAR to project properly to annotated transcripts (slightly _less_ than are mapped to the transcriptome by salmon, at least without the decoy sequence included). So, there is a very high fraction of the reads that align to the genome, but a much smaller fraction ~45%-50% that align to the transcriptome. There are many reasons something like this cou",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992:2374,Deployability,pipeline,pipeline,2374,"eads unmapped: too many mismatches | 0.00%; % of reads unmapped: too short | 8.07%; % of reads unmapped: other | 0.12%; CHIMERIC READS:; Number of chimeric reads | 0; % of chimeric reads | 0.00%; ```. *yet, only `9,310,303` reads* were determined by STAR to project properly to annotated transcripts (slightly _less_ than are mapped to the transcriptome by salmon, at least without the decoy sequence included). So, there is a very high fraction of the reads that align to the genome, but a much smaller fraction ~45%-50% that align to the transcriptome. There are many reasons something like this could happen, but it suggests that there are a lot of reads being generated from outside of annotated transcripts. This could be a mix of novel transcripts in this sample (both at entirely novel loci, as well as novel transcripts within annotated loci), as well as of noisy transcription, unannotated transcribed pseudogenes etc. I took a look at the bigwig generated by this pipeline, and STAR seems to be mapping quite a lot of reads to chr21 as well as to the mitochondrial genome (chrM). However, as evidenced by the fact that neither salmon using selective-alignment (and mapping to the transcriptome) nor the STAR->salmon pipeline see these reads mapping to annotated transcripts they must be arising from outside of these regions. There are a few option in this case. You could manually investigate where these reads are coming from by aligning them with e.g. STAR and inspecting the BAM files. Alternatively, you could attempt to assemble novel transcripts (e.g. using StringTie or Scallop) and then add them to the transcriptome for quantification. However, it does seem that getting down to the bottom of the relatively low mapping rate to the annotated transcriptome, in light of the relatively high mapping rate to the whole genome, but outside of annotated transcripts, may require a bit more digging. I'm happy to answer any other specific questions that might arise if you dig into this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992:2626,Deployability,pipeline,pipeline,2626,"eads unmapped: too many mismatches | 0.00%; % of reads unmapped: too short | 8.07%; % of reads unmapped: other | 0.12%; CHIMERIC READS:; Number of chimeric reads | 0; % of chimeric reads | 0.00%; ```. *yet, only `9,310,303` reads* were determined by STAR to project properly to annotated transcripts (slightly _less_ than are mapped to the transcriptome by salmon, at least without the decoy sequence included). So, there is a very high fraction of the reads that align to the genome, but a much smaller fraction ~45%-50% that align to the transcriptome. There are many reasons something like this could happen, but it suggests that there are a lot of reads being generated from outside of annotated transcripts. This could be a mix of novel transcripts in this sample (both at entirely novel loci, as well as novel transcripts within annotated loci), as well as of noisy transcription, unannotated transcribed pseudogenes etc. I took a look at the bigwig generated by this pipeline, and STAR seems to be mapping quite a lot of reads to chr21 as well as to the mitochondrial genome (chrM). However, as evidenced by the fact that neither salmon using selective-alignment (and mapping to the transcriptome) nor the STAR->salmon pipeline see these reads mapping to annotated transcripts they must be arising from outside of these regions. There are a few option in this case. You could manually investigate where these reads are coming from by aligning them with e.g. STAR and inspecting the BAM files. Alternatively, you could attempt to assemble novel transcripts (e.g. using StringTie or Scallop) and then add them to the transcriptome for quantification. However, it does seem that getting down to the bottom of the relatively low mapping rate to the annotated transcriptome, in light of the relatively high mapping rate to the whole genome, but outside of annotated transcripts, may require a bit more digging. I'm happy to answer any other specific questions that might arise if you dig into this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-745768992
https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-746227004:464,Safety,avoid,avoid,464,"Dear @rob-p, . Hope you are well. . Thank you so much for the time and effort that you put in for helping me. . I finally built full decoy index with more RAM allocation to 35Gb (found this in the previous posts) and successfully ran salmon quant on the samples with full decoy index. . The mapped rate of these samples with decoy sequence included, and the mapping rate dropped slightly than salmon without decoy sequence. I think this is expected, as this is to avoid spurious alignments to annotated transcripts (more conservative approach?). ![image](https://user-images.githubusercontent.com/50330051/102348292-95297380-3f99-11eb-9e7a-13b292bf0b35.png); ![image](https://user-images.githubusercontent.com/50330051/102348333-abcfca80-3f99-11eb-8b1a-079463d9af06.png); ![image](https://user-images.githubusercontent.com/50330051/102348400-c4d87b80-3f99-11eb-8859-b8fe0fbe6350.png). The explanations and reasons that you proposed are very useful and helpful. The fact that two methods of quantification suggest the same result is promising to believe that there will be more digging for the truth to be revealed. But I am a bit out of my depth on assembling novel transcripts, or extracting unmapped reads and aligning them with STAR and inspecting the BAM files. But I will give a try. Best Wishes, . David",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/603#issuecomment-746227004
https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-749596440:41,Availability,error,error,41,"Hi @gnaisha,. Thank you for the detailed error report. Would it be possible to provide the BAM file that generated this behavior, or the commands used to generate this BAM file? That way we can try to reproduce and diagnose the issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-749596440
https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-749739255:908,Energy Efficiency,allocate,allocated,908,"Hi @gnaisha,. Thank you for providing the file to reproduce the issue. So, the difference here is all in the default fragment length mean and standard deviation that salmon and eXpress use. This really only matters in single-end libraries like this, since in paired-end libraries both tools will estimate the fragment length distribution from the data itself. Nonetheless, if not given specific parameters to override the default, salmon assumes μ = 250 and σ = 25, while eXpress assumes μ = 200 and σ = 80. If you run salmon like:. ```; salmon quant -lU -t transcriptome.fa -a sample_nested_transcripts_ENST00000364953-1_ENST00000375633-5.bam --fldMean 200 --fldSD 80 -o quant_directory; ```. Then you will see the following behavior for these transcripts:. ```; ENST00000364953.1 64 23.127 1000000.000000 49.000; ENST00000375633.5 586 384.567 0.000000 0.000; ```. So that the all of the reads are, indeed, allocated to the former. The effect of the transcript length on the assignment probabilities is a direct result of the probabilistic model (and due to the length effect that actually exists in the full-length RNA-seq assay). It's unfortunate that there's not a good way to estimate the fragment length distribution in single-end data, and so we are left with having to set some defaults. Depending on the actual library, different defaults will match better or worse. On the plus side, it's easy to change these values if you have better knowledge of the parameters or reason to believe that one value will work better than another.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-749739255
https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-749739255:1296,Integrability,Depend,Depending,1296,"Hi @gnaisha,. Thank you for providing the file to reproduce the issue. So, the difference here is all in the default fragment length mean and standard deviation that salmon and eXpress use. This really only matters in single-end libraries like this, since in paired-end libraries both tools will estimate the fragment length distribution from the data itself. Nonetheless, if not given specific parameters to override the default, salmon assumes μ = 250 and σ = 25, while eXpress assumes μ = 200 and σ = 80. If you run salmon like:. ```; salmon quant -lU -t transcriptome.fa -a sample_nested_transcripts_ENST00000364953-1_ENST00000375633-5.bam --fldMean 200 --fldSD 80 -o quant_directory; ```. Then you will see the following behavior for these transcripts:. ```; ENST00000364953.1 64 23.127 1000000.000000 49.000; ENST00000375633.5 586 384.567 0.000000 0.000; ```. So that the all of the reads are, indeed, allocated to the former. The effect of the transcript length on the assignment probabilities is a direct result of the probabilistic model (and due to the length effect that actually exists in the full-length RNA-seq assay). It's unfortunate that there's not a good way to estimate the fragment length distribution in single-end data, and so we are left with having to set some defaults. Depending on the actual library, different defaults will match better or worse. On the plus side, it's easy to change these values if you have better knowledge of the parameters or reason to believe that one value will work better than another.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-749739255
https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-752064194:318,Energy Efficiency,adapt,adapter,318,"Hi Rob,. Thanks for the clarity regarding the effect of insert size distribution on quantification. That does resolve this issue, and gives me a path going forward using Salmon for this data. However, I am trying to use Salmon for small RNA-Seq data, where the insert size is equal to read length for most reads after adapter trimming. Would it be possible to add a flag to use read length as a proxy for insert size, potentially with a fallback to fldMean/fldSD in the case of full-length, untrimmed reads? This is something I would be willing to contribute myself, if it sounds appropriate. Thanks,; Gautam",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-752064194
https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-752064194:318,Integrability,adapter,adapter,318,"Hi Rob,. Thanks for the clarity regarding the effect of insert size distribution on quantification. That does resolve this issue, and gives me a path going forward using Salmon for this data. However, I am trying to use Salmon for small RNA-Seq data, where the insert size is equal to read length for most reads after adapter trimming. Would it be possible to add a flag to use read length as a proxy for insert size, potentially with a fallback to fldMean/fldSD in the case of full-length, untrimmed reads? This is something I would be willing to contribute myself, if it sounds appropriate. Thanks,; Gautam",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-752064194
https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-752064194:318,Modifiability,adapt,adapter,318,"Hi Rob,. Thanks for the clarity regarding the effect of insert size distribution on quantification. That does resolve this issue, and gives me a path going forward using Salmon for this data. However, I am trying to use Salmon for small RNA-Seq data, where the insert size is equal to read length for most reads after adapter trimming. Would it be possible to add a flag to use read length as a proxy for insert size, potentially with a fallback to fldMean/fldSD in the case of full-length, untrimmed reads? This is something I would be willing to contribute myself, if it sounds appropriate. Thanks,; Gautam",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/605#issuecomment-752064194
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046:396,Availability,down,download,396,"Hi @kai2june,. Thank you for the _detailed_ report! It's interesting because (a) those functions aren't doing anything too exotic and (b) CentOS is the OS we use on our continuous integration. We'll try and get a better handle of what is going on here. In the mean time, could you tell us if you see the same behavior with the [pre-compiled binary](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz) available from the downloads page?. P.S. One other thing worth trying. We've noticed that compiler support for interprocedural optimization isn't terrific. You can try building salmon without this option by passing `-DNO_IPO=TRUE` as an additional cmake flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046:446,Availability,avail,available,446,"Hi @kai2june,. Thank you for the _detailed_ report! It's interesting because (a) those functions aren't doing anything too exotic and (b) CentOS is the OS we use on our continuous integration. We'll try and get a better handle of what is going on here. In the mean time, could you tell us if you see the same behavior with the [pre-compiled binary](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz) available from the downloads page?. P.S. One other thing worth trying. We've noticed that compiler support for interprocedural optimization isn't terrific. You can try building salmon without this option by passing `-DNO_IPO=TRUE` as an additional cmake flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046:465,Availability,down,downloads,465,"Hi @kai2june,. Thank you for the _detailed_ report! It's interesting because (a) those functions aren't doing anything too exotic and (b) CentOS is the OS we use on our continuous integration. We'll try and get a better handle of what is going on here. In the mean time, could you tell us if you see the same behavior with the [pre-compiled binary](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz) available from the downloads page?. P.S. One other thing worth trying. We've noticed that compiler support for interprocedural optimization isn't terrific. You can try building salmon without this option by passing `-DNO_IPO=TRUE` as an additional cmake flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046:169,Deployability,continuous,continuous,169,"Hi @kai2june,. Thank you for the _detailed_ report! It's interesting because (a) those functions aren't doing anything too exotic and (b) CentOS is the OS we use on our continuous integration. We'll try and get a better handle of what is going on here. In the mean time, could you tell us if you see the same behavior with the [pre-compiled binary](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz) available from the downloads page?. P.S. One other thing worth trying. We've noticed that compiler support for interprocedural optimization isn't terrific. You can try building salmon without this option by passing `-DNO_IPO=TRUE` as an additional cmake flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046:180,Deployability,integrat,integration,180,"Hi @kai2june,. Thank you for the _detailed_ report! It's interesting because (a) those functions aren't doing anything too exotic and (b) CentOS is the OS we use on our continuous integration. We'll try and get a better handle of what is going on here. In the mean time, could you tell us if you see the same behavior with the [pre-compiled binary](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz) available from the downloads page?. P.S. One other thing worth trying. We've noticed that compiler support for interprocedural optimization isn't terrific. You can try building salmon without this option by passing `-DNO_IPO=TRUE` as an additional cmake flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046:387,Deployability,release,releases,387,"Hi @kai2june,. Thank you for the _detailed_ report! It's interesting because (a) those functions aren't doing anything too exotic and (b) CentOS is the OS we use on our continuous integration. We'll try and get a better handle of what is going on here. In the mean time, could you tell us if you see the same behavior with the [pre-compiled binary](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz) available from the downloads page?. P.S. One other thing worth trying. We've noticed that compiler support for interprocedural optimization isn't terrific. You can try building salmon without this option by passing `-DNO_IPO=TRUE` as an additional cmake flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046:180,Integrability,integrat,integration,180,"Hi @kai2june,. Thank you for the _detailed_ report! It's interesting because (a) those functions aren't doing anything too exotic and (b) CentOS is the OS we use on our continuous integration. We'll try and get a better handle of what is going on here. In the mean time, could you tell us if you see the same behavior with the [pre-compiled binary](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz) available from the downloads page?. P.S. One other thing worth trying. We've noticed that compiler support for interprocedural optimization isn't terrific. You can try building salmon without this option by passing `-DNO_IPO=TRUE` as an additional cmake flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046:573,Performance,optimiz,optimization,573,"Hi @kai2june,. Thank you for the _detailed_ report! It's interesting because (a) those functions aren't doing anything too exotic and (b) CentOS is the OS we use on our continuous integration. We'll try and get a better handle of what is going on here. In the mean time, could you tell us if you see the same behavior with the [pre-compiled binary](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz) available from the downloads page?. P.S. One other thing worth trying. We've noticed that compiler support for interprocedural optimization isn't terrific. You can try building salmon without this option by passing `-DNO_IPO=TRUE` as an additional cmake flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751366046
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751460320:676,Availability,fault,fault,676,"Hello @rob-p ,; 1. For pre-compiled binary, ""salmon index"" complete successfully, but ""salmon quant"" failed to find the read file. (""ls"" command confirms the existence of the read file SRR6269049_2.fastq); ![image](https://user-images.githubusercontent.com/24876498/103170603-66a47600-4880-11eb-9da1-336f96880e4f.png). 2. ; > cmake -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR=/usr/include -DCMAKE_BUILD_TYPE=Release -DNO_IPO=TRUE -DCMAKE_INSTALL_PREFIX=../stage ..; I add the additional ""-DNO_IPO"" flag, but ""salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode""(same as **To Reproduce** point 5) still crashed at fixFasta(), fixFastaMain() with segmentation fault. 3.; I had another issue posted yesterday reporting that the Debug mode is unabled to be compiled successfully, and I'm wondering if there's a resolution for the problem. Thank you:)); title: [salmon v1.4.0 -DCMAKE_BUILD_TYPE=Debug produce compile error: -pg and -fomit-frame-pointer are incompatible #608]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751460320
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751460320:930,Availability,error,error,930,"Hello @rob-p ,; 1. For pre-compiled binary, ""salmon index"" complete successfully, but ""salmon quant"" failed to find the read file. (""ls"" command confirms the existence of the read file SRR6269049_2.fastq); ![image](https://user-images.githubusercontent.com/24876498/103170603-66a47600-4880-11eb-9da1-336f96880e4f.png). 2. ; > cmake -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR=/usr/include -DCMAKE_BUILD_TYPE=Release -DNO_IPO=TRUE -DCMAKE_INSTALL_PREFIX=../stage ..; I add the additional ""-DNO_IPO"" flag, but ""salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode""(same as **To Reproduce** point 5) still crashed at fixFasta(), fixFastaMain() with segmentation fault. 3.; I had another issue posted yesterday reporting that the Debug mode is unabled to be compiled successfully, and I'm wondering if there's a resolution for the problem. Thank you:)); title: [salmon v1.4.0 -DCMAKE_BUILD_TYPE=Debug produce compile error: -pg and -fomit-frame-pointer are incompatible #608]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751460320
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751460320:401,Deployability,Release,Release,401,"Hello @rob-p ,; 1. For pre-compiled binary, ""salmon index"" complete successfully, but ""salmon quant"" failed to find the read file. (""ls"" command confirms the existence of the read file SRR6269049_2.fastq); ![image](https://user-images.githubusercontent.com/24876498/103170603-66a47600-4880-11eb-9da1-336f96880e4f.png). 2. ; > cmake -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR=/usr/include -DCMAKE_BUILD_TYPE=Release -DNO_IPO=TRUE -DCMAKE_INSTALL_PREFIX=../stage ..; I add the additional ""-DNO_IPO"" flag, but ""salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode""(same as **To Reproduce** point 5) still crashed at fixFasta(), fixFastaMain() with segmentation fault. 3.; I had another issue posted yesterday reporting that the Debug mode is unabled to be compiled successfully, and I'm wondering if there's a resolution for the problem. Thank you:)); title: [salmon v1.4.0 -DCMAKE_BUILD_TYPE=Debug produce compile error: -pg and -fomit-frame-pointer are incompatible #608]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751460320
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751922623:285,Availability,error,error,285,"Hi @kai2june,. I'm glad to hear the pre-compiled one works. To try and compile in debug mode, I suggest the following. Let `salmon` be the top level directory where you checked out the repository. And assume you're not in a fresh checkout (i.e. you already tried to build and got this error). Look at the file:. ```; salmon/external/pufferfish/CMakeLists.txt; ```. on line `131` you should see the following:. ```; set(DEBUG_FLAGS ""-D__STDC_FORMAT_MACROS;-DSTX_NO_STD_STRING_VIEW;-pg;-g;-gstabs""); ```. try removing the `-pg` from this so it reads . ```; set(DEBUG_FLAGS ""-D__STDC_FORMAT_MACROS;-DSTX_NO_STD_STRING_VIEW;-g;-gstabs""); ```. then try to compile again and see if that is able to complete successfully.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751922623
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056:881,Availability,fault,fault,881,"Hi @rob-p ,; After removing ""-pg"" flag in ""salmon/external/pufferfish/CMakeLists.txt"", it's able to be compiled successfully now using Debug mode. To reproduce (in salmon/build directory):; ISSUE 1: The second test failed, I'm wondering whether this should happen or not.; > /root/cmake-3.13.4-Linux-x86_64/bin/cmake -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR=/usr/include -DCMAKE_BUILD_TYPE=Debug -DNO_IPO=TRUE -DCMAKE_INSTALL_PREFIX=../stage ..; > make // to get /root/salmon/external/pufferfish/CMakeLists.txt file; > vim /root/salmon/external/pufferfish/CMakeLists.txt // remove the ""-pg"" flag on line 131 ; > make // successfully compiled after removing ""-pg"" flag; > make install ; > make test; ![second_test_failed](https://user-images.githubusercontent.com/24876498/103263448-e4809280-49e2-11eb-9be9-7bbedfa2f1a5.png). (in /mammoth/salmon_data directory):; ISSUE 2: segmentation fault occurs after ""wrote [count] cleaned references"" (the same place as Release mode); > /root/salmon/stage/bin/salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode (data from your tutorial https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/); ![image](https://user-images.githubusercontent.com/24876498/103263653-75576e00-49e3-11eb-9661-abd69de73a5e.png). gdb /root/salmon/stage/bin/salmon core.23591; (it seems to crash at cereal::OutputArchive, fixFasta, fixFastaMain, etc.); ![image](https://user-images.githubusercontent.com/24876498/103263925-2100be00-49e4-11eb-8918-01f9adf52d98.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056:672,Deployability,install,install,672,"Hi @rob-p ,; After removing ""-pg"" flag in ""salmon/external/pufferfish/CMakeLists.txt"", it's able to be compiled successfully now using Debug mode. To reproduce (in salmon/build directory):; ISSUE 1: The second test failed, I'm wondering whether this should happen or not.; > /root/cmake-3.13.4-Linux-x86_64/bin/cmake -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR=/usr/include -DCMAKE_BUILD_TYPE=Debug -DNO_IPO=TRUE -DCMAKE_INSTALL_PREFIX=../stage ..; > make // to get /root/salmon/external/pufferfish/CMakeLists.txt file; > vim /root/salmon/external/pufferfish/CMakeLists.txt // remove the ""-pg"" flag on line 131 ; > make // successfully compiled after removing ""-pg"" flag; > make install ; > make test; ![second_test_failed](https://user-images.githubusercontent.com/24876498/103263448-e4809280-49e2-11eb-9be9-7bbedfa2f1a5.png). (in /mammoth/salmon_data directory):; ISSUE 2: segmentation fault occurs after ""wrote [count] cleaned references"" (the same place as Release mode); > /root/salmon/stage/bin/salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode (data from your tutorial https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/); ![image](https://user-images.githubusercontent.com/24876498/103263653-75576e00-49e3-11eb-9661-abd69de73a5e.png). gdb /root/salmon/stage/bin/salmon core.23591; (it seems to crash at cereal::OutputArchive, fixFasta, fixFastaMain, etc.); ![image](https://user-images.githubusercontent.com/24876498/103263925-2100be00-49e4-11eb-8918-01f9adf52d98.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056:954,Deployability,Release,Release,954,"Hi @rob-p ,; After removing ""-pg"" flag in ""salmon/external/pufferfish/CMakeLists.txt"", it's able to be compiled successfully now using Debug mode. To reproduce (in salmon/build directory):; ISSUE 1: The second test failed, I'm wondering whether this should happen or not.; > /root/cmake-3.13.4-Linux-x86_64/bin/cmake -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR=/usr/include -DCMAKE_BUILD_TYPE=Debug -DNO_IPO=TRUE -DCMAKE_INSTALL_PREFIX=../stage ..; > make // to get /root/salmon/external/pufferfish/CMakeLists.txt file; > vim /root/salmon/external/pufferfish/CMakeLists.txt // remove the ""-pg"" flag on line 131 ; > make // successfully compiled after removing ""-pg"" flag; > make install ; > make test; ![second_test_failed](https://user-images.githubusercontent.com/24876498/103263448-e4809280-49e2-11eb-9be9-7bbedfa2f1a5.png). (in /mammoth/salmon_data directory):; ISSUE 2: segmentation fault occurs after ""wrote [count] cleaned references"" (the same place as Release mode); > /root/salmon/stage/bin/salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode (data from your tutorial https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/); ![image](https://user-images.githubusercontent.com/24876498/103263653-75576e00-49e3-11eb-9661-abd69de73a5e.png). gdb /root/salmon/stage/bin/salmon core.23591; (it seems to crash at cereal::OutputArchive, fixFasta, fixFastaMain, etc.); ![image](https://user-images.githubusercontent.com/24876498/103263925-2100be00-49e4-11eb-8918-01f9adf52d98.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056:210,Testability,test,test,210,"Hi @rob-p ,; After removing ""-pg"" flag in ""salmon/external/pufferfish/CMakeLists.txt"", it's able to be compiled successfully now using Debug mode. To reproduce (in salmon/build directory):; ISSUE 1: The second test failed, I'm wondering whether this should happen or not.; > /root/cmake-3.13.4-Linux-x86_64/bin/cmake -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR=/usr/include -DCMAKE_BUILD_TYPE=Debug -DNO_IPO=TRUE -DCMAKE_INSTALL_PREFIX=../stage ..; > make // to get /root/salmon/external/pufferfish/CMakeLists.txt file; > vim /root/salmon/external/pufferfish/CMakeLists.txt // remove the ""-pg"" flag on line 131 ; > make // successfully compiled after removing ""-pg"" flag; > make install ; > make test; ![second_test_failed](https://user-images.githubusercontent.com/24876498/103263448-e4809280-49e2-11eb-9be9-7bbedfa2f1a5.png). (in /mammoth/salmon_data directory):; ISSUE 2: segmentation fault occurs after ""wrote [count] cleaned references"" (the same place as Release mode); > /root/salmon/stage/bin/salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode (data from your tutorial https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/); ![image](https://user-images.githubusercontent.com/24876498/103263653-75576e00-49e3-11eb-9661-abd69de73a5e.png). gdb /root/salmon/stage/bin/salmon core.23591; (it seems to crash at cereal::OutputArchive, fixFasta, fixFastaMain, etc.); ![image](https://user-images.githubusercontent.com/24876498/103263925-2100be00-49e4-11eb-8918-01f9adf52d98.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056
https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056:689,Testability,test,test,689,"Hi @rob-p ,; After removing ""-pg"" flag in ""salmon/external/pufferfish/CMakeLists.txt"", it's able to be compiled successfully now using Debug mode. To reproduce (in salmon/build directory):; ISSUE 1: The second test failed, I'm wondering whether this should happen or not.; > /root/cmake-3.13.4-Linux-x86_64/bin/cmake -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR=/usr/include -DCMAKE_BUILD_TYPE=Debug -DNO_IPO=TRUE -DCMAKE_INSTALL_PREFIX=../stage ..; > make // to get /root/salmon/external/pufferfish/CMakeLists.txt file; > vim /root/salmon/external/pufferfish/CMakeLists.txt // remove the ""-pg"" flag on line 131 ; > make // successfully compiled after removing ""-pg"" flag; > make install ; > make test; ![second_test_failed](https://user-images.githubusercontent.com/24876498/103263448-e4809280-49e2-11eb-9be9-7bbedfa2f1a5.png). (in /mammoth/salmon_data directory):; ISSUE 2: segmentation fault occurs after ""wrote [count] cleaned references"" (the same place as Release mode); > /root/salmon/stage/bin/salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode (data from your tutorial https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/); ![image](https://user-images.githubusercontent.com/24876498/103263653-75576e00-49e3-11eb-9661-abd69de73a5e.png). gdb /root/salmon/stage/bin/salmon core.23591; (it seems to crash at cereal::OutputArchive, fixFasta, fixFastaMain, etc.); ![image](https://user-images.githubusercontent.com/24876498/103263925-2100be00-49e4-11eb-8918-01f9adf52d98.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/609#issuecomment-751968056
https://github.com/COMBINE-lab/salmon/issues/611#issuecomment-758028858:1100,Energy Efficiency,power,power,1100,"Hi @AnnaAMonaco ,. Thanks for reaching out and I agree it'd be super useful to have alevin working for both scRNA-seq and scATAC-seq multiome datasets. In short I'd say the framework is not ready yet and there are multiple challenges which we are still working-on to find the right solution. The Central issue is that the technologies to profile open-chromatin regions expects the read to align majorly to non-coding regions and salmon/alevin framework is designed to work (generally) with transcriptomic data. Having said that, one can potentially index the full genome using salmon indexing but we have not yet extensively validated the genomic alignment generated from alevin framework. Once settled, we can certainly figure out ways to run alevin without UMI, that's the easier part. What do I do now ? Basically since the scRNA-seq and scATAC-seq are two different library preps (along with the fastq), I'd still recommend using alevin for scRNA-seq, however, one might have to run other tools (like bwa-mem) to align scATAC-seq data. The are multiple reasons to recommend that, the significant power of alevin comes in with (1) multi-mapping reads but we generally expect low number of such reads with ATAC-seq data (2) UMI deduplication which is absent in the ATAC-seq data and the deduplication happens based on the aligned position. Again, I agree it's great to have a uniform workflow for the multiome data but we are thinking about the challenges in designing such workflow and how solve them. We'd let you know once we have a vignette / tutorial. -- Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/611#issuecomment-758028858
https://github.com/COMBINE-lab/salmon/issues/611#issuecomment-758028858:625,Security,validat,validated,625,"Hi @AnnaAMonaco ,. Thanks for reaching out and I agree it'd be super useful to have alevin working for both scRNA-seq and scATAC-seq multiome datasets. In short I'd say the framework is not ready yet and there are multiple challenges which we are still working-on to find the right solution. The Central issue is that the technologies to profile open-chromatin regions expects the read to align majorly to non-coding regions and salmon/alevin framework is designed to work (generally) with transcriptomic data. Having said that, one can potentially index the full genome using salmon indexing but we have not yet extensively validated the genomic alignment generated from alevin framework. Once settled, we can certainly figure out ways to run alevin without UMI, that's the easier part. What do I do now ? Basically since the scRNA-seq and scATAC-seq are two different library preps (along with the fastq), I'd still recommend using alevin for scRNA-seq, however, one might have to run other tools (like bwa-mem) to align scATAC-seq data. The are multiple reasons to recommend that, the significant power of alevin comes in with (1) multi-mapping reads but we generally expect low number of such reads with ATAC-seq data (2) UMI deduplication which is absent in the ATAC-seq data and the deduplication happens based on the aligned position. Again, I agree it's great to have a uniform workflow for the multiome data but we are thinking about the challenges in designing such workflow and how solve them. We'd let you know once we have a vignette / tutorial. -- Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/611#issuecomment-758028858
https://github.com/COMBINE-lab/salmon/issues/611#issuecomment-758575486:375,Security,validat,validated,375,"Hi @k3yavi,. Thank you for your reply!. > The Central issue is that the technologies to profile open-chromatin regions expects the read to align majorly to non-coding regions and salmon/alevin framework is designed to work (generally) with transcriptomic data. Having said that, one can potentially index the full genome using salmon indexing but we have not yet extensively validated the genomic alignment generated from alevin framework. Yes, this was the first issue I encountered and tried to work around. My attempt to a solution was - as both you and @rob-p suggested - to find a way of binning the reference so it would be treated similarly to transcripts. The underlying thought was that if for scRNA-seq the reference transcriptome acts as a collection of features that the reads are ""compared"" to, the equivalent for scATAC-seq would be known peaks of open chromatin in the given organism/developmental stage/tissue/etc. But please correct me if I'm wrong in this assumption, I am a wet lab-trained biologist trying to understand the analysis tools to my best capability. So at the moment I have a ""reference peak set"" for my scATAC-seq data. I generated this by combining MACS2-called peaks from bulk ATAC-seq I have for the same developmental time-point of both parental and hybrid lines (as I mentioned, I'm working with cross-species hybrids). I used the coordinates from this `mergedPeaks.bed` to extract a `mergedPeaks.fa` from the whole `referenceGenome.fa`, does this sound reasonable?. -Anna",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/611#issuecomment-758575486
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:828,Integrability,depend,depending,828,"Hi @rmurray2,. Thanks again for the detailed question (I answered them in reverse order, so that's why I'm saying ""again"" here). There are a few things going on that could be leading to differences. They are, in the order I think they will have an effect on the result:. * You are using RSEM in a mode that is mapping the reads to the entire genome (using STAR) and then projecting the resulting alignments to the transcriptome. You are using salmon in a way that is performing selective alignment against the transcriptome only. We have recently published [a paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8) discussing in detail the effect that some of these choices can have on transcript and gene-level abundance estimation. In general, if you don't include the genome as a mapping target, depending on your sample, there may be certain reads that are assigned to the transcriptome even though they have a better alignment to some other genomic location. This is independent of e.g. salmon and RSEM, and you'd observe the same thing if you ran RSEM using e.g. Bowtie2 as the aligner aligning against the transcriptome. Luckily, you can control this source of variation. Salmon, like RSEM, can accept alignments to the transcriptome produced by STAR. If you want to see how big of an effect this is having in your sample, you can align reads to the genome using STAR (and project them to the transcriptome) to produce a BAM file that salmon can quantify. You can check RSEM's script to see exactly how it invokes STAR, but the parameters are something like `--outFilterType BySJout --alignSJoverhangMin 8 --outFilterMultimapNmax 20 --alignSJDBoverhangMin 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --eadFilesCommand zcat --outSAMtype BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note tha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:3543,Integrability,depend,depending,3543,"e relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the alignments that it quantifies. This means that to produce RSEM-compatible input, STAR must not align reads that contain indels. While this won't generally have a big effect for many transcripts, it can certainly affect the abundance estimates for transcripts in your sample where the sample you are quantifying has (indel) variation with respect to the reference annotation. We touch upon that a bit as well in the [paper I mentioned above](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). * Finally, and likely the smallest source of potential differences, is that there are other implementation details that differ between salmon and RSEM (e.g. exactly how th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:3660,Modifiability,variab,variable,3660,"efault. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the alignments that it quantifies. This means that to produce RSEM-compatible input, STAR must not align reads that contain indels. While this won't generally have a big effect for many transcripts, it can certainly affect the abundance estimates for transcripts in your sample where the sample you are quantifying has (indel) variation with respect to the reference annotation. We touch upon that a bit as well in the [paper I mentioned above](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). * Finally, and likely the smallest source of potential differences, is that there are other implementation details that differ between salmon and RSEM (e.g. exactly how the fragment length distribution is used to compute the effective transcript length, exactly how the alignment score of a read is used to as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:467,Performance,perform,performing,467,"Hi @rmurray2,. Thanks again for the detailed question (I answered them in reverse order, so that's why I'm saying ""again"" here). There are a few things going on that could be leading to differences. They are, in the order I think they will have an effect on the result:. * You are using RSEM in a mode that is mapping the reads to the entire genome (using STAR) and then projecting the resulting alignments to the transcriptome. You are using salmon in a way that is performing selective alignment against the transcriptome only. We have recently published [a paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8) discussing in detail the effect that some of these choices can have on transcript and gene-level abundance estimation. In general, if you don't include the genome as a mapping target, depending on your sample, there may be certain reads that are assigned to the transcriptome even though they have a better alignment to some other genomic location. This is independent of e.g. salmon and RSEM, and you'd observe the same thing if you ran RSEM using e.g. Bowtie2 as the aligner aligning against the transcriptome. Luckily, you can control this source of variation. Salmon, like RSEM, can accept alignments to the transcriptome produced by STAR. If you want to see how big of an effect this is having in your sample, you can align reads to the genome using STAR (and project them to the transcriptome) to produce a BAM file that salmon can quantify. You can check RSEM's script to see exactly how it invokes STAR, but the parameters are something like `--outFilterType BySJout --alignSJoverhangMin 8 --outFilterMultimapNmax 20 --alignSJDBoverhangMin 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --eadFilesCommand zcat --outSAMtype BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note tha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:2662,Performance,optimiz,optimization,2662,"imapNmax 20 --alignSJDBoverhangMin 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --eadFilesCommand zcat --outSAMtype BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note that last parameter that I will come back to later. Also, the paper referenced above also describes a new capability present in recent versions of salmon that allow it to index the entire genome (as well as the transcriptome) to have the former act as a decoy. This allows avoiding what might otherwise be spurious mappings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate tha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:2863,Performance,optimiz,optimizing,2863," BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note that last parameter that I will come back to later. Also, the paper referenced above also describes a new capability present in recent versions of salmon that allow it to index the entire genome (as well as the transcriptome) to have the former act as a decoy. This allows avoiding what might otherwise be spurious mappings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the align",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:2271,Safety,avoid,avoiding,2271,"this is having in your sample, you can align reads to the genome using STAR (and project them to the transcriptome) to produce a BAM file that salmon can quantify. You can check RSEM's script to see exactly how it invokes STAR, but the parameters are something like `--outFilterType BySJout --alignSJoverhangMin 8 --outFilterMultimapNmax 20 --alignSJDBoverhangMin 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --eadFilesCommand zcat --outSAMtype BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note that last parameter that I will come back to later. Also, the paper referenced above also describes a new capability present in recent versions of salmon that allow it to index the entire genome (as well as the transcriptome) to have the former act as a decoy. This allows avoiding what might otherwise be spurious mappings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:3282,Testability,test,test,3282,"appings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the alignments that it quantifies. This means that to produce RSEM-compatible input, STAR must not align reads that contain indels. While this won't generally have a big effect for many transcripts, it can certainly affect the abundance estimates for transcripts in your sample where the sample you are quantifying has (indel) variation with respect to the reference annotation. We touch upon that a bit as well in the [paper I mentioned above](https://gen",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:2839,Usability,simpl,simply,2839," BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note that last parameter that I will come back to later. Also, the paper referenced above also describes a new capability present in recent versions of salmon that allow it to index the entire genome (as well as the transcriptome) to have the former act as a decoy. This allows avoiding what might otherwise be spurious mappings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the align",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:3672,Usability,simpl,simply,3672,"efault. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the alignments that it quantifies. This means that to produce RSEM-compatible input, STAR must not align reads that contain indels. While this won't generally have a big effect for many transcripts, it can certainly affect the abundance estimates for transcripts in your sample where the sample you are quantifying has (indel) variation with respect to the reference annotation. We touch upon that a bit as well in the [paper I mentioned above](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). * Finally, and likely the smallest source of potential differences, is that there are other implementation details that differ between salmon and RSEM (e.g. exactly how the fragment length distribution is used to compute the effective transcript length, exactly how the alignment score of a read is used to as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:2177,Availability,down,downstream,2177,"e result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the preci",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:3540,Availability,robust,robustly,3540,"against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:4274,Availability,reliab,reliably,4274,"alse confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in the field, these are some solutions we've come up with to address this challenge so far. And while, as a computer scientist myself, I _certainly_ appreciate the desire to be able to have e.g. exactly the same numerical output for a particular sample, we feel that doing so might convey a false sense of certainty in the resulting estimates (and it would also be very difficult to do, technically, given the streaming asynchronous phase of the method). This also means, of course, that you should be wary of the precision between runs even for methods that produce their estimates in 100% deterministic ways (e.g. RSEM, etc.); you may get identical or near identical numbers, but without an estimate of the uncertainty, th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:120,Deployability,release,released,120,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:181,Deployability,release,released,181,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:2654,Deployability,update,update,2654,"transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:3758,Deployability,release,release,3758,"riments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in the field, these are some solutions we've come up with to address this challenge so far. And while, as a computer scientist myself, I _certainly_ appreciate the desire to be able to have e.g. exactly the same numerical output for a particular sample, we feel that doing so might convey a fal",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:1568,Modifiability,variab,variables,1568,"ssue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actua",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:1207,Performance,optimiz,optimization,1207,"r. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you obser",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:1861,Performance,optimiz,optimization,1861,"mon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samp",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:3631,Performance,perform,perform,3631,"pdate order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in the field, these are some solutions we've come up with to address this challenge so far. And while, as a computer scientist myself, I _certainly_ appreciate the desire to ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:553,Safety,detect,detection,553,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:421,Testability,test,test,421,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:3914,Testability,test,test,3914," fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in the field, these are some solutions we've come up with to address this challenge so far. And while, as a computer scientist myself, I _certainly_ appreciate the desire to be able to have e.g. exactly the same numerical output for a particular sample, we feel that doing so might convey a false sense of certainty in the resulting estimates (and it would also be very difficult to do, technically, given the streaming asynchronous p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:885,Usability,simpl,simply,885,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:1753,Usability,simpl,simply,1753,"mon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samp",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:2016,Usability,simpl,simply,2016," exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the samp",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/614#issuecomment-770049957:206,Availability,down,down,206,"hey @kvittingseerup, glad to hear you find refgenie useful. . Currently, there is no way to view the recipe inputs using the web interface or server API. That said, it is possible to track this information down... For example, for [`hg38/salmon_sa_index`](http://refgenomes.databio.org/v2/asset/hg38/salmon_sa_index/splash?tag=default) asset look at the ""asset_parents"" section to find out what assets were used to create the salmon index. Then, by looking at [this file](https://github.com/refgenie/refgenomes.databio.org/blob/master/asset_pep/recipe_inputs.csv) you can find out what are the sources of the files we used to create the parent `fasta*` assets in question. . As an aside, we're working on a [new recipe system](https://github.com/refgenie/refgenie/issues/198) that will enable serving this kind of data on http://refgenomes.databio.org/.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/614#issuecomment-770049957
https://github.com/COMBINE-lab/salmon/issues/614#issuecomment-770049957:129,Integrability,interface,interface,129,"hey @kvittingseerup, glad to hear you find refgenie useful. . Currently, there is no way to view the recipe inputs using the web interface or server API. That said, it is possible to track this information down... For example, for [`hg38/salmon_sa_index`](http://refgenomes.databio.org/v2/asset/hg38/salmon_sa_index/splash?tag=default) asset look at the ""asset_parents"" section to find out what assets were used to create the salmon index. Then, by looking at [this file](https://github.com/refgenie/refgenomes.databio.org/blob/master/asset_pep/recipe_inputs.csv) you can find out what are the sources of the files we used to create the parent `fasta*` assets in question. . As an aside, we're working on a [new recipe system](https://github.com/refgenie/refgenie/issues/198) that will enable serving this kind of data on http://refgenomes.databio.org/.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/614#issuecomment-770049957
https://github.com/COMBINE-lab/salmon/issues/616#issuecomment-764188266:65,Testability,log,log,65,"Hi @gianfilippo,. Thank you for the report and for including the log File. Can you share one of the problematic samples and the reference against which you are aligning? One big difference is that the alignment rate reported by HISAT2 is to the genome, while for salmon it is with respect to the genome. For certain samples (e.g. if you get a bad sample with poor rRNA depletion etc.) you can have many reads align to the genome, but none of them align to the annotated transcriptome. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/616#issuecomment-764188266
https://github.com/COMBINE-lab/salmon/issues/616#issuecomment-764205368:506,Testability,log,log,506,"Hi,. thanks for the prompt reply. you are right, I went back to the FASTQ qc and noticed that this set of; samples all have a very low total number of reads and I would discard them.; I guess, it makes sense that the salmon did not assign fragments to any; transcripts. I should have caught it. Sorry. Thanks for your help. On Wed, Jan 20, 2021 at 9:22 PM Rob Patro <notifications@github.com> wrote:. > Hi @gianfilippo <https://github.com/gianfilippo>,; >; > Thank you for the report and for including the log File. Can you share one; > of the problematic samples and the reference against which you are; > aligning? One big difference is that the alignment rate reported by HISAT2; > is to the genome, while for salmon it is with respect to the genome. For; > certain samples (e.g. if you get a bad sample with poor rRNA depletion; > etc.) you can have many reads align to the genome, but none of them align; > to the annotated transcriptome.; >; > --Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/616#issuecomment-764188266>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACPSFVD3TOPU3IYXVD6ZWKDS26FXVANCNFSM4WL6CV6A>; > .; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/616#issuecomment-764205368
https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-764742021:83,Availability,error,error,83,"The 1 new alert is a false positive from LGTM. The rethrow is done in the uncaught error handler, so it is correct. Now this is a debugging feature to print the stack trace in case of error. It can be removed from the code (although you may be interested in keeping it maybe).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-764742021
https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-764742021:184,Availability,error,error,184,"The 1 new alert is a false positive from LGTM. The rethrow is done in the uncaught error handler, so it is correct. Now this is a debugging feature to print the stack trace in case of error. It can be removed from the code (although you may be interested in keeping it maybe).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-764742021
https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-835404939:27,Availability,error,error,27,"The last patch display the error with `std::cerr << ...`, because `log->critical(...)` does not seem to work. Not sure how to fix it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-835404939
https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-835404939:9,Deployability,patch,patch,9,"The last patch display the error with `std::cerr << ...`, because `log->critical(...)` does not seem to work. Not sure how to fix it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-835404939
https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-835404939:67,Testability,log,log,67,"The last patch display the error with `std::cerr << ...`, because `log->critical(...)` does not seem to work. Not sure how to fix it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/617#issuecomment-835404939
https://github.com/COMBINE-lab/salmon/issues/625#issuecomment-776205702:118,Testability,log,logs,118,"Yea, so Drop-seq data tends to be a bit more noisier than 10x, at least in my experience. ; I had a quick look at the logs and it seems the ""knee"" method is under estimating a bit. I highly recommend playing with the alevin flags from the discussion I forwarded, I think it should work fine with `--expectCells 3000` or `--forceCells 3000`. You can check and process the featureDump file from the alevin if you use `--dumpFeatures` flag to choose the number of barcodes to provide in the expectCells or forceCells command.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/625#issuecomment-776205702
https://github.com/COMBINE-lab/salmon/issues/625#issuecomment-776212333:249,Testability,test,test,249,"Ok, I'll give it a try with the --expectCells flag. I'm working on implementing Salmon and Alevin (along with support for full and partial decoy indexing and preprocessing to add intron flanking sequences for velocity) in GenePattern so I needed to test that flag anyway.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/625#issuecomment-776212333
https://github.com/COMBINE-lab/salmon/issues/625#issuecomment-777002541:92,Availability,error,error,92,"Hey @k3yavi,. Most of the samples ran fine with --expectCells 3000, one did not, it gave an error:; [2021-02-10 18:43:12.296] [alevinLog] [info] Done barcode density calculation.; [2021-02-10 18:43:12.296] [alevinLog] [info] # Barcodes Used: [32m135909784[0m / [31m135909784[0m.; [2021-02-10 18:43:13.337] [alevinLog] [error] Can't find right Boundary.; Please Report this issue on github. Rerunning it with --forceCells 3000 ""worked"" (output was generated). I'm attaching the alevinQC report (renamed to .txt); [MB15_alevinReport.html.txt](https://github.com/COMBINE-lab/salmon/files/5961312/MB15_alevinReport.html.txt); To see if you have any thoughts on what may be going on with this sample.; ; EDIT: Nevermind! Turns out that sample had way more cells than the others in the original study, weird.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/625#issuecomment-777002541
https://github.com/COMBINE-lab/salmon/issues/625#issuecomment-777002541:323,Availability,error,error,323,"Hey @k3yavi,. Most of the samples ran fine with --expectCells 3000, one did not, it gave an error:; [2021-02-10 18:43:12.296] [alevinLog] [info] Done barcode density calculation.; [2021-02-10 18:43:12.296] [alevinLog] [info] # Barcodes Used: [32m135909784[0m / [31m135909784[0m.; [2021-02-10 18:43:13.337] [alevinLog] [error] Can't find right Boundary.; Please Report this issue on github. Rerunning it with --forceCells 3000 ""worked"" (output was generated). I'm attaching the alevinQC report (renamed to .txt); [MB15_alevinReport.html.txt](https://github.com/COMBINE-lab/salmon/files/5961312/MB15_alevinReport.html.txt); To see if you have any thoughts on what may be going on with this sample.; ; EDIT: Nevermind! Turns out that sample had way more cells than the others in the original study, weird.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/625#issuecomment-777002541
https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-776417938:429,Safety,risk,risk,429,"Just in case it helps, I've written a script to splice out cell barcode linker sequences and shift them to before the polyA. In the process of doing this, it also does a 2-distance hamming correction of cell barcode and linker regions. All operations assume there are no INDELs:. https://gitlab.com/gringer/bioinfscripts/-/blob/master/synthSquish.pl. [usual disclaimers apply: I cannot guarantee that this works; use at your own risk]. This script could be used as a stop-gap measure to pre-process Rhapsody reads for use with Alevin via the undocumented custom length settings [--end 5, --barcodeLength 27, --umiLength 8]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-776417938
https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-776417938:542,Usability,undo,undocumented,542,"Just in case it helps, I've written a script to splice out cell barcode linker sequences and shift them to before the polyA. In the process of doing this, it also does a 2-distance hamming correction of cell barcode and linker regions. All operations assume there are no INDELs:. https://gitlab.com/gringer/bioinfscripts/-/blob/master/synthSquish.pl. [usual disclaimers apply: I cannot guarantee that this works; use at your own risk]. This script could be used as a stop-gap measure to pre-process Rhapsody reads for use with Alevin via the undocumented custom length settings [--end 5, --barcodeLength 27, --umiLength 8]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-776417938
https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-777893046:253,Availability,error,error,253,"After discovering the alternative geometry format, I see that unmodified Rhapsody reads should have the following settings:. --umi-geometry '1[53-60]' --bc-geometry '1[1-9,22-30,44-52]' --read-geometry '2[1-end]'. There's a bit of a challenge regarding error correction for the cell barcodes, in that they should be corrected in batches of 9 nucleotides (into 96 clusters of the most commonly-seen sequences).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-777893046
https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-1277030250:406,Deployability,update,update,406,"Rhapsody has introduced a new, shorter cell barcode specification to work with 51bp R1, which looks like this:. ```; 5' PFX - CLS1 - L1 - CLS2 - L2 - CLS3 - UMI - poly(T); 9 4 9 4 9 8 8; [1-9] [14-22] [27-35][36-43]; ```. The linker sequences are as follows:. L1: `GTGA`; L2: `GACA`. In other words,. --umi-geometry '1[36-43]' --bc-geometry '1[1-9,14-22,27-35]' --read-geometry '2[1-end]'. **However...** [update]. In order to remove the need for Lambda spike-ins on Illumina runs, Rhapsody has included a 0-3bp cell barcode prefix, where either nothing, or `A/GT/TCA` are added to the front of some cell barcodes. Here are the full descriptions:. ```; # Long sequence:; # 0123456789012345678901234567890123456789012345678901234567890123456789012345; # [--BC1--][----L1----][--BC2--][----L02----][--BC3--][-UMI1-][TTTTTTTTTTTTTT]; # L1 = ACTGGCCTGCGA; L2 = GGTAGCGGTGACA; # Short sequence:; # 012345678901234567890123456789012345678901234567890; # [--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTTTTT]; # L1 = GTGA; L2 = GACA; # Note: short sequence can also be prepended with A/GT/TCA to improve Illumina base; # call distributions, i.e.; # [--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTTTTT]; # A[--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTTTT]; # GT[--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTTT]; # TCA[--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTT]; ```. This means that the regions defined in the geometry specification above can appear up to 3bp away from their expected region. I've updated my barcode squishing script ([here](https://gitlab.com/gringer/bioinfscripts/-/blob/master/synthSquish.pl)) to account for this. The script identifies the cell barcode regions, corrects cell barcode sequences according to the Rhapsody Bioinformatics manual, and then shifts the linker sequences to after the UMI region, i.e.:. # 012345678901234567890123456789012345678901234567890...; # [--BC1--][--BC2--][--BC3--][-UMI1-][L1][L2][TTTTTT...]. [The prefix sequence is discarded]. After using this scr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-1277030250
https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-1277030250:1494,Deployability,update,updated,1494,"code prefix, where either nothing, or `A/GT/TCA` are added to the front of some cell barcodes. Here are the full descriptions:. ```; # Long sequence:; # 0123456789012345678901234567890123456789012345678901234567890123456789012345; # [--BC1--][----L1----][--BC2--][----L02----][--BC3--][-UMI1-][TTTTTTTTTTTTTT]; # L1 = ACTGGCCTGCGA; L2 = GGTAGCGGTGACA; # Short sequence:; # 012345678901234567890123456789012345678901234567890; # [--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTTTTT]; # L1 = GTGA; L2 = GACA; # Note: short sequence can also be prepended with A/GT/TCA to improve Illumina base; # call distributions, i.e.; # [--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTTTTT]; # A[--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTTTT]; # GT[--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTTT]; # TCA[--BC1--][L1][--BC2--][L2][--BC3--][-UMI1-][TTT]; ```. This means that the regions defined in the geometry specification above can appear up to 3bp away from their expected region. I've updated my barcode squishing script ([here](https://gitlab.com/gringer/bioinfscripts/-/blob/master/synthSquish.pl)) to account for this. The script identifies the cell barcode regions, corrects cell barcode sequences according to the Rhapsody Bioinformatics manual, and then shifts the linker sequences to after the UMI region, i.e.:. # 012345678901234567890123456789012345678901234567890...; # [--BC1--][--BC2--][--BC3--][-UMI1-][L1][L2][TTTTTT...]. [The prefix sequence is discarded]. After using this script to pre-process R1, with both the old and new cell barcode format (both use 9x9x9 cell barcodes), the following geometry can be used for `salmon alevin`:. --umi-geometry '1[28-35]' --bc-geometry '1[1-27]' --read-geometry '2[1-end]'. I've attached files containing the 96 barcodes from each region from my most recent Rhapsody single cell sequencing run (with 51bp R1 reads). These were collected by processing reads 2M-12M from R1 of one of our files, and choosing the most abundant sequences:. ```; zgrep '^[ACGT]\+$",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-1277030250
https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-1298023583:173,Modifiability,enhance,enhanced,173,"It looks like #734 would allow this barcode method to be specified directly:. Long original read geometry - `1{b[9]f[ACTGGCCTGCGA]b[9]f[GGTAGCGGTGACA]b[9]u[8]}2{r}`; Short ""enhanced"" read geometry - `1{x[0-3]b[9]f[GTGA]b[9]f[GACA]b[9]u[8]}2{r}`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-1298023583
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779409595:95,Integrability,message,message,95,"Hi Dan,. Can you share an example of a dataset where you encounter this? It's a pretty generic message, and I'm not actually certain where it's coming from without some more context. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779409595
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548:483,Integrability,wrap,wrapping,483,"Hi Rob, . For example, here is the log output when I try to index the GENCODE Human transcript set v36, using the below code;. salmon index --keepDuplicates -k 35 --gencode -t gencode.v36.transcripts.fa -i Human_v36_Index_k35. Here is where the phrase is found in the log, and is then repeated a lot until the end. Number of ones: 1309432; Number of ones per inventory item: 512; Inventory entries filled: 2558; 1309432; [2021-02-15 04:42:27.548] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2021-02-15 04:42:27.565] [puff::index::jointLog] [info] contig count for validation: 1,309,432; [2021-02-15 04:42:28.338] [puff::index::jointLog] [info] Total # of Contigs : 1,309,432; [2021-02-15 04:42:28.339] [puff::index::jointLog] [info] Total # of numerical Contigs : 1,309,432; [2021-02-15 04:42:28.404] [puff::index::jointLog] [info] Total # of contig vec entries: 7,119,643; [2021-02-15 04:42:28.404] [puff::index::jointLog] [info] bits per offset entry 23; [2021-02-15 04:42:28.590] [puff::index::jointLog] [info] Done constructing the contig vector. 1309433; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] # segments = 1,309,432; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] total length = 188,284,293; [2021-02-15 04:42:29.548] [puff::index::jointLog] [info] Reading the reference files ...; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] positional integer width = 28; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] seqSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] rankSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] edgeVecSize = 0; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] num keys = 143,763,605; len should not be greater than 64.; ...; ...; ...; len should not be greater than 64.; [2021-02-15 05:07:13.459] [puff::index::jointLog] [info] finished populating pos vector; [2021-02-15 05:07:13.460] [puff::index::jointLog] [info] wr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548:610,Security,validat,validation,610,"Hi Rob, . For example, here is the log output when I try to index the GENCODE Human transcript set v36, using the below code;. salmon index --keepDuplicates -k 35 --gencode -t gencode.v36.transcripts.fa -i Human_v36_Index_k35. Here is where the phrase is found in the log, and is then repeated a lot until the end. Number of ones: 1309432; Number of ones per inventory item: 512; Inventory entries filled: 2558; 1309432; [2021-02-15 04:42:27.548] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2021-02-15 04:42:27.565] [puff::index::jointLog] [info] contig count for validation: 1,309,432; [2021-02-15 04:42:28.338] [puff::index::jointLog] [info] Total # of Contigs : 1,309,432; [2021-02-15 04:42:28.339] [puff::index::jointLog] [info] Total # of numerical Contigs : 1,309,432; [2021-02-15 04:42:28.404] [puff::index::jointLog] [info] Total # of contig vec entries: 7,119,643; [2021-02-15 04:42:28.404] [puff::index::jointLog] [info] bits per offset entry 23; [2021-02-15 04:42:28.590] [puff::index::jointLog] [info] Done constructing the contig vector. 1309433; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] # segments = 1,309,432; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] total length = 188,284,293; [2021-02-15 04:42:29.548] [puff::index::jointLog] [info] Reading the reference files ...; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] positional integer width = 28; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] seqSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] rankSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] edgeVecSize = 0; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] num keys = 143,763,605; len should not be greater than 64.; ...; ...; ...; len should not be greater than 64.; [2021-02-15 05:07:13.459] [puff::index::jointLog] [info] finished populating pos vector; [2021-02-15 05:07:13.460] [puff::index::jointLog] [info] wr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548:35,Testability,log,log,35,"Hi Rob, . For example, here is the log output when I try to index the GENCODE Human transcript set v36, using the below code;. salmon index --keepDuplicates -k 35 --gencode -t gencode.v36.transcripts.fa -i Human_v36_Index_k35. Here is where the phrase is found in the log, and is then repeated a lot until the end. Number of ones: 1309432; Number of ones per inventory item: 512; Inventory entries filled: 2558; 1309432; [2021-02-15 04:42:27.548] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2021-02-15 04:42:27.565] [puff::index::jointLog] [info] contig count for validation: 1,309,432; [2021-02-15 04:42:28.338] [puff::index::jointLog] [info] Total # of Contigs : 1,309,432; [2021-02-15 04:42:28.339] [puff::index::jointLog] [info] Total # of numerical Contigs : 1,309,432; [2021-02-15 04:42:28.404] [puff::index::jointLog] [info] Total # of contig vec entries: 7,119,643; [2021-02-15 04:42:28.404] [puff::index::jointLog] [info] bits per offset entry 23; [2021-02-15 04:42:28.590] [puff::index::jointLog] [info] Done constructing the contig vector. 1309433; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] # segments = 1,309,432; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] total length = 188,284,293; [2021-02-15 04:42:29.548] [puff::index::jointLog] [info] Reading the reference files ...; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] positional integer width = 28; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] seqSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] rankSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] edgeVecSize = 0; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] num keys = 143,763,605; len should not be greater than 64.; ...; ...; ...; len should not be greater than 64.; [2021-02-15 05:07:13.459] [puff::index::jointLog] [info] finished populating pos vector; [2021-02-15 05:07:13.460] [puff::index::jointLog] [info] wr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548:268,Testability,log,log,268,"Hi Rob, . For example, here is the log output when I try to index the GENCODE Human transcript set v36, using the below code;. salmon index --keepDuplicates -k 35 --gencode -t gencode.v36.transcripts.fa -i Human_v36_Index_k35. Here is where the phrase is found in the log, and is then repeated a lot until the end. Number of ones: 1309432; Number of ones per inventory item: 512; Inventory entries filled: 2558; 1309432; [2021-02-15 04:42:27.548] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2021-02-15 04:42:27.565] [puff::index::jointLog] [info] contig count for validation: 1,309,432; [2021-02-15 04:42:28.338] [puff::index::jointLog] [info] Total # of Contigs : 1,309,432; [2021-02-15 04:42:28.339] [puff::index::jointLog] [info] Total # of numerical Contigs : 1,309,432; [2021-02-15 04:42:28.404] [puff::index::jointLog] [info] Total # of contig vec entries: 7,119,643; [2021-02-15 04:42:28.404] [puff::index::jointLog] [info] bits per offset entry 23; [2021-02-15 04:42:28.590] [puff::index::jointLog] [info] Done constructing the contig vector. 1309433; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] # segments = 1,309,432; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] total length = 188,284,293; [2021-02-15 04:42:29.548] [puff::index::jointLog] [info] Reading the reference files ...; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] positional integer width = 28; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] seqSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] rankSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] edgeVecSize = 0; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] num keys = 143,763,605; len should not be greater than 64.; ...; ...; ...; len should not be greater than 64.; [2021-02-15 05:07:13.459] [puff::index::jointLog] [info] finished populating pos vector; [2021-02-15 05:07:13.460] [puff::index::jointLog] [info] wr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548:2267,Testability,log,log,2267,"1-02-15 04:42:28.404] [puff::index::jointLog] [info] Total # of contig vec entries: 7,119,643; [2021-02-15 04:42:28.404] [puff::index::jointLog] [info] bits per offset entry 23; [2021-02-15 04:42:28.590] [puff::index::jointLog] [info] Done constructing the contig vector. 1309433; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] # segments = 1,309,432; [2021-02-15 04:42:29.459] [puff::index::jointLog] [info] total length = 188,284,293; [2021-02-15 04:42:29.548] [puff::index::jointLog] [info] Reading the reference files ...; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] positional integer width = 28; [2021-02-15 04:42:31.594] [puff::index::jointLog] [info] seqSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] rankSize = 188,284,293; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] edgeVecSize = 0; [2021-02-15 04:42:31.595] [puff::index::jointLog] [info] num keys = 143,763,605; len should not be greater than 64.; ...; ...; ...; len should not be greater than 64.; [2021-02-15 05:07:13.459] [puff::index::jointLog] [info] finished populating pos vector; [2021-02-15 05:07:13.460] [puff::index::jointLog] [info] writing index components; [2021-02-15 05:07:13.760] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2021-02-15 05:07:13.810] [jLog] [info] done building index. When I have just run quantification using this index, it appears again here in the log;. salmon quant -i .../Human_v36_Index_k35 -l A --seqBias --gcBias --posBias -p 12 -o ../Sample_${i}/ -1 ${r1} -2 ${r2} . [2021-02-15 19:11:59.260] [jointLog] [info] done; [2021-02-15 19:11:59.260] [jointLog] [info] Index contained 232,117 targets; [2021-02-15 19:12:05.495] [jointLog] [info] Number of decoys : 0. len should not be greater than 64.; len should not be greater than 64.; len should not be greater than 64.; ...; ...; ...; [It hasn't finished yet]. I've had this issue before, too, but couldn't work out what the problem was. . Thanks,; Dan",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779416548
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628:764,Availability,error,errors,764,"Hi @danphillips28,. Oh ok! Now I see what's going on. We really should be more stringent about catching the issue here. . The problem is that using k > 32 is not permitted in the current implementation. This is because we use a 64-bit machine word to encode k-mers, and you can only fit up to 32 nucleotides in a word. In reality, anything > 31 is not allowed, since the k-mer should be odd so that the orientation can be inferred from the canonical (lexicographically smaller) version. There is nothing fundamentally problematic about using a larger k, it's just that it would require some modifications throughout the codebase we've not yet made. Further, we've not really found cases where having such large k really help. Specifically, the larger k, the fewer errors you need to render a read unmappable (if there is an error in every k-mer, you can't map the read). On the other hand, selective alignment will do a good job of filtering out poor matches where there is insufficient similarity between the read and reference. Thus, we set a default value of k=31. You can go lower (with any odd value), but it's not currently possible to go higher. I hope this clears things up, and thanks for bringing this to our attention. I think the indexer should bail with an error in this case, until (and unless) this feature is supported. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628:824,Availability,error,error,824,"Hi @danphillips28,. Oh ok! Now I see what's going on. We really should be more stringent about catching the issue here. . The problem is that using k > 32 is not permitted in the current implementation. This is because we use a 64-bit machine word to encode k-mers, and you can only fit up to 32 nucleotides in a word. In reality, anything > 31 is not allowed, since the k-mer should be odd so that the orientation can be inferred from the canonical (lexicographically smaller) version. There is nothing fundamentally problematic about using a larger k, it's just that it would require some modifications throughout the codebase we've not yet made. Further, we've not really found cases where having such large k really help. Specifically, the larger k, the fewer errors you need to render a read unmappable (if there is an error in every k-mer, you can't map the read). On the other hand, selective alignment will do a good job of filtering out poor matches where there is insufficient similarity between the read and reference. Thus, we set a default value of k=31. You can go lower (with any odd value), but it's not currently possible to go higher. I hope this clears things up, and thanks for bringing this to our attention. I think the indexer should bail with an error in this case, until (and unless) this feature is supported. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628:1270,Availability,error,error,1270,"Hi @danphillips28,. Oh ok! Now I see what's going on. We really should be more stringent about catching the issue here. . The problem is that using k > 32 is not permitted in the current implementation. This is because we use a 64-bit machine word to encode k-mers, and you can only fit up to 32 nucleotides in a word. In reality, anything > 31 is not allowed, since the k-mer should be odd so that the orientation can be inferred from the canonical (lexicographically smaller) version. There is nothing fundamentally problematic about using a larger k, it's just that it would require some modifications throughout the codebase we've not yet made. Further, we've not really found cases where having such large k really help. Specifically, the larger k, the fewer errors you need to render a read unmappable (if there is an error in every k-mer, you can't map the read). On the other hand, selective alignment will do a good job of filtering out poor matches where there is insufficient similarity between the read and reference. Thus, we set a default value of k=31. You can go lower (with any odd value), but it's not currently possible to go higher. I hope this clears things up, and thanks for bringing this to our attention. I think the indexer should bail with an error in this case, until (and unless) this feature is supported. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628:1165,Usability,clear,clears,1165,"Hi @danphillips28,. Oh ok! Now I see what's going on. We really should be more stringent about catching the issue here. . The problem is that using k > 32 is not permitted in the current implementation. This is because we use a 64-bit machine word to encode k-mers, and you can only fit up to 32 nucleotides in a word. In reality, anything > 31 is not allowed, since the k-mer should be odd so that the orientation can be inferred from the canonical (lexicographically smaller) version. There is nothing fundamentally problematic about using a larger k, it's just that it would require some modifications throughout the codebase we've not yet made. Further, we've not really found cases where having such large k really help. Specifically, the larger k, the fewer errors you need to render a read unmappable (if there is an error in every k-mer, you can't map the read). On the other hand, selective alignment will do a good job of filtering out poor matches where there is insufficient similarity between the read and reference. Thus, we set a default value of k=31. You can go lower (with any odd value), but it's not currently possible to go higher. I hope this clears things up, and thanks for bringing this to our attention. I think the indexer should bail with an error in this case, until (and unless) this feature is supported. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779421124:91,Usability,simpl,simply,91,"Hi Rob,. Thanks! That's brilliant. Just what I needed. Now I've realised what I've done. I simply hadn't checked the manual for a while and in my mind the default (max) k had morphed from 31 into 35 (silly me, really need to check the manual more). . All the best, and thanks again!; Dan",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779421124
https://github.com/COMBINE-lab/salmon/issues/637#issuecomment-792324222:266,Availability,down,down,266,"On Sun, Mar 07, 2021 at 09:55:59AM -0800, Rob Patro wrote:; > This seems to explain the source of the issue : https://twitter.com/dpryan79/status/1368116490801717251?s=19. It looks like it will be fixed upstream (hopefully soon). excellent, thanks for tracking that down! Hopefully this issue can serve; as google bait for others wondering how to deal with this error :)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/637#issuecomment-792324222
https://github.com/COMBINE-lab/salmon/issues/637#issuecomment-792324222:362,Availability,error,error,362,"On Sun, Mar 07, 2021 at 09:55:59AM -0800, Rob Patro wrote:; > This seems to explain the source of the issue : https://twitter.com/dpryan79/status/1368116490801717251?s=19. It looks like it will be fixed upstream (hopefully soon). excellent, thanks for tracking that down! Hopefully this issue can serve; as google bait for others wondering how to deal with this error :)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/637#issuecomment-792324222
https://github.com/COMBINE-lab/salmon/issues/640#issuecomment-857337814:84,Deployability,Integrat,Integrative,84,"I appreciate to your answer.; Thanks a lot; ; ; Ki-Wook Lee; Student; Department of Integrative Biotechnology; College of Biotechnology & Bioengineering; Sungkyunkwan University; Biotechnology and Bioengineering Building 2, Rm 62156; 2066 Seobu-ro, Jangan-gu, Suwon, Gyeonggi, 16419, Republic of Korea; Tel: +82-10-5580-1770 Fax:+82-31-290-7870; ; -----Original Message-----; From: ""Anthony S. ***@***.***>; To: ***@***.***>;; Cc: ***@***.***>; ***@***.***>;; Sent: 2021-06-09 (수) 05:16:26 (GMT+09:00); Subject: Re: [COMBINE-lab/salmon] Quant.sf index issue (#640); ; Not affiliated with the Salmon team, but since you didn't get an answer here...; When building an index with transcriptomes from Gencode, you should pass the flag ""--gencode"" to the indexer. This allows salmon to split the record names on the | character and gives you the expected ""ENST00000456328.2 or ENSG00000223972.5"" style names.; —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/640#issuecomment-857337814
https://github.com/COMBINE-lab/salmon/issues/640#issuecomment-857337814:84,Integrability,Integrat,Integrative,84,"I appreciate to your answer.; Thanks a lot; ; ; Ki-Wook Lee; Student; Department of Integrative Biotechnology; College of Biotechnology & Bioengineering; Sungkyunkwan University; Biotechnology and Bioengineering Building 2, Rm 62156; 2066 Seobu-ro, Jangan-gu, Suwon, Gyeonggi, 16419, Republic of Korea; Tel: +82-10-5580-1770 Fax:+82-31-290-7870; ; -----Original Message-----; From: ""Anthony S. ***@***.***>; To: ***@***.***>;; Cc: ***@***.***>; ***@***.***>;; Sent: 2021-06-09 (수) 05:16:26 (GMT+09:00); Subject: Re: [COMBINE-lab/salmon] Quant.sf index issue (#640); ; Not affiliated with the Salmon team, but since you didn't get an answer here...; When building an index with transcriptomes from Gencode, you should pass the flag ""--gencode"" to the indexer. This allows salmon to split the record names on the | character and gives you the expected ""ENST00000456328.2 or ENSG00000223972.5"" style names.; —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/640#issuecomment-857337814
https://github.com/COMBINE-lab/salmon/issues/640#issuecomment-857337814:362,Integrability,Message,Message,362,"I appreciate to your answer.; Thanks a lot; ; ; Ki-Wook Lee; Student; Department of Integrative Biotechnology; College of Biotechnology & Bioengineering; Sungkyunkwan University; Biotechnology and Bioengineering Building 2, Rm 62156; 2066 Seobu-ro, Jangan-gu, Suwon, Gyeonggi, 16419, Republic of Korea; Tel: +82-10-5580-1770 Fax:+82-31-290-7870; ; -----Original Message-----; From: ""Anthony S. ***@***.***>; To: ***@***.***>;; Cc: ***@***.***>; ***@***.***>;; Sent: 2021-06-09 (수) 05:16:26 (GMT+09:00); Subject: Re: [COMBINE-lab/salmon] Quant.sf index issue (#640); ; Not affiliated with the Salmon team, but since you didn't get an answer here...; When building an index with transcriptomes from Gencode, you should pass the flag ""--gencode"" to the indexer. This allows salmon to split the record names on the | character and gives you the expected ""ENST00000456328.2 or ENSG00000223972.5"" style names.; —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/640#issuecomment-857337814
https://github.com/COMBINE-lab/salmon/issues/641#issuecomment-809806782:126,Deployability,release,release,126,"I noticed that I am using ""salmon quant"" version 1.4.0. My index was created; using ""salmon index version 1.3.0"". I check the release notes. It does not mention any backwards compatibility issue. Could the version miss-match be my bug?. kind regards. Andy",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/641#issuecomment-809806782
https://github.com/COMBINE-lab/salmon/issues/641#issuecomment-809864805:57,Availability,error,error,57,I tried using salmon quant v 1.3.0. I still get the same error,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/641#issuecomment-809864805
https://github.com/COMBINE-lab/salmon/issues/641#issuecomment-817390863:12,Availability,error,error,12,I think the error was caused by running out of memory,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/641#issuecomment-817390863
https://github.com/COMBINE-lab/salmon/issues/643#issuecomment-2113100892:49,Availability,down,down,49,"Nevermind, I figured it out! It seems this boils down to specifying the number of cores you request (if using HPC). I'm super new to this so I hope this was helpful!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/643#issuecomment-2113100892
https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815252574:860,Availability,error,errors,860,"Hi, ; I have same issue using Salmon 1.4.0. * `grep ""^>"" <GRCh38.d1.vd1.fa | cut -d "" "" -f 1 > decoys.txt`; * `sed -i.bak -e 's/>//g' decoys.txt`; * `cat GDC.GRCh38_gencode.v22.fasta.gz GRCh38.d1.vd1.fa.gz > GRCh38.d1.vd1_gencode.v22_gentrome.fa.gz`; * `salmon index -t GRCh38.d1.vd1_gencode.v22_gentrome.fa.gz -d decoys.txt -i salmon_1.4.0/ -p 12 -k 31 --gencode`. [2021-04-07 12:31:14.685] [puff::index::jointLog] [warning] Removed 789 transcripts that were sequence duplicates of indexed transcripts.; [2021-04-07 12:31:14.685] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-04-07 12:31:14.685] [puff::index::jointLog] [critical] The decoy file contained the names of 2779 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-04-07 12:31:15.194] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815252574
https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815252574:867,Availability,down,downstream,867,"Hi, ; I have same issue using Salmon 1.4.0. * `grep ""^>"" <GRCh38.d1.vd1.fa | cut -d "" "" -f 1 > decoys.txt`; * `sed -i.bak -e 's/>//g' decoys.txt`; * `cat GDC.GRCh38_gencode.v22.fasta.gz GRCh38.d1.vd1.fa.gz > GRCh38.d1.vd1_gencode.v22_gentrome.fa.gz`; * `salmon index -t GRCh38.d1.vd1_gencode.v22_gentrome.fa.gz -d decoys.txt -i salmon_1.4.0/ -p 12 -k 31 --gencode`. [2021-04-07 12:31:14.685] [puff::index::jointLog] [warning] Removed 789 transcripts that were sequence duplicates of indexed transcripts.; [2021-04-07 12:31:14.685] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-04-07 12:31:14.685] [puff::index::jointLog] [critical] The decoy file contained the names of 2779 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-04-07 12:31:15.194] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815252574
https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815252574:1024,Availability,error,error,1024,"Hi, ; I have same issue using Salmon 1.4.0. * `grep ""^>"" <GRCh38.d1.vd1.fa | cut -d "" "" -f 1 > decoys.txt`; * `sed -i.bak -e 's/>//g' decoys.txt`; * `cat GDC.GRCh38_gencode.v22.fasta.gz GRCh38.d1.vd1.fa.gz > GRCh38.d1.vd1_gencode.v22_gentrome.fa.gz`; * `salmon index -t GRCh38.d1.vd1_gencode.v22_gentrome.fa.gz -d decoys.txt -i salmon_1.4.0/ -p 12 -k 31 --gencode`. [2021-04-07 12:31:14.685] [puff::index::jointLog] [warning] Removed 789 transcripts that were sequence duplicates of indexed transcripts.; [2021-04-07 12:31:14.685] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-04-07 12:31:14.685] [puff::index::jointLog] [critical] The decoy file contained the names of 2779 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-04-07 12:31:15.194] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815252574
https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815273216:49,Availability,error,error,49,"Hey,. So I solved the issue over the weekend. My error was caused by my decoy file having my chromosome names with > in front of them meaning it couldn't find them when creating the index.; `; Instead of for example . > grep ""^>"" <(zcat GRCm38.primary_assembly.genome.fa.gz) | cut -d "" "" -f 1 > decoys.txt. > sed -i -e 's/>//g' decoys.txt'. `; I used the following instead. ; `. > grep ""^>"" <(gunzip -c GRCm38.primary_assembly.genome.fa.gz) | cut -d "" "" -f 1 > decoys.txt. > sed -i.bak -e 's/>//g' decoys.txt. `; I'm guessing this isn't your issue but just in case it helps! I would check your ""gentrome"" file and look if the decoy and names within your gentrome file match up properly. That's how I realized the > was disrupting my analysis!. Hope this helps. . Best Regards,; Adrian",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815273216
https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815384486:18,Availability,error,error,18,"thanks, I find my error, it was also the extract of the name. ; thanks for your advice!; Tiphaine",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/644#issuecomment-815384486
https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821477730:146,Testability,log,logging,146,"hi @jma1991 ,. Thanks for reaching out. You may have already noticed it but may I ask do you expect such a low number of read mapping ? Alevin is logging only 3% of the reads as mapped.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821477730
https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821522673:248,Availability,down,downloaded,248,"Hey @k3yavi . The data is from a public dataset hosted on the 10x genomics website:. [200 Sorted Cells from Human Glioblastoma Multiforme, 3’ LT v3.1](https://support.10xgenomics.com/single-cell-gene-expression/datasets/6.0.0/Brain_Tumor_3p_LT). I downloaded the data and subsampled the FASTQ files to 1,000 reads. It was an arbitary choice, I just needed a small dataset to test a pipeline I was building.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821522673
https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821522673:382,Deployability,pipeline,pipeline,382,"Hey @k3yavi . The data is from a public dataset hosted on the 10x genomics website:. [200 Sorted Cells from Human Glioblastoma Multiforme, 3’ LT v3.1](https://support.10xgenomics.com/single-cell-gene-expression/datasets/6.0.0/Brain_Tumor_3p_LT). I downloaded the data and subsampled the FASTQ files to 1,000 reads. It was an arbitary choice, I just needed a small dataset to test a pipeline I was building.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821522673
https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821522673:375,Testability,test,test,375,"Hey @k3yavi . The data is from a public dataset hosted on the 10x genomics website:. [200 Sorted Cells from Human Glioblastoma Multiforme, 3’ LT v3.1](https://support.10xgenomics.com/single-cell-gene-expression/datasets/6.0.0/Brain_Tumor_3p_LT). I downloaded the data and subsampled the FASTQ files to 1,000 reads. It was an arbitary choice, I just needed a small dataset to test a pipeline I was building.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821522673
https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821529523:560,Performance,optimiz,optimizer,560,"Oh, so multiple things can go wrong based on how you sampled the read like CB frequency not being aligning with the expected experiment. I'd say if you have to try a small experiment, may be sample all the reads from say ~10 Cellular barcode and specify them to alevin using `--whitelist` flag. I just tested the data it seems to work with the following log.; ```; [2021-04-16 15:57:26.183] [jointLog] [info] Mapping rate = 48.8769%. [2021-04-16 15:57:26.183] [jointLog] [info] finished quantifyLibrary(); [2021-04-16 15:57:26.360] [alevinLog] [info] Starting optimizer; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821529523
https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821529523:302,Testability,test,tested,302,"Oh, so multiple things can go wrong based on how you sampled the read like CB frequency not being aligning with the expected experiment. I'd say if you have to try a small experiment, may be sample all the reads from say ~10 Cellular barcode and specify them to alevin using `--whitelist` flag. I just tested the data it seems to work with the following log.; ```; [2021-04-16 15:57:26.183] [jointLog] [info] Mapping rate = 48.8769%. [2021-04-16 15:57:26.183] [jointLog] [info] finished quantifyLibrary(); [2021-04-16 15:57:26.360] [alevinLog] [info] Starting optimizer; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821529523
https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821529523:354,Testability,log,log,354,"Oh, so multiple things can go wrong based on how you sampled the read like CB frequency not being aligning with the expected experiment. I'd say if you have to try a small experiment, may be sample all the reads from say ~10 Cellular barcode and specify them to alevin using `--whitelist` flag. I just tested the data it seems to work with the following log.; ```; [2021-04-16 15:57:26.183] [jointLog] [info] Mapping rate = 48.8769%. [2021-04-16 15:57:26.183] [jointLog] [info] finished quantifyLibrary(); [2021-04-16 15:57:26.360] [alevinLog] [info] Starting optimizer; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/647#issuecomment-821529523
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:768,Availability,error,error,768,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:2240,Availability,error,error,2240," protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrection` flag *is* better for ONT data, though results without this flag are sub-optimal, they are not unusable. We have let ONT know about this, and I would suspect they will address it (perhaps they'll even accept a PR?). Finally, a long read error model has been created and will _hopefully_ make it to the next version of salmon. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:861,Deployability,release,release,861,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:283,Integrability,depend,dependent,283,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:340,Integrability,protocol,protocols,340,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:1444,Integrability,depend,depends,1444," protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrection` flag *is* better for ONT data, though results without this flag are sub-optimal, they are not unusable. We have let ONT know about this, and I would suspect they will address it (perhaps they'll even accept a PR?). Finally, a long read error model has been created and will _hopefully_ make it to the next version of salmon. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:703,Performance,optimiz,optimize,703,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:457,Testability,test,tested,457,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:968,Usability,simpl,simple,968,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/652#issuecomment-1138630790:162,Deployability,release,release,162,"There is not a known bug here, but this is certainly strange. Given that version 0.14.1 is very old now, I think the best thing to do is just put a caveat on the release page.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/652#issuecomment-1138630790
https://github.com/COMBINE-lab/salmon/issues/653#issuecomment-823560566:886,Availability,down,down,886,"Thanks for the detailed report @idinsmore1,. These are quite different versions of ensembl, and so changes in the underlying transcriptome can absolutely have an effect on estimated abundances. Specifically, as the newer releases of ensemble tend to annotate more and more isoforms, there are more potential explanations for the reads. Reads that may have been previously assigned to an isoform in the old annotation may better match to a new isoform in the new annotation, etc. This will affect the read assignment and TPM both to the isoform to which the reads were originally being assigned and the new isoform to which the reads are now being assigned. . One thing to look at would be to see how much things change at the gene level, where we'd expect mapping uncertainty to be much lower. Once there's an idea of the types of things that are changing, it will be possible to drill down a bit more to try and figure out exactly what's going on. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/653#issuecomment-823560566
https://github.com/COMBINE-lab/salmon/issues/653#issuecomment-823560566:221,Deployability,release,releases,221,"Thanks for the detailed report @idinsmore1,. These are quite different versions of ensembl, and so changes in the underlying transcriptome can absolutely have an effect on estimated abundances. Specifically, as the newer releases of ensemble tend to annotate more and more isoforms, there are more potential explanations for the reads. Reads that may have been previously assigned to an isoform in the old annotation may better match to a new isoform in the new annotation, etc. This will affect the read assignment and TPM both to the isoform to which the reads were originally being assigned and the new isoform to which the reads are now being assigned. . One thing to look at would be to see how much things change at the gene level, where we'd expect mapping uncertainty to be much lower. Once there's an idea of the types of things that are changing, it will be possible to drill down a bit more to try and figure out exactly what's going on. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/653#issuecomment-823560566
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-825393464:135,Availability,down,down,135,"Hi @rached-97,. First of all, thank you for the _incredibly-detailed_ report. All of the information you provided made it easy to pull down the data and to test what might be going on. I pulled down the first sample, consisting of `SRR9071838_1.fastq` and `SRR9071838_2.fastq`, which was recognized as `IU` for you. . However, since I didn't have access to the annotation you used or the specific scripts you used to extract the transcriptome reference, I instead quantified directly against [gencode v37](ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_37/gencode.v37.transcripts.fa.gz). When I did this, salmon calls the library format type as `ISR`, which is what we would expect. The `lib_format_count.json` is as such:. ```; {; ""read_files"": ""[ SRR9071838_1.fastq.gz, SRR9071838_2.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 31944161,; ""num_assigned_fragments"": 31944161,; ""num_frags_with_concordant_consistent_mappings"": 29445487,; ""num_frags_with_inconsistent_or_orphan_mappings"": 2576421,; ""strand_mapping_bias"": 0.000022006283676957945,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 648,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 29445487,; ""SF"": 1098610,; ""SR"": 1477163,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```. As you can see, the strand mapping bias reported is `0.000022006283676957945` (to an insanely higher level of precision than we actually need). This is, of course, drastically different from the value of `0.36810071818291797` that showed up in your table for this sample. While it is true that salmon is quite conservative about calling a library as stranded (i.e. it would rather make the mistake of calling stranded library as unstranded than vice-versa, as the latter would discard reads while the former would not), in this case it looks like the culprit is likely the transcriptome reference being used. When quantified under the standard gencode transcriptome, this sample is inferred as `ISR` with very high confidence ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-825393464
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-825393464:194,Availability,down,down,194,"Hi @rached-97,. First of all, thank you for the _incredibly-detailed_ report. All of the information you provided made it easy to pull down the data and to test what might be going on. I pulled down the first sample, consisting of `SRR9071838_1.fastq` and `SRR9071838_2.fastq`, which was recognized as `IU` for you. . However, since I didn't have access to the annotation you used or the specific scripts you used to extract the transcriptome reference, I instead quantified directly against [gencode v37](ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_37/gencode.v37.transcripts.fa.gz). When I did this, salmon calls the library format type as `ISR`, which is what we would expect. The `lib_format_count.json` is as such:. ```; {; ""read_files"": ""[ SRR9071838_1.fastq.gz, SRR9071838_2.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 31944161,; ""num_assigned_fragments"": 31944161,; ""num_frags_with_concordant_consistent_mappings"": 29445487,; ""num_frags_with_inconsistent_or_orphan_mappings"": 2576421,; ""strand_mapping_bias"": 0.000022006283676957945,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 648,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 29445487,; ""SF"": 1098610,; ""SR"": 1477163,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```. As you can see, the strand mapping bias reported is `0.000022006283676957945` (to an insanely higher level of precision than we actually need). This is, of course, drastically different from the value of `0.36810071818291797` that showed up in your table for this sample. While it is true that salmon is quite conservative about calling a library as stranded (i.e. it would rather make the mistake of calling stranded library as unstranded than vice-versa, as the latter would discard reads while the former would not), in this case it looks like the culprit is likely the transcriptome reference being used. When quantified under the standard gencode transcriptome, this sample is inferred as `ISR` with very high confidence ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-825393464
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-825393464:347,Security,access,access,347,"Hi @rached-97,. First of all, thank you for the _incredibly-detailed_ report. All of the information you provided made it easy to pull down the data and to test what might be going on. I pulled down the first sample, consisting of `SRR9071838_1.fastq` and `SRR9071838_2.fastq`, which was recognized as `IU` for you. . However, since I didn't have access to the annotation you used or the specific scripts you used to extract the transcriptome reference, I instead quantified directly against [gencode v37](ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_37/gencode.v37.transcripts.fa.gz). When I did this, salmon calls the library format type as `ISR`, which is what we would expect. The `lib_format_count.json` is as such:. ```; {; ""read_files"": ""[ SRR9071838_1.fastq.gz, SRR9071838_2.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 31944161,; ""num_assigned_fragments"": 31944161,; ""num_frags_with_concordant_consistent_mappings"": 29445487,; ""num_frags_with_inconsistent_or_orphan_mappings"": 2576421,; ""strand_mapping_bias"": 0.000022006283676957945,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 648,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 29445487,; ""SF"": 1098610,; ""SR"": 1477163,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```. As you can see, the strand mapping bias reported is `0.000022006283676957945` (to an insanely higher level of precision than we actually need). This is, of course, drastically different from the value of `0.36810071818291797` that showed up in your table for this sample. While it is true that salmon is quite conservative about calling a library as stranded (i.e. it would rather make the mistake of calling stranded library as unstranded than vice-versa, as the latter would discard reads while the former would not), in this case it looks like the culprit is likely the transcriptome reference being used. When quantified under the standard gencode transcriptome, this sample is inferred as `ISR` with very high confidence ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-825393464
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-825393464:156,Testability,test,test,156,"Hi @rached-97,. First of all, thank you for the _incredibly-detailed_ report. All of the information you provided made it easy to pull down the data and to test what might be going on. I pulled down the first sample, consisting of `SRR9071838_1.fastq` and `SRR9071838_2.fastq`, which was recognized as `IU` for you. . However, since I didn't have access to the annotation you used or the specific scripts you used to extract the transcriptome reference, I instead quantified directly against [gencode v37](ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_37/gencode.v37.transcripts.fa.gz). When I did this, salmon calls the library format type as `ISR`, which is what we would expect. The `lib_format_count.json` is as such:. ```; {; ""read_files"": ""[ SRR9071838_1.fastq.gz, SRR9071838_2.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 31944161,; ""num_assigned_fragments"": 31944161,; ""num_frags_with_concordant_consistent_mappings"": 29445487,; ""num_frags_with_inconsistent_or_orphan_mappings"": 2576421,; ""strand_mapping_bias"": 0.000022006283676957945,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 648,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 29445487,; ""SF"": 1098610,; ""SR"": 1477163,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```. As you can see, the strand mapping bias reported is `0.000022006283676957945` (to an insanely higher level of precision than we actually need). This is, of course, drastically different from the value of `0.36810071818291797` that showed up in your table for this sample. While it is true that salmon is quite conservative about calling a library as stranded (i.e. it would rather make the mistake of calling stranded library as unstranded than vice-versa, as the latter would discard reads while the former would not), in this case it looks like the culprit is likely the transcriptome reference being used. When quantified under the standard gencode transcriptome, this sample is inferred as `ISR` with very high confidence ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-825393464
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:1514,Deployability,integrat,integrated,1514,"at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the pr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:1514,Integrability,integrat,integrated,1514,"at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the pr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:2336,Integrability,protocol,protocol,2336,"cript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of cou",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:3164,Integrability,protocol,protocols,3164,"he premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potential problem and seek their guidance. Thank you again for all your help. Rached. ```; # setwd('wd'). options(scipen = 9999). libraries <- lapply(; X = c('data.table', 'magrittr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz' # Genome fasta from Ensembl; gencode.tx.fasta <- '../../shared_data/annotations/Gencode/gencode.v35.transcripts.fa.gz' # Gencode transcript FASTA. do",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:4276,Integrability,message,message,4276," subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potential problem and seek their guidance. Thank you again for all your help. Rached. ```; # setwd('wd'). options(scipen = 9999). libraries <- lapply(; X = c('data.table', 'magrittr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz' # Genome fasta from Ensembl; gencode.tx.fasta <- '../../shared_data/annotations/Gencode/gencode.v35.transcripts.fa.gz' # Gencode transcript FASTA. dotPlot.fname <- '../ouput/dotPlots.pdf'. ### Read exon annotations ####; message('Loading Ensembl exon annotation (1-22, X, Y, MT)...'). chromosomes <- c(1:22, 'X', 'Y', 'MT'). anot <- import(anot.gtf, feature = 'exon') %>% sort; anot <- anot[seqnames(anot) %in% chromosomes, ]. # append gene and transcript version numbers to IDs; anot$gene_id <- paste(anot$gene_id, anot$gene_version, sep = '.'); anot$transcript_id <- paste(anot$transcript_id, anot$transcript_version, sep = '.'). ### Create premature transcript annotations ####; message('Creating premature transcript annotation...'). anot.pre <- split(anot, anot$transcript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_');",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:4737,Integrability,message,message,4737,"ttr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz' # Genome fasta from Ensembl; gencode.tx.fasta <- '../../shared_data/annotations/Gencode/gencode.v35.transcripts.fa.gz' # Gencode transcript FASTA. dotPlot.fname <- '../ouput/dotPlots.pdf'. ### Read exon annotations ####; message('Loading Ensembl exon annotation (1-22, X, Y, MT)...'). chromosomes <- c(1:22, 'X', 'Y', 'MT'). anot <- import(anot.gtf, feature = 'exon') %>% sort; anot <- anot[seqnames(anot) %in% chromosomes, ]. # append gene and transcript version numbers to IDs; anot$gene_id <- paste(anot$gene_id, anot$gene_version, sep = '.'); anot$transcript_id <- paste(anot$transcript_id, anot$transcript_version, sep = '.'). ### Create premature transcript annotations ####; message('Creating premature transcript annotation...'). anot.pre <- split(anot, anot$transcript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <-",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:5692,Integrability,message,message,5692,"# Create premature transcript annotations ####; message('Creating premature transcript annotation...'). anot.pre <- split(anot, anot$transcript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <- readDNAStringSet(filepath = genome.fasta). # simplify chromosome names; names(dna) <- sapply(strsplit(names(dna), ' '), '[', 1). dna <- dna[chromosomes] # subset chrom 1-22, X, Y, MT. ### Read Gencode transcript sequences ####; gencode <- readDNAStringSet(gencode.tx.fasta); names(gencode) <- gsub(; pattern = '\\|.*', replacement = '',; x = names(gencode); ). ### Sample transcripts on + and - strand (and avoid premature transcripts with multiple mature counterparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$prema",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:7388,Integrability,message,message,7388,"emature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group] <- reverseComplement(; premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group]; ). names(premature.tx) <- anot.pre$premature_group # paste0(a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:7632,Integrability,message,message,7632,"!grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group] <- reverseComplement(; premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group]; ). names(premature.tx) <- anot.pre$premature_group # paste0(anot.pre$premature_group, '_premature') # premature rna indicator. ### Dot plots ####; smoothDot <- function(s1, s2, w = 10) {. s1a <- sapply(; X = 1:(length(s1) - w + 1),; function(z) paste(s1[ z:(z + w - 1) ], collapse = ''); ). s2a <- sapply(;",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:7943,Integrability,message,message,7943,"not.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group] <- reverseComplement(; premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group]; ). names(premature.tx) <- anot.pre$premature_group # paste0(anot.pre$premature_group, '_premature') # premature rna indicator. ### Dot plots ####; smoothDot <- function(s1, s2, w = 10) {. s1a <- sapply(; X = 1:(length(s1) - w + 1),; function(z) paste(s1[ z:(z + w - 1) ], collapse = ''); ). s2a <- sapply(; X = 1:(length(s2) - w + 1),; function(z) paste(s2[ z:(z + w - 1) ], collapse = ''); ). s2b <- sapply( # considering the reversed y sequence per window; X = 1:(length(s2) - w + 1),; function(z) paste(s2[ z:(z + w - 1) ] %>% rev, collapse = ''); ). outer(s1a, s2a, FUN = '==') | outer(s1a, s2b, FUN = '=='); }. ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:8029,Integrability,message,message,8029,"nes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group] <- reverseComplement(; premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group]; ). names(premature.tx) <- anot.pre$premature_group # paste0(anot.pre$premature_group, '_premature') # premature rna indicator. ### Dot plots ####; smoothDot <- function(s1, s2, w = 10) {. s1a <- sapply(; X = 1:(length(s1) - w + 1),; function(z) paste(s1[ z:(z + w - 1) ], collapse = ''); ). s2a <- sapply(; X = 1:(length(s2) - w + 1),; function(z) paste(s2[ z:(z + w - 1) ], collapse = ''); ). s2b <- sapply( # considering the reversed y sequence per window; X = 1:(length(s2) - w + 1),; function(z) paste(s2[ z:(z + w - 1) ] %>% rev, collapse = ''); ). outer(s1a, s2a, FUN = '==') | outer(s1a, s2b, FUN = '=='); }. to.plot <- lapply(; setNames(nm = chosenOnes),; function(x) {. # out <- outer(as.vector",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:989,Performance,load,load,989,"ob-p ,. Edit: I have resolved the problem. It is not a problem with Biostrings or GRanges. It turns out that when subsetting the premature sequences, the subsetted sequences do not retain the names of the GRanges used to subset them therefore my code could not identify minus strand transcripts and get their reverse complements. Apologies for any confusion!; ---; Thank you very much for the prompt response and for taking the time to validate Salmon's functionality. Indeed, Salmon is not the problem here. After taking a closer look at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:1062,Performance,load,loaded,1062,"d sequences do not retain the names of the GRanges used to subset them therefore my code could not identify minus strand transcripts and get their reverse complements. Apologies for any confusion!; ---; Thank you very much for the prompt response and for taking the time to validate Salmon's functionality. Indeed, Salmon is not the problem here. After taking a closer look at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:1393,Performance,load,loaded,1393," time to validate Salmon's functionality. Indeed, Salmon is not the problem here. After taking a closer look at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:4285,Performance,Load,Loading,4285," subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potential problem and seek their guidance. Thank you again for all your help. Rached. ```; # setwd('wd'). options(scipen = 9999). libraries <- lapply(; X = c('data.table', 'magrittr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz' # Genome fasta from Ensembl; gencode.tx.fasta <- '../../shared_data/annotations/Gencode/gencode.v35.transcripts.fa.gz' # Gencode transcript FASTA. dotPlot.fname <- '../ouput/dotPlots.pdf'. ### Read exon annotations ####; message('Loading Ensembl exon annotation (1-22, X, Y, MT)...'). chromosomes <- c(1:22, 'X', 'Y', 'MT'). anot <- import(anot.gtf, feature = 'exon') %>% sort; anot <- anot[seqnames(anot) %in% chromosomes, ]. # append gene and transcript version numbers to IDs; anot$gene_id <- paste(anot$gene_id, anot$gene_version, sep = '.'); anot$transcript_id <- paste(anot$transcript_id, anot$transcript_version, sep = '.'). ### Create premature transcript annotations ####; message('Creating premature transcript annotation...'). anot.pre <- split(anot, anot$transcript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_');",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:5701,Performance,Load,Loading,5701,"# Create premature transcript annotations ####; message('Creating premature transcript annotation...'). anot.pre <- split(anot, anot$transcript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <- readDNAStringSet(filepath = genome.fasta). # simplify chromosome names; names(dna) <- sapply(strsplit(names(dna), ' '), '[', 1). dna <- dna[chromosomes] # subset chrom 1-22, X, Y, MT. ### Read Gencode transcript sequences ####; gencode <- readDNAStringSet(gencode.tx.fasta); names(gencode) <- gsub(; pattern = '\\|.*', replacement = '',; x = names(gencode); ). ### Sample transcripts on + and - strand (and avoid premature transcripts with multiple mature counterparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$prema",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:11816,Performance,load,loaded,11816,"_blank() # element_rect(fill = 'grey'); ) +; facet_wrap(paste('tx strand:', tx_strand) ~ paste('tx ID:', tx_id) + paste('myFasta seq type:', myFastaSeqType), ncol = 3, scales = 'free'). print(ggp). # dev.off(). ```. R session info:; ```; R version 4.0.2 (2020-06-22); Platform: x86_64-pc-linux-gnu (64-bit); Running under: CentOS Linux 7 (Core). Matrix products: default; BLAS/LAPACK: [hidden]/easybuild/software/2017/Core/imkl/2018.3.222/compilers_and_libraries_2018.3.222/linux/mkl/lib/intel64_lin/libmkl_gf_lp64.so. locale:; [1] LC_CTYPE=en_CA.UTF-8 LC_NUMERIC=C LC_TIME=en_CA.UTF-8 LC_COLLATE=en_CA.UTF-8 ; [5] LC_MONETARY=en_CA.UTF-8 LC_MESSAGES=en_CA.UTF-8 LC_PAPER=en_CA.UTF-8 LC_NAME=C ; [9] LC_ADDRESS=C LC_TELEPHONE=C LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C . attached base packages:; [1] parallel stats4 stats graphics grDevices utils datasets methods base . other attached packages:; [1] ggplot2_3.3.3 reshape2_1.4.4 Biostrings_2.58.0 XVector_0.30.0 rtracklayer_1.50.0 ; [6] GenomicRanges_1.42.0 GenomeInfoDb_1.26.7 IRanges_2.24.1 S4Vectors_0.28.1 BiocGenerics_0.36.1 ; [11] magrittr_2.0.1 data.table_1.14.0 . loaded via a namespace (and not attached):; [1] SummarizedExperiment_1.20.0 tidyselect_1.1.0 purrr_0.3.4 lattice_0.20-41 ; [5] colorspace_2.0-0 vctrs_0.3.7 generics_0.1.0 yaml_2.2.1 ; [9] utf8_1.2.1 XML_3.99-0.6 rlang_0.4.10 pillar_1.6.0 ; [13] glue_1.4.2 withr_2.4.1 DBI_1.1.1 BiocParallel_1.24.1 ; [17] matrixStats_0.58.0 GenomeInfoDbData_1.2.4 lifecycle_1.0.0 plyr_1.8.6 ; [21] stringr_1.4.0 zlibbioc_1.36.0 MatrixGenerics_1.2.1 munsell_0.5.0 ; [25] gtable_0.3.0 labeling_0.4.2 Biobase_2.50.0 fansi_0.4.2 ; [29] Rcpp_1.0.6 scales_1.1.1 DelayedArray_0.16.3 farver_2.1.0 ; [33] Rsamtools_2.6.0 digest_0.6.27 stringi_1.5.3 dplyr_1.0.5 ; [37] grid_4.0.2 tools_4.0.2 bitops_1.0-6 RCurl_1.98-1.3 ; [41] tibble_3.1.0 crayon_1.4.1 pkgconfig_2.0.3 ellipsis_0.3.1 ; [45] Matrix_1.2-18 assertthat_0.2.1 rstudioapi_0.13 R6_2.5.0 ; [49] GenomicAlignments_1.26.0 compiler_4.0.2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:6145,Safety,avoid,avoid,6145," anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <- readDNAStringSet(filepath = genome.fasta). # simplify chromosome names; names(dna) <- sapply(strsplit(names(dna), ' '), '[', 1). dna <- dna[chromosomes] # subset chrom 1-22, X, Y, MT. ### Read Gencode transcript sequences ####; gencode <- readDNAStringSet(gencode.tx.fasta); names(gencode) <- gsub(; pattern = '\\|.*', replacement = '',; x = names(gencode); ). ### Sample transcripts on + and - strand (and avoid premature transcripts with multiple mature counterparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:7109,Safety,sanity check,sanity check,7109,"erparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:443,Security,validat,validate,443,"Dear @rob-p ,. Edit: I have resolved the problem. It is not a problem with Biostrings or GRanges. It turns out that when subsetting the premature sequences, the subsetted sequences do not retain the names of the GRanges used to subset them therefore my code could not identify minus strand transcripts and get their reverse complements. Apologies for any confusion!; ---; Thank you very much for the prompt response and for taking the time to validate Salmon's functionality. Indeed, Salmon is not the problem here. After taking a closer look at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I ha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:2328,Testability,test,test,2328,"cript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of cou",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:2979,Testability,test,test,2979," reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potential problem and seek their guidance. Thank you again for all your help. Rached. ```; # setwd('wd'). options(scipen = 9999). libraries <- lapply(; X = c('data.table', 'magrittr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:2516,Usability,feedback,feedback,2516,"Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potenti",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:3591,Usability,guid,guidance,3591," the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potential problem and seek their guidance. Thank you again for all your help. Rached. ```; # setwd('wd'). options(scipen = 9999). libraries <- lapply(; X = c('data.table', 'magrittr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz' # Genome fasta from Ensembl; gencode.tx.fasta <- '../../shared_data/annotations/Gencode/gencode.v35.transcripts.fa.gz' # Gencode transcript FASTA. dotPlot.fname <- '../ouput/dotPlots.pdf'. ### Read exon annotations ####; message('Loading Ensembl exon annotation (1-22, X, Y, MT)...'). chromosomes <- c(1:22, 'X', 'Y', 'MT'). anot <- import(anot.gtf, feature = 'exon') %>% sort; anot <- anot[seqnames(anot) %in% chromosomes, ]. # append gene and transcript version numbers to IDs; a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:5783,Usability,simpl,simplify,5783,"ranscript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <- readDNAStringSet(filepath = genome.fasta). # simplify chromosome names; names(dna) <- sapply(strsplit(names(dna), ' '), '[', 1). dna <- dna[chromosomes] # subset chrom 1-22, X, Y, MT. ### Read Gencode transcript sequences ####; gencode <- readDNAStringSet(gencode.tx.fasta); names(gencode) <- gsub(; pattern = '\\|.*', replacement = '',; x = names(gencode); ). ### Sample transcripts on + and - strand (and avoid premature transcripts with multiple mature counterparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source =",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:6211,Usability,simpl,simplicity,6211," anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <- readDNAStringSet(filepath = genome.fasta). # simplify chromosome names; names(dna) <- sapply(strsplit(names(dna), ' '), '[', 1). dna <- dna[chromosomes] # subset chrom 1-22, X, Y, MT. ### Read Gencode transcript sequences ####; gencode <- readDNAStringSet(gencode.tx.fasta); names(gencode) <- gsub(; pattern = '\\|.*', replacement = '',; x = names(gencode); ). ### Sample transcripts on + and - strand (and avoid premature transcripts with multiple mature counterparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:10493,Usability,guid,guide,10493,"er,; s2 = as.vector(gencode[[x]]) %>% as.character; ). out2 <- reshape2::melt(out2) %>% as.data.table; colnames(out2) <- c('myFasta', 'GencodeMatureFasta', 'seqMatch'); out2[ , myFastaSeqType := 'premature']. # my premature complement vs gencode mature; out3 <- smoothDot(; s1 = as.vector(premature.tx[[x]] %>% complement) %>% as.character,; s2 = as.vector(gencode[[x]]) %>% as.character; ). out3 <- reshape2::melt(out3) %>% as.data.table; colnames(out3) <- c('myFasta', 'GencodeMatureFasta', 'seqMatch'); out3[ , myFastaSeqType := 'premature (complement)']. out <- rbind(out1, out2, out3); out[ , tx_strand := strand(anot[which(anot$transcript_id == x)[1], ]) %>% as.character]. return(out); }; ) %>% rbindlist(., idcol = 'tx_id'). # pdf(dotPlot.fname, width = 8, height = 6). ggp <- ggplot(mapping = aes(x = myFasta, y = GencodeMatureFasta, fill = seqMatch), data = to.plot) +; geom_tile(width = 1, height = 1) +; scale_fill_manual(; values = c('TRUE' = 'black', 'FALSE' = 'white'),; guide = F; ) + scale_x_continuous(; expand = c(0, 0); ) + scale_y_continuous(; expand = c(0, 0); ) +; theme_bw(base_size = 10) +; theme(; panel.grid = element_blank(), panel.background = element_blank() # element_rect(fill = 'grey'); ) +; facet_wrap(paste('tx strand:', tx_strand) ~ paste('tx ID:', tx_id) + paste('myFasta seq type:', myFastaSeqType), ncol = 3, scales = 'free'). print(ggp). # dev.off(). ```. R session info:; ```; R version 4.0.2 (2020-06-22); Platform: x86_64-pc-linux-gnu (64-bit); Running under: CentOS Linux 7 (Core). Matrix products: default; BLAS/LAPACK: [hidden]/easybuild/software/2017/Core/imkl/2018.3.222/compilers_and_libraries_2018.3.222/linux/mkl/lib/intel64_lin/libmkl_gf_lp64.so. locale:; [1] LC_CTYPE=en_CA.UTF-8 LC_NUMERIC=C LC_TIME=en_CA.UTF-8 LC_COLLATE=en_CA.UTF-8 ; [5] LC_MONETARY=en_CA.UTF-8 LC_MESSAGES=en_CA.UTF-8 LC_PAPER=en_CA.UTF-8 LC_NAME=C ; [9] LC_ADDRESS=C LC_TELEPHONE=C LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C . attached base packages:; [1] parallel stats4",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826117612:319,Availability,fault,fault,319,"Thank you @rached-97, for the incredibly thorough follow-up to the original issue. I agree that the work you put into not only reporting the issue thoroughly to begin with, but following up with your findings, will certainly help others who might encounter related issues in the future. Since salmon seems not to be at fault here, I'll close the issue. I do recommend taking this over to the bioconductor forums, where the community is _incredibly_ responsive; I imagine you'll have a resolution in no time. Thanks again for the excellent report and follow-up!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826117612
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826117612:449,Usability,responsiv,responsive,449,"Thank you @rached-97, for the incredibly thorough follow-up to the original issue. I agree that the work you put into not only reporting the issue thoroughly to begin with, but following up with your findings, will certainly help others who might encounter related issues in the future. Since salmon seems not to be at fault here, I'll close the issue. I do recommend taking this over to the bioconductor forums, where the community is _incredibly_ responsive; I imagine you'll have a resolution in no time. Thanks again for the excellent report and follow-up!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826117612
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753:210,Deployability,update,updated,210,"Hi @aedavids,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a _decoy_ sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753:237,Deployability,update,update,237,"Hi @aedavids,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a _decoy_ sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753:48,Usability,undo,undocumented,48,"Hi @aedavids,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a _decoy_ sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332:555,Deployability,update,updated,555,"thanks. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332:582,Deployability,update,update,582,"thanks. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332:290,Usability,undo,undocumented,290,"thanks. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332:395,Usability,undo,undocumented,395,"thanks. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:520,Availability,down,downstream,520,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:1272,Deployability,update,updated,1272,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:1299,Deployability,update,update,1299,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:1007,Usability,undo,undocumented,1007,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:1112,Usability,undo,undocumented,1112,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-841492751:2120,Usability,simpl,simply,2120,"ecoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file.; ; Well, it's the case the decoys are *not* be quantified. That is, only the target transcripts will appear in the `quant.sf` file, no decoys should be present there. The main purpose of the decoys is to account for reads _not_ from target transcripts that might otherwise be sequenced in the sample.; ; The reason we report the decoy mapping fragments in the unmapped names file is, as I said, a historical contingency. Basically, since we're not mapping the decoys to targets and counting them toward quantification, one might be interested in knowing _where_ the decoy sequences come from. At some point, the easiest way to do this was just to place the name of these fragments in the unmapped names file (with the `d` tag) and then grab the reads and go fishing with them in some other way. However, I totally understand why including them in the unmapped names file is confusing. During selective-alignment, if we assign a fragment as best mapping to a decoy, it doesn't get assigned to a quantifiable target, but it's not technically unmapped in the same sense as the other unmapped reads. That is, we know it comes from the decoy sequence, that the alignment score is at least the minimum required, and that it maps better to the decoy than to any non-decoy target. That's quite different that ""truly"" unmapped fragments where we find no mapping for the fragment within the required score threshold. Anyway, I hope the answer is useful for you. If you want to select only unmapped reads that were matched to *neither* your target sequences (transcripts) *nor* to the decoy sequences, then your `grep` command should do the trick. However, if you want to look at all of the reads that simply didn't contribute to the counts in the `quant.sf` file, then you'd want to look at everything in the unmapped names file. Let me know if you have any other questions!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-841492751
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-843255628:323,Usability,undo,undocumented,323,"Hi Rob. Thanks for the explination. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Friday, May 14, 2021 at 1:41 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Sorry for the delay in replying to your reply here. So I think what you suggest is the right solution, and there are some strange historical reasons we write the decoy names out in the unmapped file. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. Well, it's the case the decoys are not be quantified. That is, only the target transcripts will appear in the quant.sf file, no decoys should be present there. The main purpose of the decoys is to account for reads not from target transcripts that might otherwise be sequenced in the sample. The reason we report the decoy mapping fragments in the unmapped names file is, as I said, a historical contingency. Basically, since we're not mapping the decoys to targets and counting them toward quantification, one might be interested in knowing where the decoy sequences come from. At some point, the easiest way to do this was just to place the name of these fragments in the unmapped names file (with the d tag) and then grab the reads and go fishing with them in some other way. However, I totally understand why including them in the unmapped names file is confusing. During selective-alignment, if we assign a fragment as best mapping to a decoy, it doesn't get assigned to a quantifiable target, but it's not technically unmapped in the same sense as the other unmapped reads. That is, we know it comes from the decoy sequence, that the alignment score is at least the minimum required, and tha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-843255628
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-843255628:2473,Usability,simpl,simply,2473,"be present there. The main purpose of the decoys is to account for reads not from target transcripts that might otherwise be sequenced in the sample. The reason we report the decoy mapping fragments in the unmapped names file is, as I said, a historical contingency. Basically, since we're not mapping the decoys to targets and counting them toward quantification, one might be interested in knowing where the decoy sequences come from. At some point, the easiest way to do this was just to place the name of these fragments in the unmapped names file (with the d tag) and then grab the reads and go fishing with them in some other way. However, I totally understand why including them in the unmapped names file is confusing. During selective-alignment, if we assign a fragment as best mapping to a decoy, it doesn't get assigned to a quantifiable target, but it's not technically unmapped in the same sense as the other unmapped reads. That is, we know it comes from the decoy sequence, that the alignment score is at least the minimum required, and that it maps better to the decoy than to any non-decoy target. That's quite different that ""truly"" unmapped fragments where we find no mapping for the fragment within the required score threshold. Anyway, I hope the answer is useful for you. If you want to select only unmapped reads that were matched to neither your target sequences (transcripts) nor to the decoy sequences, then your grep command should do the trick. However, if you want to look at all of the reads that simply didn't contribute to the counts in the quant.sf file, then you'd want to look at everything in the unmapped names file. Let me know if you have any other questions!. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-841492751>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAUROTOCPHB6SKMA2ETTNWDF5ANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-843255628
https://github.com/COMBINE-lab/salmon/issues/659#issuecomment-841483705:621,Safety,detect,detection,621,"Hi @mooreann,. So, as you can see the strand mapping bias is calculated as `0.5172763911708863`, so ~51.7%. A *perfectly* unbiased ratio would be 50%. However, there are many reasons you might see small deviations from this in your sample. Salmon is *super* conservative in reporting a _potential_ bias (anything > 1% deviation from 50/50), since raising the warning doesn't cost anything (we compute these statistics anyway), and it's better that someone who could be expecting such a thing is a aware of it than that someone notice the warning and decide it's not a problem. In this case, your sample is 0.7% above the detection threshold, and so the warning is raised. Unless you're noticing anything else that looks strange with the results, I would suggest that this probably isn't anything you need to worry about. As I mentioned, one might expect some natural variation from a ""perfect"" 50% split between reads arising in each orientation, and the bias in this data looks pretty slight; it's just that salmon is very conservative in reporting the issue. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/659#issuecomment-841483705
https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372:363,Energy Efficiency,adapt,adapter,363,"Hi @Ci-TJ,. This suggests that the FASTQ files were ""desynchronized"" during / after trimming somehow. Salmon requires that the FASTQ files are synchronized. So, if the trimmer decides to discard a read from the first read file, it must also discard the corresponding read from the second read file. I'm not specifically familiar with RabbitQC, but most quality / adapter trimmers have an option to separate out any reads that become orphaned during trimming so that the output paired FASTQ files remain synchronized. You should make sure that any such options are passed during QC. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372
https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372:143,Integrability,synchroniz,synchronized,143,"Hi @Ci-TJ,. This suggests that the FASTQ files were ""desynchronized"" during / after trimming somehow. Salmon requires that the FASTQ files are synchronized. So, if the trimmer decides to discard a read from the first read file, it must also discard the corresponding read from the second read file. I'm not specifically familiar with RabbitQC, but most quality / adapter trimmers have an option to separate out any reads that become orphaned during trimming so that the output paired FASTQ files remain synchronized. You should make sure that any such options are passed during QC. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372
https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372:363,Integrability,adapter,adapter,363,"Hi @Ci-TJ,. This suggests that the FASTQ files were ""desynchronized"" during / after trimming somehow. Salmon requires that the FASTQ files are synchronized. So, if the trimmer decides to discard a read from the first read file, it must also discard the corresponding read from the second read file. I'm not specifically familiar with RabbitQC, but most quality / adapter trimmers have an option to separate out any reads that become orphaned during trimming so that the output paired FASTQ files remain synchronized. You should make sure that any such options are passed during QC. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372
https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372:503,Integrability,synchroniz,synchronized,503,"Hi @Ci-TJ,. This suggests that the FASTQ files were ""desynchronized"" during / after trimming somehow. Salmon requires that the FASTQ files are synchronized. So, if the trimmer decides to discard a read from the first read file, it must also discard the corresponding read from the second read file. I'm not specifically familiar with RabbitQC, but most quality / adapter trimmers have an option to separate out any reads that become orphaned during trimming so that the output paired FASTQ files remain synchronized. You should make sure that any such options are passed during QC. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372
https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372:363,Modifiability,adapt,adapter,363,"Hi @Ci-TJ,. This suggests that the FASTQ files were ""desynchronized"" during / after trimming somehow. Salmon requires that the FASTQ files are synchronized. So, if the trimmer decides to discard a read from the first read file, it must also discard the corresponding read from the second read file. I'm not specifically familiar with RabbitQC, but most quality / adapter trimmers have an option to separate out any reads that become orphaned during trimming so that the output paired FASTQ files remain synchronized. You should make sure that any such options are passed during QC. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/660#issuecomment-846252372
https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054:435,Availability,down,down,435,"Hi @bounlu,. Thank you for the detailed report. The mapping rate certainly does seem very low. To answer your specific questions first:; 1) I'm not sure --- let's try tor find out; 2) I don't think so (if they are part of your index, they should be aligned against); 3) If there are many transcripts / targets you expect to be sequenced but which aren't present in this set, that can affect the mapping rate, but not likely to take it down to 6%. Here are the things I'd investigate --- roughly in order: . 1) In addition to the fraction of reads STAR mapped (which you report above), what fraction of the reads are assigned to features by featureCounts? In some cases, when there is a failure of rRNA depletion of polyA selection, you can end up with an experiment where most of the sequenced RNA comes from rRNA not present in the reference transcriptome. In this case, STAR will be able to align the reads to the genome, but you won't see these reads mapping to annotated features (and you also won't see them showing up in your transcript level quantifications). So, it may be worth to take a look at the count of reads assigned to the feature set of genes by featureCounts. 2) Above, it looks like a considerable number of fragments were discarded due to no alignment reaching the required alignment score (`11,448,458` fragments discarded because of this). Have you tried to adapter / quality trim the data? Does this have any effect on the mapping rate?. 3) If the above don't reveal any clues, I'd be happy to try to take a look at the data if you can share it. I'd be quite surprised if STAR is aligning a lot of reads *to transcriptome features* that are being missed by salmon. Nonetheless, if you pass the proper flags to STAR (including `--quantMode TranscriptomeSAM`), then you can use the SAM/BAM file generated by STAR to perform quantification with salmon (i.e. use STAR's alignments to do _transcript-level_ quantification). I'd be happy to help dig further on any of these, so pleas",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054
https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054:686,Availability,failure,failure,686,"Hi @bounlu,. Thank you for the detailed report. The mapping rate certainly does seem very low. To answer your specific questions first:; 1) I'm not sure --- let's try tor find out; 2) I don't think so (if they are part of your index, they should be aligned against); 3) If there are many transcripts / targets you expect to be sequenced but which aren't present in this set, that can affect the mapping rate, but not likely to take it down to 6%. Here are the things I'd investigate --- roughly in order: . 1) In addition to the fraction of reads STAR mapped (which you report above), what fraction of the reads are assigned to features by featureCounts? In some cases, when there is a failure of rRNA depletion of polyA selection, you can end up with an experiment where most of the sequenced RNA comes from rRNA not present in the reference transcriptome. In this case, STAR will be able to align the reads to the genome, but you won't see these reads mapping to annotated features (and you also won't see them showing up in your transcript level quantifications). So, it may be worth to take a look at the count of reads assigned to the feature set of genes by featureCounts. 2) Above, it looks like a considerable number of fragments were discarded due to no alignment reaching the required alignment score (`11,448,458` fragments discarded because of this). Have you tried to adapter / quality trim the data? Does this have any effect on the mapping rate?. 3) If the above don't reveal any clues, I'd be happy to try to take a look at the data if you can share it. I'd be quite surprised if STAR is aligning a lot of reads *to transcriptome features* that are being missed by salmon. Nonetheless, if you pass the proper flags to STAR (including `--quantMode TranscriptomeSAM`), then you can use the SAM/BAM file generated by STAR to perform quantification with salmon (i.e. use STAR's alignments to do _transcript-level_ quantification). I'd be happy to help dig further on any of these, so pleas",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054
https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054:1381,Energy Efficiency,adapt,adapter,1381,"inly does seem very low. To answer your specific questions first:; 1) I'm not sure --- let's try tor find out; 2) I don't think so (if they are part of your index, they should be aligned against); 3) If there are many transcripts / targets you expect to be sequenced but which aren't present in this set, that can affect the mapping rate, but not likely to take it down to 6%. Here are the things I'd investigate --- roughly in order: . 1) In addition to the fraction of reads STAR mapped (which you report above), what fraction of the reads are assigned to features by featureCounts? In some cases, when there is a failure of rRNA depletion of polyA selection, you can end up with an experiment where most of the sequenced RNA comes from rRNA not present in the reference transcriptome. In this case, STAR will be able to align the reads to the genome, but you won't see these reads mapping to annotated features (and you also won't see them showing up in your transcript level quantifications). So, it may be worth to take a look at the count of reads assigned to the feature set of genes by featureCounts. 2) Above, it looks like a considerable number of fragments were discarded due to no alignment reaching the required alignment score (`11,448,458` fragments discarded because of this). Have you tried to adapter / quality trim the data? Does this have any effect on the mapping rate?. 3) If the above don't reveal any clues, I'd be happy to try to take a look at the data if you can share it. I'd be quite surprised if STAR is aligning a lot of reads *to transcriptome features* that are being missed by salmon. Nonetheless, if you pass the proper flags to STAR (including `--quantMode TranscriptomeSAM`), then you can use the SAM/BAM file generated by STAR to perform quantification with salmon (i.e. use STAR's alignments to do _transcript-level_ quantification). I'd be happy to help dig further on any of these, so please feel free to reach out if you find anything interesting. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054
https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054:1381,Integrability,adapter,adapter,1381,"inly does seem very low. To answer your specific questions first:; 1) I'm not sure --- let's try tor find out; 2) I don't think so (if they are part of your index, they should be aligned against); 3) If there are many transcripts / targets you expect to be sequenced but which aren't present in this set, that can affect the mapping rate, but not likely to take it down to 6%. Here are the things I'd investigate --- roughly in order: . 1) In addition to the fraction of reads STAR mapped (which you report above), what fraction of the reads are assigned to features by featureCounts? In some cases, when there is a failure of rRNA depletion of polyA selection, you can end up with an experiment where most of the sequenced RNA comes from rRNA not present in the reference transcriptome. In this case, STAR will be able to align the reads to the genome, but you won't see these reads mapping to annotated features (and you also won't see them showing up in your transcript level quantifications). So, it may be worth to take a look at the count of reads assigned to the feature set of genes by featureCounts. 2) Above, it looks like a considerable number of fragments were discarded due to no alignment reaching the required alignment score (`11,448,458` fragments discarded because of this). Have you tried to adapter / quality trim the data? Does this have any effect on the mapping rate?. 3) If the above don't reveal any clues, I'd be happy to try to take a look at the data if you can share it. I'd be quite surprised if STAR is aligning a lot of reads *to transcriptome features* that are being missed by salmon. Nonetheless, if you pass the proper flags to STAR (including `--quantMode TranscriptomeSAM`), then you can use the SAM/BAM file generated by STAR to perform quantification with salmon (i.e. use STAR's alignments to do _transcript-level_ quantification). I'd be happy to help dig further on any of these, so please feel free to reach out if you find anything interesting. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054
https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054:1381,Modifiability,adapt,adapter,1381,"inly does seem very low. To answer your specific questions first:; 1) I'm not sure --- let's try tor find out; 2) I don't think so (if they are part of your index, they should be aligned against); 3) If there are many transcripts / targets you expect to be sequenced but which aren't present in this set, that can affect the mapping rate, but not likely to take it down to 6%. Here are the things I'd investigate --- roughly in order: . 1) In addition to the fraction of reads STAR mapped (which you report above), what fraction of the reads are assigned to features by featureCounts? In some cases, when there is a failure of rRNA depletion of polyA selection, you can end up with an experiment where most of the sequenced RNA comes from rRNA not present in the reference transcriptome. In this case, STAR will be able to align the reads to the genome, but you won't see these reads mapping to annotated features (and you also won't see them showing up in your transcript level quantifications). So, it may be worth to take a look at the count of reads assigned to the feature set of genes by featureCounts. 2) Above, it looks like a considerable number of fragments were discarded due to no alignment reaching the required alignment score (`11,448,458` fragments discarded because of this). Have you tried to adapter / quality trim the data? Does this have any effect on the mapping rate?. 3) If the above don't reveal any clues, I'd be happy to try to take a look at the data if you can share it. I'd be quite surprised if STAR is aligning a lot of reads *to transcriptome features* that are being missed by salmon. Nonetheless, if you pass the proper flags to STAR (including `--quantMode TranscriptomeSAM`), then you can use the SAM/BAM file generated by STAR to perform quantification with salmon (i.e. use STAR's alignments to do _transcript-level_ quantification). I'd be happy to help dig further on any of these, so please feel free to reach out if you find anything interesting. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054
https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054:1838,Performance,perform,perform,1838,"inly does seem very low. To answer your specific questions first:; 1) I'm not sure --- let's try tor find out; 2) I don't think so (if they are part of your index, they should be aligned against); 3) If there are many transcripts / targets you expect to be sequenced but which aren't present in this set, that can affect the mapping rate, but not likely to take it down to 6%. Here are the things I'd investigate --- roughly in order: . 1) In addition to the fraction of reads STAR mapped (which you report above), what fraction of the reads are assigned to features by featureCounts? In some cases, when there is a failure of rRNA depletion of polyA selection, you can end up with an experiment where most of the sequenced RNA comes from rRNA not present in the reference transcriptome. In this case, STAR will be able to align the reads to the genome, but you won't see these reads mapping to annotated features (and you also won't see them showing up in your transcript level quantifications). So, it may be worth to take a look at the count of reads assigned to the feature set of genes by featureCounts. 2) Above, it looks like a considerable number of fragments were discarded due to no alignment reaching the required alignment score (`11,448,458` fragments discarded because of this). Have you tried to adapter / quality trim the data? Does this have any effect on the mapping rate?. 3) If the above don't reveal any clues, I'd be happy to try to take a look at the data if you can share it. I'd be quite surprised if STAR is aligning a lot of reads *to transcriptome features* that are being missed by salmon. Nonetheless, if you pass the proper flags to STAR (including `--quantMode TranscriptomeSAM`), then you can use the SAM/BAM file generated by STAR to perform quantification with salmon (i.e. use STAR's alignments to do _transcript-level_ quantification). I'd be happy to help dig further on any of these, so please feel free to reach out if you find anything interesting. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-846251054
https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-847091597:835,Energy Efficiency,adapt,adapter,835,"Hi @rob-p . Thanks for the quick reply. Indeed my salmon index does not include lncRNAs, but my sequencing does. For indexing, I only used UCSC RefSeq transcripts (which I believe contains only protein coding transcripts that exclude most of lncRNAs). But this does not seem to suffice to explain the low mapping rate as Wikipedia says ""[Quantitatively, lncRNAs demonstrate ~10-fold lower abundance than mRNAs in a population of cells.](https://en.wikipedia.org/wiki/Long_non-coding_RNA#Abundance)"". To answer your questions:; 1. I used `htseq-count`, and here are the overall statistics (out of 149347870 record pairs processed):; ```; stat	""-s yes""	""-s reverse""; __no_feature	135258158	44917653; __ambiguous	39301	594958; __too_low_aQual	0	0; __not_aligned	0	0; __alignment_not_unique	7430169	7430169; ```. 2. I haven't done quality/adapter trimming as the data really looks clean and of high quality according to FastQC report. 3. Unfortunately I can't share the raw data yet but I will try your suggestion to quantify with STAR at the transcript level.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-847091597
https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-847091597:835,Integrability,adapter,adapter,835,"Hi @rob-p . Thanks for the quick reply. Indeed my salmon index does not include lncRNAs, but my sequencing does. For indexing, I only used UCSC RefSeq transcripts (which I believe contains only protein coding transcripts that exclude most of lncRNAs). But this does not seem to suffice to explain the low mapping rate as Wikipedia says ""[Quantitatively, lncRNAs demonstrate ~10-fold lower abundance than mRNAs in a population of cells.](https://en.wikipedia.org/wiki/Long_non-coding_RNA#Abundance)"". To answer your questions:; 1. I used `htseq-count`, and here are the overall statistics (out of 149347870 record pairs processed):; ```; stat	""-s yes""	""-s reverse""; __no_feature	135258158	44917653; __ambiguous	39301	594958; __too_low_aQual	0	0; __not_aligned	0	0; __alignment_not_unique	7430169	7430169; ```. 2. I haven't done quality/adapter trimming as the data really looks clean and of high quality according to FastQC report. 3. Unfortunately I can't share the raw data yet but I will try your suggestion to quantify with STAR at the transcript level.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-847091597
https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-847091597:835,Modifiability,adapt,adapter,835,"Hi @rob-p . Thanks for the quick reply. Indeed my salmon index does not include lncRNAs, but my sequencing does. For indexing, I only used UCSC RefSeq transcripts (which I believe contains only protein coding transcripts that exclude most of lncRNAs). But this does not seem to suffice to explain the low mapping rate as Wikipedia says ""[Quantitatively, lncRNAs demonstrate ~10-fold lower abundance than mRNAs in a population of cells.](https://en.wikipedia.org/wiki/Long_non-coding_RNA#Abundance)"". To answer your questions:; 1. I used `htseq-count`, and here are the overall statistics (out of 149347870 record pairs processed):; ```; stat	""-s yes""	""-s reverse""; __no_feature	135258158	44917653; __ambiguous	39301	594958; __too_low_aQual	0	0; __not_aligned	0	0; __alignment_not_unique	7430169	7430169; ```. 2. I haven't done quality/adapter trimming as the data really looks clean and of high quality according to FastQC report. 3. Unfortunately I can't share the raw data yet but I will try your suggestion to quantify with STAR at the transcript level.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/661#issuecomment-847091597
https://github.com/COMBINE-lab/salmon/issues/662#issuecomment-845350603:108,Testability,log,log,108,Realized I haven't run the gene quantification yet and its looking for quant.genes.sf not quant.sf like the log says?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/662#issuecomment-845350603
https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665:728,Availability,error,error,728,"Thanks for your quickly reply! It really worked @k3yavi; Then I run the command line ; `salmon alevin -l ISR /home/lailab/disk/gjw/Ascite-1_R1.fq.gz -2 /home/lailab/disk/gjw/Ascite-1_R2.fq.gz --chromium --index /home/lailab/disk/gjw/default/ -p 10 -o /home/lailab/disk/gjw/alevin_out --tgMap /home/lailab/disk/gjw/txp2gene.tsv`. ```; Version Server Response: Not Found; Logs will be written to /home/lailab/disk/gjw/alevin_out/logs; [2021-05-27 14:31:00.318] [jointLog] [info] setting maxHashResizeThreads to 10; [2021-05-27 14:31:00.318] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2021-05-27 14:31:00.318] [jointLog] [error] You passed paired-end files to salmon, but you passed 0 files to --mates1 and 1 files to --mates2. You must pass the same number of files to both flags; [2021-05-27 14:31:00.318] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2021-05-27 14:31:00.318] [alevinLog] [error] Could not properly process salmon-level options!; ```; Is it the problem with my data?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665
https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665:1088,Availability,error,error,1088,"Thanks for your quickly reply! It really worked @k3yavi; Then I run the command line ; `salmon alevin -l ISR /home/lailab/disk/gjw/Ascite-1_R1.fq.gz -2 /home/lailab/disk/gjw/Ascite-1_R2.fq.gz --chromium --index /home/lailab/disk/gjw/default/ -p 10 -o /home/lailab/disk/gjw/alevin_out --tgMap /home/lailab/disk/gjw/txp2gene.tsv`. ```; Version Server Response: Not Found; Logs will be written to /home/lailab/disk/gjw/alevin_out/logs; [2021-05-27 14:31:00.318] [jointLog] [info] setting maxHashResizeThreads to 10; [2021-05-27 14:31:00.318] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2021-05-27 14:31:00.318] [jointLog] [error] You passed paired-end files to salmon, but you passed 0 files to --mates1 and 1 files to --mates2. You must pass the same number of files to both flags; [2021-05-27 14:31:00.318] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2021-05-27 14:31:00.318] [alevinLog] [error] Could not properly process salmon-level options!; ```; Is it the problem with my data?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665
https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665:627,Security,validat,validation,627,"Thanks for your quickly reply! It really worked @k3yavi; Then I run the command line ; `salmon alevin -l ISR /home/lailab/disk/gjw/Ascite-1_R1.fq.gz -2 /home/lailab/disk/gjw/Ascite-1_R2.fq.gz --chromium --index /home/lailab/disk/gjw/default/ -p 10 -o /home/lailab/disk/gjw/alevin_out --tgMap /home/lailab/disk/gjw/txp2gene.tsv`. ```; Version Server Response: Not Found; Logs will be written to /home/lailab/disk/gjw/alevin_out/logs; [2021-05-27 14:31:00.318] [jointLog] [info] setting maxHashResizeThreads to 10; [2021-05-27 14:31:00.318] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2021-05-27 14:31:00.318] [jointLog] [error] You passed paired-end files to salmon, but you passed 0 files to --mates1 and 1 files to --mates2. You must pass the same number of files to both flags; [2021-05-27 14:31:00.318] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2021-05-27 14:31:00.318] [alevinLog] [error] Could not properly process salmon-level options!; ```; Is it the problem with my data?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665
https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665:641,Security,validat,validateMappings,641,"Thanks for your quickly reply! It really worked @k3yavi; Then I run the command line ; `salmon alevin -l ISR /home/lailab/disk/gjw/Ascite-1_R1.fq.gz -2 /home/lailab/disk/gjw/Ascite-1_R2.fq.gz --chromium --index /home/lailab/disk/gjw/default/ -p 10 -o /home/lailab/disk/gjw/alevin_out --tgMap /home/lailab/disk/gjw/txp2gene.tsv`. ```; Version Server Response: Not Found; Logs will be written to /home/lailab/disk/gjw/alevin_out/logs; [2021-05-27 14:31:00.318] [jointLog] [info] setting maxHashResizeThreads to 10; [2021-05-27 14:31:00.318] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2021-05-27 14:31:00.318] [jointLog] [error] You passed paired-end files to salmon, but you passed 0 files to --mates1 and 1 files to --mates2. You must pass the same number of files to both flags; [2021-05-27 14:31:00.318] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2021-05-27 14:31:00.318] [alevinLog] [error] Could not properly process salmon-level options!; ```; Is it the problem with my data?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665
https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665:677,Security,validat,validation,677,"Thanks for your quickly reply! It really worked @k3yavi; Then I run the command line ; `salmon alevin -l ISR /home/lailab/disk/gjw/Ascite-1_R1.fq.gz -2 /home/lailab/disk/gjw/Ascite-1_R2.fq.gz --chromium --index /home/lailab/disk/gjw/default/ -p 10 -o /home/lailab/disk/gjw/alevin_out --tgMap /home/lailab/disk/gjw/txp2gene.tsv`. ```; Version Server Response: Not Found; Logs will be written to /home/lailab/disk/gjw/alevin_out/logs; [2021-05-27 14:31:00.318] [jointLog] [info] setting maxHashResizeThreads to 10; [2021-05-27 14:31:00.318] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2021-05-27 14:31:00.318] [jointLog] [error] You passed paired-end files to salmon, but you passed 0 files to --mates1 and 1 files to --mates2. You must pass the same number of files to both flags; [2021-05-27 14:31:00.318] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2021-05-27 14:31:00.318] [alevinLog] [error] Could not properly process salmon-level options!; ```; Is it the problem with my data?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665
https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665:370,Testability,Log,Logs,370,"Thanks for your quickly reply! It really worked @k3yavi; Then I run the command line ; `salmon alevin -l ISR /home/lailab/disk/gjw/Ascite-1_R1.fq.gz -2 /home/lailab/disk/gjw/Ascite-1_R2.fq.gz --chromium --index /home/lailab/disk/gjw/default/ -p 10 -o /home/lailab/disk/gjw/alevin_out --tgMap /home/lailab/disk/gjw/txp2gene.tsv`. ```; Version Server Response: Not Found; Logs will be written to /home/lailab/disk/gjw/alevin_out/logs; [2021-05-27 14:31:00.318] [jointLog] [info] setting maxHashResizeThreads to 10; [2021-05-27 14:31:00.318] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2021-05-27 14:31:00.318] [jointLog] [error] You passed paired-end files to salmon, but you passed 0 files to --mates1 and 1 files to --mates2. You must pass the same number of files to both flags; [2021-05-27 14:31:00.318] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2021-05-27 14:31:00.318] [alevinLog] [error] Could not properly process salmon-level options!; ```; Is it the problem with my data?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665
https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665:427,Testability,log,logs,427,"Thanks for your quickly reply! It really worked @k3yavi; Then I run the command line ; `salmon alevin -l ISR /home/lailab/disk/gjw/Ascite-1_R1.fq.gz -2 /home/lailab/disk/gjw/Ascite-1_R2.fq.gz --chromium --index /home/lailab/disk/gjw/default/ -p 10 -o /home/lailab/disk/gjw/alevin_out --tgMap /home/lailab/disk/gjw/txp2gene.tsv`. ```; Version Server Response: Not Found; Logs will be written to /home/lailab/disk/gjw/alevin_out/logs; [2021-05-27 14:31:00.318] [jointLog] [info] setting maxHashResizeThreads to 10; [2021-05-27 14:31:00.318] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2021-05-27 14:31:00.318] [jointLog] [error] You passed paired-end files to salmon, but you passed 0 files to --mates1 and 1 files to --mates2. You must pass the same number of files to both flags; [2021-05-27 14:31:00.318] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2021-05-27 14:31:00.318] [alevinLog] [error] Could not properly process salmon-level options!; ```; Is it the problem with my data?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/663#issuecomment-850151665
https://github.com/COMBINE-lab/salmon/issues/664#issuecomment-847978707:101,Performance,optimiz,optimization,101,"Hi @sjaenick,. This is a bug in the compiler which GCC has not yet fixed related to inter-procedural optimization. To compile successfully, please add `-DNO_IPO=TRUE` to the CMake flags. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/664#issuecomment-847978707
https://github.com/COMBINE-lab/salmon/issues/667#issuecomment-1016917355:72,Availability,down,down,72,> I have the exact same issue. Any solutions?. I think I ended up going down to version 1.4.0 and it worked.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/667#issuecomment-1016917355
https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216:50,Availability,recover,recoverOrphans,50,"UPDATE: On @rob-p 's suggestion, I removed the `--recoverOrphans` option and then all 60 samples did finished without segfaulting. Perhaps there were too many orphans to handle - alignments rates were a dismal 0.5-23%. These were heavily degraded samples that the sequencing center recommended not to sequence but the PI wanted to try it anyway. If you want a pair of fastq files (full or cutdown to ~5 M reads) to test this weird edge case, I can see about getting them to you. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216
https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216:238,Availability,degraded,degraded,238,"UPDATE: On @rob-p 's suggestion, I removed the `--recoverOrphans` option and then all 60 samples did finished without segfaulting. Perhaps there were too many orphans to handle - alignments rates were a dismal 0.5-23%. These were heavily degraded samples that the sequencing center recommended not to sequence but the PI wanted to try it anyway. If you want a pair of fastq files (full or cutdown to ~5 M reads) to test this weird edge case, I can see about getting them to you. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216
https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216:0,Deployability,UPDATE,UPDATE,0,"UPDATE: On @rob-p 's suggestion, I removed the `--recoverOrphans` option and then all 60 samples did finished without segfaulting. Perhaps there were too many orphans to handle - alignments rates were a dismal 0.5-23%. These were heavily degraded samples that the sequencing center recommended not to sequence but the PI wanted to try it anyway. If you want a pair of fastq files (full or cutdown to ~5 M reads) to test this weird edge case, I can see about getting them to you. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216
https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216:50,Safety,recover,recoverOrphans,50,"UPDATE: On @rob-p 's suggestion, I removed the `--recoverOrphans` option and then all 60 samples did finished without segfaulting. Perhaps there were too many orphans to handle - alignments rates were a dismal 0.5-23%. These were heavily degraded samples that the sequencing center recommended not to sequence but the PI wanted to try it anyway. If you want a pair of fastq files (full or cutdown to ~5 M reads) to test this weird edge case, I can see about getting them to you. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216
https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216:415,Testability,test,test,415,"UPDATE: On @rob-p 's suggestion, I removed the `--recoverOrphans` option and then all 60 samples did finished without segfaulting. Perhaps there were too many orphans to handle - alignments rates were a dismal 0.5-23%. These were heavily degraded samples that the sequencing center recommended not to sequence but the PI wanted to try it anyway. If you want a pair of fastq files (full or cutdown to ~5 M reads) to test this weird edge case, I can see about getting them to you. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/668#issuecomment-862525216
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-859016646:234,Security,access,access,234,"Hi @curtisd0886,. Indeed; thanks for sharing! @k3yavi -- I think we should take a look [here](https://github.com/COMBINE-lab/salmon/blob/develop/src/SalmonAlevin.cpp#L152) and at the resulting implications. We've thus far had limited access to data with barcode lengths > 16, so I think we should try to evaluate if there are any other places we make such assumptions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-859016646
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860704025:92,Deployability,release,release,92,"Hi @curtisd0886,. So, issues relevant to processing this data should be resolved in the new release (v1.5.1). However, for technical reasons in the way different modes are handled internally, we had to simplify the mixing and matching of certain different options. Specifically, one can no longer use the `--citeseq` flag in conjunction with the custom geometry flags. So, if you have non-standard `--citeseq` geometry, the recommendation is to just use the new barcode specification format (e.g. `--umi-geometry`, `--bc-geometry` and `--read-geometry`), along with a couple of other flags. Specifically, you should explicitly provide `--keepCBFraction 1.0` and a tgMap file (even if it is just a trivial one mapping each feature to itself). @k3yavi can elaborate further if I've overlooked anything. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860704025
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860704025:202,Usability,simpl,simplify,202,"Hi @curtisd0886,. So, issues relevant to processing this data should be resolved in the new release (v1.5.1). However, for technical reasons in the way different modes are handled internally, we had to simplify the mixing and matching of certain different options. Specifically, one can no longer use the `--citeseq` flag in conjunction with the custom geometry flags. So, if you have non-standard `--citeseq` geometry, the recommendation is to just use the new barcode specification format (e.g. `--umi-geometry`, `--bc-geometry` and `--read-geometry`), along with a couple of other flags. Specifically, you should explicitly provide `--keepCBFraction 1.0` and a tgMap file (even if it is just a trivial one mapping each feature to itself). @k3yavi can elaborate further if I've overlooked anything. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860704025
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860924195:28,Deployability,update,update,28,"Hi @rob-p ,; Thanks for the update. This will be great. I tried using the 1.5.1 version today and I am still only getting the 16nt barcodes. I used the below commands. . sudo ~/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR -i ~/Data/salmon/cell_hash -1 R1.fq.gz -2 R2.fq.gz --read-geometry 2[1-end] --bc-geometry 1[3-8,24-29,45-50] --umi-geometry 1[51-56] -o /home/cndd3/Data/Multi_3/hash_1.5.1/ --citeseq --featureStart 0 --featureLength 15 —keepCBFraction 1 . I made sure to get rid of the --citeseq flag, but I am not sure if I am missing something else to get it working. . Thanks for your help with this!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860924195
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860925409:43,Testability,test,tested,43,"Ok, I'm tagging @k3yavi since I believe he tested the hot fix with the data you shared. Hey may have some more insight on what's going on here. By the way, the command you quote above still contains the `--citeseq` flag, but I assume that's just a typo.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860925409
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860927005:470,Availability,error,error,470,"hey try the following command, I double checked on 1.5.1 and it seemed to give the 18 length CBs:; ```; sudo ~/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR -i ~/Data/salmon/cell_hash -1 R1.fq.gz -2 R2.fq.gz --read-geometry 2[1-15] --bc-geometry 1[3-8,24-29,45-50] --umi-geometry 1[51-56] -o /home/cndd3/Data/Multi_3/hash_1.5.1/ —keepCBFraction 1 --tgMap <might have to create a tsv file with feature name tab feature name>; ```. If the program is not exiting with error with the command you shared then probably there is some error on the update as it should throw error when you simultaneously provide with `citeseq` and `geometry` flags.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860927005
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860927005:532,Availability,error,error,532,"hey try the following command, I double checked on 1.5.1 and it seemed to give the 18 length CBs:; ```; sudo ~/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR -i ~/Data/salmon/cell_hash -1 R1.fq.gz -2 R2.fq.gz --read-geometry 2[1-15] --bc-geometry 1[3-8,24-29,45-50] --umi-geometry 1[51-56] -o /home/cndd3/Data/Multi_3/hash_1.5.1/ —keepCBFraction 1 --tgMap <might have to create a tsv file with feature name tab feature name>; ```. If the program is not exiting with error with the command you shared then probably there is some error on the update as it should throw error when you simultaneously provide with `citeseq` and `geometry` flags.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860927005
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860927005:571,Availability,error,error,571,"hey try the following command, I double checked on 1.5.1 and it seemed to give the 18 length CBs:; ```; sudo ~/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR -i ~/Data/salmon/cell_hash -1 R1.fq.gz -2 R2.fq.gz --read-geometry 2[1-15] --bc-geometry 1[3-8,24-29,45-50] --umi-geometry 1[51-56] -o /home/cndd3/Data/Multi_3/hash_1.5.1/ —keepCBFraction 1 --tgMap <might have to create a tsv file with feature name tab feature name>; ```. If the program is not exiting with error with the command you shared then probably there is some error on the update as it should throw error when you simultaneously provide with `citeseq` and `geometry` flags.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860927005
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860927005:545,Deployability,update,update,545,"hey try the following command, I double checked on 1.5.1 and it seemed to give the 18 length CBs:; ```; sudo ~/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR -i ~/Data/salmon/cell_hash -1 R1.fq.gz -2 R2.fq.gz --read-geometry 2[1-15] --bc-geometry 1[3-8,24-29,45-50] --umi-geometry 1[51-56] -o /home/cndd3/Data/Multi_3/hash_1.5.1/ —keepCBFraction 1 --tgMap <might have to create a tsv file with feature name tab feature name>; ```. If the program is not exiting with error with the command you shared then probably there is some error on the update as it should throw error when you simultaneously provide with `citeseq` and `geometry` flags.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860927005
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860935816:39,Availability,fault,fault,39,@curtisd0886 -- I think this may be my fault. I think the pre-compiled binary I uploaded may be cut from the wrong tag. Let me fix it and report back here.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860935816
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860947066:269,Deployability,update,updated,269,"Ok @curtisd0886, it should be fixed now! Sorry for the mixup. Everything else (bioconda, docker, etc.) were cut from the tag, but the pre-compiled excitable was mistakenly copied over from the master branch (before the changes were merged in) rather than the tag. I've updated the executable.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860947066
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-861099975:140,Availability,down,down,140,"Thank you guys. The new software did the trick and now I am getting 18 nt barcodes, however it appears that the mapping efficiency has gone down significantly. Previously it was about 8% of reads now it like 5.3e-5%. Any ideas where the issue might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-861099975
https://github.com/COMBINE-lab/salmon/issues/671#issuecomment-860792782:81,Availability,error,error,81,"Hi @callumparr,. Thank you for the detailed report. I think this is primarily an error in documentation. The `gencode` flag is intended to be used only with ""mapping-based"" mode and when creating the index. It ensures that the index (and hence the resulting quantifications) contain the shortened names for gencode transcripts. In alignment-based mode, the behavior of the flag would have to be slightly different. It would have to re-normalize not only the names of the reference sequences in the fasta file, but it would also have to re-normalize the names in the BAM header so that they match. Specifically, the requirement is that the names of the sequences in the input fasta file are a 1-1 match with the names in the BAM header so that transcripts can be matched up properly with their sequences for training and applying the error model. However, if your BAM file already contains the stripped transcript names (i.e. if the BAM file header has the names without everything past the initial `|`), then I believe you can use the following command to have salmon do the same to the fasta file on the fly, so that the names match. ```{bash}; salmon-1.5.1_linux_x86_64/bin/salmon quant --ont -p 4 -t <(awk '{ if ($0 ~ ""^>"") { split($0,a,""|""); print a[1] } else { print $0 } }' Genome_files/gencode.vM24.transcripts.fa) -l U -a Documents/Day2_03_DRS_pass.bam -o Documents/counts/Day2_03_DRS_pass; ```. If the BAM file contains the ""full"" transcript name however, I think the current options are either to let salmon use the full transcript name from the fasta file, or to modify the GTF when running with minimap2, so that the BAM file itself contains the shortened names. Finally, I'd like to mention that the way you _intended_ to use the `--gencode` flag in alignment mode actually makes _a lot_ of sense, and I think it would be a very nice feature. Basically, the idea would be to apply stripping everything after the first `|` from *both* the fasta header and the BAM header, and using the red",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/671#issuecomment-860792782
https://github.com/COMBINE-lab/salmon/issues/671#issuecomment-860792782:833,Availability,error,error,833,"Hi @callumparr,. Thank you for the detailed report. I think this is primarily an error in documentation. The `gencode` flag is intended to be used only with ""mapping-based"" mode and when creating the index. It ensures that the index (and hence the resulting quantifications) contain the shortened names for gencode transcripts. In alignment-based mode, the behavior of the flag would have to be slightly different. It would have to re-normalize not only the names of the reference sequences in the fasta file, but it would also have to re-normalize the names in the BAM header so that they match. Specifically, the requirement is that the names of the sequences in the input fasta file are a 1-1 match with the names in the BAM header so that transcripts can be matched up properly with their sequences for training and applying the error model. However, if your BAM file already contains the stripped transcript names (i.e. if the BAM file header has the names without everything past the initial `|`), then I believe you can use the following command to have salmon do the same to the fasta file on the fly, so that the names match. ```{bash}; salmon-1.5.1_linux_x86_64/bin/salmon quant --ont -p 4 -t <(awk '{ if ($0 ~ ""^>"") { split($0,a,""|""); print a[1] } else { print $0 } }' Genome_files/gencode.vM24.transcripts.fa) -l U -a Documents/Day2_03_DRS_pass.bam -o Documents/counts/Day2_03_DRS_pass; ```. If the BAM file contains the ""full"" transcript name however, I think the current options are either to let salmon use the full transcript name from the fasta file, or to modify the GTF when running with minimap2, so that the BAM file itself contains the shortened names. Finally, I'd like to mention that the way you _intended_ to use the `--gencode` flag in alignment mode actually makes _a lot_ of sense, and I think it would be a very nice feature. Basically, the idea would be to apply stripping everything after the first `|` from *both* the fasta header and the BAM header, and using the red",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/671#issuecomment-860792782
https://github.com/COMBINE-lab/salmon/issues/671#issuecomment-860792782:2101,Deployability,release,release,2101,"ting the index. It ensures that the index (and hence the resulting quantifications) contain the shortened names for gencode transcripts. In alignment-based mode, the behavior of the flag would have to be slightly different. It would have to re-normalize not only the names of the reference sequences in the fasta file, but it would also have to re-normalize the names in the BAM header so that they match. Specifically, the requirement is that the names of the sequences in the input fasta file are a 1-1 match with the names in the BAM header so that transcripts can be matched up properly with their sequences for training and applying the error model. However, if your BAM file already contains the stripped transcript names (i.e. if the BAM file header has the names without everything past the initial `|`), then I believe you can use the following command to have salmon do the same to the fasta file on the fly, so that the names match. ```{bash}; salmon-1.5.1_linux_x86_64/bin/salmon quant --ont -p 4 -t <(awk '{ if ($0 ~ ""^>"") { split($0,a,""|""); print a[1] } else { print $0 } }' Genome_files/gencode.vM24.transcripts.fa) -l U -a Documents/Day2_03_DRS_pass.bam -o Documents/counts/Day2_03_DRS_pass; ```. If the BAM file contains the ""full"" transcript name however, I think the current options are either to let salmon use the full transcript name from the fasta file, or to modify the GTF when running with minimap2, so that the BAM file itself contains the shortened names. Finally, I'd like to mention that the way you _intended_ to use the `--gencode` flag in alignment mode actually makes _a lot_ of sense, and I think it would be a very nice feature. Basically, the idea would be to apply stripping everything after the first `|` from *both* the fasta header and the BAM header, and using the reduced names for `quant.sf` outputs. We'll certainly. look into adding this functionality in a future release. Apologies for confusion caused by the ambiguous documentation of this flag. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/671#issuecomment-860792782
https://github.com/COMBINE-lab/salmon/issues/671#issuecomment-860792782:1998,Energy Efficiency,reduce,reduced,1998,"ting the index. It ensures that the index (and hence the resulting quantifications) contain the shortened names for gencode transcripts. In alignment-based mode, the behavior of the flag would have to be slightly different. It would have to re-normalize not only the names of the reference sequences in the fasta file, but it would also have to re-normalize the names in the BAM header so that they match. Specifically, the requirement is that the names of the sequences in the input fasta file are a 1-1 match with the names in the BAM header so that transcripts can be matched up properly with their sequences for training and applying the error model. However, if your BAM file already contains the stripped transcript names (i.e. if the BAM file header has the names without everything past the initial `|`), then I believe you can use the following command to have salmon do the same to the fasta file on the fly, so that the names match. ```{bash}; salmon-1.5.1_linux_x86_64/bin/salmon quant --ont -p 4 -t <(awk '{ if ($0 ~ ""^>"") { split($0,a,""|""); print a[1] } else { print $0 } }' Genome_files/gencode.vM24.transcripts.fa) -l U -a Documents/Day2_03_DRS_pass.bam -o Documents/counts/Day2_03_DRS_pass; ```. If the BAM file contains the ""full"" transcript name however, I think the current options are either to let salmon use the full transcript name from the fasta file, or to modify the GTF when running with minimap2, so that the BAM file itself contains the shortened names. Finally, I'd like to mention that the way you _intended_ to use the `--gencode` flag in alignment mode actually makes _a lot_ of sense, and I think it would be a very nice feature. Basically, the idea would be to apply stripping everything after the first `|` from *both* the fasta header and the BAM header, and using the reduced names for `quant.sf` outputs. We'll certainly. look into adding this functionality in a future release. Apologies for confusion caused by the ambiguous documentation of this flag. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/671#issuecomment-860792782
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-866187414:546,Deployability,pipeline,pipeline,546,"Hi @rob-p,. Thank you for your reply.; It's a BWA-mem2 aligment with this command:. ```; ""bwa-mem2 mem -M -t {threads} -v 2 {input.reference} {input.reads} | samtools view -q 20 -F 3844 --threads {threads} -Sb -> {output.inter_bam} && ""; ""samtools sort -@ {threads} -O bam {output.inter_bam} > {output.final_bam} && samtools index -@ {threads} {output.final_bam} && samtools flagstat {output.final_bam} > {output.flag} ""; ```; I don't if it's because I use BWA which is a non-splicing aligner? Or because I sorted my BAM file?; I am developing a pipeline and the first step is to test the data with bwa-mem2 with salmon. Is it really a problem to use the results with the MU lib? The 60% may be wrong and reflect alignments that don't exist and ignore good alignments because of the wrong lib?. ### Edit. I tried to do the mapping and the aligment with salmon:; ```. {; ""read_files"": ""[ ../results/trimmed/3373-1_CCGCGGTT-CTAGCGCT-AHV5HLDSXY_L004_R1_trimmed.fastq.gz, ../results/trimmed/3373-1_CCGCGGTT-CTAGCGCT-AHV5HLDSXY_L004_R2_trimmed.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 35843765,; ""num_assigned_fragments"": 35843765,; ""num_frags_with_concordant_consistent_mappings"": 29709658,; ""num_frags_with_inconsistent_or_orphan_mappings"": 6209768,; ""strand_mapping_bias"": 0.000008381042727599249,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 249,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 29709658,; ""SF"": 2520360,; ""SR"": 3689159,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```; It's the ISR library but I have only 40% of mapping , it's really confusing.... Best,; Kisekya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-866187414
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-866187414:580,Testability,test,test,580,"Hi @rob-p,. Thank you for your reply.; It's a BWA-mem2 aligment with this command:. ```; ""bwa-mem2 mem -M -t {threads} -v 2 {input.reference} {input.reads} | samtools view -q 20 -F 3844 --threads {threads} -Sb -> {output.inter_bam} && ""; ""samtools sort -@ {threads} -O bam {output.inter_bam} > {output.final_bam} && samtools index -@ {threads} {output.final_bam} && samtools flagstat {output.final_bam} > {output.flag} ""; ```; I don't if it's because I use BWA which is a non-splicing aligner? Or because I sorted my BAM file?; I am developing a pipeline and the first step is to test the data with bwa-mem2 with salmon. Is it really a problem to use the results with the MU lib? The 60% may be wrong and reflect alignments that don't exist and ignore good alignments because of the wrong lib?. ### Edit. I tried to do the mapping and the aligment with salmon:; ```. {; ""read_files"": ""[ ../results/trimmed/3373-1_CCGCGGTT-CTAGCGCT-AHV5HLDSXY_L004_R1_trimmed.fastq.gz, ../results/trimmed/3373-1_CCGCGGTT-CTAGCGCT-AHV5HLDSXY_L004_R2_trimmed.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 35843765,; ""num_assigned_fragments"": 35843765,; ""num_frags_with_concordant_consistent_mappings"": 29709658,; ""num_frags_with_inconsistent_or_orphan_mappings"": 6209768,; ""strand_mapping_bias"": 0.000008381042727599249,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 249,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 29709658,; ""SF"": 2520360,; ""SR"": 3689159,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```; It's the ISR library but I have only 40% of mapping , it's really confusing.... Best,; Kisekya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-866187414
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-873519594:1254,Availability,reliab,reliable,1254,"Hi @Kisekya,. So BWA-MEM and BWA-MEM2 are somewhat of a problem to begin with because they perform local alignment, which isn't really ideal for aligning RNA-seq reads to the transcriptome. If you really wish to use an aligner, we've had good experiences with Bowtie2 (when used in the appropriate end-to-end alignment mode) and with STAR (using the alignments projected to the transcriptome with `--quantMode TranscriptomeSAM` flag to output the alignments in transcriptomic coordinates as required by salmon). Apart from the local alignment issue, sorting the BAM file is _absolutely_ a problem for salmon, and is likely why you get the strange library type. When run in alignment mode, just like RSEM, salmon requires the alignments for the the mates of a read pair to appear subsequently in the file, and for all alignments for a given read to appear contiguously in the file. This allows parsing the reads without having to require potentially unbounded memory (holding the record for one end of a fragment in memory while waiting for the record for the other end). In fact, given that you've sorted the alignments here, I'm surprised you're not getting the ""suspicious pair"" warnings in your logs. The ISR library with 40% mapping is likely a more reliable number. The obvious question here is why might the mapping rate be this low? There are a few reasons you might see something like this. One, for example, is poor ribosomal depletion, paired with not having all of the rRNA sequences in your index. In this case, you have many fewer reads coming from the rest of the transcriptome and you get depleted mapping rates like this. . Could you say a bit more about the experimental setup? Is this in a well-annotated organism like human / mouse etc.? Is this a polyA selection or ribosomal depletion prep? Anything else that might be relevant to sample quality?. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-873519594
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-873519594:91,Performance,perform,perform,91,"Hi @Kisekya,. So BWA-MEM and BWA-MEM2 are somewhat of a problem to begin with because they perform local alignment, which isn't really ideal for aligning RNA-seq reads to the transcriptome. If you really wish to use an aligner, we've had good experiences with Bowtie2 (when used in the appropriate end-to-end alignment mode) and with STAR (using the alignments projected to the transcriptome with `--quantMode TranscriptomeSAM` flag to output the alignments in transcriptomic coordinates as required by salmon). Apart from the local alignment issue, sorting the BAM file is _absolutely_ a problem for salmon, and is likely why you get the strange library type. When run in alignment mode, just like RSEM, salmon requires the alignments for the the mates of a read pair to appear subsequently in the file, and for all alignments for a given read to appear contiguously in the file. This allows parsing the reads without having to require potentially unbounded memory (holding the record for one end of a fragment in memory while waiting for the record for the other end). In fact, given that you've sorted the alignments here, I'm surprised you're not getting the ""suspicious pair"" warnings in your logs. The ISR library with 40% mapping is likely a more reliable number. The obvious question here is why might the mapping rate be this low? There are a few reasons you might see something like this. One, for example, is poor ribosomal depletion, paired with not having all of the rRNA sequences in your index. In this case, you have many fewer reads coming from the rest of the transcriptome and you get depleted mapping rates like this. . Could you say a bit more about the experimental setup? Is this in a well-annotated organism like human / mouse etc.? Is this a polyA selection or ribosomal depletion prep? Anything else that might be relevant to sample quality?. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-873519594
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-873519594:1198,Testability,log,logs,1198,"Hi @Kisekya,. So BWA-MEM and BWA-MEM2 are somewhat of a problem to begin with because they perform local alignment, which isn't really ideal for aligning RNA-seq reads to the transcriptome. If you really wish to use an aligner, we've had good experiences with Bowtie2 (when used in the appropriate end-to-end alignment mode) and with STAR (using the alignments projected to the transcriptome with `--quantMode TranscriptomeSAM` flag to output the alignments in transcriptomic coordinates as required by salmon). Apart from the local alignment issue, sorting the BAM file is _absolutely_ a problem for salmon, and is likely why you get the strange library type. When run in alignment mode, just like RSEM, salmon requires the alignments for the the mates of a read pair to appear subsequently in the file, and for all alignments for a given read to appear contiguously in the file. This allows parsing the reads without having to require potentially unbounded memory (holding the record for one end of a fragment in memory while waiting for the record for the other end). In fact, given that you've sorted the alignments here, I'm surprised you're not getting the ""suspicious pair"" warnings in your logs. The ISR library with 40% mapping is likely a more reliable number. The obvious question here is why might the mapping rate be this low? There are a few reasons you might see something like this. One, for example, is poor ribosomal depletion, paired with not having all of the rRNA sequences in your index. In this case, you have many fewer reads coming from the rest of the transcriptome and you get depleted mapping rates like this. . Could you say a bit more about the experimental setup? Is this in a well-annotated organism like human / mouse etc.? Is this a polyA selection or ribosomal depletion prep? Anything else that might be relevant to sample quality?. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-873519594
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:4633,Availability,fault,fault,4633,"too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	0; % of reads unmapped: too short |	0.00%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%; ```. It was really better but I am afraid that I have really low quality (I try the parameter 0.3 when I wrote these lines ), I filtered again with samtools -f 2 -F3840 and the salmon counts which is still very low : 24323720 counts. I used samtools flagstat to see what happens after the filter and we get this?; ```; 48983692 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 48983692 + 0 mapped (100.00% : N/A); 48983692 + 0 paired in sequencing; 24491846 + 0 read1; 24491846 + 0 read2; 48983692 + 0 properly paired (100.00% : N/A); 48983692 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. I don't understand why I'm losing so many counts, is it because I'm filtering? But still I have to filter to get the properly pairs... For the sorting it's totally my fault I read the doc wrong but even by not sorting I get very low results not usable less than 26%. The experimentation is done on oak, on 4 times 3 late samples and 3 early samples of dormancy were recovered and we made a TruSeq stranded illumina on these samples. I use a gene model built by my team with the 25808 genes that the oak has as reference. For this part ""Is this a polyA selection or ribosomal depletion prep"" I don't know, I'll find out. To be honest I am totally lost because I don't understand what's wrong in my analysis.... Thank you very much for your help once again . Kisekya. EDIT:. I discover that I have 59 millions of duplicates in my data...; I tried to delete it after filtering my proper pair I get bad records 38% of mapping ...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:4832,Availability,recover,recovered,4832,"too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	0; % of reads unmapped: too short |	0.00%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%; ```. It was really better but I am afraid that I have really low quality (I try the parameter 0.3 when I wrote these lines ), I filtered again with samtools -f 2 -F3840 and the salmon counts which is still very low : 24323720 counts. I used samtools flagstat to see what happens after the filter and we get this?; ```; 48983692 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 48983692 + 0 mapped (100.00% : N/A); 48983692 + 0 paired in sequencing; 24491846 + 0 read1; 24491846 + 0 read2; 48983692 + 0 properly paired (100.00% : N/A); 48983692 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. I don't understand why I'm losing so many counts, is it because I'm filtering? But still I have to filter to get the properly pairs... For the sorting it's totally my fault I read the doc wrong but even by not sorting I get very low results not usable less than 26%. The experimentation is done on oak, on 4 times 3 late samples and 3 early samples of dormancy were recovered and we made a TruSeq stranded illumina on these samples. I use a gene model built by my team with the 25808 genes that the oak has as reference. For this part ""Is this a polyA selection or ribosomal depletion prep"" I don't know, I'll find out. To be honest I am totally lost because I don't understand what's wrong in my analysis.... Thank you very much for your help once again . Kisekya. EDIT:. I discover that I have 59 millions of duplicates in my data...; I tried to delete it after filtering my proper pair I get bad records 38% of mapping ...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:1867,Energy Efficiency,reduce,reduce,1867,"mber of splices: GC/AG |	39101; Number of splices: AT/AC |	13983; Number of splices: Non-canonical |	478779; Mismatch rate per base, % |	0.56%; Deletion rate per base |	0.03%; Deletion average length |	4.89; Insertion rate per base |	0.03%; Insertion average length |	4.88; MULTI-MAPPING READS:; Number of reads mapped to multiple loci |	1029261; % of reads mapped to multiple loci |	1.20%; Number of reads mapped to too many loci |	565; % of reads mapped to too many loci |	0.00%; UNMAPPED READS:; Number of reads unmapped: too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	47533174; % of reads unmapped: too short |	55.56%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%. ```. I filtered it by samtools -f 2 -F 3840 . and Salmon gave me this result which is still very weak: 24323638 counts. So I decided to reduce the parameters as indicated in this link: https://github.com/alexdobin/STAR/issues/169; Because I trimmed my sequence and some can have a size between 100pb -150pb. ` ""STAR --runThreadN {threads} --runMode alignReads --genomeDir {input.ref} --readFilesIn {input.fq1} {input.fq2} --readFilesCommand zcat --outSAMtype BAM Unsorted SortedByCoordinate --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --quantMode TranscriptomeSAM GeneCounts --outFileNamePrefix {output} --outStd Log {log} ""`. I got this final.out:; ```; Started job on |	Jul 05 14:25:19; Started mapping on |	Jul 05 14:25:23; Finished on |	Jul 05 16:37:44; Mapping speed, Million of reads per hour |	38.78. Number of input reads |	85547657; Average input read length |	298; UNIQUE READS:; Uniquely mapped reads number |	70090369; Uniquely mapped reads % |	81.93%; Average mapped length |	191.51; Number of splices: Total |	1068826; Number of splices: Annotated (sjdb) |	0; Number of splices: GT/AG |	470490; Number of splices: GC/AG |	43525",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:4832,Safety,recover,recovered,4832,"too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	0; % of reads unmapped: too short |	0.00%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%; ```. It was really better but I am afraid that I have really low quality (I try the parameter 0.3 when I wrote these lines ), I filtered again with samtools -f 2 -F3840 and the salmon counts which is still very low : 24323720 counts. I used samtools flagstat to see what happens after the filter and we get this?; ```; 48983692 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 48983692 + 0 mapped (100.00% : N/A); 48983692 + 0 paired in sequencing; 24491846 + 0 read1; 24491846 + 0 read2; 48983692 + 0 properly paired (100.00% : N/A); 48983692 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. I don't understand why I'm losing so many counts, is it because I'm filtering? But still I have to filter to get the properly pairs... For the sorting it's totally my fault I read the doc wrong but even by not sorting I get very low results not usable less than 26%. The experimentation is done on oak, on 4 times 3 late samples and 3 early samples of dormancy were recovered and we made a TruSeq stranded illumina on these samples. I use a gene model built by my team with the 25808 genes that the oak has as reference. For this part ""Is this a polyA selection or ribosomal depletion prep"" I don't know, I'll find out. To be honest I am totally lost because I don't understand what's wrong in my analysis.... Thank you very much for your help once again . Kisekya. EDIT:. I discover that I have 59 millions of duplicates in my data...; I tried to delete it after filtering my proper pair I get bad records 38% of mapping ...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:388,Testability,Log,Log,388,"Hi @rob-p,; I am aware of that, but we were off on bwa anyway. I decided to follow your advice and used STAR with this order:. ` ""STAR --runThreadN {threads} --runMode alignReads --genomeDir {input.ref} --readFilesIn {input.fq1} {input.fq2} --readFilesCommand zcat --outSAMtype BAM Unsorted SortedByCoordinate --quantMode TranscriptomeSAM GeneCounts --outFileNamePrefix {output} --outStd Log {log} ""`. I then got this final.out:; ```. Started job on |	Jul 05 07:51:09; Started mapping on |	Jul 05 07:51:13; Finished on |	Jul 05 10:01:38; Mapping speed, Million of reads per hour |	39.36. Number of input reads |	85547657; Average input read length |	298; UNIQUE READS:; Uniquely mapped reads number |	36980651; Uniquely mapped reads % |	43.23%; Average mapped length |	283.47; Number of splices: Total |	943061; Number of splices: Annotated (sjdb) |	0; Number of splices: GT/AG |	411198; Number of splices: GC/AG |	39101; Number of splices: AT/AC |	13983; Number of splices: Non-canonical |	478779; Mismatch rate per base, % |	0.56%; Deletion rate per base |	0.03%; Deletion average length |	4.89; Insertion rate per base |	0.03%; Insertion average length |	4.88; MULTI-MAPPING READS:; Number of reads mapped to multiple loci |	1029261; % of reads mapped to multiple loci |	1.20%; Number of reads mapped to too many loci |	565; % of reads mapped to too many loci |	0.00%; UNMAPPED READS:; Number of reads unmapped: too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	47533174; % of reads unmapped: too short |	55.56%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%. ```. I filtered it by samtools -f 2 -F 3840 . and Salmon gave me this result which is still very weak: 24323638 counts. So I decided to reduce the parameters as indicated in this link: https://github.com/alexdobin/STAR/issues/169; Because I trimmed my sequence and some ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:393,Testability,log,log,393,"Hi @rob-p,; I am aware of that, but we were off on bwa anyway. I decided to follow your advice and used STAR with this order:. ` ""STAR --runThreadN {threads} --runMode alignReads --genomeDir {input.ref} --readFilesIn {input.fq1} {input.fq2} --readFilesCommand zcat --outSAMtype BAM Unsorted SortedByCoordinate --quantMode TranscriptomeSAM GeneCounts --outFileNamePrefix {output} --outStd Log {log} ""`. I then got this final.out:; ```. Started job on |	Jul 05 07:51:09; Started mapping on |	Jul 05 07:51:13; Finished on |	Jul 05 10:01:38; Mapping speed, Million of reads per hour |	39.36. Number of input reads |	85547657; Average input read length |	298; UNIQUE READS:; Uniquely mapped reads number |	36980651; Uniquely mapped reads % |	43.23%; Average mapped length |	283.47; Number of splices: Total |	943061; Number of splices: Annotated (sjdb) |	0; Number of splices: GT/AG |	411198; Number of splices: GC/AG |	39101; Number of splices: AT/AC |	13983; Number of splices: Non-canonical |	478779; Mismatch rate per base, % |	0.56%; Deletion rate per base |	0.03%; Deletion average length |	4.89; Insertion rate per base |	0.03%; Insertion average length |	4.88; MULTI-MAPPING READS:; Number of reads mapped to multiple loci |	1029261; % of reads mapped to multiple loci |	1.20%; Number of reads mapped to too many loci |	565; % of reads mapped to too many loci |	0.00%; UNMAPPED READS:; Number of reads unmapped: too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	47533174; % of reads unmapped: too short |	55.56%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%. ```. I filtered it by samtools -f 2 -F 3840 . and Salmon gave me this result which is still very weak: 24323638 counts. So I decided to reduce the parameters as indicated in this link: https://github.com/alexdobin/STAR/issues/169; Because I trimmed my sequence and some ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:2363,Testability,Log,Log,2363,"iple loci |	1.20%; Number of reads mapped to too many loci |	565; % of reads mapped to too many loci |	0.00%; UNMAPPED READS:; Number of reads unmapped: too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	47533174; % of reads unmapped: too short |	55.56%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%. ```. I filtered it by samtools -f 2 -F 3840 . and Salmon gave me this result which is still very weak: 24323638 counts. So I decided to reduce the parameters as indicated in this link: https://github.com/alexdobin/STAR/issues/169; Because I trimmed my sequence and some can have a size between 100pb -150pb. ` ""STAR --runThreadN {threads} --runMode alignReads --genomeDir {input.ref} --readFilesIn {input.fq1} {input.fq2} --readFilesCommand zcat --outSAMtype BAM Unsorted SortedByCoordinate --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --quantMode TranscriptomeSAM GeneCounts --outFileNamePrefix {output} --outStd Log {log} ""`. I got this final.out:; ```; Started job on |	Jul 05 14:25:19; Started mapping on |	Jul 05 14:25:23; Finished on |	Jul 05 16:37:44; Mapping speed, Million of reads per hour |	38.78. Number of input reads |	85547657; Average input read length |	298; UNIQUE READS:; Uniquely mapped reads number |	70090369; Uniquely mapped reads % |	81.93%; Average mapped length |	191.51; Number of splices: Total |	1068826; Number of splices: Annotated (sjdb) |	0; Number of splices: GT/AG |	470490; Number of splices: GC/AG |	43525; Number of splices: AT/AC |	15865; Number of splices: Non-canonical |	538946; Mismatch rate per base, % |	1.29%; Deletion rate per base |	0.03%; Deletion average length |	4.76; Insertion rate per base |	0.03%; Insertion average length |	5.23; MULTI-MAPPING READS:; Number of reads mapped to multiple loci |	15205492; % of reads mapped to multiple loci |	17.77%; Number o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:2368,Testability,log,log,2368,"iple loci |	1.20%; Number of reads mapped to too many loci |	565; % of reads mapped to too many loci |	0.00%; UNMAPPED READS:; Number of reads unmapped: too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	47533174; % of reads unmapped: too short |	55.56%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%. ```. I filtered it by samtools -f 2 -F 3840 . and Salmon gave me this result which is still very weak: 24323638 counts. So I decided to reduce the parameters as indicated in this link: https://github.com/alexdobin/STAR/issues/169; Because I trimmed my sequence and some can have a size between 100pb -150pb. ` ""STAR --runThreadN {threads} --runMode alignReads --genomeDir {input.ref} --readFilesIn {input.fq1} {input.fq2} --readFilesCommand zcat --outSAMtype BAM Unsorted SortedByCoordinate --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --quantMode TranscriptomeSAM GeneCounts --outFileNamePrefix {output} --outStd Log {log} ""`. I got this final.out:; ```; Started job on |	Jul 05 14:25:19; Started mapping on |	Jul 05 14:25:23; Finished on |	Jul 05 16:37:44; Mapping speed, Million of reads per hour |	38.78. Number of input reads |	85547657; Average input read length |	298; UNIQUE READS:; Uniquely mapped reads number |	70090369; Uniquely mapped reads % |	81.93%; Average mapped length |	191.51; Number of splices: Total |	1068826; Number of splices: Annotated (sjdb) |	0; Number of splices: GT/AG |	470490; Number of splices: GC/AG |	43525; Number of splices: AT/AC |	15865; Number of splices: Non-canonical |	538946; Mismatch rate per base, % |	1.29%; Deletion rate per base |	0.03%; Deletion average length |	4.76; Insertion rate per base |	0.03%; Insertion average length |	5.23; MULTI-MAPPING READS:; Number of reads mapped to multiple loci |	15205492; % of reads mapped to multiple loci |	17.77%; Number o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:4711,Usability,usab,usable,4711,"too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	0; % of reads unmapped: too short |	0.00%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%; ```. It was really better but I am afraid that I have really low quality (I try the parameter 0.3 when I wrote these lines ), I filtered again with samtools -f 2 -F3840 and the salmon counts which is still very low : 24323720 counts. I used samtools flagstat to see what happens after the filter and we get this?; ```; 48983692 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 48983692 + 0 mapped (100.00% : N/A); 48983692 + 0 paired in sequencing; 24491846 + 0 read1; 24491846 + 0 read2; 48983692 + 0 properly paired (100.00% : N/A); 48983692 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. I don't understand why I'm losing so many counts, is it because I'm filtering? But still I have to filter to get the properly pairs... For the sorting it's totally my fault I read the doc wrong but even by not sorting I get very low results not usable less than 26%. The experimentation is done on oak, on 4 times 3 late samples and 3 early samples of dormancy were recovered and we made a TruSeq stranded illumina on these samples. I use a gene model built by my team with the 25808 genes that the oak has as reference. For this part ""Is this a polyA selection or ribosomal depletion prep"" I don't know, I'll find out. To be honest I am totally lost because I don't understand what's wrong in my analysis.... Thank you very much for your help once again . Kisekya. EDIT:. I discover that I have 59 millions of duplicates in my data...; I tried to delete it after filtering my proper pair I get bad records 38% of mapping ...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/678#issuecomment-1138623879:98,Usability,clear,clearly,98,"I am not sure what this `-m` flag refers to. It is not currently an option, and doesn't appear to clearly have been one in the past either.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/678#issuecomment-1138623879
https://github.com/COMBINE-lab/salmon/issues/679#issuecomment-1036315058:340,Availability,failure,failure,340,"Hi @kate-simonova,. How low is your mapping rate? Can you explicitly pass the flag `-l IU` for library type? This strand bias means that the data look stranded however. Also, can you mention how the mapping rate changes if you add `--softclip` and/or if you lower `--minScoreFraction`? If the mapping rate is very low, this could signify a failure of the sample to match the reference well. Could you post the contents of `meta_info.json`?. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/679#issuecomment-1036315058
https://github.com/COMBINE-lab/salmon/issues/679#issuecomment-1036524215:1175,Deployability,pipeline,pipeline,1175,"Hi @kate-simonova,. While I would certainly recommend updating to the latest version of salmon (which, given the pre 1.0.0 to post 1.0.0 difference would require you to rebuild the index), I don't think that would have a substantial effect on a mapping rate that is this low. If the Fastqscreen report suggests that most of the reads map to the genome (>75%), but you are seeing an 8% mapping rate in salmon, this highly suggests that most of the reads are, for some reason, arising from outside of an annotated gene. I would then have two suggestions to test out:. 1.) Check for mtRNA contamination. Try adding extra mitochondrial RNA to your reference fasta, re-indexing, and re-quantifying. If mtRNA depletion or polyA enrichment failed, then it's possible that you have most of your RNA-seq reads coming from mt genes. I've seen this before a number of times and it results in a situation where most of the reads map back to the genome — but not the annotated transcriptome, which often has an incomplete set of mtRNA sequences. 2.) Try mapping the reads to the genome and see how many reads overlap known genes. This is what you would do with a ""counting-based"" RNA-seq pipeline, so something like STAR+feature-counts or subread+feature-counts. While I would generally not recommend this for quantification, it can be instructive to see the fraction of reads that map to the genome but not to known transcripts. Likewise, you could (with the newest salmon) build an index on the transcriptome with the genome added as a decoy (see about our decoy-aware indexing), then the `meta_info.json` will let you know the fraction of reads that were discarded because they were best matched to a decoy sequence (in this case, the genome, but not some annotated transcript). This should help clarify what's going on, and might suggest some issues with the sample that are preventing a reasonable mapping rate to the annotated transcriptome. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/679#issuecomment-1036524215
https://github.com/COMBINE-lab/salmon/issues/679#issuecomment-1036524215:555,Testability,test,test,555,"Hi @kate-simonova,. While I would certainly recommend updating to the latest version of salmon (which, given the pre 1.0.0 to post 1.0.0 difference would require you to rebuild the index), I don't think that would have a substantial effect on a mapping rate that is this low. If the Fastqscreen report suggests that most of the reads map to the genome (>75%), but you are seeing an 8% mapping rate in salmon, this highly suggests that most of the reads are, for some reason, arising from outside of an annotated gene. I would then have two suggestions to test out:. 1.) Check for mtRNA contamination. Try adding extra mitochondrial RNA to your reference fasta, re-indexing, and re-quantifying. If mtRNA depletion or polyA enrichment failed, then it's possible that you have most of your RNA-seq reads coming from mt genes. I've seen this before a number of times and it results in a situation where most of the reads map back to the genome — but not the annotated transcriptome, which often has an incomplete set of mtRNA sequences. 2.) Try mapping the reads to the genome and see how many reads overlap known genes. This is what you would do with a ""counting-based"" RNA-seq pipeline, so something like STAR+feature-counts or subread+feature-counts. While I would generally not recommend this for quantification, it can be instructive to see the fraction of reads that map to the genome but not to known transcripts. Likewise, you could (with the newest salmon) build an index on the transcriptome with the genome added as a decoy (see about our decoy-aware indexing), then the `meta_info.json` will let you know the fraction of reads that were discarded because they were best matched to a decoy sequence (in this case, the genome, but not some annotated transcript). This should help clarify what's going on, and might suggest some issues with the sample that are preventing a reasonable mapping rate to the annotated transcriptome. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/679#issuecomment-1036524215
https://github.com/COMBINE-lab/salmon/issues/681#issuecomment-873025258:1074,Availability,down,downstream,1074,"Hi @joshstolz,. Is there a reason you want to restrict the possible mapping to this set of 1800 transcripts? In general, mapping to a small subset of the reference is not ideal, because alignment works based on asking if the ""best hit"" for a read is good enough, so its easy to come up with scenarios where the is a read that matches some transcript very well, and another transcript ""alright"" --- if the transcript it matches very well is in the reference, then it will map there, otherwise it will likely map to the transcript where the match is just ""alright"". TLDR; the set of references included can affect the mappings. That being said, if there is a good technical reason you have for only including a subset of transcripts, that's easy to do. You just build your index on only those transcripts. Salmon treats the reference sequences you feed to it as the ""transcriptome"" and will map to that. Also, you could map to the full transcriptome and just extract the rows for these targets from the `quant.sf` files at the end. Of course, depending on what you want to do downstream, you'll have to be aware of how these 1800 transcripts fit into the bigger picture and how the reads that mapped to the other transcripts affect your belief about things like the effective library size etc.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/681#issuecomment-873025258
https://github.com/COMBINE-lab/salmon/issues/681#issuecomment-873025258:1041,Integrability,depend,depending,1041,"Hi @joshstolz,. Is there a reason you want to restrict the possible mapping to this set of 1800 transcripts? In general, mapping to a small subset of the reference is not ideal, because alignment works based on asking if the ""best hit"" for a read is good enough, so its easy to come up with scenarios where the is a read that matches some transcript very well, and another transcript ""alright"" --- if the transcript it matches very well is in the reference, then it will map there, otherwise it will likely map to the transcript where the match is just ""alright"". TLDR; the set of references included can affect the mappings. That being said, if there is a good technical reason you have for only including a subset of transcripts, that's easy to do. You just build your index on only those transcripts. Salmon treats the reference sequences you feed to it as the ""transcriptome"" and will map to that. Also, you could map to the full transcriptome and just extract the rows for these targets from the `quant.sf` files at the end. Of course, depending on what you want to do downstream, you'll have to be aware of how these 1800 transcripts fit into the bigger picture and how the reads that mapped to the other transcripts affect your belief about things like the effective library size etc.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/681#issuecomment-873025258
https://github.com/COMBINE-lab/salmon/issues/681#issuecomment-873058012:134,Deployability,pipeline,pipeline,134,These 1800 transcripts have a degradation signal we are looking to allow other researchers to model. The thought was we could build a pipeline to do so and that by restricting it to the 1800 for salmon it might not be computationally inaccessible.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/681#issuecomment-873058012
https://github.com/COMBINE-lab/salmon/issues/681#issuecomment-1138621099:79,Availability,down,down,79,"I think the recommended approach would be to map against everything and filter down to only the information for these 1800 transcripts. Let us know if that process leads to further questions. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/681#issuecomment-1138621099
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-876797474:92,Testability,test,testing,92,"Thank you very much, @k3yavi ! The references to the other discussion is very helpful. I am testing what happens if I use `-lISF` (instead of `-lISR`). Thanks again!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-876797474
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669:66,Availability,error,error,66,"I tested changing the parameters, and I am still getting the same error message:. ```; [2021-07-08 16:05:50.979] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-08 16:05:50.979] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. However, I will look into the other information and see if I can understand what is happening. I can also test not using a white list and see if that changes the number to be something like an order of magnitude different that what I would expect from the other sample. So, I will post an update when I can run alevin without an error message, and try to give some sense of the results that are quantified.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669:207,Availability,error,error,207,"I tested changing the parameters, and I am still getting the same error message:. ```; [2021-07-08 16:05:50.979] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-08 16:05:50.979] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. However, I will look into the other information and see if I can understand what is happening. I can also test not using a white list and see if that changes the number to be something like an order of magnitude different that what I would expect from the other sample. So, I will post an update when I can run alevin without an error message, and try to give some sense of the results that are quantified.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669:651,Availability,error,error,651,"I tested changing the parameters, and I am still getting the same error message:. ```; [2021-07-08 16:05:50.979] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-08 16:05:50.979] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. However, I will look into the other information and see if I can understand what is happening. I can also test not using a white list and see if that changes the number to be something like an order of magnitude different that what I would expect from the other sample. So, I will post an update when I can run alevin without an error message, and try to give some sense of the results that are quantified.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669:611,Deployability,update,update,611,"I tested changing the parameters, and I am still getting the same error message:. ```; [2021-07-08 16:05:50.979] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-08 16:05:50.979] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. However, I will look into the other information and see if I can understand what is happening. I can also test not using a white list and see if that changes the number to be something like an order of magnitude different that what I would expect from the other sample. So, I will post an update when I can run alevin without an error message, and try to give some sense of the results that are quantified.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669:72,Integrability,message,message,72,"I tested changing the parameters, and I am still getting the same error message:. ```; [2021-07-08 16:05:50.979] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-08 16:05:50.979] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. However, I will look into the other information and see if I can understand what is happening. I can also test not using a white list and see if that changes the number to be something like an order of magnitude different that what I would expect from the other sample. So, I will post an update when I can run alevin without an error message, and try to give some sense of the results that are quantified.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669:657,Integrability,message,message,657,"I tested changing the parameters, and I am still getting the same error message:. ```; [2021-07-08 16:05:50.979] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-08 16:05:50.979] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. However, I will look into the other information and see if I can understand what is happening. I can also test not using a white list and see if that changes the number to be something like an order of magnitude different that what I would expect from the other sample. So, I will post an update when I can run alevin without an error message, and try to give some sense of the results that are quantified.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669:2,Testability,test,tested,2,"I tested changing the parameters, and I am still getting the same error message:. ```; [2021-07-08 16:05:50.979] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-08 16:05:50.979] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. However, I will look into the other information and see if I can understand what is happening. I can also test not using a white list and see if that changes the number to be something like an order of magnitude different that what I would expect from the other sample. So, I will post an update when I can run alevin without an error message, and try to give some sense of the results that are quantified.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669:428,Testability,test,test,428,"I tested changing the parameters, and I am still getting the same error message:. ```; [2021-07-08 16:05:50.979] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-08 16:05:50.979] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. However, I will look into the other information and see if I can understand what is happening. I can also test not using a white list and see if that changes the number to be something like an order of magnitude different that what I would expect from the other sample. So, I will post an update when I can run alevin without an error message, and try to give some sense of the results that are quantified.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877303669
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877325227:120,Availability,error,error,120,"Hi @cwarden45 ,. So `--whitelist` flag is not intended to consume the 737k list of CB you are providing, that's why the error. Thanks for the issue in the doc, I updated it. Idea behind the flag is to externally provide a list of high confidence barcodes potentially identified by different tools like Cellranger, and quantify *only* those barcodes. One use case would be to take the output file `barcodes.tsv` of the cellranger and provide it to flag and quantify only those barcodes. Briefly, alevin doesn't use the 737k list of barcodes at all, it identifies barcodes empirically. If you'd like to quantify using the 737k list of barcodes, checkout the next generation of alevin tool, called [alevin-fry](https://github.com/COMBINE-lab/alevin-fry).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877325227
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877325227:162,Deployability,update,updated,162,"Hi @cwarden45 ,. So `--whitelist` flag is not intended to consume the 737k list of CB you are providing, that's why the error. Thanks for the issue in the doc, I updated it. Idea behind the flag is to externally provide a list of high confidence barcodes potentially identified by different tools like Cellranger, and quantify *only* those barcodes. One use case would be to take the output file `barcodes.tsv` of the cellranger and provide it to flag and quantify only those barcodes. Briefly, alevin doesn't use the 737k list of barcodes at all, it identifies barcodes empirically. If you'd like to quantify using the 737k list of barcodes, checkout the next generation of alevin tool, called [alevin-fry](https://github.com/COMBINE-lab/alevin-fry).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877325227
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877526812:310,Testability,test,test,310,"Thank you very much, @k3yavi !. There was a different 10x white list that worked for a different sample (with a different 10x design), and there is a 3rd dataset that is a BD Rhapsody experiment (where I can't use CellRanger). However, for this particular sample, I also have the CellRanger results. So, I can test as you have described, and close the ticket. If I have additional questions about the different cell counts from the different methods, then I will open a different ticket for that separate topic. Thanks again!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-877526812
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878607734:242,Testability,test,testing,242,"FYI - for the sample in question, there are **9,974** ""filtered"" barcodes and **737,280** ""raw"" barcodes (from CellRanger). If I take the larger list of ""raw"" barcodes and I remove the ""-1"" from each of them, then that is what I am currently testing for Alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878607734
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294:128,Availability,error,error,128,"Strictly speaking, if I use the barcodes.tsv (decompressed) files from CellRanger (for the _same sample_), I still get the same error message. I get the error message below, with or without ""-1"" at the end of the barcodes:. ```; [2021-07-12 15:55:48.717] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-12 15:55:48.717] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. If I **don't** use any white list, then I think that should be one possible resolution (mentioned before). However, in case others have a similar question, I have at least temporarily re-opened the ticket. This is the command that I am currently using:. ```; ID=5309-CT-2; R1=../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz; R2=../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz. TYPE=10xV2; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv. /path/to/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. **Is there anything that you think I should change, if I wanted to confirm that I can run the analysis with some sort of white list?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294:153,Availability,error,error,153,"Strictly speaking, if I use the barcodes.tsv (decompressed) files from CellRanger (for the _same sample_), I still get the same error message. I get the error message below, with or without ""-1"" at the end of the barcodes:. ```; [2021-07-12 15:55:48.717] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-12 15:55:48.717] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. If I **don't** use any white list, then I think that should be one possible resolution (mentioned before). However, in case others have a similar question, I have at least temporarily re-opened the ticket. This is the command that I am currently using:. ```; ID=5309-CT-2; R1=../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz; R2=../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz. TYPE=10xV2; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv. /path/to/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. **Is there anything that you think I should change, if I wanted to confirm that I can run the analysis with some sort of white list?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294:349,Availability,error,error,349,"Strictly speaking, if I use the barcodes.tsv (decompressed) files from CellRanger (for the _same sample_), I still get the same error message. I get the error message below, with or without ""-1"" at the end of the barcodes:. ```; [2021-07-12 15:55:48.717] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-12 15:55:48.717] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. If I **don't** use any white list, then I think that should be one possible resolution (mentioned before). However, in case others have a similar question, I have at least temporarily re-opened the ticket. This is the command that I am currently using:. ```; ID=5309-CT-2; R1=../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz; R2=../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz. TYPE=10xV2; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv. /path/to/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. **Is there anything that you think I should change, if I wanted to confirm that I can run the analysis with some sort of white list?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294:855,Availability,down,download,855,"Strictly speaking, if I use the barcodes.tsv (decompressed) files from CellRanger (for the _same sample_), I still get the same error message. I get the error message below, with or without ""-1"" at the end of the barcodes:. ```; [2021-07-12 15:55:48.717] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-12 15:55:48.717] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. If I **don't** use any white list, then I think that should be one possible resolution (mentioned before). However, in case others have a similar question, I have at least temporarily re-opened the ticket. This is the command that I am currently using:. ```; ID=5309-CT-2; R1=../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz; R2=../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz. TYPE=10xV2; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv. /path/to/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. **Is there anything that you think I should change, if I wanted to confirm that I can run the analysis with some sort of white list?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294:134,Integrability,message,message,134,"Strictly speaking, if I use the barcodes.tsv (decompressed) files from CellRanger (for the _same sample_), I still get the same error message. I get the error message below, with or without ""-1"" at the end of the barcodes:. ```; [2021-07-12 15:55:48.717] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-12 15:55:48.717] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. If I **don't** use any white list, then I think that should be one possible resolution (mentioned before). However, in case others have a similar question, I have at least temporarily re-opened the ticket. This is the command that I am currently using:. ```; ID=5309-CT-2; R1=../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz; R2=../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz. TYPE=10xV2; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv. /path/to/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. **Is there anything that you think I should change, if I wanted to confirm that I can run the analysis with some sort of white list?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294:159,Integrability,message,message,159,"Strictly speaking, if I use the barcodes.tsv (decompressed) files from CellRanger (for the _same sample_), I still get the same error message. I get the error message below, with or without ""-1"" at the end of the barcodes:. ```; [2021-07-12 15:55:48.717] [alevinLog] [info] Done importing white-list Barcodes; [2021-07-12 15:55:48.717] [alevinLog] [error] Wrong whitelist provided; Please check https://salmon.readthedocs.io/en/develop/alevin.html#whitelist; ```. If I **don't** use any white list, then I think that should be one possible resolution (mentioned before). However, in case others have a similar question, I have at least temporarily re-opened the ticket. This is the command that I am currently using:. ```; ID=5309-CT-2; R1=../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz; R2=../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz. TYPE=10xV2; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv. /path/to/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. **Is there anything that you think I should change, if I wanted to confirm that I can run the analysis with some sort of white list?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878688294
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878693769:723,Availability,error,error,723,"Strictly speaking, barcodes.tsv**.gz** files are provided. I was viewing the uncompressed file in Notepad++, so I think that number (from the line numbers on the left) should be correct. I will double-check the next time that I am on my work computer. I will test using the smaller number of barcodes. However, unlike the larger file, I think this would be different for every sample. If part of the reason that I wanted to run Alevin is that I wanted an independent quantification (which takes less time), then that may be a notable limitation. However, for whatever reason, this seems to only be an issue with the v2 sample (the v3 sample worked fine). So, I will test that, and I will at least confirm if I see the same error message or not.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878693769
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878693769:729,Integrability,message,message,729,"Strictly speaking, barcodes.tsv**.gz** files are provided. I was viewing the uncompressed file in Notepad++, so I think that number (from the line numbers on the left) should be correct. I will double-check the next time that I am on my work computer. I will test using the smaller number of barcodes. However, unlike the larger file, I think this would be different for every sample. If part of the reason that I wanted to run Alevin is that I wanted an independent quantification (which takes less time), then that may be a notable limitation. However, for whatever reason, this seems to only be an issue with the v2 sample (the v3 sample worked fine). So, I will test that, and I will at least confirm if I see the same error message or not.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878693769
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878693769:259,Testability,test,test,259,"Strictly speaking, barcodes.tsv**.gz** files are provided. I was viewing the uncompressed file in Notepad++, so I think that number (from the line numbers on the left) should be correct. I will double-check the next time that I am on my work computer. I will test using the smaller number of barcodes. However, unlike the larger file, I think this would be different for every sample. If part of the reason that I wanted to run Alevin is that I wanted an independent quantification (which takes less time), then that may be a notable limitation. However, for whatever reason, this seems to only be an issue with the v2 sample (the v3 sample worked fine). So, I will test that, and I will at least confirm if I see the same error message or not.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878693769
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878693769:666,Testability,test,test,666,"Strictly speaking, barcodes.tsv**.gz** files are provided. I was viewing the uncompressed file in Notepad++, so I think that number (from the line numbers on the left) should be correct. I will double-check the next time that I am on my work computer. I will test using the smaller number of barcodes. However, unlike the larger file, I think this would be different for every sample. If part of the reason that I wanted to run Alevin is that I wanted an independent quantification (which takes less time), then that may be a notable limitation. However, for whatever reason, this seems to only be an issue with the v2 sample (the v3 sample worked fine). So, I will test that, and I will at least confirm if I see the same error message or not.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878693769
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878695672:297,Availability,error,error,297,"Yep, I think that's right and it is expected that every sample would have different number of cells/cellular barcodes. The general idea is to use a frequency distribution to separate high quality barcodes from low quality post quantification. I'm sorry that you are facing issues with v2, but the error simply means you are providing the full list of 737k barcodes which is not an expected behavior for the `--whitelist` flag and in a typical 10x/ Dropseq based experiment one would expect ~10-12k cells/CB.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878695672
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878695672:303,Usability,simpl,simply,303,"Yep, I think that's right and it is expected that every sample would have different number of cells/cellular barcodes. The general idea is to use a frequency distribution to separate high quality barcodes from low quality post quantification. I'm sorry that you are facing issues with v2, but the error simply means you are providing the full list of 737k barcodes which is not an expected behavior for the `--whitelist` flag and in a typical 10x/ Dropseq based experiment one would expect ~10-12k cells/CB.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878695672
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665:99,Availability,error,error,99,"Maybe it is a little off-topic, but here is the code for the v3 sample that ran without generating error messages:. ```; ID=5k_pbmc_v3. R1a=../Reads/5k_pbmc_v3_S1_L001_R1_001.fastq.gz; R1b=../Reads/5k_pbmc_v3_S1_L002_R1_001.fastq.gz; R1c=../Reads/5k_pbmc_v3_S1_L003_R1_001.fastq.gz; R1d=../Reads/5k_pbmc_v3_S1_L004_R1_001.fastq.gz. R2a=../Reads/5k_pbmc_v3_S1_L001_R2_001.fastq.gz; R2b=../Reads/5k_pbmc_v3_S1_L002_R2_001.fastq.gz; R2c=../Reads/5k_pbmc_v3_S1_L003_R2_001.fastq.gz; R2d=../Reads/5k_pbmc_v3_S1_L004_R2_001.fastq.gz. TYPE=10xV3; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=/net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt. ../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR --chromium -1 $R1a $R1b $R1c $R1d -2 $R2a $R2b $R2c $R2d -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. That v3 white list is even **larger** than the v2 white list:. ```; cwarden$ wc -l /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; 6794880 /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; ```. So, that is part of why I was confused. However, before I start running the analysis with the shorter set of barcodes unique for this sample, here are the commands that I believe you were asking about. ```; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; 737280 ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; 9974 ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; ```. I would prefer to have an option where I could potentially conclude the cell count is different than provided by CellRanger. . However, I will at least check to confirm this solves the problem with the error message that I am seei",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665:550,Availability,down,download,550,"Maybe it is a little off-topic, but here is the code for the v3 sample that ran without generating error messages:. ```; ID=5k_pbmc_v3. R1a=../Reads/5k_pbmc_v3_S1_L001_R1_001.fastq.gz; R1b=../Reads/5k_pbmc_v3_S1_L002_R1_001.fastq.gz; R1c=../Reads/5k_pbmc_v3_S1_L003_R1_001.fastq.gz; R1d=../Reads/5k_pbmc_v3_S1_L004_R1_001.fastq.gz. R2a=../Reads/5k_pbmc_v3_S1_L001_R2_001.fastq.gz; R2b=../Reads/5k_pbmc_v3_S1_L002_R2_001.fastq.gz; R2c=../Reads/5k_pbmc_v3_S1_L003_R2_001.fastq.gz; R2d=../Reads/5k_pbmc_v3_S1_L004_R2_001.fastq.gz. TYPE=10xV3; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=/net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt. ../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR --chromium -1 $R1a $R1b $R1c $R1d -2 $R2a $R2b $R2c $R2d -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. That v3 white list is even **larger** than the v2 white list:. ```; cwarden$ wc -l /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; 6794880 /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; ```. So, that is part of why I was confused. However, before I start running the analysis with the shorter set of barcodes unique for this sample, here are the commands that I believe you were asking about. ```; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; 737280 ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; 9974 ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; ```. I would prefer to have an option where I could potentially conclude the cell count is different than provided by CellRanger. . However, I will at least check to confirm this solves the problem with the error message that I am seei",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665:1973,Availability,error,error,1973,"e code for the v3 sample that ran without generating error messages:. ```; ID=5k_pbmc_v3. R1a=../Reads/5k_pbmc_v3_S1_L001_R1_001.fastq.gz; R1b=../Reads/5k_pbmc_v3_S1_L002_R1_001.fastq.gz; R1c=../Reads/5k_pbmc_v3_S1_L003_R1_001.fastq.gz; R1d=../Reads/5k_pbmc_v3_S1_L004_R1_001.fastq.gz. R2a=../Reads/5k_pbmc_v3_S1_L001_R2_001.fastq.gz; R2b=../Reads/5k_pbmc_v3_S1_L002_R2_001.fastq.gz; R2c=../Reads/5k_pbmc_v3_S1_L003_R2_001.fastq.gz; R2d=../Reads/5k_pbmc_v3_S1_L004_R2_001.fastq.gz. TYPE=10xV3; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=/net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt. ../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR --chromium -1 $R1a $R1b $R1c $R1d -2 $R2a $R2b $R2c $R2d -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. That v3 white list is even **larger** than the v2 white list:. ```; cwarden$ wc -l /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; 6794880 /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; ```. So, that is part of why I was confused. However, before I start running the analysis with the shorter set of barcodes unique for this sample, here are the commands that I believe you were asking about. ```; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; 737280 ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; 9974 ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; ```. I would prefer to have an option where I could potentially conclude the cell count is different than provided by CellRanger. . However, I will at least check to confirm this solves the problem with the error message that I am seeing (with the much smaller cell barcode list).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665:105,Integrability,message,messages,105,"Maybe it is a little off-topic, but here is the code for the v3 sample that ran without generating error messages:. ```; ID=5k_pbmc_v3. R1a=../Reads/5k_pbmc_v3_S1_L001_R1_001.fastq.gz; R1b=../Reads/5k_pbmc_v3_S1_L002_R1_001.fastq.gz; R1c=../Reads/5k_pbmc_v3_S1_L003_R1_001.fastq.gz; R1d=../Reads/5k_pbmc_v3_S1_L004_R1_001.fastq.gz. R2a=../Reads/5k_pbmc_v3_S1_L001_R2_001.fastq.gz; R2b=../Reads/5k_pbmc_v3_S1_L002_R2_001.fastq.gz; R2c=../Reads/5k_pbmc_v3_S1_L003_R2_001.fastq.gz; R2d=../Reads/5k_pbmc_v3_S1_L004_R2_001.fastq.gz. TYPE=10xV3; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=/net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt. ../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR --chromium -1 $R1a $R1b $R1c $R1d -2 $R2a $R2b $R2c $R2d -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. That v3 white list is even **larger** than the v2 white list:. ```; cwarden$ wc -l /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; 6794880 /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; ```. So, that is part of why I was confused. However, before I start running the analysis with the shorter set of barcodes unique for this sample, here are the commands that I believe you were asking about. ```; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; 737280 ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; 9974 ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; ```. I would prefer to have an option where I could potentially conclude the cell count is different than provided by CellRanger. . However, I will at least check to confirm this solves the problem with the error message that I am seei",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665:1979,Integrability,message,message,1979,"e code for the v3 sample that ran without generating error messages:. ```; ID=5k_pbmc_v3. R1a=../Reads/5k_pbmc_v3_S1_L001_R1_001.fastq.gz; R1b=../Reads/5k_pbmc_v3_S1_L002_R1_001.fastq.gz; R1c=../Reads/5k_pbmc_v3_S1_L003_R1_001.fastq.gz; R1d=../Reads/5k_pbmc_v3_S1_L004_R1_001.fastq.gz. R2a=../Reads/5k_pbmc_v3_S1_L001_R2_001.fastq.gz; R2b=../Reads/5k_pbmc_v3_S1_L002_R2_001.fastq.gz; R2c=../Reads/5k_pbmc_v3_S1_L003_R2_001.fastq.gz; R2d=../Reads/5k_pbmc_v3_S1_L004_R2_001.fastq.gz. TYPE=10xV3; #for MAP, download from UCSC Table Browser, and remove 1st line (and then manually add SARS-COV-2 genes); MAP=../../Ref_Generation/SARS_COV_2-hg38_RefSeq_2column.txt; REF=../../Ref_Generation/SARS_COV_2-hg38_salmon; CBWL=/net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt. ../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISR --chromium -1 $R1a $R1b $R1c $R1d -2 $R2a $R2b $R2c $R2d -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL; ```. That v3 white list is even **larger** than the v2 white list:. ```; cwarden$ wc -l /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; 6794880 /net/isi-dcnl/ifs/user_data/Seq/Chromium_data/3M-february-2018.txt; ```. So, that is part of why I was confused. However, before I start running the analysis with the shorter set of barcodes unique for this sample, here are the commands that I believe you were asking about. ```; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; 737280 ../CellRanger/5309-CT-2/outs/raw_feature_bc_matrix/barcodes.tsv; cwarden$ wc -l ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; 9974 ../CellRanger/5309-CT-2/outs/filtered_feature_bc_matrix/barcodes.tsv; ```. I would prefer to have an option where I could potentially conclude the cell count is different than provided by CellRanger. . However, I will at least check to confirm this solves the problem with the error message that I am seeing (with the much smaller cell barcode list).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879347665
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561:91,Availability,error,error,91,"If i use the smaller set of barcodes, then I progress further. However, I still receive an error message (and there no **quants_mat_rows.txt** file):. ```; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0.00 UMI after deduplicating.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 BiDirected Edges.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 UniDirected Edges.; [2021-07-13 13:59:07.134] [alevinLog] [info] Finished optimizer; /var/spool/slurmd/job3050767/slurm_script: line 23: 10494 Floating point exception../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL. ```. If the barcode is on the opposite read, then I am not sure if I should really be using the reverse or reverse complement (possibly even for the full barcode list)?. However, for the sake of this discussion, I will now test not providing any white list. If that works, then I will close the ticket again. **Update (7/14/2021)**: I have added the full log file here: [cluster_log.log](https://github.com/COMBINE-lab/salmon/files/6819402/cluster_log.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561:990,Deployability,Update,Update,990,"If i use the smaller set of barcodes, then I progress further. However, I still receive an error message (and there no **quants_mat_rows.txt** file):. ```; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0.00 UMI after deduplicating.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 BiDirected Edges.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 UniDirected Edges.; [2021-07-13 13:59:07.134] [alevinLog] [info] Finished optimizer; /var/spool/slurmd/job3050767/slurm_script: line 23: 10494 Floating point exception../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL. ```. If the barcode is on the opposite read, then I am not sure if I should really be using the reverse or reverse complement (possibly even for the full barcode list)?. However, for the sake of this discussion, I will now test not providing any white list. If that works, then I will close the ticket again. **Update (7/14/2021)**: I have added the full log file here: [cluster_log.log](https://github.com/COMBINE-lab/salmon/files/6819402/cluster_log.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561:97,Integrability,message,message,97,"If i use the smaller set of barcodes, then I progress further. However, I still receive an error message (and there no **quants_mat_rows.txt** file):. ```; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0.00 UMI after deduplicating.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 BiDirected Edges.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 UniDirected Edges.; [2021-07-13 13:59:07.134] [alevinLog] [info] Finished optimizer; /var/spool/slurmd/job3050767/slurm_script: line 23: 10494 Floating point exception../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL. ```. If the barcode is on the opposite read, then I am not sure if I should really be using the reverse or reverse complement (possibly even for the full barcode list)?. However, for the sake of this discussion, I will now test not providing any white list. If that works, then I will close the ticket again. **Update (7/14/2021)**: I have added the full log file here: [cluster_log.log](https://github.com/COMBINE-lab/salmon/files/6819402/cluster_log.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561:437,Performance,optimiz,optimizer,437,"If i use the smaller set of barcodes, then I progress further. However, I still receive an error message (and there no **quants_mat_rows.txt** file):. ```; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0.00 UMI after deduplicating.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 BiDirected Edges.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 UniDirected Edges.; [2021-07-13 13:59:07.134] [alevinLog] [info] Finished optimizer; /var/spool/slurmd/job3050767/slurm_script: line 23: 10494 Floating point exception../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL. ```. If the barcode is on the opposite read, then I am not sure if I should really be using the reverse or reverse complement (possibly even for the full barcode list)?. However, for the sake of this discussion, I will now test not providing any white list. If that works, then I will close the ticket again. **Update (7/14/2021)**: I have added the full log file here: [cluster_log.log](https://github.com/COMBINE-lab/salmon/files/6819402/cluster_log.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561:902,Testability,test,test,902,"If i use the smaller set of barcodes, then I progress further. However, I still receive an error message (and there no **quants_mat_rows.txt** file):. ```; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0.00 UMI after deduplicating.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 BiDirected Edges.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 UniDirected Edges.; [2021-07-13 13:59:07.134] [alevinLog] [info] Finished optimizer; /var/spool/slurmd/job3050767/slurm_script: line 23: 10494 Floating point exception../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL. ```. If the barcode is on the opposite read, then I am not sure if I should really be using the reverse or reverse complement (possibly even for the full barcode list)?. However, for the sake of this discussion, I will now test not providing any white list. If that works, then I will close the ticket again. **Update (7/14/2021)**: I have added the full log file here: [cluster_log.log](https://github.com/COMBINE-lab/salmon/files/6819402/cluster_log.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561:1034,Testability,log,log,1034,"If i use the smaller set of barcodes, then I progress further. However, I still receive an error message (and there no **quants_mat_rows.txt** file):. ```; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0.00 UMI after deduplicating.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 BiDirected Edges.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 UniDirected Edges.; [2021-07-13 13:59:07.134] [alevinLog] [info] Finished optimizer; /var/spool/slurmd/job3050767/slurm_script: line 23: 10494 Floating point exception../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL. ```. If the barcode is on the opposite read, then I am not sure if I should really be using the reverse or reverse complement (possibly even for the full barcode list)?. However, for the sake of this discussion, I will now test not providing any white list. If that works, then I will close the ticket again. **Update (7/14/2021)**: I have added the full log file here: [cluster_log.log](https://github.com/COMBINE-lab/salmon/files/6819402/cluster_log.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561:1062,Testability,log,log,1062,"If i use the smaller set of barcodes, then I progress further. However, I still receive an error message (and there no **quants_mat_rows.txt** file):. ```; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0.00 UMI after deduplicating.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 BiDirected Edges.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 UniDirected Edges.; [2021-07-13 13:59:07.134] [alevinLog] [info] Finished optimizer; /var/spool/slurmd/job3050767/slurm_script: line 23: 10494 Floating point exception../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL. ```. If the barcode is on the opposite read, then I am not sure if I should really be using the reverse or reverse complement (possibly even for the full barcode list)?. However, for the sake of this discussion, I will now test not providing any white list. If that works, then I will close the ticket again. **Update (7/14/2021)**: I have added the full log file here: [cluster_log.log](https://github.com/COMBINE-lab/salmon/files/6819402/cluster_log.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561:1131,Testability,log,log,1131,"If i use the smaller set of barcodes, then I progress further. However, I still receive an error message (and there no **quants_mat_rows.txt** file):. ```; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0.00 UMI after deduplicating.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 BiDirected Edges.; [2021-07-13 13:59:07.109] [alevinLog] [info] Total 0 UniDirected Edges.; [2021-07-13 13:59:07.134] [alevinLog] [info] Finished optimizer; /var/spool/slurmd/job3050767/slurm_script: line 23: 10494 Floating point exception../../Ref_Generation/salmon-1.5.1_linux_x86_64/bin/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP --whitelist $CBWL. ```. If the barcode is on the opposite read, then I am not sure if I should really be using the reverse or reverse complement (possibly even for the full barcode list)?. However, for the sake of this discussion, I will now test not providing any white list. If that works, then I will close the ticket again. **Update (7/14/2021)**: I have added the full log file here: [cluster_log.log](https://github.com/COMBINE-lab/salmon/files/6819402/cluster_log.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-879497561
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749:1737,Availability,error,error,1737,"In terms of an intermediate update:. **Setting 1**:. _Command 1_:; `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting to make feature Matrix; [2021-07-13 20:12:34.654] [alevinLog] [info] Done making feature Matrix; [2021-07-13 20:12:35.447] [alevinLog] [info] Finished white listing; [2021-07-13 20:12:36.158] [alevinLog] [info] Finished optimizer; 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. **Setting 2:**:; _Command 1_:; `/path/to/salmon alevin -l ISR --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting to make feature Matrix; [2021-07-14 09:51:38.566] [alevinLog] [info] Done making feature Matrix; [2021-07-14 09:51:39.347] [alevinLog] [info] Finished white listing; [2021-07-14 09:51:39.541] [alevinLog] [info] Finished optimizer; [2021-07-14 09:51:39.564] [jointLog] [warning] NOTE: Read Lib [[ ../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz, ../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz]] :. Greater than 5% of the fragments disagreed with the provided library type; check the file: 5309-CT-2/lib_format_counts.json for details. 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. Technically, this means that the program ran without generating an error message, but this seems strange to me. So, I think I would prefer to keep the issue open a little bit longer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749:28,Deployability,update,update,28,"In terms of an intermediate update:. **Setting 1**:. _Command 1_:; `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting to make feature Matrix; [2021-07-13 20:12:34.654] [alevinLog] [info] Done making feature Matrix; [2021-07-13 20:12:35.447] [alevinLog] [info] Finished white listing; [2021-07-13 20:12:36.158] [alevinLog] [info] Finished optimizer; 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. **Setting 2:**:; _Command 1_:; `/path/to/salmon alevin -l ISR --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting to make feature Matrix; [2021-07-14 09:51:38.566] [alevinLog] [info] Done making feature Matrix; [2021-07-14 09:51:39.347] [alevinLog] [info] Finished white listing; [2021-07-14 09:51:39.541] [alevinLog] [info] Finished optimizer; [2021-07-14 09:51:39.564] [jointLog] [warning] NOTE: Read Lib [[ ../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz, ../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz]] :. Greater than 5% of the fragments disagreed with the provided library type; check the file: 5309-CT-2/lib_format_counts.json for details. 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. Technically, this means that the program ran without generating an error message, but this seems strange to me. So, I think I would prefer to keep the issue open a little bit longer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749:1743,Integrability,message,message,1743,"In terms of an intermediate update:. **Setting 1**:. _Command 1_:; `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting to make feature Matrix; [2021-07-13 20:12:34.654] [alevinLog] [info] Done making feature Matrix; [2021-07-13 20:12:35.447] [alevinLog] [info] Finished white listing; [2021-07-13 20:12:36.158] [alevinLog] [info] Finished optimizer; 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. **Setting 2:**:; _Command 1_:; `/path/to/salmon alevin -l ISR --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting to make feature Matrix; [2021-07-14 09:51:38.566] [alevinLog] [info] Done making feature Matrix; [2021-07-14 09:51:39.347] [alevinLog] [info] Finished white listing; [2021-07-14 09:51:39.541] [alevinLog] [info] Finished optimizer; [2021-07-14 09:51:39.564] [jointLog] [warning] NOTE: Read Lib [[ ../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz, ../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz]] :. Greater than 5% of the fragments disagreed with the provided library type; check the file: 5309-CT-2/lib_format_counts.json for details. 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. Technically, this means that the program ran without generating an error message, but this seems strange to me. So, I think I would prefer to keep the issue open a little bit longer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749:536,Performance,optimiz,optimizer,536,"In terms of an intermediate update:. **Setting 1**:. _Command 1_:; `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting to make feature Matrix; [2021-07-13 20:12:34.654] [alevinLog] [info] Done making feature Matrix; [2021-07-13 20:12:35.447] [alevinLog] [info] Finished white listing; [2021-07-13 20:12:36.158] [alevinLog] [info] Finished optimizer; 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. **Setting 2:**:; _Command 1_:; `/path/to/salmon alevin -l ISR --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting to make feature Matrix; [2021-07-14 09:51:38.566] [alevinLog] [info] Done making feature Matrix; [2021-07-14 09:51:39.347] [alevinLog] [info] Finished white listing; [2021-07-14 09:51:39.541] [alevinLog] [info] Finished optimizer; [2021-07-14 09:51:39.564] [jointLog] [warning] NOTE: Read Lib [[ ../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz, ../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz]] :. Greater than 5% of the fragments disagreed with the provided library type; check the file: 5309-CT-2/lib_format_counts.json for details. 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. Technically, this means that the program ran without generating an error message, but this seems strange to me. So, I think I would prefer to keep the issue open a little bit longer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749:1205,Performance,optimiz,optimizer,1205,"In terms of an intermediate update:. **Setting 1**:. _Command 1_:; `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting to make feature Matrix; [2021-07-13 20:12:34.654] [alevinLog] [info] Done making feature Matrix; [2021-07-13 20:12:35.447] [alevinLog] [info] Finished white listing; [2021-07-13 20:12:36.158] [alevinLog] [info] Finished optimizer; 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. **Setting 2:**:; _Command 1_:; `/path/to/salmon alevin -l ISR --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting to make feature Matrix; [2021-07-14 09:51:38.566] [alevinLog] [info] Done making feature Matrix; [2021-07-14 09:51:39.347] [alevinLog] [info] Finished white listing; [2021-07-14 09:51:39.541] [alevinLog] [info] Finished optimizer; [2021-07-14 09:51:39.564] [jointLog] [warning] NOTE: Read Lib [[ ../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz, ../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz]] :. Greater than 5% of the fragments disagreed with the provided library type; check the file: 5309-CT-2/lib_format_counts.json for details. 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. Technically, this means that the program ran without generating an error message, but this seems strange to me. So, I think I would prefer to keep the issue open a little bit longer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749:166,Testability,Log,Log,166,"In terms of an intermediate update:. **Setting 1**:. _Command 1_:; `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting to make feature Matrix; [2021-07-13 20:12:34.654] [alevinLog] [info] Done making feature Matrix; [2021-07-13 20:12:35.447] [alevinLog] [info] Finished white listing; [2021-07-13 20:12:36.158] [alevinLog] [info] Finished optimizer; 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. **Setting 2:**:; _Command 1_:; `/path/to/salmon alevin -l ISR --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting to make feature Matrix; [2021-07-14 09:51:38.566] [alevinLog] [info] Done making feature Matrix; [2021-07-14 09:51:39.347] [alevinLog] [info] Finished white listing; [2021-07-14 09:51:39.541] [alevinLog] [info] Finished optimizer; [2021-07-14 09:51:39.564] [jointLog] [warning] NOTE: Read Lib [[ ../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz, ../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz]] :. Greater than 5% of the fragments disagreed with the provided library type; check the file: 5309-CT-2/lib_format_counts.json for details. 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. Technically, this means that the program ran without generating an error message, but this seems strange to me. So, I think I would prefer to keep the issue open a little bit longer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749:835,Testability,Log,Log,835,"In terms of an intermediate update:. **Setting 1**:. _Command 1_:; `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-13 20:12:34.651] [alevinLog] [info] Starting to make feature Matrix; [2021-07-13 20:12:34.654] [alevinLog] [info] Done making feature Matrix; [2021-07-13 20:12:35.447] [alevinLog] [info] Finished white listing; [2021-07-13 20:12:36.158] [alevinLog] [info] Finished optimizer; 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. **Setting 2:**:; _Command 1_:; `/path/to/salmon alevin -l ISR --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`; _End of Log 1_:; ```; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting white listing of 814 cells; [2021-07-14 09:51:38.564] [alevinLog] [info] Starting to make feature Matrix; [2021-07-14 09:51:38.566] [alevinLog] [info] Done making feature Matrix; [2021-07-14 09:51:39.347] [alevinLog] [info] Finished white listing; [2021-07-14 09:51:39.541] [alevinLog] [info] Finished optimizer; [2021-07-14 09:51:39.564] [jointLog] [warning] NOTE: Read Lib [[ ../Reads/5309-CT-2_S01_L005_R1_001.fastq.gz, ../Reads/5309-CT-2_S01_L005_R2_001.fastq.gz]] :. Greater than 5% of the fragments disagreed with the provided library type; check the file: 5309-CT-2/lib_format_counts.json for details. 0.0408521	8.9925e-05	0.000114595	636780	18682.9	; 0.0290163	6.61624e-05	0.000111685	230922	8010.3	; ```; _Size of quants_mat_rows.txt 1_: 814 lines/barcodes. Technically, this means that the program ran without generating an error message, but this seems strange to me. So, I think I would prefer to keep the issue open a little bit longer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-880273749
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:25,Availability,error,error,25,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:137,Availability,error,error,137,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:1494,Availability,error,errors,1494,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:31,Integrability,message,message,31,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:143,Integrability,message,message,143,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:238,Usability,clear,clear,238,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-883784536:135,Deployability,update,update,135,"I did not previously test that, but I am currently running the analysis with the extra `--expectCells 10000` parameter. I will post an update. If all goes well, I will then close the ticket. Thank you again for your help!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-883784536
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-883784536:21,Testability,test,test,21,"I did not previously test that, but I am currently running the analysis with the extra `--expectCells 10000` parameter. I will post an update. If all goes well, I will then close the ticket. Thank you again for your help!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-883784536
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-884309748:80,Availability,error,error,80,"With the extra `--expectCells 10000` parameter, Alevin finished running without error messages and 10,641 cell barcodes in **quants_mat_rows.txt**. I am not sure how much of a difference it makes if I expect 8,000 cells versus 10,000 cells. However, this looks much closer to the expected number, and I am closing the ticket. Thanks again!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-884309748
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-884309748:86,Integrability,message,messages,86,"With the extra `--expectCells 10000` parameter, Alevin finished running without error messages and 10,641 cell barcodes in **quants_mat_rows.txt**. I am not sure how much of a difference it makes if I expect 8,000 cells versus 10,000 cells. However, this looks much closer to the expected number, and I am closing the ticket. Thanks again!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-884309748
https://github.com/COMBINE-lab/salmon/issues/683#issuecomment-879989898:135,Integrability,interface,interface,135,"Hi @rudondamba,. There are a few options. If it's less than ~10MB, then you can just drop the file into the text box in the GitHub web interface here, and it will upload it and provide a link. If it's a few hundred MB or so, as is typical of many transcriptomes, the best thing to do might be to put it up on Google Drive or Box or Dropbox (whatever you have access to) and share a link. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/683#issuecomment-879989898
https://github.com/COMBINE-lab/salmon/issues/683#issuecomment-879989898:359,Security,access,access,359,"Hi @rudondamba,. There are a few options. If it's less than ~10MB, then you can just drop the file into the text box in the GitHub web interface here, and it will upload it and provide a link. If it's a few hundred MB or so, as is typical of many transcriptomes, the best thing to do might be to put it up on Google Drive or Box or Dropbox (whatever you have access to) and share a link. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/683#issuecomment-879989898
https://github.com/COMBINE-lab/salmon/issues/683#issuecomment-880059577:258,Availability,error,error,258,"Hi @rudondamba,. The problem is that the sequence on line `30403` is parsed as ""empty"", since there is a space after the initial `>` and before the sequence name. This messes up the state of the underlying parser leading to the (in this case, uninformative) error message you are observing. If you fix the name online `30403`, then you should be able to index the transcriptome as expected. Let me know if this works for you. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/683#issuecomment-880059577
https://github.com/COMBINE-lab/salmon/issues/683#issuecomment-880059577:264,Integrability,message,message,264,"Hi @rudondamba,. The problem is that the sequence on line `30403` is parsed as ""empty"", since there is a space after the initial `>` and before the sequence name. This messes up the state of the underlying parser leading to the (in this case, uninformative) error message you are observing. If you fix the name online `30403`, then you should be able to index the transcriptome as expected. Let me know if this works for you. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/683#issuecomment-880059577
https://github.com/COMBINE-lab/salmon/issues/684#issuecomment-884695240:94,Deployability,pipeline,pipeline,94,"Closing this as it's not an issue with Salmon. ; The main problem was that the nf-core/rnaseq pipeline didn't call `umi_tools` with the `--paired` flag. . Some more fine-tuning of the UMI-tools output might be necessary to make sure unpaired reads are properly counted by Salmon. For this, see https://github.com/CGATOxford/UMI-tools/issues/465.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/684#issuecomment-884695240
https://github.com/COMBINE-lab/salmon/issues/685#issuecomment-881557370:150,Availability,Error,Error,150,"Hi @jmaggiore,. The problem is that an outdated index is being mapped against. Specifically, that is the nature of this exception:. ```; Exception : [Error: This version of salmon does not support indexing using the RapMap index.]; ```. To fix this, you should re-index your reference using a recent version of salmon (ideally, v1.5.1 which you are using to quantify). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/685#issuecomment-881557370
https://github.com/COMBINE-lab/salmon/issues/687#issuecomment-885886146:813,Testability,test,test,813,"Hi @mdeyssen,. Thanks for reporting this. This is really strange, as the compute note you're using here should be much more than capable of building this index reasonably quickly. Furthermore, stage 0 is the construction of the compacted de Bruijn graph for which we use a modified version of TwoPaCo. One strange thing seems to be that there is no disk read / disk write happening at this point, though TwoPaCo should have written its intermediate files to disk by this point in the algorithm.; ; I'm not sure what the best way to try to debug is at this point. Does the indexing complete correctly when you build it on just the transcriptome (leaving out the decoys)? Can you provide any information about the specific instance type used (and particularly the storage)? We'll see if we can think how to further test what might be going on in this case. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/687#issuecomment-885886146
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-882817058:194,Availability,down,down,194,"Hi @jashapiro,. Thanks for the report! I am curious if this is one issue or two. Specifically, are you saying that with 1.5.1 you get *no* `cmd_info.json`, while with 1.4.0 you got the stripped-down one, or that you get the stripped down version in both but you expect a full `json` file. I agree that, in any case, having some more info in the `cmd_info.json` output in RAD mode would be useful. It follows a separate code path, so this is likely a matter of making sure the function to write the json file is invoked at the right point. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-882817058
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-882817058:233,Availability,down,down,233,"Hi @jashapiro,. Thanks for the report! I am curious if this is one issue or two. Specifically, are you saying that with 1.5.1 you get *no* `cmd_info.json`, while with 1.4.0 you got the stripped-down one, or that you get the stripped down version in both but you expect a full `json` file. I agree that, in any case, having some more info in the `cmd_info.json` output in RAD mode would be useful. It follows a separate code path, so this is likely a matter of making sure the function to write the json file is invoked at the right point. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-882817058
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-882833076:56,Availability,down,down,56,"I suppose it is two issues. With 1.4.0 I get a stripped down `cmd_info.json` file, and with 1.5.1 I get none at all.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-882833076
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883029429:259,Deployability,release,release,259,"Hi @jashapiro,. I've just pushed a commit that should address this, adding both a ""full"" `cmd_info.json` and a reasonable `meta_info.json`. If this is something you need on short notice we can try an push out a 1.5.2 soon, otherwise, this will be in the next release. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883029429
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883442154:274,Deployability,pipeline,pipeline,274,"Hi @rob-p,. It turns out I slightly misdiagnosed the problem... The ""no `cmd_info.json`"" was correct 1.5.1, but as you may have discovered, it is also missing in earlier versions. What I was seeing was a `cmd_info.json` file that is somehow generated along the `alevin-fry` pipeline we are using. For extra confusion, that was reporting a salmon version of 1.4.0, even when salmon 1.5.1 was used. We will dive in a bit deeper to see if we can find exactly where that file was generated.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883442154
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883468182:22,Testability,test,testing,22,"I didn't do more full testing, but it looks like the latter problem is indeed coming from `alevin-fry quant` (and this is a temporary but expected behavior). https://github.com/COMBINE-lab/alevin-fry/blob/967f5cbb404fb86a71291d88a73afa071570b575/libradicl/src/quant.rs#L1622-L1651. I'm not familiar enough with rust to know whether this will result in overwriting `cmd_info.json` and `meta_info.json` files that exist, but it does seem to me a good behavior would be to merge the info from `salmon alevin` and `alevin-fry`, particularly for the `meta_info.json` file, where both tools add useful information.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883468182
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669:1803,Availability,down,downstream,1803,"Hi @jashapiro,. So there are definitely a few things going on here. The first is that you correctly diagnosed the missing cmd_info.json information when `alevin` is run in RAD mode. That was simply an oversight, and there is no reason that file shouldn't have been written. Second, there is also useful information that belongs in `meta_info.json` in the `aux_info` directory (like the SHA hash of the reference sequences); that was also missing but has now been added.; ; In addition to salmon's `alevin` command, each step of `alevin-fry` also writes some useful metadata when it executes. For example, there is a json file written by the `generate-permit-list` step, one written by the `collate` step, and one written by the `quant` step. We've never run into the problem of the output of `alevin-fry` overwriting the output of `alevin` because we use a directory structure where the output quantifications reside in a separate directory from the input RAD file. However, I can now see that if you're writing the quants in the same place as the input, then there will be a conflict in the file names, and the existing files will be overwritten with the new ones. I agree that both tools output useful information. I'm a *bit* ambivalent about assuming the salmon-generated files exist, and merging them into one output file, as I think there might be cases where those files aren't present and `alevin-fry` should still run properly since it doesn't require them to perform it's processing. One option would be to rename the `alevin-fry` output files to prefix/postfix them so they don't collide with the salmon files even if they live in the same directory. Then, one could (now or later) write a small command to merge the relevant json files into a unified output if that would be more convenient downstream. Let me know your thoughts. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669:1469,Performance,perform,perform,1469,"Hi @jashapiro,. So there are definitely a few things going on here. The first is that you correctly diagnosed the missing cmd_info.json information when `alevin` is run in RAD mode. That was simply an oversight, and there is no reason that file shouldn't have been written. Second, there is also useful information that belongs in `meta_info.json` in the `aux_info` directory (like the SHA hash of the reference sequences); that was also missing but has now been added.; ; In addition to salmon's `alevin` command, each step of `alevin-fry` also writes some useful metadata when it executes. For example, there is a json file written by the `generate-permit-list` step, one written by the `collate` step, and one written by the `quant` step. We've never run into the problem of the output of `alevin-fry` overwriting the output of `alevin` because we use a directory structure where the output quantifications reside in a separate directory from the input RAD file. However, I can now see that if you're writing the quants in the same place as the input, then there will be a conflict in the file names, and the existing files will be overwritten with the new ones. I agree that both tools output useful information. I'm a *bit* ambivalent about assuming the salmon-generated files exist, and merging them into one output file, as I think there might be cases where those files aren't present and `alevin-fry` should still run properly since it doesn't require them to perform it's processing. One option would be to rename the `alevin-fry` output files to prefix/postfix them so they don't collide with the salmon files even if they live in the same directory. Then, one could (now or later) write a small command to merge the relevant json files into a unified output if that would be more convenient downstream. Let me know your thoughts. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669:390,Security,hash,hash,390,"Hi @jashapiro,. So there are definitely a few things going on here. The first is that you correctly diagnosed the missing cmd_info.json information when `alevin` is run in RAD mode. That was simply an oversight, and there is no reason that file shouldn't have been written. Second, there is also useful information that belongs in `meta_info.json` in the `aux_info` directory (like the SHA hash of the reference sequences); that was also missing but has now been added.; ; In addition to salmon's `alevin` command, each step of `alevin-fry` also writes some useful metadata when it executes. For example, there is a json file written by the `generate-permit-list` step, one written by the `collate` step, and one written by the `quant` step. We've never run into the problem of the output of `alevin-fry` overwriting the output of `alevin` because we use a directory structure where the output quantifications reside in a separate directory from the input RAD file. However, I can now see that if you're writing the quants in the same place as the input, then there will be a conflict in the file names, and the existing files will be overwritten with the new ones. I agree that both tools output useful information. I'm a *bit* ambivalent about assuming the salmon-generated files exist, and merging them into one output file, as I think there might be cases where those files aren't present and `alevin-fry` should still run properly since it doesn't require them to perform it's processing. One option would be to rename the `alevin-fry` output files to prefix/postfix them so they don't collide with the salmon files even if they live in the same directory. Then, one could (now or later) write a small command to merge the relevant json files into a unified output if that would be more convenient downstream. Let me know your thoughts. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669:191,Usability,simpl,simply,191,"Hi @jashapiro,. So there are definitely a few things going on here. The first is that you correctly diagnosed the missing cmd_info.json information when `alevin` is run in RAD mode. That was simply an oversight, and there is no reason that file shouldn't have been written. Second, there is also useful information that belongs in `meta_info.json` in the `aux_info` directory (like the SHA hash of the reference sequences); that was also missing but has now been added.; ; In addition to salmon's `alevin` command, each step of `alevin-fry` also writes some useful metadata when it executes. For example, there is a json file written by the `generate-permit-list` step, one written by the `collate` step, and one written by the `quant` step. We've never run into the problem of the output of `alevin-fry` overwriting the output of `alevin` because we use a directory structure where the output quantifications reside in a separate directory from the input RAD file. However, I can now see that if you're writing the quants in the same place as the input, then there will be a conflict in the file names, and the existing files will be overwritten with the new ones. I agree that both tools output useful information. I'm a *bit* ambivalent about assuming the salmon-generated files exist, and merging them into one output file, as I think there might be cases where those files aren't present and `alevin-fry` should still run properly since it doesn't require them to perform it's processing. One option would be to rename the `alevin-fry` output files to prefix/postfix them so they don't collide with the salmon files even if they live in the same directory. Then, one could (now or later) write a small command to merge the relevant json files into a unified output if that would be more convenient downstream. Let me know your thoughts. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316:325,Deployability,pipeline,pipeline,325,"Hi @rob-p,. Ah, I hadn't looked carefully at the outputs, so I was overlooking the fact that the `meta_info.json` for `salmon alevin` is in the `aux_info` subdirectory, while the one for `alevin-fry quant` is in the output directory, so I don't _think_ there will be a conflict. (We have been using the same directory in our pipeline for simplicity.) However, I do think having them named the same thing is a potential source of confusion. Is there a reason not to name the file produced by `alevin-fry quant` something more like `quant.json` or `quant_info.json` to be more in parallel with the `collate.json` and `generate_permit_list.json` files generated at those steps? Either way, I agree that merging the files within `alevin-fry` is probably _not_ the best solution. . The `cmd_info.json` file, seems a special case: I am not sure what the ultimate goal for that file is; it seems now to be included ""for R compatibility"" though I am not fully clear on what that means (with `.mtx` input we don't need it, but `tximeta`/`tximport` may be looking for it?). If the final quant output directory does need the file, it would seem to make sense to copy it along somehow from the `salmon alevin --rad` output directory (with a stop along the way in the `collate` output I guess?). Presumably the `aux_info` would also be desired for `tximeta` if/when `alevin-fry` support is implemented there?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316:338,Usability,simpl,simplicity,338,"Hi @rob-p,. Ah, I hadn't looked carefully at the outputs, so I was overlooking the fact that the `meta_info.json` for `salmon alevin` is in the `aux_info` subdirectory, while the one for `alevin-fry quant` is in the output directory, so I don't _think_ there will be a conflict. (We have been using the same directory in our pipeline for simplicity.) However, I do think having them named the same thing is a potential source of confusion. Is there a reason not to name the file produced by `alevin-fry quant` something more like `quant.json` or `quant_info.json` to be more in parallel with the `collate.json` and `generate_permit_list.json` files generated at those steps? Either way, I agree that merging the files within `alevin-fry` is probably _not_ the best solution. . The `cmd_info.json` file, seems a special case: I am not sure what the ultimate goal for that file is; it seems now to be included ""for R compatibility"" though I am not fully clear on what that means (with `.mtx` input we don't need it, but `tximeta`/`tximport` may be looking for it?). If the final quant output directory does need the file, it would seem to make sense to copy it along somehow from the `salmon alevin --rad` output directory (with a stop along the way in the `collate` output I guess?). Presumably the `aux_info` would also be desired for `tximeta` if/when `alevin-fry` support is implemented there?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316:952,Usability,clear,clear,952,"Hi @rob-p,. Ah, I hadn't looked carefully at the outputs, so I was overlooking the fact that the `meta_info.json` for `salmon alevin` is in the `aux_info` subdirectory, while the one for `alevin-fry quant` is in the output directory, so I don't _think_ there will be a conflict. (We have been using the same directory in our pipeline for simplicity.) However, I do think having them named the same thing is a potential source of confusion. Is there a reason not to name the file produced by `alevin-fry quant` something more like `quant.json` or `quant_info.json` to be more in parallel with the `collate.json` and `generate_permit_list.json` files generated at those steps? Either way, I agree that merging the files within `alevin-fry` is probably _not_ the best solution. . The `cmd_info.json` file, seems a special case: I am not sure what the ultimate goal for that file is; it seems now to be included ""for R compatibility"" though I am not fully clear on what that means (with `.mtx` input we don't need it, but `tximeta`/`tximport` may be looking for it?). If the final quant output directory does need the file, it would seem to make sense to copy it along somehow from the `salmon alevin --rad` output directory (with a stop along the way in the `collate` output I guess?). Presumably the `aux_info` would also be desired for `tximeta` if/when `alevin-fry` support is implemented there?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885209656:544,Availability,avail,available,544,"> Hi @jashapiro,; > ; > I've just pushed a commit that should address this, adding both a ""full"" `cmd_info.json` and a reasonable `meta_info.json`. If this is something you need on short notice we can try an push out a 1.5.2 soon, otherwise, this will be in the next release.; > ; > Best,; > Rob. Hi Rob- . I was happy to wait, but since [you were so quick to push out `alevin-fry` 0.4.1](https://github.com/COMBINE-lab/alevin-fry/issues/22#issuecomment-885195051) , it might be nice to have the version bump here too, so we can use all of the available info in our pipeline!. Thanks for your quick responses!; -Josh",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885209656
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885209656:267,Deployability,release,release,267,"> Hi @jashapiro,; > ; > I've just pushed a commit that should address this, adding both a ""full"" `cmd_info.json` and a reasonable `meta_info.json`. If this is something you need on short notice we can try an push out a 1.5.2 soon, otherwise, this will be in the next release.; > ; > Best,; > Rob. Hi Rob- . I was happy to wait, but since [you were so quick to push out `alevin-fry` 0.4.1](https://github.com/COMBINE-lab/alevin-fry/issues/22#issuecomment-885195051) , it might be nice to have the version bump here too, so we can use all of the available info in our pipeline!. Thanks for your quick responses!; -Josh",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885209656
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885209656:566,Deployability,pipeline,pipeline,566,"> Hi @jashapiro,; > ; > I've just pushed a commit that should address this, adding both a ""full"" `cmd_info.json` and a reasonable `meta_info.json`. If this is something you need on short notice we can try an push out a 1.5.2 soon, otherwise, this will be in the next release.; > ; > Best,; > Rob. Hi Rob- . I was happy to wait, but since [you were so quick to push out `alevin-fry` 0.4.1](https://github.com/COMBINE-lab/alevin-fry/issues/22#issuecomment-885195051) , it might be nice to have the version bump here too, so we can use all of the available info in our pipeline!. Thanks for your quick responses!; -Josh",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885209656
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885640975:16,Deployability,release,released,16,"Hi Josh,. We've released 1.5.2 that incorporates these changes. Let me know if everything works from your end. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885640975
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885683822:45,Deployability,update,updates,45,"Hi Rob,. Seems to work great! Thanks for the updates!. I would consider this issue closed!. -Josh",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-885683822
https://github.com/COMBINE-lab/salmon/issues/689#issuecomment-883541895:50,Availability,error,error,50,"Hi @hsi88,. It looks like there was an allocation error while reading the input. This exception terminated the program (which resulted in the FastxParser destructor error happening simultaneously in a different thread of execution). Can you see if you can run this command successfully without including the decoy sequence and, if so, attempt index construction on a machine with more RAM?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/689#issuecomment-883541895
https://github.com/COMBINE-lab/salmon/issues/689#issuecomment-883541895:165,Availability,error,error,165,"Hi @hsi88,. It looks like there was an allocation error while reading the input. This exception terminated the program (which resulted in the FastxParser destructor error happening simultaneously in a different thread of execution). Can you see if you can run this command successfully without including the decoy sequence and, if so, attempt index construction on a machine with more RAM?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/689#issuecomment-883541895
https://github.com/COMBINE-lab/salmon/issues/690#issuecomment-886279790:259,Deployability,release,releases,259,"Hi @saipra003,. Thank you for posting the issue, and also following up with the resolution. It’s not immediately clear why there would have been an issue with 1.2.1, but we’ll be sure to make not of this for anyone else who runs into such an issue with older releases. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/690#issuecomment-886279790
https://github.com/COMBINE-lab/salmon/issues/690#issuecomment-886279790:113,Usability,clear,clear,113,"Hi @saipra003,. Thank you for posting the issue, and also following up with the resolution. It’s not immediately clear why there would have been an issue with 1.2.1, but we’ll be sure to make not of this for anyone else who runs into such an issue with older releases. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/690#issuecomment-886279790
https://github.com/COMBINE-lab/salmon/issues/691#issuecomment-921865678:85,Deployability,release,release,85,"Thanks for the super-detailed report, @allyhawkins. We'll fix this in the next point release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/691#issuecomment-921865678
https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-916889943:403,Availability,reliab,reliably,403,"I am assuming what you are looking for is the ""effective lengths"" of the transcripts i.e. not just the original transcript lengths but instead corrected based on the quantification model. I think it's going to be tricky to generate that because of two major reasons: (1) salmon model does not perform length correction in single-cell mode mainly due to 3' single-end sequencing of the read it's hard to reliably estimate the fragment lengths (2) salmon in single-cell mode performs quantification at gene-level which makes it harder to predict effective length of the transcripts.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-916889943
https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-916889943:293,Performance,perform,perform,293,"I am assuming what you are looking for is the ""effective lengths"" of the transcripts i.e. not just the original transcript lengths but instead corrected based on the quantification model. I think it's going to be tricky to generate that because of two major reasons: (1) salmon model does not perform length correction in single-cell mode mainly due to 3' single-end sequencing of the read it's hard to reliably estimate the fragment lengths (2) salmon in single-cell mode performs quantification at gene-level which makes it harder to predict effective length of the transcripts.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-916889943
https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-916889943:473,Performance,perform,performs,473,"I am assuming what you are looking for is the ""effective lengths"" of the transcripts i.e. not just the original transcript lengths but instead corrected based on the quantification model. I think it's going to be tricky to generate that because of two major reasons: (1) salmon model does not perform length correction in single-cell mode mainly due to 3' single-end sequencing of the read it's hard to reliably estimate the fragment lengths (2) salmon in single-cell mode performs quantification at gene-level which makes it harder to predict effective length of the transcripts.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-916889943
https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-916889943:536,Safety,predict,predict,536,"I am assuming what you are looking for is the ""effective lengths"" of the transcripts i.e. not just the original transcript lengths but instead corrected based on the quantification model. I think it's going to be tricky to generate that because of two major reasons: (1) salmon model does not perform length correction in single-cell mode mainly due to 3' single-end sequencing of the read it's hard to reliably estimate the fragment lengths (2) salmon in single-cell mode performs quantification at gene-level which makes it harder to predict effective length of the transcripts.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-916889943
https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-917202872:306,Availability,down,down,306,"Thanks, Avi- actually, I am interested in obtaining the **original** transcript lengths . For now, I'm going through the effort of mapping the salmon quant output (quant.sf) ""Length"" column to the raw counts. Not quite there yet, but as long as there isn't anything already in place for this, I'll proceed down this avenue. . Thanks again!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/693#issuecomment-917202872
https://github.com/COMBINE-lab/salmon/issues/694#issuecomment-892079287:1078,Availability,error,error,1078,"Hi @rudondamba,. Thanks for the report. At salmon v1.0.0, there was a breaking change in the index used in salmon. We moved from using the RapMap index to using a [pufferfish based index](https://github.com/COMBINE-lab/pufferfish). This means that, you'll want to rebuild your index using a recent version of salmon. Further, the move to the pufferfish index drops support for the FMD-like index, so you can remove the `--type` argument. With a new version of salmon you should be able to do:. ```{bash}; salmon index -t galGal6.gene+cluster+repBase+tRNA.fa -i SalmonIndex -k 31 -p 8; ```. the `-p` is optional and tells salmon to use 8 threads during indexing. Also, the default `k` is 31, so you don't need to pass that explicitly if don't want. Best,; Rob. P.S. It's worth noting that, even after this, there is still one small problem with your FASTA input file. You have an empty sequence name on line `30445`. More specifically, you have a record named `> NM_001318772.1_Unfound_-`. Note the space between the `>` and the record name. This is not allowed, and leads to an error in parsing the file. If you fix this header line, I can verify that the index builds successfully with the above command using the latest version of salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/694#issuecomment-892079287
https://github.com/COMBINE-lab/salmon/issues/696#issuecomment-1903505996:168,Availability,Error,Error,168,"In my case, the issue was that I was giving a relative path instead of absolute path to the `salmon quant -i` argument. Giving the absolute path made the `Exception : [Error: The index version file salmon_index/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]` error go away.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/696#issuecomment-1903505996
https://github.com/COMBINE-lab/salmon/issues/696#issuecomment-1903505996:294,Availability,error,error,294,"In my case, the issue was that I was giving a relative path instead of absolute path to the `salmon quant -i` argument. Giving the absolute path made the `Exception : [Error: The index version file salmon_index/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]` error go away.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/696#issuecomment-1903505996
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-921879305:153,Availability,ping,ping,153,"Hi @jeremymsimon, . Somehow, the notification for this in my e-mail got classified as SPAM. Anyway, thank you for the detailed description! I'm going to ping @Gaura here. @Gaura — this is the alternative protocol I was discussing with you yesterday. As you can see, the main issue here is the ""noisy"" barcodes. Let me know what you think would be necessary to add support for this, and I'm happy to schedule a technical discussion if you want to discuss some options.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-921879305
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-921879305:399,Energy Efficiency,schedul,schedule,399,"Hi @jeremymsimon, . Somehow, the notification for this in my e-mail got classified as SPAM. Anyway, thank you for the detailed description! I'm going to ping @Gaura here. @Gaura — this is the alternative protocol I was discussing with you yesterday. As you can see, the main issue here is the ""noisy"" barcodes. Let me know what you think would be necessary to add support for this, and I'm happy to schedule a technical discussion if you want to discuss some options.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-921879305
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-921879305:204,Integrability,protocol,protocol,204,"Hi @jeremymsimon, . Somehow, the notification for this in my e-mail got classified as SPAM. Anyway, thank you for the detailed description! I'm going to ping @Gaura here. @Gaura — this is the alternative protocol I was discussing with you yesterday. As you can see, the main issue here is the ""noisy"" barcodes. Let me know what you think would be necessary to add support for this, and I'm happy to schedule a technical discussion if you want to discuss some options.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-921879305
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936300287:22,Deployability,update,update,22,@rob-p and @Gaura and update on this? Would be good to know if this is feasible,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936300287
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:329,Availability,avail,available,329,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:51,Deployability,pipeline,pipeline,51,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:103,Deployability,pipeline,pipeline,103,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:33,Integrability,protocol,protocol,33,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:149,Integrability,protocol,protocol,149,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:247,Testability,test,testing,247,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:362,Usability,clear,clear,362,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:482,Availability,down,down,482,"Thanks @Gaura! Sounds promising. . Can you clarify what differences you saw between v1/v2 protocols? My understanding was that the only changes were slight differences in barcode positions within the barcode read, ie something that could be handled with different `bc-geometry`, but maybe that's all you meant in terms of differing implementations. Regarding the BC1 and how it could be two sequences for the same sample - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of sal",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:90,Integrability,protocol,protocols,90,"Thanks @Gaura! Sounds promising. . Can you clarify what differences you saw between v1/v2 protocols? My understanding was that the only changes were slight differences in barcode positions within the barcode read, ie something that could be handled with different `bc-geometry`, but maybe that's all you meant in terms of differing implementations. Regarding the BC1 and how it could be two sequences for the same sample - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of sal",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:554,Integrability,protocol,protocol,554,"Thanks @Gaura! Sounds promising. . Can you clarify what differences you saw between v1/v2 protocols? My understanding was that the only changes were slight differences in barcode positions within the barcode read, ie something that could be handled with different `bc-geometry`, but maybe that's all you meant in terms of differing implementations. Regarding the BC1 and how it could be two sequences for the same sample - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of sal",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:1273,Integrability,rout,routes,1273,"within the barcode read, ie something that could be handled with different `bc-geometry`, but maybe that's all you meant in terms of differing implementations. Regarding the BC1 and how it could be two sequences for the same sample - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of salmon/alevin as a preprocessing step, like what my slow perl script can do, or it can be handled internally. Having alevin do the collapsing would likely be a lot faster and means the FASTQs",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:2333,Usability,clear,clear,2333,"mple - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of salmon/alevin as a preprocessing step, like what my slow perl script can do, or it can be handled internally. Having alevin do the collapsing would likely be a lot faster and means the FASTQs don't need any editing, which would be preferable, but I would understand if that is out of scope for you all. . Hopefully that explanation is clear, but if you have any other questions on this I'd be happy to meet and discuss",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951080577:425,Deployability,pipeline,pipeline,425,"Thanks Jeremy! Yes, that's what I was hinting at with different v1/v2 protocols. From their code, you can see differences in amplicon sequences:; - For v1: 'NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGAGGCCAGAGCATTCGIIIIIIII'; - For v2: 'NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGACTGTGGIIIIIIII'; where the `IIIIIIII` sequence corresponds to barcode. This is from the pipeline code I mentioned earlier used for [this paper](https://www.nature.com/articles/s41593-021-00872-y). Do you have a the pairing file for the BC1 barcodes? Is it the Supp Table S12 in the Rosenberg paper? It is needed for development and testing.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951080577
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951080577:70,Integrability,protocol,protocols,70,"Thanks Jeremy! Yes, that's what I was hinting at with different v1/v2 protocols. From their code, you can see differences in amplicon sequences:; - For v1: 'NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGAGGCCAGAGCATTCGIIIIIIII'; - For v2: 'NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGACTGTGGIIIIIIII'; where the `IIIIIIII` sequence corresponds to barcode. This is from the pipeline code I mentioned earlier used for [this paper](https://www.nature.com/articles/s41593-021-00872-y). Do you have a the pairing file for the BC1 barcodes? Is it the Supp Table S12 in the Rosenberg paper? It is needed for development and testing.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951080577
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951080577:669,Testability,test,testing,669,"Thanks Jeremy! Yes, that's what I was hinting at with different v1/v2 protocols. From their code, you can see differences in amplicon sequences:; - For v1: 'NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGAGGCCAGAGCATTCGIIIIIIII'; - For v2: 'NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGACTGTGGIIIIIIII'; where the `IIIIIIII` sequence corresponds to barcode. This is from the pipeline code I mentioned earlier used for [this paper](https://www.nature.com/articles/s41593-021-00872-y). Do you have a the pairing file for the BC1 barcodes? Is it the Supp Table S12 in the Rosenberg paper? It is needed for development and testing.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951080577
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951974369:85,Integrability,depend,depend,85,"Yup this geometry is correct! As for a pairing file for the BC1 barcodes, this _may_ depend on the version and/or implementation in the lab. For ParseBio (""v2""), it seems to be consistent thus far, and I've linked a pairing table for that below. For homemade SPLiT-seq (""v1""), which barcodes end up in which wells, and which wells actually get utilized may vary. Additionally, some other labs may not use random hexamers at all, meaning we should have some flexibility such that users provide their own table and also an option for whether this pairing table is strictly required. In [this barcode sharing file](https://github.com/COMBINE-lab/salmon/files/7418722/ParseBio_barcodeSharing.txt), the first column represents the oligo-dT BC1 sequences, and the second column are the paired random hexamer BC1 sequences. Note this file is the same as [this one here on my github](https://github.com/jeremymsimon/SPLITseq).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951974369
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108:395,Availability,down,downstream,395,"Hi @jeremymsimon,. I've discussed the support for SPLiT-seq/ParseBio with @Gaura in some depth. Honestly, I think the cleanest solution right now is just to have a more streamlined (and streaming) way to match / replace the random hexamers upstream of alevin-fry. By my understanding, if we can simply replace barcode 1 appropriately (as your Perl script currently does), everything should work downstream in alevin/alevin-fry.; ; To that end, I've thrown together a small rust program based on your Perl script. Currently that lives [here](https://github.com/COMBINE-lab/splitp). It reads the same basic parameters as the Perl script, and writes its output to stdout so that it can be used with named pipes. For example, something like:; ; ```; <normal salmon command> -1 read_file_1.fq -2 <(splitp --read-file read_file_2.fq --bc-map bcSharing_example.txt --start 79 --end 86 --one-hamming); ```. which will transform the second fastq file and stream the transformed reads out which can then be read by alevin-fry. One important thing to note is that while *alevin* requires the input reads to be a real file (i.e. you can't stream reads in because it does 2 passes), if you are mapping these reads for processing with *alevin-fry* you can use the process substitution trick above. As you hinted, this program works considerably faster than the Perl script. For example, for the first 10,000,000 reads in `SRR6750042`, the Perl script took 2m 48s to transform the reads and `splitp` took ~6s (if the output wasn't being written to a file on disk it took <4s). This should generally be fast enough to not be a speed bottleneck. So, perhaps the next step is to try to help you walk through this approach with a test dataset (and ideally using alevin-fry) to see if things are turning out as expected?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108:1617,Performance,bottleneck,bottleneck,1617,"Hi @jeremymsimon,. I've discussed the support for SPLiT-seq/ParseBio with @Gaura in some depth. Honestly, I think the cleanest solution right now is just to have a more streamlined (and streaming) way to match / replace the random hexamers upstream of alevin-fry. By my understanding, if we can simply replace barcode 1 appropriately (as your Perl script currently does), everything should work downstream in alevin/alevin-fry.; ; To that end, I've thrown together a small rust program based on your Perl script. Currently that lives [here](https://github.com/COMBINE-lab/splitp). It reads the same basic parameters as the Perl script, and writes its output to stdout so that it can be used with named pipes. For example, something like:; ; ```; <normal salmon command> -1 read_file_1.fq -2 <(splitp --read-file read_file_2.fq --bc-map bcSharing_example.txt --start 79 --end 86 --one-hamming); ```. which will transform the second fastq file and stream the transformed reads out which can then be read by alevin-fry. One important thing to note is that while *alevin* requires the input reads to be a real file (i.e. you can't stream reads in because it does 2 passes), if you are mapping these reads for processing with *alevin-fry* you can use the process substitution trick above. As you hinted, this program works considerably faster than the Perl script. For example, for the first 10,000,000 reads in `SRR6750042`, the Perl script took 2m 48s to transform the reads and `splitp` took ~6s (if the output wasn't being written to a file on disk it took <4s). This should generally be fast enough to not be a speed bottleneck. So, perhaps the next step is to try to help you walk through this approach with a test dataset (and ideally using alevin-fry) to see if things are turning out as expected?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108:1711,Testability,test,test,1711,"Hi @jeremymsimon,. I've discussed the support for SPLiT-seq/ParseBio with @Gaura in some depth. Honestly, I think the cleanest solution right now is just to have a more streamlined (and streaming) way to match / replace the random hexamers upstream of alevin-fry. By my understanding, if we can simply replace barcode 1 appropriately (as your Perl script currently does), everything should work downstream in alevin/alevin-fry.; ; To that end, I've thrown together a small rust program based on your Perl script. Currently that lives [here](https://github.com/COMBINE-lab/splitp). It reads the same basic parameters as the Perl script, and writes its output to stdout so that it can be used with named pipes. For example, something like:; ; ```; <normal salmon command> -1 read_file_1.fq -2 <(splitp --read-file read_file_2.fq --bc-map bcSharing_example.txt --start 79 --end 86 --one-hamming); ```. which will transform the second fastq file and stream the transformed reads out which can then be read by alevin-fry. One important thing to note is that while *alevin* requires the input reads to be a real file (i.e. you can't stream reads in because it does 2 passes), if you are mapping these reads for processing with *alevin-fry* you can use the process substitution trick above. As you hinted, this program works considerably faster than the Perl script. For example, for the first 10,000,000 reads in `SRR6750042`, the Perl script took 2m 48s to transform the reads and `splitp` took ~6s (if the output wasn't being written to a file on disk it took <4s). This should generally be fast enough to not be a speed bottleneck. So, perhaps the next step is to try to help you walk through this approach with a test dataset (and ideally using alevin-fry) to see if things are turning out as expected?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108:295,Usability,simpl,simply,295,"Hi @jeremymsimon,. I've discussed the support for SPLiT-seq/ParseBio with @Gaura in some depth. Honestly, I think the cleanest solution right now is just to have a more streamlined (and streaming) way to match / replace the random hexamers upstream of alevin-fry. By my understanding, if we can simply replace barcode 1 appropriately (as your Perl script currently does), everything should work downstream in alevin/alevin-fry.; ; To that end, I've thrown together a small rust program based on your Perl script. Currently that lives [here](https://github.com/COMBINE-lab/splitp). It reads the same basic parameters as the Perl script, and writes its output to stdout so that it can be used with named pipes. For example, something like:; ; ```; <normal salmon command> -1 read_file_1.fq -2 <(splitp --read-file read_file_2.fq --bc-map bcSharing_example.txt --start 79 --end 86 --one-hamming); ```. which will transform the second fastq file and stream the transformed reads out which can then be read by alevin-fry. One important thing to note is that while *alevin* requires the input reads to be a real file (i.e. you can't stream reads in because it does 2 passes), if you are mapping these reads for processing with *alevin-fry* you can use the process substitution trick above. As you hinted, this program works considerably faster than the Perl script. For example, for the first 10,000,000 reads in `SRR6750042`, the Perl script took 2m 48s to transform the reads and `splitp` took ~6s (if the output wasn't being written to a file on disk it took <4s). This should generally be fast enough to not be a speed bottleneck. So, perhaps the next step is to try to help you walk through this approach with a test dataset (and ideally using alevin-fry) to see if things are turning out as expected?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-978168267:681,Availability,down,downstream,681,"Hey @rob-p and @Gaura - thanks for this! My understanding is that your `splitp` will replace my slow perl script, which is great, and then alevin-fry should work pretty much like any other run, yes? If so, can you comment on the low alignment rate and other oddities I encountered running regular alevin following editing of my R2 FASTQ in this way (documented above), and whether there's something inherently different about alevin-fry that should address those issues? Because I currently detect only a tiny fraction of the cells expected. . I'm more than happy/eager to give splitp+alevin-fry a try, but I suspect there's some secondary issue at hand that we'll need to address downstream",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-978168267
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-978168267:491,Safety,detect,detect,491,"Hey @rob-p and @Gaura - thanks for this! My understanding is that your `splitp` will replace my slow perl script, which is great, and then alevin-fry should work pretty much like any other run, yes? If so, can you comment on the low alignment rate and other oddities I encountered running regular alevin following editing of my R2 FASTQ in this way (documented above), and whether there's something inherently different about alevin-fry that should address those issues? Because I currently detect only a tiny fraction of the cells expected. . I'm more than happy/eager to give splitp+alevin-fry a try, but I suspect there's some secondary issue at hand that we'll need to address downstream",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-978168267
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-982878463:359,Integrability,wrap,wrap,359,"Hi @jeremymsimon! In order to test and validate the implementation I would need a count matrix generated on samples. Do you have a sample and count matrix from that? The Rosenberg submission of the data has an unclear way of specifying barcodes and I have emailed him about it. If you have count matrix and matching fastqs that we can use to validate, we can wrap it up soon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-982878463
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-982878463:39,Security,validat,validate,39,"Hi @jeremymsimon! In order to test and validate the implementation I would need a count matrix generated on samples. Do you have a sample and count matrix from that? The Rosenberg submission of the data has an unclear way of specifying barcodes and I have emailed him about it. If you have count matrix and matching fastqs that we can use to validate, we can wrap it up soon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-982878463
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-982878463:342,Security,validat,validate,342,"Hi @jeremymsimon! In order to test and validate the implementation I would need a count matrix generated on samples. Do you have a sample and count matrix from that? The Rosenberg submission of the data has an unclear way of specifying barcodes and I have emailed him about it. If you have count matrix and matching fastqs that we can use to validate, we can wrap it up soon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-982878463
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-982878463:30,Testability,test,test,30,"Hi @jeremymsimon! In order to test and validate the implementation I would need a count matrix generated on samples. Do you have a sample and count matrix from that? The Rosenberg submission of the data has an unclear way of specifying barcodes and I have emailed him about it. If you have count matrix and matching fastqs that we can use to validate, we can wrap it up soon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-982878463
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-983984806:465,Testability,test,test,465,"Apologies- I deleted my earlier comment because I realized you don't really need any alevin output from me. . I also can't seem to find actual barcode sequences on the Rosenberg data, the cells seem to just be indexed from 0-163068, which is not going to be helpful here. I assume you haven't heard back from the authors yet about the actual barcode sequences for these matrices? If not, I think I know of a few other published papers that we may be able to use as test cases instead",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-983984806
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-983999718:81,Security,access,accessible,81,"Yeah, I haven't heard back yet. Any test case is fine where the data is publicly accessible. Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-983999718
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-983999718:36,Testability,test,test,36,"Yeah, I haven't heard back yet. Any test case is fine where the data is publicly accessible. Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-983999718
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-984646381:314,Security,access,accessible,314,"Hey @Gaura - I have a test dataset for us to use. I'm about to set up an alevin run of my own, but wanted to pass it on to you in the meantime. I haven't yet done any testing or exploration of my own yet, though the data comes from a collaborator of ours. The raw (FASTQ) and processed data (UMI counts matrix) is accessible from GEO at [GSE137941](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE137941) and SRA (`fastq-dump --split-files SRR10174292`). These data were generated with the original SPLiT-seq method (your ""v1""). The caveat is that they did NOT combine the oligo-dT and random hexamer barcodes, meaning they are separate cells/columns in their processed data matrix. This means we should be able to run `alevin`/`alevin-fry` directly on these FASTQs, bypassing `splitp` for now, and get something that hopefully matches their processed data matrix. . According to the methods section [of their paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7366517/), they used GENCODE v28 annotations, and ultimately kept 6,888 nuclei after filtering. Their processed data matrix seems to have 25,000 columns, so I suppose this is pre-filtering. . Let me know what you think!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-984646381
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-984646381:22,Testability,test,test,22,"Hey @Gaura - I have a test dataset for us to use. I'm about to set up an alevin run of my own, but wanted to pass it on to you in the meantime. I haven't yet done any testing or exploration of my own yet, though the data comes from a collaborator of ours. The raw (FASTQ) and processed data (UMI counts matrix) is accessible from GEO at [GSE137941](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE137941) and SRA (`fastq-dump --split-files SRR10174292`). These data were generated with the original SPLiT-seq method (your ""v1""). The caveat is that they did NOT combine the oligo-dT and random hexamer barcodes, meaning they are separate cells/columns in their processed data matrix. This means we should be able to run `alevin`/`alevin-fry` directly on these FASTQs, bypassing `splitp` for now, and get something that hopefully matches their processed data matrix. . According to the methods section [of their paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7366517/), they used GENCODE v28 annotations, and ultimately kept 6,888 nuclei after filtering. Their processed data matrix seems to have 25,000 columns, so I suppose this is pre-filtering. . Let me know what you think!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-984646381
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-984646381:167,Testability,test,testing,167,"Hey @Gaura - I have a test dataset for us to use. I'm about to set up an alevin run of my own, but wanted to pass it on to you in the meantime. I haven't yet done any testing or exploration of my own yet, though the data comes from a collaborator of ours. The raw (FASTQ) and processed data (UMI counts matrix) is accessible from GEO at [GSE137941](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE137941) and SRA (`fastq-dump --split-files SRR10174292`). These data were generated with the original SPLiT-seq method (your ""v1""). The caveat is that they did NOT combine the oligo-dT and random hexamer barcodes, meaning they are separate cells/columns in their processed data matrix. This means we should be able to run `alevin`/`alevin-fry` directly on these FASTQs, bypassing `splitp` for now, and get something that hopefully matches their processed data matrix. . According to the methods section [of their paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7366517/), they used GENCODE v28 annotations, and ultimately kept 6,888 nuclei after filtering. Their processed data matrix seems to have 25,000 columns, so I suppose this is pre-filtering. . Let me know what you think!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-984646381
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-984788978:61,Availability,down,download,61,"This is great find! Thanks @jeremymsimon! I have started the download, will run it today.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-984788978
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985038970:82,Safety,detect,detected,82,"Okay my alevin run finished, and I got a mapping rate of just 6.1% and 2254 cells detected. My `lib_format_counts.json` contains the following:. ```; {; ""read_files"": ""[ SRR10174292_2.fastq.gz, SRR10174292_1.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 15259749,; ""num_assigned_fragments"": 15259749,; ""num_frags_with_concordant_consistent_mappings"": 0,; ""num_frags_with_inconsistent_or_orphan_mappings"": 61866895,; ""strand_mapping_bias"": 0.0,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 0,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 0,; ""SF"": 0,; ""SR"": 0,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```. so lots of fragments are discarded for one reason or another, and it's not clear whether the library type assignment is working properly, sort of like my initial example above. Separately, I'm running zUMIs on the same files and will report back with those data when the run is complete",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985038970
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985038970:709,Usability,clear,clear,709,"Okay my alevin run finished, and I got a mapping rate of just 6.1% and 2254 cells detected. My `lib_format_counts.json` contains the following:. ```; {; ""read_files"": ""[ SRR10174292_2.fastq.gz, SRR10174292_1.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 15259749,; ""num_assigned_fragments"": 15259749,; ""num_frags_with_concordant_consistent_mappings"": 0,; ""num_frags_with_inconsistent_or_orphan_mappings"": 61866895,; ""strand_mapping_bias"": 0.0,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 0,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 0,; ""SF"": 0,; ""SR"": 0,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```. so lots of fragments are discarded for one reason or another, and it's not clear whether the library type assignment is working properly, sort of like my initial example above. Separately, I'm running zUMIs on the same files and will report back with those data when the run is complete",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985038970
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985043620:147,Integrability,protocol,protocol,147,"Hi @jeremymsimon --- I am noticing this as jumping out:. ```; ""num_frags_with_inconsistent_or_orphan_mappings"": 61866895,; ```. Is `ISR` the right protocol for this data, in the manner in which the reads are provided to `alevin`? @Gaura is doing a run with alevin-fry and we'll discuss those results here when we have them. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985043620
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985521197:382,Availability,error,error,382,"Yesterday, I got a mapping rate of about 16% with `--split-seqV1` flag and alevin-fry using salmon index based on GENCODE v31 reference and salmon index without decoy. The number of nuclei identified with knee-distance filter is 6928. However, the number of barcodes that match b/w the submission and run is 305 which I am trying to figure out why. ; Earlier, we spotted a position error in about 10% of the split-seq reads in a subset of 2.5M reads. The fixed sequence `ATCCACGTGCTTGAGAGGCCAGAGCATTCG` doesn't always occur at its position and sometimes it's a base or 2 off, which changes the expected barcodes. For example, the sequence in red is part fixed sequence and part 8-bp barcode and it should be in the end of read but sometimes it is not. ; ![image](https://user-images.githubusercontent.com/12998572/144609847-635e1997-fcf3-4e30-8992-ce2a54e21329.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985521197
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1307,Availability,error,errors,1307," here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:2489,Availability,error,errors,2489,"tead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps that can be achieved with some Levenshtein distance flexibility using the entire 24bp barcode sequence detected...? . Anyway sorry for the brainstorming dump, but the short answer is: we're probably stuck losing a bunch of reads due to positional errors like this",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:535,Modifiability,flexible,flexible,535,"@Gaura this sort of frameshift in the barcodes is a known issue, and can be computationally challenging (at least for existing methods). zUMIs, for example, does an automatic barcode detection based on fixed barcode positions like we're doing here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:183,Safety,detect,detection,183,"@Gaura this sort of frameshift in the barcodes is a known issue, and can be computationally challenging (at least for existing methods). zUMIs, for example, does an automatic barcode detection based on fixed barcode positions like we're doing here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:277,Safety,detect,detect,277,"@Gaura this sort of frameshift in the barcodes is a known issue, and can be computationally challenging (at least for existing methods). zUMIs, for example, does an automatic barcode detection based on fixed barcode positions like we're doing here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1226,Safety,detect,detection,1226," here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1604,Safety,detect,detection,1604,"sible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps that can be achieved with some Levenshtein distance flexibility using the entire 24bp barcode sequence detected...? . Anyway sorry for the brainstorming dump, but the short answer is: we're probably stuck losing a bunch of ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:2345,Safety,detect,detected,2345,"tead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps that can be achieved with some Levenshtein distance flexibility using the entire 24bp barcode sequence detected...? . Anyway sorry for the brainstorming dump, but the short answer is: we're probably stuck losing a bunch of reads due to positional errors like this",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1019,Testability,test,test,1019,"be computationally challenging (at least for existing methods). zUMIs, for example, does an automatic barcode detection based on fixed barcode positions like we're doing here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1176,Usability,simpl,simplest,1176," here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985583153:250,Deployability,release,release,250,"Hi @jeremymsimon and @Gaura,. After redoing the analysis with the correct barcode geometry (still with --sketch-mode), using knee filtering with alevin-fry leads to the following:. ```; robp@ocean1:/data007/users/rob/SRR10174292$ ~/alevin-fry/target/release/alevin-fry generate-permit-list -k -i SRR10174292_map -o SRR10174292_quant -d both; 2021-12-03 09:49:37 INFO paired : false, ref_count : 228,754, num_chunks : 11,194; 2021-12-03 09:49:37 INFO read 2 file-level tags; 2021-12-03 09:49:37 INFO read 2 read-level tags; 2021-12-03 09:49:37 INFO read 1 alignemnt-level tags; 2021-12-03 09:49:37 INFO File-level tag values FileTags { bclen: 24, umilen: 10 }; 2021-12-03 09:49:50 INFO observed 55,952,280 reads in 11,194 chunks --- max ambiguity read occurs in 2,330 refs; 2021-12-03 09:49:50 INFO max_idx = 268803; 2021-12-03 09:49:50 INFO max_idx = 86532; 2021-12-03 09:49:50 INFO max_idx = 30016; 2021-12-03 09:49:50 INFO max_idx = 12061; 2021-12-03 09:49:50 INFO max_idx = 8292; 2021-12-03 09:49:50 INFO max_idx = 7396; 2021-12-03 09:49:50 INFO max_idx = 7112; 2021-12-03 09:49:50 INFO max_idx = 7012; 2021-12-03 09:49:50 INFO max_idx = 6969; 2021-12-03 09:49:50 INFO max_idx = 6952; 2021-12-03 09:49:50 INFO knee-finding iter = 10; 2021-12-03 09:49:50 INFO max_idx = 6944; 2021-12-03 09:49:50 INFO max_idx = 6938; 2021-12-03 09:49:50 INFO max_idx = 6937; 2021-12-03 09:49:50 INFO knee distance method resulted in the selection of 6938 permitted barcodes.; 2021-12-03 09:49:52 INFO total number of distinct corrected barcodes : 391,939; ````. so 6938 detected cells seems about right. Of course, since I'm not using the external unfiltered permit list here, they are likely not the same barcodes as in the deposited data due to the issue that @Gaura mentioned.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985583153
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985583153:1555,Safety,detect,detected,1555,"Hi @jeremymsimon and @Gaura,. After redoing the analysis with the correct barcode geometry (still with --sketch-mode), using knee filtering with alevin-fry leads to the following:. ```; robp@ocean1:/data007/users/rob/SRR10174292$ ~/alevin-fry/target/release/alevin-fry generate-permit-list -k -i SRR10174292_map -o SRR10174292_quant -d both; 2021-12-03 09:49:37 INFO paired : false, ref_count : 228,754, num_chunks : 11,194; 2021-12-03 09:49:37 INFO read 2 file-level tags; 2021-12-03 09:49:37 INFO read 2 read-level tags; 2021-12-03 09:49:37 INFO read 1 alignemnt-level tags; 2021-12-03 09:49:37 INFO File-level tag values FileTags { bclen: 24, umilen: 10 }; 2021-12-03 09:49:50 INFO observed 55,952,280 reads in 11,194 chunks --- max ambiguity read occurs in 2,330 refs; 2021-12-03 09:49:50 INFO max_idx = 268803; 2021-12-03 09:49:50 INFO max_idx = 86532; 2021-12-03 09:49:50 INFO max_idx = 30016; 2021-12-03 09:49:50 INFO max_idx = 12061; 2021-12-03 09:49:50 INFO max_idx = 8292; 2021-12-03 09:49:50 INFO max_idx = 7396; 2021-12-03 09:49:50 INFO max_idx = 7112; 2021-12-03 09:49:50 INFO max_idx = 7012; 2021-12-03 09:49:50 INFO max_idx = 6969; 2021-12-03 09:49:50 INFO max_idx = 6952; 2021-12-03 09:49:50 INFO knee-finding iter = 10; 2021-12-03 09:49:50 INFO max_idx = 6944; 2021-12-03 09:49:50 INFO max_idx = 6938; 2021-12-03 09:49:50 INFO max_idx = 6937; 2021-12-03 09:49:50 INFO knee distance method resulted in the selection of 6938 permitted barcodes.; 2021-12-03 09:49:52 INFO total number of distinct corrected barcodes : 391,939; ````. so 6938 detected cells seems about right. Of course, since I'm not using the external unfiltered permit list here, they are likely not the same barcodes as in the deposited data due to the issue that @Gaura mentioned.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985583153
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987136859:134,Availability,recover,recovered,134,"Hey @Gaura and @rob-p - just as a point of comparison, I ran zUMIs on the same exact files. With nominal filtering I get 12,942 cells recovered, of which 10,386 (80%) were also contained in the published matrix (exact sequence matches). . Happy to provide any of those files if that's useful. But it does seem to argue something is funky when it comes to alevin's detection of these barcodes, right?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987136859
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987136859:134,Safety,recover,recovered,134,"Hey @Gaura and @rob-p - just as a point of comparison, I ran zUMIs on the same exact files. With nominal filtering I get 12,942 cells recovered, of which 10,386 (80%) were also contained in the published matrix (exact sequence matches). . Happy to provide any of those files if that's useful. But it does seem to argue something is funky when it comes to alevin's detection of these barcodes, right?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987136859
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987136859:364,Safety,detect,detection,364,"Hey @Gaura and @rob-p - just as a point of comparison, I ran zUMIs on the same exact files. With nominal filtering I get 12,942 cells recovered, of which 10,386 (80%) were also contained in the published matrix (exact sequence matches). . Happy to provide any of those files if that's useful. But it does seem to argue something is funky when it comes to alevin's detection of these barcodes, right?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987136859
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987334414:108,Integrability,protocol,protocol,108,"Hey @jeremymsimon! I figured out what the issue was. Alevin is fine, but my implementation of the split-seq protocol had an issue. I was able to get good correlation with the submitted counts and good match between barcodes. Here's what I did: . - Used fastp to clean the fastq files. ; - Ran `salmon alevin` with custom geometry flags and `--justAlign`. For index I used human GENCODE v31 reference. ; ```; salmon alevin -i /biodb/human/gencode/v31/salmon_index_wo_decoy/ -l A -1 SRR10174292_trimmed_1.fastq.gz -2 SRR10174292_trimmed_2.fastq.gz -p 44 --bc-geometry 2[11-18,49-56,87-94] --umi-geo 2[1-10] --read-geo 1[1-end] -o runtestdata_custombc --justAlign; ``` ; - Used `alevin-fry` on the output; ```; $ cd runtestdata_custombc; $ alevin-fry generate-permit-list -i ./ -d both --output-dir run -k; 2021-12-06 15:36:39 INFO paired : false, ref_count : 226,030, num_chunks : 7,9282021-12-06 15:36:39 INFO read 2 file-level tags2021-12-06 15:36:39 INFO read 2 read-level tags; 2021-12-06 15:36:39 INFO read 1 alignemnt-level tags; 2021-12-06 15:36:39 INFO File-level tag values FileTags { bclen: 24, umilen: 10 }2021-12-06 15:36:45 INFO observed 39,536,527 reads in 7,928 chunks --- max ambiguity read occurs in 196 refs2021-12-06 15:36:45 INFO max_idx = 170639; 2021-12-06 15:36:45 INFO max_idx = 59128; 2021-12-06 15:36:45 INFO max_idx = 21206; 2021-12-06 15:36:45 INFO max_idx = 10151; 2021-12-06 15:36:45 INFO max_idx = 7852; 2021-12-06 15:36:45 INFO max_idx = 72462021-12-06 15:36:45 INFO max_idx = 70432021-12-06 15:36:45 INFO max_idx = 69602021-12-06 15:36:45 INFO max_idx = 6937; 2021-12-06 15:36:45 INFO max_idx = 6925; 2021-12-06 15:36:45 INFO knee-finding iter = 10; 2021-12-06 15:36:45 INFO max_idx = 6922; 2021-12-06 15:36:45 INFO knee distance method resulted in the selection of 6923 permitted barcodes.; 2021-12-06 15:36:46 INFO total number of distinct corrected barcodes : 333,352; $ alevin-fry collate -i run/ -t 16 -r ./ ; 2021-12-06 15:37:02 INFO filter_type = Filtered; 2021-1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987334414
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987334414:6647,Integrability,protocol,protocol,6647,"true, write: false }; 2021-12-06 15:37:20 INFO paired : false, ref_count : 226,030, num_chunks : 6,923; 2021-12-06 15:37:21 INFO tg-map contained 60,603 genes mapping to 226,030 transcripts.; 2021-12-06 15:37:21 INFO read 2 file-level tags; 2021-12-06 15:37:21 INFO read 2 read-level tags; 2021-12-06 15:37:21 INFO read 1 alignemnt-level tags; 2021-12-06 15:37:21 INFO File-level tag values FileTags { bclen: 24, umilen: 10 }; ⠓ [00:00:00] [╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠐ [00:00:00] [╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠁ [00:00:00] [╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠴ [00:00:01] [╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠤ [00:00:01] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠁ [00:00:01] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠤ [00:00:01] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠠ [00:00:02] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢░╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠐ [00:00:03] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢░╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠋ [00:00:03] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢░╟╟╟╟╟╟╟╟╟╟] ⠄ [00:00:04] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢░╟╟╟╟╟╟╟] ⠈ [00:00:04] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟] ⠙ [00:00:04] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟] 6551/6923 ; 2021-12-06 15:37:26 WARN ; found connected component with 30679 vertices, resolved into 18 UMIs over 10 genes with trivial resolution.; [00:00:07] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢] 6923/6923 finished quantifying 6,923 cells.2021-12-06 15:37:28 INFO processed 26,250,078 total read records; ```. - Found that 6913 out of 6923 (>99%) barcodes are present in the submitted data.; - Finally ran a correlation b/w the alevin-fry output (located in `res/alevin`) and submitted data. Here are the results:. ![image](https://user-images.githubusercontent.com/12998572/144936078-b4e0ab3e-de1e-4b5d-8000-8c71109f27ae.png). ```; Min. 1st Qu. Median Mean 3rd Qu. Max. ; 0.03701 0.74469 0.88377 0.83131 0.94898 1.00000 ; ```. This demonstrates that alevin performs well with split-seq protocol. Let me know what you think, @jeremymsimon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987334414
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987334414:6618,Performance,perform,performs,6618,"true, write: false }; 2021-12-06 15:37:20 INFO paired : false, ref_count : 226,030, num_chunks : 6,923; 2021-12-06 15:37:21 INFO tg-map contained 60,603 genes mapping to 226,030 transcripts.; 2021-12-06 15:37:21 INFO read 2 file-level tags; 2021-12-06 15:37:21 INFO read 2 read-level tags; 2021-12-06 15:37:21 INFO read 1 alignemnt-level tags; 2021-12-06 15:37:21 INFO File-level tag values FileTags { bclen: 24, umilen: 10 }; ⠓ [00:00:00] [╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠐ [00:00:00] [╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠁ [00:00:00] [╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠴ [00:00:01] [╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠤ [00:00:01] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠁ [00:00:01] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠤ [00:00:01] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠠ [00:00:02] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢░╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠐ [00:00:03] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢░╟╟╟╟╟╟╟╟╟╟╟╟╟] ⠋ [00:00:03] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢░╟╟╟╟╟╟╟╟╟╟] ⠄ [00:00:04] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢░╟╟╟╟╟╟╟] ⠈ [00:00:04] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟╟╟╟] ⠙ [00:00:04] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢▌╟╟] 6551/6923 ; 2021-12-06 15:37:26 WARN ; found connected component with 30679 vertices, resolved into 18 UMIs over 10 genes with trivial resolution.; [00:00:07] [╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢╢] 6923/6923 finished quantifying 6,923 cells.2021-12-06 15:37:28 INFO processed 26,250,078 total read records; ```. - Found that 6913 out of 6923 (>99%) barcodes are present in the submitted data.; - Finally ran a correlation b/w the alevin-fry output (located in `res/alevin`) and submitted data. Here are the results:. ![image](https://user-images.githubusercontent.com/12998572/144936078-b4e0ab3e-de1e-4b5d-8000-8c71109f27ae.png). ```; Min. 1st Qu. Median Mean 3rd Qu. Max. ; 0.03701 0.74469 0.88377 0.83131 0.94898 1.00000 ; ```. This demonstrates that alevin performs well with split-seq protocol. Let me know what you think, @jeremymsimon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987334414
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445:832,Availability,error,errors,832,"@Gaura that definitely seems promising! A few questions:. -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate. -I see you specified `-l A` - can you comment on what the detected/correct library type was here?. -I assume all of this will also work in conjunction with `--expectCells` or `--keepCBFraction` if those parameters were needed? Your ~7k cells detected is very close to the published number _post-filtering_, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here. . -Is there any prospect of dealing with frameshift errors in the barcode detection step? Or is that out of scope?. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445:220,Safety,detect,detection,220,"@Gaura that definitely seems promising! A few questions:. -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate. -I see you specified `-l A` - can you comment on what the detected/correct library type was here?. -I assume all of this will also work in conjunction with `--expectCells` or `--keepCBFraction` if those parameters were needed? Your ~7k cells detected is very close to the published number _post-filtering_, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here. . -Is there any prospect of dealing with frameshift errors in the barcode detection step? Or is that out of scope?. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445:306,Safety,detect,detected,306,"@Gaura that definitely seems promising! A few questions:. -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate. -I see you specified `-l A` - can you comment on what the detected/correct library type was here?. -I assume all of this will also work in conjunction with `--expectCells` or `--keepCBFraction` if those parameters were needed? Your ~7k cells detected is very close to the published number _post-filtering_, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here. . -Is there any prospect of dealing with frameshift errors in the barcode detection step? Or is that out of scope?. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445:490,Safety,detect,detected,490,"@Gaura that definitely seems promising! A few questions:. -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate. -I see you specified `-l A` - can you comment on what the detected/correct library type was here?. -I assume all of this will also work in conjunction with `--expectCells` or `--keepCBFraction` if those parameters were needed? Your ~7k cells detected is very close to the published number _post-filtering_, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here. . -Is there any prospect of dealing with frameshift errors in the barcode detection step? Or is that out of scope?. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445:854,Safety,detect,detection,854,"@Gaura that definitely seems promising! A few questions:. -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate. -I see you specified `-l A` - can you comment on what the detected/correct library type was here?. -I assume all of this will also work in conjunction with `--expectCells` or `--keepCBFraction` if those parameters were needed? Your ~7k cells detected is very close to the published number _post-filtering_, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here. . -Is there any prospect of dealing with frameshift errors in the barcode detection step? Or is that out of scope?. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-987910445
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633:1659,Availability,reliab,reliable,1659,"Hi @jeremymsimon,. A few quick thoughts on this.; ; > -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate.; ; I don't think we've been able to successfully obtain the same concordance with alevin yet (as opposed to alevin-fry). There is more sophisticated _internal_ barcode logic going on there, and we may need to pull @k3yavi in to see what is happening outside of the RAD -> fry pipeline.; ; > -I see you specified -l A - can you comment on what the detected/correct library type was here?; ; Unlike `alevin`, when you run with in `--rad` or `--sketch` mode, the library type isn't really relevant. All mappings are passed through to the rad file. Subsequently, in `alevin-fry` there is a `-d` (direction) flag that is used to filter mappings that don't concord with the expected orientation. I'm not sure what @Gaura used in the run above — the default is to keep reads from either orientation.; ; > -I assume all of this will also work in conjunction with --expectCells or --keepCBFraction if those parameters were needed? Your ~7k cells detected is very close to the published number post-filtering, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here.; ; According to the commands listed, @Gaura used `alevin-fry`'s built-in knee-like filtering. This tries to use a knee on the cumulative read count histogram to determine a good cutoff for ""reliable"" cells versus poor quality cells. Alternatively, one can provide an external permit list with `-u` (unfiltered permit list) to quantify all cells that match any known barcode. This will generally result in *many* more quantified cells, which you will then want to filter post-quantification.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633:520,Deployability,pipeline,pipeline,520,"Hi @jeremymsimon,. A few quick thoughts on this.; ; > -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate.; ; I don't think we've been able to successfully obtain the same concordance with alevin yet (as opposed to alevin-fry). There is more sophisticated _internal_ barcode logic going on there, and we may need to pull @k3yavi in to see what is happening outside of the RAD -> fry pipeline.; ; > -I see you specified -l A - can you comment on what the detected/correct library type was here?; ; Unlike `alevin`, when you run with in `--rad` or `--sketch` mode, the library type isn't really relevant. All mappings are passed through to the rad file. Subsequently, in `alevin-fry` there is a `-d` (direction) flag that is used to filter mappings that don't concord with the expected orientation. I'm not sure what @Gaura used in the run above — the default is to keep reads from either orientation.; ; > -I assume all of this will also work in conjunction with --expectCells or --keepCBFraction if those parameters were needed? Your ~7k cells detected is very close to the published number post-filtering, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here.; ; According to the commands listed, @Gaura used `alevin-fry`'s built-in knee-like filtering. This tries to use a knee on the cumulative read count histogram to determine a good cutoff for ""reliable"" cells versus poor quality cells. Alternatively, one can provide an external permit list with `-u` (unfiltered permit list) to quantify all cells that match any known barcode. This will generally result in *many* more quantified cells, which you will then want to filter post-quantification.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633:216,Safety,detect,detection,216,"Hi @jeremymsimon,. A few quick thoughts on this.; ; > -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate.; ; I don't think we've been able to successfully obtain the same concordance with alevin yet (as opposed to alevin-fry). There is more sophisticated _internal_ barcode logic going on there, and we may need to pull @k3yavi in to see what is happening outside of the RAD -> fry pipeline.; ; > -I see you specified -l A - can you comment on what the detected/correct library type was here?; ; Unlike `alevin`, when you run with in `--rad` or `--sketch` mode, the library type isn't really relevant. All mappings are passed through to the rad file. Subsequently, in `alevin-fry` there is a `-d` (direction) flag that is used to filter mappings that don't concord with the expected orientation. I'm not sure what @Gaura used in the run above — the default is to keep reads from either orientation.; ; > -I assume all of this will also work in conjunction with --expectCells or --keepCBFraction if those parameters were needed? Your ~7k cells detected is very close to the published number post-filtering, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here.; ; According to the commands listed, @Gaura used `alevin-fry`'s built-in knee-like filtering. This tries to use a knee on the cumulative read count histogram to determine a good cutoff for ""reliable"" cells versus poor quality cells. Alternatively, one can provide an external permit list with `-u` (unfiltered permit list) to quantify all cells that match any known barcode. This will generally result in *many* more quantified cells, which you will then want to filter post-quantification.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633:591,Safety,detect,detected,591,"Hi @jeremymsimon,. A few quick thoughts on this.; ; > -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate.; ; I don't think we've been able to successfully obtain the same concordance with alevin yet (as opposed to alevin-fry). There is more sophisticated _internal_ barcode logic going on there, and we may need to pull @k3yavi in to see what is happening outside of the RAD -> fry pipeline.; ; > -I see you specified -l A - can you comment on what the detected/correct library type was here?; ; Unlike `alevin`, when you run with in `--rad` or `--sketch` mode, the library type isn't really relevant. All mappings are passed through to the rad file. Subsequently, in `alevin-fry` there is a `-d` (direction) flag that is used to filter mappings that don't concord with the expected orientation. I'm not sure what @Gaura used in the run above — the default is to keep reads from either orientation.; ; > -I assume all of this will also work in conjunction with --expectCells or --keepCBFraction if those parameters were needed? Your ~7k cells detected is very close to the published number post-filtering, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here.; ; According to the commands listed, @Gaura used `alevin-fry`'s built-in knee-like filtering. This tries to use a knee on the cumulative read count histogram to determine a good cutoff for ""reliable"" cells versus poor quality cells. Alternatively, one can provide an external permit list with `-u` (unfiltered permit list) to quantify all cells that match any known barcode. This will generally result in *many* more quantified cells, which you will then want to filter post-quantification.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633:1181,Safety,detect,detected,1181,"Hi @jeremymsimon,. A few quick thoughts on this.; ; > -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate.; ; I don't think we've been able to successfully obtain the same concordance with alevin yet (as opposed to alevin-fry). There is more sophisticated _internal_ barcode logic going on there, and we may need to pull @k3yavi in to see what is happening outside of the RAD -> fry pipeline.; ; > -I see you specified -l A - can you comment on what the detected/correct library type was here?; ; Unlike `alevin`, when you run with in `--rad` or `--sketch` mode, the library type isn't really relevant. All mappings are passed through to the rad file. Subsequently, in `alevin-fry` there is a `-d` (direction) flag that is used to filter mappings that don't concord with the expected orientation. I'm not sure what @Gaura used in the run above — the default is to keep reads from either orientation.; ; > -I assume all of this will also work in conjunction with --expectCells or --keepCBFraction if those parameters were needed? Your ~7k cells detected is very close to the published number post-filtering, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here.; ; According to the commands listed, @Gaura used `alevin-fry`'s built-in knee-like filtering. This tries to use a knee on the cumulative read count histogram to determine a good cutoff for ""reliable"" cells versus poor quality cells. Alternatively, one can provide an external permit list with `-u` (unfiltered permit list) to quantify all cells that match any known barcode. This will generally result in *many* more quantified cells, which you will then want to filter post-quantification.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633:412,Testability,log,logic,412,"Hi @jeremymsimon,. A few quick thoughts on this.; ; > -Is quantification via alevin-fry (rather than alevin itself) mandatory here? I ask since your run seems successful whereas my full alevin run had a very poor BC detection and mapping rate.; ; I don't think we've been able to successfully obtain the same concordance with alevin yet (as opposed to alevin-fry). There is more sophisticated _internal_ barcode logic going on there, and we may need to pull @k3yavi in to see what is happening outside of the RAD -> fry pipeline.; ; > -I see you specified -l A - can you comment on what the detected/correct library type was here?; ; Unlike `alevin`, when you run with in `--rad` or `--sketch` mode, the library type isn't really relevant. All mappings are passed through to the rad file. Subsequently, in `alevin-fry` there is a `-d` (direction) flag that is used to filter mappings that don't concord with the expected orientation. I'm not sure what @Gaura used in the run above — the default is to keep reads from either orientation.; ; > -I assume all of this will also work in conjunction with --expectCells or --keepCBFraction if those parameters were needed? Your ~7k cells detected is very close to the published number post-filtering, but no similar filtering has been done here yet. My guess is that the proportion of cells that pass these filters will be higher for alevin, but we may still be under-estimating the number of real cells by a little bit here.; ; According to the commands listed, @Gaura used `alevin-fry`'s built-in knee-like filtering. This tries to use a knee on the cumulative read count histogram to determine a good cutoff for ""reliable"" cells versus poor quality cells. Alternatively, one can provide an external permit list with `-u` (unfiltered permit list) to quantify all cells that match any known barcode. This will generally result in *many* more quantified cells, which you will then want to filter post-quantification.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988011633
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988023540:66,Availability,error,errors,66,Okay got it. And is there any prospect of dealing with frameshift errors in the barcode detection step? Or is that out of scope?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988023540
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988023540:88,Safety,detect,detection,88,Okay got it. And is there any prospect of dealing with frameshift errors in the barcode detection step? Or is that out of scope?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988023540
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:141,Availability,error,errors,141,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:1087,Availability,error,errors,1087,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:511,Energy Efficiency,efficient,efficiently,511,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:565,Integrability,protocol,protocols,565,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:326,Safety,avoid,avoid,326,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:287,Testability,log,logic,287,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:608,Usability,simpl,simple,608,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988184912:731,Integrability,inject,inject,731,"[I confirmed with the developer of zUMIs](https://github.com/sdparekh/zUMIs/issues/298) that no frameshift detection/correction is happening in their approach for SPLiT-seq libraries, so the barcode discovery should be fairly consistent with what alevin is already doing (ie with fixed geometry positions). So, likely no need to incorporate this into `splitp` at the moment but if we/others determine that frameshifts are frequent enough and the data can improve in some noticeable way with correcting them, we can revisit later as you suggested. . As for the barcode detection - my usual approach with `alevin` at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject `--expectCells ncells` and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for `alevin-fry` as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988184912
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988184912:107,Safety,detect,detection,107,"[I confirmed with the developer of zUMIs](https://github.com/sdparekh/zUMIs/issues/298) that no frameshift detection/correction is happening in their approach for SPLiT-seq libraries, so the barcode discovery should be fairly consistent with what alevin is already doing (ie with fixed geometry positions). So, likely no need to incorporate this into `splitp` at the moment but if we/others determine that frameshifts are frequent enough and the data can improve in some noticeable way with correcting them, we can revisit later as you suggested. . As for the barcode detection - my usual approach with `alevin` at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject `--expectCells ncells` and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for `alevin-fry` as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988184912
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988184912:568,Safety,detect,detection,568,"[I confirmed with the developer of zUMIs](https://github.com/sdparekh/zUMIs/issues/298) that no frameshift detection/correction is happening in their approach for SPLiT-seq libraries, so the barcode discovery should be fairly consistent with what alevin is already doing (ie with fixed geometry positions). So, likely no need to incorporate this into `splitp` at the moment but if we/others determine that frameshifts are frequent enough and the data can improve in some noticeable way with correcting them, we can revisit later as you suggested. . As for the barcode detection - my usual approach with `alevin` at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject `--expectCells ncells` and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for `alevin-fry` as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988184912
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988184912:731,Security,inject,inject,731,"[I confirmed with the developer of zUMIs](https://github.com/sdparekh/zUMIs/issues/298) that no frameshift detection/correction is happening in their approach for SPLiT-seq libraries, so the barcode discovery should be fairly consistent with what alevin is already doing (ie with fixed geometry positions). So, likely no need to incorporate this into `splitp` at the moment but if we/others determine that frameshifts are frequent enough and the data can improve in some noticeable way with correcting them, we can revisit later as you suggested. . As for the barcode detection - my usual approach with `alevin` at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject `--expectCells ncells` and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for `alevin-fry` as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988184912
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988185505:108,Deployability,pipeline,pipeline,108,"Hey @jeremymsimon! I generated the barcode list using all combinations of 8-bp barcode sequence in [sci-seq-pipeline github repo](https://github.com/yjzhang/split-seq-pipeline/tree/master/split_seq/barcodes) for version v1. Using `alevin-fry generate-permit-list -u barcode_list` instead of `-k` resulted in 142,667 nuclei. After using Seurat filtering on the generated counts, `min.cells = 3, min.features = 200` as done [here](https://satijalab.org/seurat/articles/pbmc3k_tutorial.html), I got 7,442 nuclei. All of the barcodes were present in the submission. The correlation b/w alevin-fry and submission is also good:; ```; Min. 1st Qu. Median Mean 3rd Qu. Max. ; 0.2966 0.7127 0.8689 0.8163 0.9448 0.9963; ```; Addressing your previous question, I would recommend using `salmon alevin` and `alevin-fry` for single cell quantification. It is much faster than using `alevin`. Let me know if you have any questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988185505
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988185505:167,Deployability,pipeline,pipeline,167,"Hey @jeremymsimon! I generated the barcode list using all combinations of 8-bp barcode sequence in [sci-seq-pipeline github repo](https://github.com/yjzhang/split-seq-pipeline/tree/master/split_seq/barcodes) for version v1. Using `alevin-fry generate-permit-list -u barcode_list` instead of `-k` resulted in 142,667 nuclei. After using Seurat filtering on the generated counts, `min.cells = 3, min.features = 200` as done [here](https://satijalab.org/seurat/articles/pbmc3k_tutorial.html), I got 7,442 nuclei. All of the barcodes were present in the submission. The correlation b/w alevin-fry and submission is also good:; ```; Min. 1st Qu. Median Mean 3rd Qu. Max. ; 0.2966 0.7127 0.8689 0.8163 0.9448 0.9963; ```; Addressing your previous question, I would recommend using `salmon alevin` and `alevin-fry` for single cell quantification. It is much faster than using `alevin`. Let me know if you have any questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988185505
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759:707,Deployability,pipeline,pipeline,707,"Hi @jeremymsimon, to answer your last question:. > As for the barcode detection - my usual approach with alevin at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject --expectCells ncells and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for alevin-fry as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?. So one of the nice aspects of the alevin to alevin-fry pipeline is that it's relatively easy to try different filtering approaches since the initial mapping process only has to happen once. In general, the knee detection method is pretty good, and often gives a reasonable cell count. However, this isn't always the case. What we find in the alevin-fry pre-print is that it tends to be slightly more conservative than if you did e.g. unfiltered quantification followed by filtering with something like `DropletUtils` (but usually only slightly). The knee method is basically the iterative knee finding procedure from UMI-tools, with some slight tweaks to the parameters. However, unlike alevin, alevin-fry also supports unfiltered quantification. In this case, you provide an `unfiltered-permitlist`, which is a set of acceptable barcodes (not necessarily all expected to be present), and alevin-fry will correct against this. This will tend to produce a _lot_ of quantified cells, since we quantify any barcode matching 10 or more reads (by default, this value is modifiable on the command line). So, such unfiltered matrices definitely need to be filtered after quantification. However, for protocols with an external permit list, or those where you can reasonably derive a list of potential expected barcodes, it's less stringent and therefore po",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759:231,Integrability,inject,inject,231,"Hi @jeremymsimon, to answer your last question:. > As for the barcode detection - my usual approach with alevin at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject --expectCells ncells and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for alevin-fry as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?. So one of the nice aspects of the alevin to alevin-fry pipeline is that it's relatively easy to try different filtering approaches since the initial mapping process only has to happen once. In general, the knee detection method is pretty good, and often gives a reasonable cell count. However, this isn't always the case. What we find in the alevin-fry pre-print is that it tends to be slightly more conservative than if you did e.g. unfiltered quantification followed by filtering with something like `DropletUtils` (but usually only slightly). The knee method is basically the iterative knee finding procedure from UMI-tools, with some slight tweaks to the parameters. However, unlike alevin, alevin-fry also supports unfiltered quantification. In this case, you provide an `unfiltered-permitlist`, which is a set of acceptable barcodes (not necessarily all expected to be present), and alevin-fry will correct against this. This will tend to produce a _lot_ of quantified cells, since we quantify any barcode matching 10 or more reads (by default, this value is modifiable on the command line). So, such unfiltered matrices definitely need to be filtered after quantification. However, for protocols with an external permit list, or those where you can reasonably derive a list of potential expected barcodes, it's less stringent and therefore po",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759:1845,Integrability,protocol,protocols,1845,"the barcode detection - my usual approach with alevin at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject --expectCells ncells and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for alevin-fry as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?. So one of the nice aspects of the alevin to alevin-fry pipeline is that it's relatively easy to try different filtering approaches since the initial mapping process only has to happen once. In general, the knee detection method is pretty good, and often gives a reasonable cell count. However, this isn't always the case. What we find in the alevin-fry pre-print is that it tends to be slightly more conservative than if you did e.g. unfiltered quantification followed by filtering with something like `DropletUtils` (but usually only slightly). The knee method is basically the iterative knee finding procedure from UMI-tools, with some slight tweaks to the parameters. However, unlike alevin, alevin-fry also supports unfiltered quantification. In this case, you provide an `unfiltered-permitlist`, which is a set of acceptable barcodes (not necessarily all expected to be present), and alevin-fry will correct against this. This will tend to produce a _lot_ of quantified cells, since we quantify any barcode matching 10 or more reads (by default, this value is modifiable on the command line). So, such unfiltered matrices definitely need to be filtered after quantification. However, for protocols with an external permit list, or those where you can reasonably derive a list of potential expected barcodes, it's less stringent and therefore potentially a bit more sensitive than knee-based filtering.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759:70,Safety,detect,detection,70,"Hi @jeremymsimon, to answer your last question:. > As for the barcode detection - my usual approach with alevin at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject --expectCells ncells and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for alevin-fry as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?. So one of the nice aspects of the alevin to alevin-fry pipeline is that it's relatively easy to try different filtering approaches since the initial mapping process only has to happen once. In general, the knee detection method is pretty good, and often gives a reasonable cell count. However, this isn't always the case. What we find in the alevin-fry pre-print is that it tends to be slightly more conservative than if you did e.g. unfiltered quantification followed by filtering with something like `DropletUtils` (but usually only slightly). The knee method is basically the iterative knee finding procedure from UMI-tools, with some slight tweaks to the parameters. However, unlike alevin, alevin-fry also supports unfiltered quantification. In this case, you provide an `unfiltered-permitlist`, which is a set of acceptable barcodes (not necessarily all expected to be present), and alevin-fry will correct against this. This will tend to produce a _lot_ of quantified cells, since we quantify any barcode matching 10 or more reads (by default, this value is modifiable on the command line). So, such unfiltered matrices definitely need to be filtered after quantification. However, for protocols with an external permit list, or those where you can reasonably derive a list of potential expected barcodes, it's less stringent and therefore po",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759:863,Safety,detect,detection,863,"Hi @jeremymsimon, to answer your last question:. > As for the barcode detection - my usual approach with alevin at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject --expectCells ncells and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for alevin-fry as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?. So one of the nice aspects of the alevin to alevin-fry pipeline is that it's relatively easy to try different filtering approaches since the initial mapping process only has to happen once. In general, the knee detection method is pretty good, and often gives a reasonable cell count. However, this isn't always the case. What we find in the alevin-fry pre-print is that it tends to be slightly more conservative than if you did e.g. unfiltered quantification followed by filtering with something like `DropletUtils` (but usually only slightly). The knee method is basically the iterative knee finding procedure from UMI-tools, with some slight tweaks to the parameters. However, unlike alevin, alevin-fry also supports unfiltered quantification. In this case, you provide an `unfiltered-permitlist`, which is a set of acceptable barcodes (not necessarily all expected to be present), and alevin-fry will correct against this. This will tend to produce a _lot_ of quantified cells, since we quantify any barcode matching 10 or more reads (by default, this value is modifiable on the command line). So, such unfiltered matrices definitely need to be filtered after quantification. However, for protocols with an external permit list, or those where you can reasonably derive a list of potential expected barcodes, it's less stringent and therefore po",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759:231,Security,inject,inject,231,"Hi @jeremymsimon, to answer your last question:. > As for the barcode detection - my usual approach with alevin at least is to let it try to estimate a ""real"" cell number, but if it's way off from our experimental expectations, to inject --expectCells ncells and let that serve as a starting point (with subsequent filtering). That has worked reasonably well in the past for me , and seems to be an option for alevin-fry as well. I don't know whether that is poor practice in the long run...it came from a place of seeing far too many weak knee plots early in the droplet scRNA-seq days. Are you generally more trusting of these estimates these days?. So one of the nice aspects of the alevin to alevin-fry pipeline is that it's relatively easy to try different filtering approaches since the initial mapping process only has to happen once. In general, the knee detection method is pretty good, and often gives a reasonable cell count. However, this isn't always the case. What we find in the alevin-fry pre-print is that it tends to be slightly more conservative than if you did e.g. unfiltered quantification followed by filtering with something like `DropletUtils` (but usually only slightly). The knee method is basically the iterative knee finding procedure from UMI-tools, with some slight tweaks to the parameters. However, unlike alevin, alevin-fry also supports unfiltered quantification. In this case, you provide an `unfiltered-permitlist`, which is a set of acceptable barcodes (not necessarily all expected to be present), and alevin-fry will correct against this. This will tend to produce a _lot_ of quantified cells, since we quantify any barcode matching 10 or more reads (by default, this value is modifiable on the command line). So, such unfiltered matrices definitely need to be filtered after quantification. However, for protocols with an external permit list, or those where you can reasonably derive a list of potential expected barcodes, it's less stringent and therefore po",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988967759
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-921864891:40,Availability,ping,ping,40,Thanks for the report @callumparr! I'll ping @gmarcais on this.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-921864891
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1328992610:13,Deployability,update,update,13,Is there any update on this? I am seeing the same warnings and I don't think I totally understand what they mean and whether they can be safely ignored... Thanks!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1328992610
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1328992610:137,Safety,safe,safely,137,Is there any update on this? I am seeing the same warnings and I don't think I totally understand what they mean and whether they can be safely ignored... Thanks!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1328992610
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1419784692:46,Availability,error,error,46,"Does anyone have an answer to this, are those error models 'safe' to ignore?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1419784692
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1419784692:60,Safety,safe,safe,60,"Does anyone have an answer to this, are those error models 'safe' to ignore?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1419784692
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1420973769:11,Availability,ping,pinging,11,Thanks for pinging on this — I'll poke the model implementor (@gmarcais) via other channels and get back to you as soon as I can.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1420973769
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240:218,Availability,error,error,218,"This happens when there was no read in the same length bin during training period (the first so many reads are used for training) as the read under consideration. So Salmon can't assigned a valid log likelihood and an error is reported. The 3 errors are really the same one. The error log likelihood of 3 models are added (based on position of first mismatch/indel, length of clipping at each end of the read). If the length bin is empty for 1 model, it is likely empty for the other 2 models, and 3 warnings are printed when 1 would have been enough. This read gets a error likelihood of 1 and is mostly ignored by Salmon after that. Such reads should be rare by definition (unless the input BAM was not randomized, or there is bug) and this warning should be rare as well. So unless you see many such warnings, you can safely ignore it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240:243,Availability,error,errors,243,"This happens when there was no read in the same length bin during training period (the first so many reads are used for training) as the read under consideration. So Salmon can't assigned a valid log likelihood and an error is reported. The 3 errors are really the same one. The error log likelihood of 3 models are added (based on position of first mismatch/indel, length of clipping at each end of the read). If the length bin is empty for 1 model, it is likely empty for the other 2 models, and 3 warnings are printed when 1 would have been enough. This read gets a error likelihood of 1 and is mostly ignored by Salmon after that. Such reads should be rare by definition (unless the input BAM was not randomized, or there is bug) and this warning should be rare as well. So unless you see many such warnings, you can safely ignore it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240:279,Availability,error,error,279,"This happens when there was no read in the same length bin during training period (the first so many reads are used for training) as the read under consideration. So Salmon can't assigned a valid log likelihood and an error is reported. The 3 errors are really the same one. The error log likelihood of 3 models are added (based on position of first mismatch/indel, length of clipping at each end of the read). If the length bin is empty for 1 model, it is likely empty for the other 2 models, and 3 warnings are printed when 1 would have been enough. This read gets a error likelihood of 1 and is mostly ignored by Salmon after that. Such reads should be rare by definition (unless the input BAM was not randomized, or there is bug) and this warning should be rare as well. So unless you see many such warnings, you can safely ignore it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240:569,Availability,error,error,569,"This happens when there was no read in the same length bin during training period (the first so many reads are used for training) as the read under consideration. So Salmon can't assigned a valid log likelihood and an error is reported. The 3 errors are really the same one. The error log likelihood of 3 models are added (based on position of first mismatch/indel, length of clipping at each end of the read). If the length bin is empty for 1 model, it is likely empty for the other 2 models, and 3 warnings are printed when 1 would have been enough. This read gets a error likelihood of 1 and is mostly ignored by Salmon after that. Such reads should be rare by definition (unless the input BAM was not randomized, or there is bug) and this warning should be rare as well. So unless you see many such warnings, you can safely ignore it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240:821,Safety,safe,safely,821,"This happens when there was no read in the same length bin during training period (the first so many reads are used for training) as the read under consideration. So Salmon can't assigned a valid log likelihood and an error is reported. The 3 errors are really the same one. The error log likelihood of 3 models are added (based on position of first mismatch/indel, length of clipping at each end of the read). If the length bin is empty for 1 model, it is likely empty for the other 2 models, and 3 warnings are printed when 1 would have been enough. This read gets a error likelihood of 1 and is mostly ignored by Salmon after that. Such reads should be rare by definition (unless the input BAM was not randomized, or there is bug) and this warning should be rare as well. So unless you see many such warnings, you can safely ignore it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240:196,Testability,log,log,196,"This happens when there was no read in the same length bin during training period (the first so many reads are used for training) as the read under consideration. So Salmon can't assigned a valid log likelihood and an error is reported. The 3 errors are really the same one. The error log likelihood of 3 models are added (based on position of first mismatch/indel, length of clipping at each end of the read). If the length bin is empty for 1 model, it is likely empty for the other 2 models, and 3 warnings are printed when 1 would have been enough. This read gets a error likelihood of 1 and is mostly ignored by Salmon after that. Such reads should be rare by definition (unless the input BAM was not randomized, or there is bug) and this warning should be rare as well. So unless you see many such warnings, you can safely ignore it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240:285,Testability,log,log,285,"This happens when there was no read in the same length bin during training period (the first so many reads are used for training) as the read under consideration. So Salmon can't assigned a valid log likelihood and an error is reported. The 3 errors are really the same one. The error log likelihood of 3 models are added (based on position of first mismatch/indel, length of clipping at each end of the read). If the length bin is empty for 1 model, it is likely empty for the other 2 models, and 3 warnings are printed when 1 would have been enough. This read gets a error likelihood of 1 and is mostly ignored by Salmon after that. Such reads should be rare by definition (unless the input BAM was not randomized, or there is bug) and this warning should be rare as well. So unless you see many such warnings, you can safely ignore it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1421264240
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1422213426:258,Availability,error,error,258,"Thank @rob-p and @gmarcais, that clarifies it. There's only a handful of reads that seem to fall in this category.; Just a suggestion would it better to have just a one line summary on the amount of reads that are categorised as such and then make quite the error messages?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1422213426
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1422213426:264,Integrability,message,messages,264,"Thank @rob-p and @gmarcais, that clarifies it. There's only a handful of reads that seem to fall in this category.; Just a suggestion would it better to have just a one line summary on the amount of reads that are categorised as such and then make quite the error messages?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1422213426
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939:116,Availability,error,error,116,"I am unsure what qualifies as many but on an input of 20M reads and an unsorted BAM file, I get around 600 lines of error in the log file, some of the reads have many alignments so the errors are redundant to some extent. The no. of unique reads' alignments that cause an error is perhaps 300.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939:185,Availability,error,errors,185,"I am unsure what qualifies as many but on an input of 20M reads and an unsorted BAM file, I get around 600 lines of error in the log file, some of the reads have many alignments so the errors are redundant to some extent. The no. of unique reads' alignments that cause an error is perhaps 300.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939:196,Availability,redundant,redundant,196,"I am unsure what qualifies as many but on an input of 20M reads and an unsorted BAM file, I get around 600 lines of error in the log file, some of the reads have many alignments so the errors are redundant to some extent. The no. of unique reads' alignments that cause an error is perhaps 300.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939:272,Availability,error,error,272,"I am unsure what qualifies as many but on an input of 20M reads and an unsorted BAM file, I get around 600 lines of error in the log file, some of the reads have many alignments so the errors are redundant to some extent. The no. of unique reads' alignments that cause an error is perhaps 300.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939:196,Safety,redund,redundant,196,"I am unsure what qualifies as many but on an input of 20M reads and an unsorted BAM file, I get around 600 lines of error in the log file, some of the reads have many alignments so the errors are redundant to some extent. The no. of unique reads' alignments that cause an error is perhaps 300.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939:129,Testability,log,log,129,"I am unsure what qualifies as many but on an input of 20M reads and an unsorted BAM file, I get around 600 lines of error in the log file, some of the reads have many alignments so the errors are redundant to some extent. The no. of unique reads' alignments that cause an error is perhaps 300.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1427049939
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1928552794:148,Availability,error,errors,148,"hi @callumparr, I'm running into something similar, just wondering if you have a better idea now on your last comment (as in what qualifies as many errors)? thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1928552794
https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1981452288:150,Availability,error,errors,150,"> hi @callumparr, I'm running into something similar, just wondering if you have a better idea now on your last comment (as in what qualifies as many errors)? thanks. Hi sorry, I did not get this resolved. In the end, I kind of just ignored the warnings. But in any case for those particular datasets, we also tried quantification through IsoQuant and Bambu. You may try those to compare to Salmon. . I am also intending to try oarfish, https://combine-lab.github.io/oarfish/ which was designed for long-read RNA seq datasets. From the looks of it you follow a similar strategy, first align to the transcriptome and then feed the BAM to oarfish to sort out multimapping reads.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/701#issuecomment-1981452288
https://github.com/COMBINE-lab/salmon/issues/702#issuecomment-916883967:31,Performance,perform,performs,31,The single-cell mode of salmon performs CB correction and knee based thresholding before mapping and quantification. That's why some of the reads from CB with very low frequency would never map.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/702#issuecomment-916883967
https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920921204:248,Deployability,release,release,248,"That's amazing @Gaura. This feature has been frequently requested by multiple users but I never got a chance to work on this, thanks a lot for the PR. Give me some time to go over the PR and if everything looks Ok, we can merge it in into the next release cycle. May I ask previous version of inDrop had an issue with variable length barcodes, did they solved that issue in v2 ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920921204
https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920921204:318,Modifiability,variab,variable,318,"That's amazing @Gaura. This feature has been frequently requested by multiple users but I never got a chance to work on this, thanks a lot for the PR. Give me some time to go over the PR and if everything looks Ok, we can merge it in into the next release cycle. May I ask previous version of inDrop had an issue with variable length barcodes, did they solved that issue in v2 ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920921204
https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920995572:69,Testability,test,tested,69,Thanks @k3yavi! I'm not aware of the issues with previous version. I tested it with the [test data](https://github.com/indrops/indrops/tree/master/test/seq_runs/run_v2_single_file) and the data I mentioned earlier and it worked fine. Let me know how the review goes and if I have missed something.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920995572
https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920995572:89,Testability,test,test,89,Thanks @k3yavi! I'm not aware of the issues with previous version. I tested it with the [test data](https://github.com/indrops/indrops/tree/master/test/seq_runs/run_v2_single_file) and the data I mentioned earlier and it worked fine. Let me know how the review goes and if I have missed something.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920995572
https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920995572:147,Testability,test,test,147,Thanks @k3yavi! I'm not aware of the issues with previous version. I tested it with the [test data](https://github.com/indrops/indrops/tree/master/test/seq_runs/run_v2_single_file) and the data I mentioned earlier and it worked fine. Let me know how the review goes and if I have missed something.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/703#issuecomment-920995572
https://github.com/COMBINE-lab/salmon/issues/706#issuecomment-1183300898:608,Deployability,pipeline,pipeline,608,"Hi @rahulnutron,. Currently, the recommendation is to move to [`alevin-fry`](https://github.com/COMBINE-lab/alevin-fry). If you don't have a good annotation (i.e. transcripts as well as introns) then you can index just the transcriptome as normal. The big differences are that, rather than letting alevin do the quantification, you would ask `salmon alevin` to just a do the mapping phase and produce a rad file. This can be done with (assuming chromium v3 chemistry): . ```; salmon alevin -i <index> --chromiumV3 -l A -1 <reads1> -2 <reads2> -p 16 -o <outdir> --sketch; ```. note that with the `alevin-fry` pipeline, there is no need to provide the t2g map at this phase (it's used later). Further, the `--sketch` flag tells alevin just to map the reads and to prepare the RAD file for subsequent quantification with `alevin-fry`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/706#issuecomment-1183300898
https://github.com/COMBINE-lab/salmon/issues/708#issuecomment-923452593:12,Energy Efficiency,reduce,reduceGCMemory,12,Using the --reduceGCMemory flag fixed it.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/708#issuecomment-923452593
https://github.com/COMBINE-lab/salmon/issues/709#issuecomment-928588815:4,Availability,error,error,4,The error code comes from the underlying `kseq` parser. It means some read had an underlying quality string that was truncated (shorter than the corresponding read). See [this](https://www.biostars.org/p/342945/#405026) biostars post.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/709#issuecomment-928588815
https://github.com/COMBINE-lab/salmon/issues/710#issuecomment-1165611828:379,Availability,avail,availability,379,"You can try installing from bioconda, or compiling from source. In the cases above, I presume this has to be resolved via the module system on the cluster / server where this is being run. In the pre-compiled binaries, salmon attempts to link against a specific, old version of libm to maximize compatibility among the operating systems on which it will run. However, given it's availability on bioconda, Dockerhub, and compiled via source, the pre-compiled executable for linux is probably the least preferred way to obtain and run salmon on linux.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/710#issuecomment-1165611828
https://github.com/COMBINE-lab/salmon/issues/710#issuecomment-1165611828:12,Deployability,install,installing,12,"You can try installing from bioconda, or compiling from source. In the cases above, I presume this has to be resolved via the module system on the cluster / server where this is being run. In the pre-compiled binaries, salmon attempts to link against a specific, old version of libm to maximize compatibility among the operating systems on which it will run. However, given it's availability on bioconda, Dockerhub, and compiled via source, the pre-compiled executable for linux is probably the least preferred way to obtain and run salmon on linux.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/710#issuecomment-1165611828
https://github.com/COMBINE-lab/salmon/issues/710#issuecomment-1165640786:31,Availability,echo,echo,31,"Thank you!. For a fix, I did: `echo $LD_LIBRARY_PATH`. ; this shows `salmonpath/1.3.0/lib:originalpath`; Doing ` export LD_LIBRARY_PATH=originalpath` fixed this in my environment",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/710#issuecomment-1165640786
https://github.com/COMBINE-lab/salmon/issues/710#issuecomment-1165663420:15,Deployability,update,update,15,"Thanks for the update, and I'm glad to hear that you were able to find a local fix!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/710#issuecomment-1165663420
https://github.com/COMBINE-lab/salmon/issues/713#issuecomment-941839528:678,Modifiability,variab,variable,678,"Hi @oligomyeggo,. Thank you for the **incredibly** detailed report :). The problem is the following (derived from your `B13_MeOH_cells_Jurkat_Cas9_EGR1_1_simulated.out.err.txt` log above):. ```; ### [ index ] => { /beevol/home/winklerc/projects/scifi_pipeline/scifi/ref/idx/complete_ref_lens.bin }; ```. So it looks like what your rule is passing to the mapping command is not the path to the index directory, but the path to this specific file, `complete_ref_lens.bin` **within** the index directory. The argument passed to the `-i` flag of `salmon alevin` must be the directory where all of the index files live. I think you just need to have the directory itself stored in a variable upon index creation, and then you can pass it to the mapping rule. Let me know if this helps!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/713#issuecomment-941839528
https://github.com/COMBINE-lab/salmon/issues/713#issuecomment-941839528:177,Testability,log,log,177,"Hi @oligomyeggo,. Thank you for the **incredibly** detailed report :). The problem is the following (derived from your `B13_MeOH_cells_Jurkat_Cas9_EGR1_1_simulated.out.err.txt` log above):. ```; ### [ index ] => { /beevol/home/winklerc/projects/scifi_pipeline/scifi/ref/idx/complete_ref_lens.bin }; ```. So it looks like what your rule is passing to the mapping command is not the path to the index directory, but the path to this specific file, `complete_ref_lens.bin` **within** the index directory. The argument passed to the `-i` flag of `salmon alevin` must be the directory where all of the index files live. I think you just need to have the directory itself stored in a variable upon index creation, and then you can pass it to the mapping rule. Let me know if this helps!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/713#issuecomment-941839528
https://github.com/COMBINE-lab/salmon/issues/713#issuecomment-941866580:177,Deployability,pipeline,pipeline,177,"Ah, I see what went wrong! When I initially looked at the `B13_MeOH_cells_Jurkat_Cas9_EGR1_1_simulated.out.err` I saw all the index files listed and thought that that meant the pipeline found all of them, but I see that it was just grabbing that first file and trying to use it as the entire index directory. I have adjusted my snakemake rule per your suggestion and it is now working. Thank you so much @rob-p! I appreciate it, and sorry for the oversight on my part!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/713#issuecomment-941866580
https://github.com/COMBINE-lab/salmon/issues/714#issuecomment-948175672:43,Availability,avail,available,43,"Hi @BenSamy2020,. Judging from the options available to the indexing command, it seems you're running quite an old version of salmon (before the `-d` flag was even introduced). Specifically, it seems to be a version prior to 1.0 (when the new index was introduced). I'd recommend updating to the latest version of salmon, then you should be able to properly provide the decoys file. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/714#issuecomment-948175672
https://github.com/COMBINE-lab/salmon/issues/715#issuecomment-952521555:195,Safety,detect,detected,195,"Just following up on this, I figured out that the dataset was the problem. I had subset it from a larger scifi-RNA-seq dataset and was too aggressive. I had just over 1000 aligned reads per cell detected by alevin. I ran two 10X datasets using this procedure and both worked as expected. The 10X mixing experiment found about 50:50 mixture of human and mouse cells with a few collisions, and a 10X human PBMC dataset returned no mouse cells at all. . I'll go ahead and close this now. For future reference, using a concatenated reference along with the `--resolution trivial` flag allows alevin to handle species mixture data just fine.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/715#issuecomment-952521555
https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234:422,Availability,down,download,422,"Hi @paulitikka,. This isn't something that I have control over (in terms of the official package repositories), and we don't have the resources to independently package salmon for the huge variety of different linux distributions. However, there are already ways to install the salmon binary directly across linux and OSX. Specifically, there is a pre-compiled binary [here](https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz) --- tagged on the release page. Additionally, you can install salmon directly on linux or osx using bioconda — as outlined [here](https://anaconda.org/bioconda/salmon). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234
https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234:266,Deployability,install,install,266,"Hi @paulitikka,. This isn't something that I have control over (in terms of the official package repositories), and we don't have the resources to independently package salmon for the huge variety of different linux distributions. However, there are already ways to install the salmon binary directly across linux and OSX. Specifically, there is a pre-compiled binary [here](https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz) --- tagged on the release page. Additionally, you can install salmon directly on linux or osx using bioconda — as outlined [here](https://anaconda.org/bioconda/salmon). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234
https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234:413,Deployability,release,releases,413,"Hi @paulitikka,. This isn't something that I have control over (in terms of the official package repositories), and we don't have the resources to independently package salmon for the huge variety of different linux distributions. However, there are already ways to install the salmon binary directly across linux and OSX. Specifically, there is a pre-compiled binary [here](https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz) --- tagged on the release page. Additionally, you can install salmon directly on linux or osx using bioconda — as outlined [here](https://anaconda.org/bioconda/salmon). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234
https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234:490,Deployability,release,release,490,"Hi @paulitikka,. This isn't something that I have control over (in terms of the official package repositories), and we don't have the resources to independently package salmon for the huge variety of different linux distributions. However, there are already ways to install the salmon binary directly across linux and OSX. Specifically, there is a pre-compiled binary [here](https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz) --- tagged on the release page. Additionally, you can install salmon directly on linux or osx using bioconda — as outlined [here](https://anaconda.org/bioconda/salmon). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234
https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234:526,Deployability,install,install,526,"Hi @paulitikka,. This isn't something that I have control over (in terms of the official package repositories), and we don't have the resources to independently package salmon for the huge variety of different linux distributions. However, there are already ways to install the salmon binary directly across linux and OSX. Specifically, there is a pre-compiled binary [here](https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz) --- tagged on the release page. Additionally, you can install salmon directly on linux or osx using bioconda — as outlined [here](https://anaconda.org/bioconda/salmon). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/716#issuecomment-962084234
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307:60,Energy Efficiency,allocate,allocate,60,"Hi @lubios,. This suggests that the machine was not able to allocate enough memory to perform the requested operation. I would try the following things in order to see if they fix the issue. First, try quantifying without the decoy-aware index. This doesn't provide the benefits of the decoy sequence, but it will ensure that this is, in fact, the problem you are having. If that works, try building the decoy-aware index with the `--sparse` parameter. This will build the sparse index instead of the dense index, which is a bit smaller and may therefore fit in RAM on the machine where you are doing quantification. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307:86,Performance,perform,perform,86,"Hi @lubios,. This suggests that the machine was not able to allocate enough memory to perform the requested operation. I would try the following things in order to see if they fix the issue. First, try quantifying without the decoy-aware index. This doesn't provide the benefits of the decoy sequence, but it will ensure that this is, in fact, the problem you are having. If that works, try building the decoy-aware index with the `--sparse` parameter. This will build the sparse index instead of the dense index, which is a bit smaller and may therefore fit in RAM on the machine where you are doing quantification. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631:52,Availability,avail,availability,52,"Ok, thank you very much.; The problem I had was RAM availability. I enlarged it for 48 and it works.; However, to quantify I had another problem.; I use this command line and I increase to 56 RAM. srun ./salmon-1.5.2_linux_x86_64/bin/salmon quant -i salmon_index \; -l A \; -1 ERR3537668_1.fastq.gz \; -2 ERR3537668_2.fastq.gz \; -o transcripts_DecoyQuant \; --validateMappings \; --numBootstraps 100 \; --gcBias \; --seqBias\; -p 12. And I got this error message:; [2021-11-08 14:35:28.348] [jointLog] [info] Finished Bootstrapping; ERROR: Could not create the directory [""transcripts_quant""]. Please check; that. But actually, it was created.; I really don't understand the message error. Best wishes,; Luciana. On Fri, Nov 5, 2021 at 5:56 PM Rob Patro ***@***.***> wrote:. > Hi @lubios <https://github.com/lubios>,; >; > This suggests that the machine was not able to allocate enough memory to; > perform the requested operation. I would try the following things in order; > to see if they fix the issue. First, try quantifying without the; > decoy-aware index. This doesn't provide the benefits of the decoy sequence,; > but it will ensure that this is, in fact, the problem you are having. If; > that works, try building the decoy-aware index with the --sparse; > parameter. This will build the sparse index instead of the dense index,; > which is a bit smaller and may therefore fit in RAM on the machine where; > you are doing quantification.; >; > Best,; > Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADRT5CUYGXBSY3UOX24RTYDUKQLETANCNFSM5HOIMSQQ>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631:450,Availability,error,error,450,"Ok, thank you very much.; The problem I had was RAM availability. I enlarged it for 48 and it works.; However, to quantify I had another problem.; I use this command line and I increase to 56 RAM. srun ./salmon-1.5.2_linux_x86_64/bin/salmon quant -i salmon_index \; -l A \; -1 ERR3537668_1.fastq.gz \; -2 ERR3537668_2.fastq.gz \; -o transcripts_DecoyQuant \; --validateMappings \; --numBootstraps 100 \; --gcBias \; --seqBias\; -p 12. And I got this error message:; [2021-11-08 14:35:28.348] [jointLog] [info] Finished Bootstrapping; ERROR: Could not create the directory [""transcripts_quant""]. Please check; that. But actually, it was created.; I really don't understand the message error. Best wishes,; Luciana. On Fri, Nov 5, 2021 at 5:56 PM Rob Patro ***@***.***> wrote:. > Hi @lubios <https://github.com/lubios>,; >; > This suggests that the machine was not able to allocate enough memory to; > perform the requested operation. I would try the following things in order; > to see if they fix the issue. First, try quantifying without the; > decoy-aware index. This doesn't provide the benefits of the decoy sequence,; > but it will ensure that this is, in fact, the problem you are having. If; > that works, try building the decoy-aware index with the --sparse; > parameter. This will build the sparse index instead of the dense index,; > which is a bit smaller and may therefore fit in RAM on the machine where; > you are doing quantification.; >; > Best,; > Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADRT5CUYGXBSY3UOX24RTYDUKQLETANCNFSM5HOIMSQQ>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631:534,Availability,ERROR,ERROR,534,"Ok, thank you very much.; The problem I had was RAM availability. I enlarged it for 48 and it works.; However, to quantify I had another problem.; I use this command line and I increase to 56 RAM. srun ./salmon-1.5.2_linux_x86_64/bin/salmon quant -i salmon_index \; -l A \; -1 ERR3537668_1.fastq.gz \; -2 ERR3537668_2.fastq.gz \; -o transcripts_DecoyQuant \; --validateMappings \; --numBootstraps 100 \; --gcBias \; --seqBias\; -p 12. And I got this error message:; [2021-11-08 14:35:28.348] [jointLog] [info] Finished Bootstrapping; ERROR: Could not create the directory [""transcripts_quant""]. Please check; that. But actually, it was created.; I really don't understand the message error. Best wishes,; Luciana. On Fri, Nov 5, 2021 at 5:56 PM Rob Patro ***@***.***> wrote:. > Hi @lubios <https://github.com/lubios>,; >; > This suggests that the machine was not able to allocate enough memory to; > perform the requested operation. I would try the following things in order; > to see if they fix the issue. First, try quantifying without the; > decoy-aware index. This doesn't provide the benefits of the decoy sequence,; > but it will ensure that this is, in fact, the problem you are having. If; > that works, try building the decoy-aware index with the --sparse; > parameter. This will build the sparse index instead of the dense index,; > which is a bit smaller and may therefore fit in RAM on the machine where; > you are doing quantification.; >; > Best,; > Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADRT5CUYGXBSY3UOX24RTYDUKQLETANCNFSM5HOIMSQQ>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631:684,Availability,error,error,684,"Ok, thank you very much.; The problem I had was RAM availability. I enlarged it for 48 and it works.; However, to quantify I had another problem.; I use this command line and I increase to 56 RAM. srun ./salmon-1.5.2_linux_x86_64/bin/salmon quant -i salmon_index \; -l A \; -1 ERR3537668_1.fastq.gz \; -2 ERR3537668_2.fastq.gz \; -o transcripts_DecoyQuant \; --validateMappings \; --numBootstraps 100 \; --gcBias \; --seqBias\; -p 12. And I got this error message:; [2021-11-08 14:35:28.348] [jointLog] [info] Finished Bootstrapping; ERROR: Could not create the directory [""transcripts_quant""]. Please check; that. But actually, it was created.; I really don't understand the message error. Best wishes,; Luciana. On Fri, Nov 5, 2021 at 5:56 PM Rob Patro ***@***.***> wrote:. > Hi @lubios <https://github.com/lubios>,; >; > This suggests that the machine was not able to allocate enough memory to; > perform the requested operation. I would try the following things in order; > to see if they fix the issue. First, try quantifying without the; > decoy-aware index. This doesn't provide the benefits of the decoy sequence,; > but it will ensure that this is, in fact, the problem you are having. If; > that works, try building the decoy-aware index with the --sparse; > parameter. This will build the sparse index instead of the dense index,; > which is a bit smaller and may therefore fit in RAM on the machine where; > you are doing quantification.; >; > Best,; > Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADRT5CUYGXBSY3UOX24RTYDUKQLETANCNFSM5HOIMSQQ>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631:871,Energy Efficiency,allocate,allocate,871,"Ok, thank you very much.; The problem I had was RAM availability. I enlarged it for 48 and it works.; However, to quantify I had another problem.; I use this command line and I increase to 56 RAM. srun ./salmon-1.5.2_linux_x86_64/bin/salmon quant -i salmon_index \; -l A \; -1 ERR3537668_1.fastq.gz \; -2 ERR3537668_2.fastq.gz \; -o transcripts_DecoyQuant \; --validateMappings \; --numBootstraps 100 \; --gcBias \; --seqBias\; -p 12. And I got this error message:; [2021-11-08 14:35:28.348] [jointLog] [info] Finished Bootstrapping; ERROR: Could not create the directory [""transcripts_quant""]. Please check; that. But actually, it was created.; I really don't understand the message error. Best wishes,; Luciana. On Fri, Nov 5, 2021 at 5:56 PM Rob Patro ***@***.***> wrote:. > Hi @lubios <https://github.com/lubios>,; >; > This suggests that the machine was not able to allocate enough memory to; > perform the requested operation. I would try the following things in order; > to see if they fix the issue. First, try quantifying without the; > decoy-aware index. This doesn't provide the benefits of the decoy sequence,; > but it will ensure that this is, in fact, the problem you are having. If; > that works, try building the decoy-aware index with the --sparse; > parameter. This will build the sparse index instead of the dense index,; > which is a bit smaller and may therefore fit in RAM on the machine where; > you are doing quantification.; >; > Best,; > Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADRT5CUYGXBSY3UOX24RTYDUKQLETANCNFSM5HOIMSQQ>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631:456,Integrability,message,message,456,"Ok, thank you very much.; The problem I had was RAM availability. I enlarged it for 48 and it works.; However, to quantify I had another problem.; I use this command line and I increase to 56 RAM. srun ./salmon-1.5.2_linux_x86_64/bin/salmon quant -i salmon_index \; -l A \; -1 ERR3537668_1.fastq.gz \; -2 ERR3537668_2.fastq.gz \; -o transcripts_DecoyQuant \; --validateMappings \; --numBootstraps 100 \; --gcBias \; --seqBias\; -p 12. And I got this error message:; [2021-11-08 14:35:28.348] [jointLog] [info] Finished Bootstrapping; ERROR: Could not create the directory [""transcripts_quant""]. Please check; that. But actually, it was created.; I really don't understand the message error. Best wishes,; Luciana. On Fri, Nov 5, 2021 at 5:56 PM Rob Patro ***@***.***> wrote:. > Hi @lubios <https://github.com/lubios>,; >; > This suggests that the machine was not able to allocate enough memory to; > perform the requested operation. I would try the following things in order; > to see if they fix the issue. First, try quantifying without the; > decoy-aware index. This doesn't provide the benefits of the decoy sequence,; > but it will ensure that this is, in fact, the problem you are having. If; > that works, try building the decoy-aware index with the --sparse; > parameter. This will build the sparse index instead of the dense index,; > which is a bit smaller and may therefore fit in RAM on the machine where; > you are doing quantification.; >; > Best,; > Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADRT5CUYGXBSY3UOX24RTYDUKQLETANCNFSM5HOIMSQQ>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631:676,Integrability,message,message,676,"Ok, thank you very much.; The problem I had was RAM availability. I enlarged it for 48 and it works.; However, to quantify I had another problem.; I use this command line and I increase to 56 RAM. srun ./salmon-1.5.2_linux_x86_64/bin/salmon quant -i salmon_index \; -l A \; -1 ERR3537668_1.fastq.gz \; -2 ERR3537668_2.fastq.gz \; -o transcripts_DecoyQuant \; --validateMappings \; --numBootstraps 100 \; --gcBias \; --seqBias\; -p 12. And I got this error message:; [2021-11-08 14:35:28.348] [jointLog] [info] Finished Bootstrapping; ERROR: Could not create the directory [""transcripts_quant""]. Please check; that. But actually, it was created.; I really don't understand the message error. Best wishes,; Luciana. On Fri, Nov 5, 2021 at 5:56 PM Rob Patro ***@***.***> wrote:. > Hi @lubios <https://github.com/lubios>,; >; > This suggests that the machine was not able to allocate enough memory to; > perform the requested operation. I would try the following things in order; > to see if they fix the issue. First, try quantifying without the; > decoy-aware index. This doesn't provide the benefits of the decoy sequence,; > but it will ensure that this is, in fact, the problem you are having. If; > that works, try building the decoy-aware index with the --sparse; > parameter. This will build the sparse index instead of the dense index,; > which is a bit smaller and may therefore fit in RAM on the machine where; > you are doing quantification.; >; > Best,; > Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADRT5CUYGXBSY3UOX24RTYDUKQLETANCNFSM5HOIMSQQ>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631:900,Performance,perform,perform,900,"Ok, thank you very much.; The problem I had was RAM availability. I enlarged it for 48 and it works.; However, to quantify I had another problem.; I use this command line and I increase to 56 RAM. srun ./salmon-1.5.2_linux_x86_64/bin/salmon quant -i salmon_index \; -l A \; -1 ERR3537668_1.fastq.gz \; -2 ERR3537668_2.fastq.gz \; -o transcripts_DecoyQuant \; --validateMappings \; --numBootstraps 100 \; --gcBias \; --seqBias\; -p 12. And I got this error message:; [2021-11-08 14:35:28.348] [jointLog] [info] Finished Bootstrapping; ERROR: Could not create the directory [""transcripts_quant""]. Please check; that. But actually, it was created.; I really don't understand the message error. Best wishes,; Luciana. On Fri, Nov 5, 2021 at 5:56 PM Rob Patro ***@***.***> wrote:. > Hi @lubios <https://github.com/lubios>,; >; > This suggests that the machine was not able to allocate enough memory to; > perform the requested operation. I would try the following things in order; > to see if they fix the issue. First, try quantifying without the; > decoy-aware index. This doesn't provide the benefits of the decoy sequence,; > but it will ensure that this is, in fact, the problem you are having. If; > that works, try building the decoy-aware index with the --sparse; > parameter. This will build the sparse index instead of the dense index,; > which is a bit smaller and may therefore fit in RAM on the machine where; > you are doing quantification.; >; > Best,; > Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADRT5CUYGXBSY3UOX24RTYDUKQLETANCNFSM5HOIMSQQ>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631:361,Security,validat,validateMappings,361,"Ok, thank you very much.; The problem I had was RAM availability. I enlarged it for 48 and it works.; However, to quantify I had another problem.; I use this command line and I increase to 56 RAM. srun ./salmon-1.5.2_linux_x86_64/bin/salmon quant -i salmon_index \; -l A \; -1 ERR3537668_1.fastq.gz \; -2 ERR3537668_2.fastq.gz \; -o transcripts_DecoyQuant \; --validateMappings \; --numBootstraps 100 \; --gcBias \; --seqBias\; -p 12. And I got this error message:; [2021-11-08 14:35:28.348] [jointLog] [info] Finished Bootstrapping; ERROR: Could not create the directory [""transcripts_quant""]. Please check; that. But actually, it was created.; I really don't understand the message error. Best wishes,; Luciana. On Fri, Nov 5, 2021 at 5:56 PM Rob Patro ***@***.***> wrote:. > Hi @lubios <https://github.com/lubios>,; >; > This suggests that the machine was not able to allocate enough memory to; > perform the requested operation. I would try the following things in order; > to see if they fix the issue. First, try quantifying without the; > decoy-aware index. This doesn't provide the benefits of the decoy sequence,; > but it will ensure that this is, in fact, the problem you are having. If; > that works, try building the decoy-aware index with the --sparse; > parameter. This will build the sparse index instead of the dense index,; > which is a bit smaller and may therefore fit in RAM on the machine where; > you are doing quantification.; >; > Best,; > Rob; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-962058307>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADRT5CUYGXBSY3UOX24RTYDUKQLETANCNFSM5HOIMSQQ>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-963995631
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-964546340:216,Availability,error,error,216,"Hi @lubios,. I'm glad that you were able to address the first issue. The thing that's strange about the second is that somehow the output path you are providing in the command doesn't match the directory name in the error message. Specifically, your command has the output directory as `transcripts_DecoyQuant`, but the error reports not being able to create the directory `transcripts_quant`. Are the command and error here properly paired?. The only situations under which one might expect this issue to occur is if either (1) your user doesn't have sufficient permission to create the location where the output is to be written or (2) the disk on which the output is to be written has insufficient space. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-964546340
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-964546340:320,Availability,error,error,320,"Hi @lubios,. I'm glad that you were able to address the first issue. The thing that's strange about the second is that somehow the output path you are providing in the command doesn't match the directory name in the error message. Specifically, your command has the output directory as `transcripts_DecoyQuant`, but the error reports not being able to create the directory `transcripts_quant`. Are the command and error here properly paired?. The only situations under which one might expect this issue to occur is if either (1) your user doesn't have sufficient permission to create the location where the output is to be written or (2) the disk on which the output is to be written has insufficient space. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-964546340
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-964546340:414,Availability,error,error,414,"Hi @lubios,. I'm glad that you were able to address the first issue. The thing that's strange about the second is that somehow the output path you are providing in the command doesn't match the directory name in the error message. Specifically, your command has the output directory as `transcripts_DecoyQuant`, but the error reports not being able to create the directory `transcripts_quant`. Are the command and error here properly paired?. The only situations under which one might expect this issue to occur is if either (1) your user doesn't have sufficient permission to create the location where the output is to be written or (2) the disk on which the output is to be written has insufficient space. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-964546340
https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-964546340:222,Integrability,message,message,222,"Hi @lubios,. I'm glad that you were able to address the first issue. The thing that's strange about the second is that somehow the output path you are providing in the command doesn't match the directory name in the error message. Specifically, your command has the output directory as `transcripts_DecoyQuant`, but the error reports not being able to create the directory `transcripts_quant`. Are the command and error here properly paired?. The only situations under which one might expect this issue to occur is if either (1) your user doesn't have sufficient permission to create the location where the output is to be written or (2) the disk on which the output is to be written has insufficient space. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/717#issuecomment-964546340
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-984930979:102,Availability,ping,ping,102,Hi @vbontempi96 — I don't know how bioconda is actually handling building for the new Mac archs. I'll ping @dpryan79 who would know what's currently being done and what the plan is.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-984930979
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-984952328:179,Modifiability,config,config,179,"@vbontempi96,. Thanks to @genomax, working from an M1, it seems likely that the issue is your order of Conda channels. Given an empty channel list, try the following:. ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda create -n salmon salmon; ```. This should allow you to get the latest salmon (currently 1.6.0) on an M1 from bioconda. Please feel free to re-open if this doesn't work for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-984952328
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-984952328:217,Modifiability,config,config,217,"@vbontempi96,. Thanks to @genomax, working from an M1, it seems likely that the issue is your order of Conda channels. Given an empty channel list, try the following:. ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda create -n salmon salmon; ```. This should allow you to get the latest salmon (currently 1.6.0) on an M1 from bioconda. Please feel free to re-open if this doesn't work for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-984952328
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-984952328:255,Modifiability,config,config,255,"@vbontempi96,. Thanks to @genomax, working from an M1, it seems likely that the issue is your order of Conda channels. Given an empty channel list, try the following:. ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda create -n salmon salmon; ```. This should allow you to get the latest salmon (currently 1.6.0) on an M1 from bioconda. Please feel free to re-open if this doesn't work for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-984952328
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855:207,Availability,error,error,207,"I'm having a similar problem, also using an M1 mac. I've tried your solution @rob-p, as well as updating conda, creating a new environment specifically for salmon, it still doesn't work. I get the following error:. ```; $ conda install -c bioconda salmon 3s Py salmon; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - salmon. Current channels:. - https://conda.anaconda.org/bioconda/osx-arm64; - https://conda.anaconda.org/bioconda/noarch; - https://conda.anaconda.org/conda-forge/osx-arm64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/osx-arm64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/osx-arm64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855:604,Availability,avail,available,604,"I'm having a similar problem, also using an M1 mac. I've tried your solution @rob-p, as well as updating conda, creating a new environment specifically for salmon, it still doesn't work. I get the following error:. ```; $ conda install -c bioconda salmon 3s Py salmon; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - salmon. Current channels:. - https://conda.anaconda.org/bioconda/osx-arm64; - https://conda.anaconda.org/bioconda/noarch; - https://conda.anaconda.org/conda-forge/osx-arm64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/osx-arm64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/osx-arm64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855:228,Deployability,install,install,228,"I'm having a similar problem, also using an M1 mac. I've tried your solution @rob-p, as well as updating conda, creating a new environment specifically for salmon, it still doesn't work. I get the following error:. ```; $ conda install -c bioconda salmon 3s Py salmon; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - salmon. Current channels:. - https://conda.anaconda.org/bioconda/osx-arm64; - https://conda.anaconda.org/bioconda/noarch; - https://conda.anaconda.org/conda-forge/osx-arm64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/osx-arm64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/osx-arm64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855:397,Modifiability,flexible,flexible,397,"I'm having a similar problem, also using an M1 mac. I've tried your solution @rob-p, as well as updating conda, creating a new environment specifically for salmon, it still doesn't work. I get the following error:. ```; $ conda install -c bioconda salmon 3s Py salmon; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - salmon. Current channels:. - https://conda.anaconda.org/bioconda/osx-arm64; - https://conda.anaconda.org/bioconda/noarch; - https://conda.anaconda.org/conda-forge/osx-arm64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/osx-arm64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/osx-arm64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855:534,Modifiability,flexible,flexible,534,"I'm having a similar problem, also using an M1 mac. I've tried your solution @rob-p, as well as updating conda, creating a new environment specifically for salmon, it still doesn't work. I get the following error:. ```; $ conda install -c bioconda salmon 3s Py salmon; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - salmon. Current channels:. - https://conda.anaconda.org/bioconda/osx-arm64; - https://conda.anaconda.org/bioconda/noarch; - https://conda.anaconda.org/conda-forge/osx-arm64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/osx-arm64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/osx-arm64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137109855
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:156,Availability,error,error,156,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:11,Deployability,install,installing,11,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:78,Deployability,install,install,78,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:53,Integrability,message,message,53,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:64,Usability,simpl,simple,64,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137172685:61,Availability,error,error,61,Hmm strange! But `conda install salmon` still gives the same error for me.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137172685
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137172685:24,Deployability,install,install,24,Hmm strange! But `conda install salmon` still gives the same error for me.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137172685
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671:63,Availability,error,error,63,"> Hmm strange! But `conda install salmon` still gives the same error for me. Hi @charlotte-west,. Can you please try the following?. ```; CONDA_SUBDIR=osx-64 conda create -n rosetta # create a new environment; conda activate rosetta; conda env config vars set CONDA_SUBDIR=osx-64 # subsequent commands use intel packages; conda install salmon; ```. These instructions came from (here)[https://github.com/Haydnspass/miniforge#rosetta-on-mac-with-apple-silicon-hardware] and have worked for me on my M1 laptop. My understanding is that conda / bioconda are still working on having worker nodes that can pre-compile native M1 executables. So, until that work is complete, it may be necessary to run certain native packages in a Rosetta environment. Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671:26,Deployability,install,install,26,"> Hmm strange! But `conda install salmon` still gives the same error for me. Hi @charlotte-west,. Can you please try the following?. ```; CONDA_SUBDIR=osx-64 conda create -n rosetta # create a new environment; conda activate rosetta; conda env config vars set CONDA_SUBDIR=osx-64 # subsequent commands use intel packages; conda install salmon; ```. These instructions came from (here)[https://github.com/Haydnspass/miniforge#rosetta-on-mac-with-apple-silicon-hardware] and have worked for me on my M1 laptop. My understanding is that conda / bioconda are still working on having worker nodes that can pre-compile native M1 executables. So, until that work is complete, it may be necessary to run certain native packages in a Rosetta environment. Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671:328,Deployability,install,install,328,"> Hmm strange! But `conda install salmon` still gives the same error for me. Hi @charlotte-west,. Can you please try the following?. ```; CONDA_SUBDIR=osx-64 conda create -n rosetta # create a new environment; conda activate rosetta; conda env config vars set CONDA_SUBDIR=osx-64 # subsequent commands use intel packages; conda install salmon; ```. These instructions came from (here)[https://github.com/Haydnspass/miniforge#rosetta-on-mac-with-apple-silicon-hardware] and have worked for me on my M1 laptop. My understanding is that conda / bioconda are still working on having worker nodes that can pre-compile native M1 executables. So, until that work is complete, it may be necessary to run certain native packages in a Rosetta environment. Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671:244,Modifiability,config,config,244,"> Hmm strange! But `conda install salmon` still gives the same error for me. Hi @charlotte-west,. Can you please try the following?. ```; CONDA_SUBDIR=osx-64 conda create -n rosetta # create a new environment; conda activate rosetta; conda env config vars set CONDA_SUBDIR=osx-64 # subsequent commands use intel packages; conda install salmon; ```. These instructions came from (here)[https://github.com/Haydnspass/miniforge#rosetta-on-mac-with-apple-silicon-hardware] and have worked for me on my M1 laptop. My understanding is that conda / bioconda are still working on having worker nodes that can pre-compile native M1 executables. So, until that work is complete, it may be necessary to run certain native packages in a Rosetta environment. Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137266420:46,Deployability,update,update,46,Awesome! Thanks for looping back around. I'll update the discussion Q&A on GitHub about this and update that when bioconda has native M1 support.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137266420
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137266420:97,Deployability,update,update,97,Awesome! Thanks for looping back around. I'll update the discussion Q&A on GitHub about this and update that when bioconda has native M1 support.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137266420
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:271,Availability,error,error,271,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:326,Availability,error,errors,326,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:49,Deployability,install,install,49,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:509,Modifiability,config,config,509,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:363,Usability,simpl,simple,363,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:246,Availability,ERROR,ERROR,246,"When I do with proxy I got : . ```; Last login: Thu Jun 30 15:10:26 on ttys001; Benjamin@u932-ulm-2-57030119-6834 ~ % all_proxy= url:port conda install salmon; Collecting package metadata (current_repodata.json): failed. # >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<. Traceback (most recent call last):; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/exceptions.py"", line 1082, in __call__; return func(*args, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main.py"", line 87, in _main; exit_code = do_call(args, p); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/conda_argparse.py"", line 84, in do_call; return getattr(module, func_name)(args, parser); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main_install.py"", line 20, in execute; install(args, parser, 'install'); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/install.py"", line 260, in install; unlink_link_transaction = solver.solve_for_transaction(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 152, in solve_for_transaction; unlink_precs, link_precs = self.solve_for_diff(update_modifier, deps_modifier,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 195, in solve_for_diff; final_precs = self.solve_final_state(update_modifier, deps_modifier, prune, ignore_pinned,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 300, in solve_final_state; ssc = self._collect_all_metadata(ssc); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/common/io.py"", line 88, in decorated; return f(*args, **kwds); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Ca",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:6216,Availability,error,error,6216,"rge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkgs; /Users/Benjamin/.conda/pkgs; envs directories : /usr/local/Caskroom/miniforge/base/envs; /Users/Benjamin/.conda/envs; platform : osx-arm64; user-agent : conda/4.12.0 requests/2.27.1 CPython/3.9.13 Darwin/21.5.0 OSX/12.4; UID:GID : 501:20; netrc file : None; offline mode : False. An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core maintainers to improve; future releases of conda.; Would you like conda to send this report to the core maintainers?. [y/N]: y; Upload did not complete. Thank you for helping to improve conda.; Opt-in to always sending reports (and not see this message again); by running. $ conda config --set report_errors true; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:144,Deployability,install,install,144,"When I do with proxy I got : . ```; Last login: Thu Jun 30 15:10:26 on ttys001; Benjamin@u932-ulm-2-57030119-6834 ~ % all_proxy= url:port conda install salmon; Collecting package metadata (current_repodata.json): failed. # >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<. Traceback (most recent call last):; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/exceptions.py"", line 1082, in __call__; return func(*args, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main.py"", line 87, in _main; exit_code = do_call(args, p); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/conda_argparse.py"", line 84, in do_call; return getattr(module, func_name)(args, parser); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main_install.py"", line 20, in execute; install(args, parser, 'install'); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/install.py"", line 260, in install; unlink_link_transaction = solver.solve_for_transaction(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 152, in solve_for_transaction; unlink_precs, link_precs = self.solve_for_diff(update_modifier, deps_modifier,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 195, in solve_for_diff; final_precs = self.solve_final_state(update_modifier, deps_modifier, prune, ignore_pinned,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 300, in solve_final_state; ssc = self._collect_all_metadata(ssc); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/common/io.py"", line 88, in decorated; return f(*args, **kwds); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Ca",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:889,Deployability,install,install,889,"When I do with proxy I got : . ```; Last login: Thu Jun 30 15:10:26 on ttys001; Benjamin@u932-ulm-2-57030119-6834 ~ % all_proxy= url:port conda install salmon; Collecting package metadata (current_repodata.json): failed. # >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<. Traceback (most recent call last):; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/exceptions.py"", line 1082, in __call__; return func(*args, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main.py"", line 87, in _main; exit_code = do_call(args, p); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/conda_argparse.py"", line 84, in do_call; return getattr(module, func_name)(args, parser); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main_install.py"", line 20, in execute; install(args, parser, 'install'); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/install.py"", line 260, in install; unlink_link_transaction = solver.solve_for_transaction(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 152, in solve_for_transaction; unlink_precs, link_precs = self.solve_for_diff(update_modifier, deps_modifier,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 195, in solve_for_diff; final_precs = self.solve_final_state(update_modifier, deps_modifier, prune, ignore_pinned,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 300, in solve_final_state; ssc = self._collect_all_metadata(ssc); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/common/io.py"", line 88, in decorated; return f(*args, **kwds); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Ca",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:912,Deployability,install,install,912,"When I do with proxy I got : . ```; Last login: Thu Jun 30 15:10:26 on ttys001; Benjamin@u932-ulm-2-57030119-6834 ~ % all_proxy= url:port conda install salmon; Collecting package metadata (current_repodata.json): failed. # >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<. Traceback (most recent call last):; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/exceptions.py"", line 1082, in __call__; return func(*args, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main.py"", line 87, in _main; exit_code = do_call(args, p); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/conda_argparse.py"", line 84, in do_call; return getattr(module, func_name)(args, parser); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main_install.py"", line 20, in execute; install(args, parser, 'install'); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/install.py"", line 260, in install; unlink_link_transaction = solver.solve_for_transaction(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 152, in solve_for_transaction; unlink_precs, link_precs = self.solve_for_diff(update_modifier, deps_modifier,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 195, in solve_for_diff; final_precs = self.solve_final_state(update_modifier, deps_modifier, prune, ignore_pinned,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 300, in solve_final_state; ssc = self._collect_all_metadata(ssc); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/common/io.py"", line 88, in decorated; return f(*args, **kwds); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Ca",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:1002,Deployability,install,install,1002,"When I do with proxy I got : . ```; Last login: Thu Jun 30 15:10:26 on ttys001; Benjamin@u932-ulm-2-57030119-6834 ~ % all_proxy= url:port conda install salmon; Collecting package metadata (current_repodata.json): failed. # >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<. Traceback (most recent call last):; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/exceptions.py"", line 1082, in __call__; return func(*args, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main.py"", line 87, in _main; exit_code = do_call(args, p); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/conda_argparse.py"", line 84, in do_call; return getattr(module, func_name)(args, parser); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main_install.py"", line 20, in execute; install(args, parser, 'install'); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/install.py"", line 260, in install; unlink_link_transaction = solver.solve_for_transaction(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 152, in solve_for_transaction; unlink_precs, link_precs = self.solve_for_diff(update_modifier, deps_modifier,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 195, in solve_for_diff; final_precs = self.solve_final_state(update_modifier, deps_modifier, prune, ignore_pinned,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 300, in solve_final_state; ssc = self._collect_all_metadata(ssc); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/common/io.py"", line 88, in decorated; return f(*args, **kwds); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Ca",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:1028,Deployability,install,install,1028," login: Thu Jun 30 15:10:26 on ttys001; Benjamin@u932-ulm-2-57030119-6834 ~ % all_proxy= url:port conda install salmon; Collecting package metadata (current_repodata.json): failed. # >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<. Traceback (most recent call last):; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/exceptions.py"", line 1082, in __call__; return func(*args, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main.py"", line 87, in _main; exit_code = do_call(args, p); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/conda_argparse.py"", line 84, in do_call; return getattr(module, func_name)(args, parser); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main_install.py"", line 20, in execute; install(args, parser, 'install'); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/install.py"", line 260, in install; unlink_link_transaction = solver.solve_for_transaction(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 152, in solve_for_transaction; unlink_precs, link_precs = self.solve_for_diff(update_modifier, deps_modifier,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 195, in solve_for_diff; final_precs = self.solve_final_state(update_modifier, deps_modifier, prune, ignore_pinned,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 300, in solve_final_state; ssc = self._collect_all_metadata(ssc); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/common/io.py"", line 88, in decorated; return f(*args, **kwds); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/sit",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4923,Deployability,install,install,4923,"get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkg",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:5482,Deployability,install,installed,5482,"se/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkgs; /Users/Benjamin/.conda/pkgs; envs directories : /usr/local/Caskroom/miniforge/base/envs; /Users/Benjamin/.conda/envs; platform : osx-arm64; user-agent : conda/4.12.0 requests/2.27.1 CPython/3.9.13 Darwin/21.5.0 OSX/12.4; UID:GID : 501:20; netrc file : None; offline mode : False. An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core maintainers to improve; future releases of conda.; Would you like conda to send this report to the core maintainers?. [y/N]: y; Upload did not complete. Thank you ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:6351,Deployability,release,releases,6351,"rge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkgs; /Users/Benjamin/.conda/pkgs; envs directories : /usr/local/Caskroom/miniforge/base/envs; /Users/Benjamin/.conda/envs; platform : osx-arm64; user-agent : conda/4.12.0 requests/2.27.1 CPython/3.9.13 Darwin/21.5.0 OSX/12.4; UID:GID : 501:20; netrc file : None; offline mode : False. An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core maintainers to improve; future releases of conda.; Would you like conda to send this report to the core maintainers?. [y/N]: y; Upload did not complete. Thank you for helping to improve conda.; Opt-in to always sending reports (and not see this message again); by running. $ conda config --set report_errors true; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4411,Energy Efficiency,adapt,adapter,4411,"r_data.py"", line 145, in query; self.load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Cas",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4522,Energy Efficiency,adapt,adapters,4522,"/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4683,Energy Efficiency,adapt,adapters,4683,"ubdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4411,Integrability,adapter,adapter,4411,"r_data.py"", line 145, in query; self.load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Cas",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4522,Integrability,adapter,adapters,4522,"/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4683,Integrability,adapter,adapters,4683,"ubdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:6565,Integrability,message,message,6565,"rge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkgs; /Users/Benjamin/.conda/pkgs; envs directories : /usr/local/Caskroom/miniforge/base/envs; /Users/Benjamin/.conda/envs; platform : osx-arm64; user-agent : conda/4.12.0 requests/2.27.1 CPython/3.9.13 Darwin/21.5.0 OSX/12.4; UID:GID : 501:20; netrc file : None; offline mode : False. An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core maintainers to improve; future releases of conda.; Would you like conda to send this report to the core maintainers?. [y/N]: y; Upload did not complete. Thank you for helping to improve conda.; Opt-in to always sending reports (and not see this message again); by running. $ conda config --set report_errors true; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4411,Modifiability,adapt,adapter,4411,"r_data.py"", line 145, in query; self.load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Cas",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4522,Modifiability,adapt,adapters,4522,"/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4683,Modifiability,adapt,adapters,4683,"ubdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:4952,Modifiability,variab,variables,4952,"quest('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkgs; /Users/Benjamin/.conda/pkgs; envs directories : /usr/local/Caskroom/miniforge/base/envs; /Users/Benjamin/.conda/envs; platform : osx-arm64; user-agent : conda/4.12.0 requests/2.27.1 CPython/3.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:5322,Modifiability,config,config,5322,"/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkgs; /Users/Benjamin/.conda/pkgs; envs directories : /usr/local/Caskroom/miniforge/base/envs; /Users/Benjamin/.conda/envs; platform : osx-arm64; user-agent : conda/4.12.0 requests/2.27.1 CPython/3.9.13 Darwin/21.5.0 OSX/12.4; UID:GID : 501:20; netrc file : None; offline mode : False. An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core m",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:5372,Modifiability,config,config,5372,"line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = self.get_connection(request.url, proxies); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkgs; /Users/Benjamin/.conda/pkgs; envs directories : /usr/local/Caskroom/miniforge/base/envs; /Users/Benjamin/.conda/envs; platform : osx-arm64; user-agent : conda/4.12.0 requests/2.27.1 CPython/3.9.13 Darwin/21.5.0 OSX/12.4; UID:GID : 501:20; netrc file : None; offline mode : False. An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core maintainers to improve; future releases of conda.; Would you like co",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:6601,Modifiability,config,config,6601,"rge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkgs; /Users/Benjamin/.conda/pkgs; envs directories : /usr/local/Caskroom/miniforge/base/envs; /Users/Benjamin/.conda/envs; platform : osx-arm64; user-agent : conda/4.12.0 requests/2.27.1 CPython/3.9.13 Darwin/21.5.0 OSX/12.4; UID:GID : 501:20; netrc file : None; offline mode : False. An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core maintainers to improve; future releases of conda.; Would you like conda to send this report to the core maintainers?. [y/N]: y; Upload did not complete. Thank you for helping to improve conda.; Opt-in to always sending reports (and not see this message again); by running. $ conda config --set report_errors true; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:2603,Performance,concurren,concurrent,2603,"in solve_final_state; ssc = self._collect_all_metadata(ssc); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/common/io.py"", line 88, in decorated; return f(*args, **kwds); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 1058, in _prepare; reduced_index = get_reduced_index(self.prefix, self.channels,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/index.py"", line 288, in get_reduced_index; new_records = SubdirData.query_all(spec, channels=channels, subdirs=subdirs,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 140, in query_all; result = tuple(concat(executor.map(subdir_query, channel_urls))); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 609, in result_iterator; yield fs.pop().result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 446, in result; return self.__get_result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 391, in __get_result; raise self._exception; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 132, in <lambda>; subdir_query = lambda url: tuple(SubdirData(Channel(url), repodata_fn=repodata_fn).query(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 145, in query; self.load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Cask",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:2743,Performance,concurren,concurrent,2743,"on/io.py"", line 88, in decorated; return f(*args, **kwds); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 1058, in _prepare; reduced_index = get_reduced_index(self.prefix, self.channels,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/index.py"", line 288, in get_reduced_index; new_records = SubdirData.query_all(spec, channels=channels, subdirs=subdirs,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 140, in query_all; result = tuple(concat(executor.map(subdir_query, channel_urls))); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 609, in result_iterator; yield fs.pop().result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 446, in result; return self.__get_result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 391, in __get_result; raise self._exception; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 132, in <lambda>; subdir_query = lambda url: tuple(SubdirData(Channel(url), repodata_fn=repodata_fn).query(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 145, in query; self.load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_req",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:2877,Performance,concurren,concurrent,2877,"core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 1058, in _prepare; reduced_index = get_reduced_index(self.prefix, self.channels,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/index.py"", line 288, in get_reduced_index; new_records = SubdirData.query_all(spec, channels=channels, subdirs=subdirs,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 140, in query_all; result = tuple(concat(executor.map(subdir_query, channel_urls))); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 609, in result_iterator; yield fs.pop().result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 446, in result; return self.__get_result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 391, in __get_result; raise self._exception; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 132, in <lambda>; subdir_query = lambda url: tuple(SubdirData(Channel(url), repodata_fn=repodata_fn).query(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 145, in query; self.load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_re",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:3012,Performance,concurren,concurrent,3012,"ib/python3.9/site-packages/conda/core/solve.py"", line 1058, in _prepare; reduced_index = get_reduced_index(self.prefix, self.channels,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/index.py"", line 288, in get_reduced_index; new_records = SubdirData.query_all(spec, channels=channels, subdirs=subdirs,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 140, in query_all; result = tuple(concat(executor.map(subdir_query, channel_urls))); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 609, in result_iterator; yield fs.pop().result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 446, in result; return self.__get_result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 391, in __get_result; raise self._exception; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 132, in <lambda>; subdir_query = lambda url: tuple(SubdirData(Channel(url), repodata_fn=repodata_fn).query(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 145, in query; self.load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforg",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:3438,Performance,load,load,3438,"in query_all; result = tuple(concat(executor.map(subdir_query, channel_urls))); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 609, in result_iterator; yield fs.pop().result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 446, in result; return self.__get_result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 391, in __get_result; raise self._exception; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 132, in <lambda>; subdir_query = lambda url: tuple(SubdirData(Channel(url), repodata_fn=repodata_fn).query(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 145, in query; self.load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:3556,Performance,load,load,3556,"cal/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 609, in result_iterator; yield fs.pop().result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 446, in result; return self.__get_result(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py"", line 391, in __get_result; raise self._exception; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 132, in <lambda>; subdir_query = lambda url: tuple(SubdirData(Channel(url), repodata_fn=repodata_fn).query(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 145, in query; self.load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 210, in load; _internal_state = self._load(); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 375, in _load; raw_repodata_str = fetch_repodata_remote_request(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/subdir_data.py"", line 701, in fetch_repodata_remote_request; resp = session.get(join_url(url, filename), headers=headers, proxies=session.proxies,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 542, in get; return self.request('GET', url, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request; resp = self.send(prep, **send_kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send; r = adapter.send(request, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/adapters.py"", line 413, in send; conn = ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:5873,Performance,cache,cache,5873,"rge/base/lib/python3.9/site-packages/requests/adapters.py"", line 308, in get_connection; raise InvalidProxyURL(""Please check proxy URL. It is malformed""; requests.exceptions.InvalidProxyURL: Please check proxy URL. It is malformed and could be missing the host. `$ /usr/local/bin/conda install salmon`. environment variables:; CIO_TEST=<not set>; CONDA_ROOT=/usr/local/Caskroom/miniforge/base; CURL_CA_BUNDLE=<not set>; INFOPATH=/opt/homebrew/share/info:; MANPATH=/opt/homebrew/share/man::; PATH=/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr; /sbin:/sbin; REQUESTS_CA_BUNDLE=<not set>; SSL_CERT_FILE=<not set>; all_proxy=<set>. active environment : None; user config file : /Users/Benjamin/.condarc; populated config files : /usr/local/Caskroom/miniforge/base/.condarc; conda version : 4.12.0; conda-build version : not installed; python version : 3.9.13.final.0; virtual packages : __osx=12.4=0; __unix=0=0; __archspec=1=arm64; base environment : /usr/local/Caskroom/miniforge/base (writable); conda av data dir : /usr/local/Caskroom/miniforge/base/etc/conda; conda av metadata url : None; channel URLs : https://conda.anaconda.org/conda-forge/osx-arm64; https://conda.anaconda.org/conda-forge/noarch; package cache : /usr/local/Caskroom/miniforge/base/pkgs; /Users/Benjamin/.conda/pkgs; envs directories : /usr/local/Caskroom/miniforge/base/envs; /Users/Benjamin/.conda/envs; platform : osx-arm64; user-agent : conda/4.12.0 requests/2.27.1 CPython/3.9.13 Darwin/21.5.0 OSX/12.4; UID:GID : 501:20; netrc file : None; offline mode : False. An unexpected error has occurred. Conda has prepared the above report. If submitted, this report will be used by core maintainers to improve; future releases of conda.; Would you like conda to send this report to the core maintainers?. [y/N]: y; Upload did not complete. Thank you for helping to improve conda.; Opt-in to always sending reports (and not see this message again); by running. $ conda config --set report_errors true; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515:41,Testability,log,login,41,"When I do with proxy I got : . ```; Last login: Thu Jun 30 15:10:26 on ttys001; Benjamin@u932-ulm-2-57030119-6834 ~ % all_proxy= url:port conda install salmon; Collecting package metadata (current_repodata.json): failed. # >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<. Traceback (most recent call last):; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/exceptions.py"", line 1082, in __call__; return func(*args, **kwargs); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main.py"", line 87, in _main; exit_code = do_call(args, p); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/conda_argparse.py"", line 84, in do_call; return getattr(module, func_name)(args, parser); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/main_install.py"", line 20, in execute; install(args, parser, 'install'); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/cli/install.py"", line 260, in install; unlink_link_transaction = solver.solve_for_transaction(; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 152, in solve_for_transaction; unlink_precs, link_precs = self.solve_for_diff(update_modifier, deps_modifier,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 195, in solve_for_diff; final_precs = self.solve_final_state(update_modifier, deps_modifier, prune, ignore_pinned,; File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 300, in solve_final_state; ssc = self._collect_all_metadata(ssc); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/common/io.py"", line 88, in decorated; return f(*args, **kwds); File ""/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/conda/core/solve.py"", line 463, in _collect_all_metadata; index, r = self._prepare(prepared_specs); File ""/usr/local/Ca",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171208515
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171217271:131,Availability,error,error,131,"One thing is that bioconda is not in your channels list, only conda forge. Though I don't think that's the ultimate source of this error.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171217271
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:17,Availability,error,error,17,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:597,Availability,error,error,597,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:652,Availability,error,errors,652,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:375,Deployability,install,install,375,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:150,Modifiability,config,config,150,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:188,Modifiability,config,config,188,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:226,Modifiability,config,config,226,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:835,Modifiability,config,config,835,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:67,Testability,log,login,67,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:689,Usability,simpl,simple,689,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1384299365:65,Availability,error,error,65,"> > Hmm strange! But `conda install salmon` still gives the same error for me.; > ; > Hi @charlotte-west,; > ; > Can you please try the following?; > ; > ```; > CONDA_SUBDIR=osx-64 conda create -n rosetta # create a new environment; > conda activate rosetta; > conda env config vars set CONDA_SUBDIR=osx-64 # subsequent commands use intel packages; > conda install salmon; > ```; > ; > These instructions came from (here)[https://github.com/Haydnspass/miniforge#rosetta-on-mac-with-apple-silicon-hardware] and have worked for me on my M1 laptop. My understanding is that conda / bioconda are still working on having worker nodes that can pre-compile native M1 executables. So, until that work is complete, it may be necessary to run certain native packages in a Rosetta environment. Let me know if this works for you.; > ; > Best, Rob. it worked for me but i had to remove the comments starting with the `#`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1384299365
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1384299365:28,Deployability,install,install,28,"> > Hmm strange! But `conda install salmon` still gives the same error for me.; > ; > Hi @charlotte-west,; > ; > Can you please try the following?; > ; > ```; > CONDA_SUBDIR=osx-64 conda create -n rosetta # create a new environment; > conda activate rosetta; > conda env config vars set CONDA_SUBDIR=osx-64 # subsequent commands use intel packages; > conda install salmon; > ```; > ; > These instructions came from (here)[https://github.com/Haydnspass/miniforge#rosetta-on-mac-with-apple-silicon-hardware] and have worked for me on my M1 laptop. My understanding is that conda / bioconda are still working on having worker nodes that can pre-compile native M1 executables. So, until that work is complete, it may be necessary to run certain native packages in a Rosetta environment. Let me know if this works for you.; > ; > Best, Rob. it worked for me but i had to remove the comments starting with the `#`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1384299365
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1384299365:357,Deployability,install,install,357,"> > Hmm strange! But `conda install salmon` still gives the same error for me.; > ; > Hi @charlotte-west,; > ; > Can you please try the following?; > ; > ```; > CONDA_SUBDIR=osx-64 conda create -n rosetta # create a new environment; > conda activate rosetta; > conda env config vars set CONDA_SUBDIR=osx-64 # subsequent commands use intel packages; > conda install salmon; > ```; > ; > These instructions came from (here)[https://github.com/Haydnspass/miniforge#rosetta-on-mac-with-apple-silicon-hardware] and have worked for me on my M1 laptop. My understanding is that conda / bioconda are still working on having worker nodes that can pre-compile native M1 executables. So, until that work is complete, it may be necessary to run certain native packages in a Rosetta environment. Let me know if this works for you.; > ; > Best, Rob. it worked for me but i had to remove the comments starting with the `#`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1384299365
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1384299365:271,Modifiability,config,config,271,"> > Hmm strange! But `conda install salmon` still gives the same error for me.; > ; > Hi @charlotte-west,; > ; > Can you please try the following?; > ; > ```; > CONDA_SUBDIR=osx-64 conda create -n rosetta # create a new environment; > conda activate rosetta; > conda env config vars set CONDA_SUBDIR=osx-64 # subsequent commands use intel packages; > conda install salmon; > ```; > ; > These instructions came from (here)[https://github.com/Haydnspass/miniforge#rosetta-on-mac-with-apple-silicon-hardware] and have worked for me on my M1 laptop. My understanding is that conda / bioconda are still working on having worker nodes that can pre-compile native M1 executables. So, until that work is complete, it may be necessary to run certain native packages in a Rosetta environment. Let me know if this works for you.; > ; > Best, Rob. it worked for me but i had to remove the comments starting with the `#`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1384299365
https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524:54,Availability,ping,pinging,54,"Hi @mishaprochazka and @jdidion,. Thanks @jdidion for pinging me on this. Somehow, Gmail has decided that all GitHub notifications (except those that explicitly tag me, but somehow I missed this one) should go to SPAM. So, I've been missing some of the newer issues here. The short answer is that the documentation needs to be updated. When salmon was originally published, we made use of [RapMap](https://github.com/COMBINE-lab/RapMap) as the underlying mapper, which performed quasi-mapping against an index that consisted of a suffix array and a hash over k-mers pointing to prefixes in the suffix array (similar to the strategy used by STAR, but using much longer k-mers to improve lookup speed). We referred to this index as the quasi-index. As the software evolved and we continued to improve the mapping methodology, we eventually transitioned over to an index based on [our pufferfish data structure](https://github.com/COMBINE-lab/pufferfish). In addition to the new data structure, this coincided with our move over to selective-alignment as the mapping algorithm, and all of this happened at the 1.0.0 release (this is why, for example, indices built before 1.0.0 are not compatible with salmon > 1.0.0; a topic on which there have been a few GitHub issues). However, given the fact that the documentation and software are linked only through manual human intervention (we haven't leveled up to e.g. having salmon be a [literate program](https://en.wikipedia.org/wiki/Literate_programming) yet), these two sometimes get out of sync. This is an instance of that. We have maintained the functionality of the `--writeMappings` feature, and in fact, even augmented it. However, we have not replaced the antiquated `quasi-index` terminology in the documentation. The TLDR is that you can use `--writeMappings` with the index you built with the `salmon index` command, and it should work fine. If you are mapping against an index without decoy sequences, then the output format will be basically ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524
https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524:327,Deployability,update,updated,327,"Hi @mishaprochazka and @jdidion,. Thanks @jdidion for pinging me on this. Somehow, Gmail has decided that all GitHub notifications (except those that explicitly tag me, but somehow I missed this one) should go to SPAM. So, I've been missing some of the newer issues here. The short answer is that the documentation needs to be updated. When salmon was originally published, we made use of [RapMap](https://github.com/COMBINE-lab/RapMap) as the underlying mapper, which performed quasi-mapping against an index that consisted of a suffix array and a hash over k-mers pointing to prefixes in the suffix array (similar to the strategy used by STAR, but using much longer k-mers to improve lookup speed). We referred to this index as the quasi-index. As the software evolved and we continued to improve the mapping methodology, we eventually transitioned over to an index based on [our pufferfish data structure](https://github.com/COMBINE-lab/pufferfish). In addition to the new data structure, this coincided with our move over to selective-alignment as the mapping algorithm, and all of this happened at the 1.0.0 release (this is why, for example, indices built before 1.0.0 are not compatible with salmon > 1.0.0; a topic on which there have been a few GitHub issues). However, given the fact that the documentation and software are linked only through manual human intervention (we haven't leveled up to e.g. having salmon be a [literate program](https://en.wikipedia.org/wiki/Literate_programming) yet), these two sometimes get out of sync. This is an instance of that. We have maintained the functionality of the `--writeMappings` feature, and in fact, even augmented it. However, we have not replaced the antiquated `quasi-index` terminology in the documentation. The TLDR is that you can use `--writeMappings` with the index you built with the `salmon index` command, and it should work fine. If you are mapping against an index without decoy sequences, then the output format will be basically ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524
https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524:1113,Deployability,release,release,1113,"ose that explicitly tag me, but somehow I missed this one) should go to SPAM. So, I've been missing some of the newer issues here. The short answer is that the documentation needs to be updated. When salmon was originally published, we made use of [RapMap](https://github.com/COMBINE-lab/RapMap) as the underlying mapper, which performed quasi-mapping against an index that consisted of a suffix array and a hash over k-mers pointing to prefixes in the suffix array (similar to the strategy used by STAR, but using much longer k-mers to improve lookup speed). We referred to this index as the quasi-index. As the software evolved and we continued to improve the mapping methodology, we eventually transitioned over to an index based on [our pufferfish data structure](https://github.com/COMBINE-lab/pufferfish). In addition to the new data structure, this coincided with our move over to selective-alignment as the mapping algorithm, and all of this happened at the 1.0.0 release (this is why, for example, indices built before 1.0.0 are not compatible with salmon > 1.0.0; a topic on which there have been a few GitHub issues). However, given the fact that the documentation and software are linked only through manual human intervention (we haven't leveled up to e.g. having salmon be a [literate program](https://en.wikipedia.org/wiki/Literate_programming) yet), these two sometimes get out of sync. This is an instance of that. We have maintained the functionality of the `--writeMappings` feature, and in fact, even augmented it. However, we have not replaced the antiquated `quasi-index` terminology in the documentation. The TLDR is that you can use `--writeMappings` with the index you built with the `salmon index` command, and it should work fine. If you are mapping against an index without decoy sequences, then the output format will be basically the same as older (pre 1.0.0) versions. If you have decoy sequences in your index, then there is an optional SAM flag with each record that ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524
https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524:2549,Deployability,release,release,2549,"inting to prefixes in the suffix array (similar to the strategy used by STAR, but using much longer k-mers to improve lookup speed). We referred to this index as the quasi-index. As the software evolved and we continued to improve the mapping methodology, we eventually transitioned over to an index based on [our pufferfish data structure](https://github.com/COMBINE-lab/pufferfish). In addition to the new data structure, this coincided with our move over to selective-alignment as the mapping algorithm, and all of this happened at the 1.0.0 release (this is why, for example, indices built before 1.0.0 are not compatible with salmon > 1.0.0; a topic on which there have been a few GitHub issues). However, given the fact that the documentation and software are linked only through manual human intervention (we haven't leveled up to e.g. having salmon be a [literate program](https://en.wikipedia.org/wiki/Literate_programming) yet), these two sometimes get out of sync. This is an instance of that. We have maintained the functionality of the `--writeMappings` feature, and in fact, even augmented it. However, we have not replaced the antiquated `quasi-index` terminology in the documentation. The TLDR is that you can use `--writeMappings` with the index you built with the `salmon index` command, and it should work fine. If you are mapping against an index without decoy sequences, then the output format will be basically the same as older (pre 1.0.0) versions. If you have decoy sequences in your index, then there is an optional SAM flag with each record that tells you if the mapping is to a target or a decoy. Specifically, `XT:A:T` signifies that the mapping was to a non-decoy target and `XT:A:T` specifies that the mapping was to a sequence marked in the index as a decoy. I hope this diversion into historical leakage into the current documentation answers your question. Thanks for reporting this, and we'll try to address it upstream so it's fixed in the next release. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524
https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524:763,Modifiability,evolve,evolved,763,"Hi @mishaprochazka and @jdidion,. Thanks @jdidion for pinging me on this. Somehow, Gmail has decided that all GitHub notifications (except those that explicitly tag me, but somehow I missed this one) should go to SPAM. So, I've been missing some of the newer issues here. The short answer is that the documentation needs to be updated. When salmon was originally published, we made use of [RapMap](https://github.com/COMBINE-lab/RapMap) as the underlying mapper, which performed quasi-mapping against an index that consisted of a suffix array and a hash over k-mers pointing to prefixes in the suffix array (similar to the strategy used by STAR, but using much longer k-mers to improve lookup speed). We referred to this index as the quasi-index. As the software evolved and we continued to improve the mapping methodology, we eventually transitioned over to an index based on [our pufferfish data structure](https://github.com/COMBINE-lab/pufferfish). In addition to the new data structure, this coincided with our move over to selective-alignment as the mapping algorithm, and all of this happened at the 1.0.0 release (this is why, for example, indices built before 1.0.0 are not compatible with salmon > 1.0.0; a topic on which there have been a few GitHub issues). However, given the fact that the documentation and software are linked only through manual human intervention (we haven't leveled up to e.g. having salmon be a [literate program](https://en.wikipedia.org/wiki/Literate_programming) yet), these two sometimes get out of sync. This is an instance of that. We have maintained the functionality of the `--writeMappings` feature, and in fact, even augmented it. However, we have not replaced the antiquated `quasi-index` terminology in the documentation. The TLDR is that you can use `--writeMappings` with the index you built with the `salmon index` command, and it should work fine. If you are mapping against an index without decoy sequences, then the output format will be basically ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524
https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524:469,Performance,perform,performed,469,"Hi @mishaprochazka and @jdidion,. Thanks @jdidion for pinging me on this. Somehow, Gmail has decided that all GitHub notifications (except those that explicitly tag me, but somehow I missed this one) should go to SPAM. So, I've been missing some of the newer issues here. The short answer is that the documentation needs to be updated. When salmon was originally published, we made use of [RapMap](https://github.com/COMBINE-lab/RapMap) as the underlying mapper, which performed quasi-mapping against an index that consisted of a suffix array and a hash over k-mers pointing to prefixes in the suffix array (similar to the strategy used by STAR, but using much longer k-mers to improve lookup speed). We referred to this index as the quasi-index. As the software evolved and we continued to improve the mapping methodology, we eventually transitioned over to an index based on [our pufferfish data structure](https://github.com/COMBINE-lab/pufferfish). In addition to the new data structure, this coincided with our move over to selective-alignment as the mapping algorithm, and all of this happened at the 1.0.0 release (this is why, for example, indices built before 1.0.0 are not compatible with salmon > 1.0.0; a topic on which there have been a few GitHub issues). However, given the fact that the documentation and software are linked only through manual human intervention (we haven't leveled up to e.g. having salmon be a [literate program](https://en.wikipedia.org/wiki/Literate_programming) yet), these two sometimes get out of sync. This is an instance of that. We have maintained the functionality of the `--writeMappings` feature, and in fact, even augmented it. However, we have not replaced the antiquated `quasi-index` terminology in the documentation. The TLDR is that you can use `--writeMappings` with the index you built with the `salmon index` command, and it should work fine. If you are mapping against an index without decoy sequences, then the output format will be basically ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524
https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524:549,Security,hash,hash,549,"Hi @mishaprochazka and @jdidion,. Thanks @jdidion for pinging me on this. Somehow, Gmail has decided that all GitHub notifications (except those that explicitly tag me, but somehow I missed this one) should go to SPAM. So, I've been missing some of the newer issues here. The short answer is that the documentation needs to be updated. When salmon was originally published, we made use of [RapMap](https://github.com/COMBINE-lab/RapMap) as the underlying mapper, which performed quasi-mapping against an index that consisted of a suffix array and a hash over k-mers pointing to prefixes in the suffix array (similar to the strategy used by STAR, but using much longer k-mers to improve lookup speed). We referred to this index as the quasi-index. As the software evolved and we continued to improve the mapping methodology, we eventually transitioned over to an index based on [our pufferfish data structure](https://github.com/COMBINE-lab/pufferfish). In addition to the new data structure, this coincided with our move over to selective-alignment as the mapping algorithm, and all of this happened at the 1.0.0 release (this is why, for example, indices built before 1.0.0 are not compatible with salmon > 1.0.0; a topic on which there have been a few GitHub issues). However, given the fact that the documentation and software are linked only through manual human intervention (we haven't leveled up to e.g. having salmon be a [literate program](https://en.wikipedia.org/wiki/Literate_programming) yet), these two sometimes get out of sync. This is an instance of that. We have maintained the functionality of the `--writeMappings` feature, and in fact, even augmented it. However, we have not replaced the antiquated `quasi-index` terminology in the documentation. The TLDR is that you can use `--writeMappings` with the index you built with the `salmon index` command, and it should work fine. If you are mapping against an index without decoy sequences, then the output format will be basically ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/727#issuecomment-996192524
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1022365298:360,Usability,clear,clear,360,"Hi @k3yavi . Thank you very much for the answer. . I am interested in the equivalence class counts (ECC) per cell and the transcripts that belong to each equivalence class. I think bfh.txt is file that contains that information, but I couldn't figure out how the file is structured. I looked at the function you linked to, but the schema hasn't become totally clear. Could you provide some more information? . Thanks in advance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1022365298
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1028272923:162,Availability,down,down,162,"Hi @k3yavi . Thank you for linking me to your script to parse the bfh file from alevin. I think I could figure out the structure of the bfh file. I will write it down underneath with the help of an example. Could you confirm it is correct? . Everything up and until the listing of the barcodes is clear. I will start with a line from after the barcodes. ; ""7	90480	90486	107507	107990	108641	109149	112915	1	1	105	1	TGGGATTT	1"". I think that each such line corresponds to an equivalence class (EC). The first entry on each row is the number of transcripts in the EC. This is followed by the transcripts (more correctly, indices you can use to obtain the transcripts). Then you have the number of reads with in the EC, followed by the number of barcodes (~cells). For each barcode, you have an index that can be used to retrieve the identity of the barcode, followed by the number of UMIs within that barcode, the sequence of the UMI and lastly the number of reads associated with that UMI. . My goal is create a expression matrix where the ECs are the rows and the columns are the cells. If I want the UMI counts, do I need to count the number of reads associated with each UMI or just the number of UMIs per cell? . Thanks in advance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1028272923
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1028272923:297,Usability,clear,clear,297,"Hi @k3yavi . Thank you for linking me to your script to parse the bfh file from alevin. I think I could figure out the structure of the bfh file. I will write it down underneath with the help of an example. Could you confirm it is correct? . Everything up and until the listing of the barcodes is clear. I will start with a line from after the barcodes. ; ""7	90480	90486	107507	107990	108641	109149	112915	1	1	105	1	TGGGATTT	1"". I think that each such line corresponds to an equivalence class (EC). The first entry on each row is the number of transcripts in the EC. This is followed by the transcripts (more correctly, indices you can use to obtain the transcripts). Then you have the number of reads with in the EC, followed by the number of barcodes (~cells). For each barcode, you have an index that can be used to retrieve the identity of the barcode, followed by the number of UMIs within that barcode, the sequence of the UMI and lastly the number of reads associated with that UMI. . My goal is create a expression matrix where the ECs are the rows and the columns are the cells. If I want the UMI counts, do I need to count the number of reads associated with each UMI or just the number of UMIs per cell? . Thanks in advance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1028272923
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1029328067:664,Integrability,depend,depends,664,"Hi @tmms1 ,. > I think that each such line corresponds to an equivalence class (EC). The first entry on each row is the number of transcripts in the EC. This is followed by the transcripts (more correctly, indices you can use to obtain the transcripts). Then you have the number of reads with in the EC, followed by the number of barcodes (~cells). For each barcode, you have an index that can be used to retrieve the identity of the barcode, followed by the number of UMIs within that barcode, the sequence of the UMI and lastly the number of reads associated with that UMI. This is correct !. Unfortunately you goal is not very clear to me and the output matrix depends on that. I understand that you wan't a matrix of dimension |eq_class X cells| but if you wan't the values in the matrix to be read count then you have to add the counts of all the UMIs in class across the cells; and if you wan't the values in the matrix to be the frequency of the unique UMIs then just count the UMIs you wan't instead of reads.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1029328067
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1029328067:630,Usability,clear,clear,630,"Hi @tmms1 ,. > I think that each such line corresponds to an equivalence class (EC). The first entry on each row is the number of transcripts in the EC. This is followed by the transcripts (more correctly, indices you can use to obtain the transcripts). Then you have the number of reads with in the EC, followed by the number of barcodes (~cells). For each barcode, you have an index that can be used to retrieve the identity of the barcode, followed by the number of UMIs within that barcode, the sequence of the UMI and lastly the number of reads associated with that UMI. This is correct !. Unfortunately you goal is not very clear to me and the output matrix depends on that. I understand that you wan't a matrix of dimension |eq_class X cells| but if you wan't the values in the matrix to be read count then you have to add the counts of all the UMIs in class across the cells; and if you wan't the values in the matrix to be the frequency of the unique UMIs then just count the UMIs you wan't instead of reads.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1029328067
https://github.com/COMBINE-lab/salmon/issues/729#issuecomment-993410585:17,Deployability,Release,Release,17,"The pre-compiled Release now finally seems to work, so nevermind!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/729#issuecomment-993410585
https://github.com/COMBINE-lab/salmon/issues/730#issuecomment-1010446444:88,Availability,avail,available,88,Since I can't reproduce this locally? I'm going to close this until more information is available.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/730#issuecomment-1010446444
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467:959,Availability,error,error,959,"Btw the same thing happens when using gencode transcriptome. ```bash; wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.transcripts.fa.gz; # better name; mv gencode.vM25.transcripts.fa.gz Mus_musculus_GENCODE_v25_GRCm38.fa.gz ; ````. ```; ./generateDecoyTranscriptome.sh \; -a /no_backup/genome_annots/Mus_musculus_ENSEMBL_v_102_GRCm38.gtf \; -g /no_backup/genome_seqs/Mmu10_gDNA.fasta \; -t /no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa.gz \; -o /no_backup/indexes/salmon/gencode_mm10; ```. Which generates 2 files:; ```; ls -1 /no_backup/indexes/salmon/gencode_mm10; decoys.txt; gentrome.fa; ```. And then if I try to build an index with:. ```; salmon index \; -t /no_backup/indexes/salmon/gencode_mm10/gentrome.fa \; -i /no_backup/indexes/salmon/gencode_mm10 \; -d /no_backup/indexes/salmon/gencode_mm10/decoys.txt \; -k 28 --threads 8; ```. the job starts running but dies immediately for the same error:. ```; tail -n 4 /no_backup/indexes/salmon/gencode_mm10/ref_indexing.log . [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ````. I do believe that the names in `decoys.txt` match some fasta headers in `gentrome.fa` as show here:. ```; head -n 1 decoys.txt; GL456210.1. zgrep ""GL456210.1"" gentrome.fa ; >GL456210.1; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467:1431,Availability,error,errors,1431,"Btw the same thing happens when using gencode transcriptome. ```bash; wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.transcripts.fa.gz; # better name; mv gencode.vM25.transcripts.fa.gz Mus_musculus_GENCODE_v25_GRCm38.fa.gz ; ````. ```; ./generateDecoyTranscriptome.sh \; -a /no_backup/genome_annots/Mus_musculus_ENSEMBL_v_102_GRCm38.gtf \; -g /no_backup/genome_seqs/Mmu10_gDNA.fasta \; -t /no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa.gz \; -o /no_backup/indexes/salmon/gencode_mm10; ```. Which generates 2 files:; ```; ls -1 /no_backup/indexes/salmon/gencode_mm10; decoys.txt; gentrome.fa; ```. And then if I try to build an index with:. ```; salmon index \; -t /no_backup/indexes/salmon/gencode_mm10/gentrome.fa \; -i /no_backup/indexes/salmon/gencode_mm10 \; -d /no_backup/indexes/salmon/gencode_mm10/decoys.txt \; -k 28 --threads 8; ```. the job starts running but dies immediately for the same error:. ```; tail -n 4 /no_backup/indexes/salmon/gencode_mm10/ref_indexing.log . [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ````. I do believe that the names in `decoys.txt` match some fasta headers in `gentrome.fa` as show here:. ```; head -n 1 decoys.txt; GL456210.1. zgrep ""GL456210.1"" gentrome.fa ; >GL456210.1; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467:1438,Availability,down,downstream,1438,"Btw the same thing happens when using gencode transcriptome. ```bash; wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.transcripts.fa.gz; # better name; mv gencode.vM25.transcripts.fa.gz Mus_musculus_GENCODE_v25_GRCm38.fa.gz ; ````. ```; ./generateDecoyTranscriptome.sh \; -a /no_backup/genome_annots/Mus_musculus_ENSEMBL_v_102_GRCm38.gtf \; -g /no_backup/genome_seqs/Mmu10_gDNA.fasta \; -t /no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa.gz \; -o /no_backup/indexes/salmon/gencode_mm10; ```. Which generates 2 files:; ```; ls -1 /no_backup/indexes/salmon/gencode_mm10; decoys.txt; gentrome.fa; ```. And then if I try to build an index with:. ```; salmon index \; -t /no_backup/indexes/salmon/gencode_mm10/gentrome.fa \; -i /no_backup/indexes/salmon/gencode_mm10 \; -d /no_backup/indexes/salmon/gencode_mm10/decoys.txt \; -k 28 --threads 8; ```. the job starts running but dies immediately for the same error:. ```; tail -n 4 /no_backup/indexes/salmon/gencode_mm10/ref_indexing.log . [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ````. I do believe that the names in `decoys.txt` match some fasta headers in `gentrome.fa` as show here:. ```; head -n 1 decoys.txt; GL456210.1. zgrep ""GL456210.1"" gentrome.fa ; >GL456210.1; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467:1569,Availability,error,error,1569,"Btw the same thing happens when using gencode transcriptome. ```bash; wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.transcripts.fa.gz; # better name; mv gencode.vM25.transcripts.fa.gz Mus_musculus_GENCODE_v25_GRCm38.fa.gz ; ````. ```; ./generateDecoyTranscriptome.sh \; -a /no_backup/genome_annots/Mus_musculus_ENSEMBL_v_102_GRCm38.gtf \; -g /no_backup/genome_seqs/Mmu10_gDNA.fasta \; -t /no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa.gz \; -o /no_backup/indexes/salmon/gencode_mm10; ```. Which generates 2 files:; ```; ls -1 /no_backup/indexes/salmon/gencode_mm10; decoys.txt; gentrome.fa; ```. And then if I try to build an index with:. ```; salmon index \; -t /no_backup/indexes/salmon/gencode_mm10/gentrome.fa \; -i /no_backup/indexes/salmon/gencode_mm10 \; -d /no_backup/indexes/salmon/gencode_mm10/decoys.txt \; -k 28 --threads 8; ```. the job starts running but dies immediately for the same error:. ```; tail -n 4 /no_backup/indexes/salmon/gencode_mm10/ref_indexing.log . [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ````. I do believe that the names in `decoys.txt` match some fasta headers in `gentrome.fa` as show here:. ```; head -n 1 decoys.txt; GL456210.1. zgrep ""GL456210.1"" gentrome.fa ; >GL456210.1; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467:1034,Testability,log,log,1034,"Btw the same thing happens when using gencode transcriptome. ```bash; wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.transcripts.fa.gz; # better name; mv gencode.vM25.transcripts.fa.gz Mus_musculus_GENCODE_v25_GRCm38.fa.gz ; ````. ```; ./generateDecoyTranscriptome.sh \; -a /no_backup/genome_annots/Mus_musculus_ENSEMBL_v_102_GRCm38.gtf \; -g /no_backup/genome_seqs/Mmu10_gDNA.fasta \; -t /no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa.gz \; -o /no_backup/indexes/salmon/gencode_mm10; ```. Which generates 2 files:; ```; ls -1 /no_backup/indexes/salmon/gencode_mm10; decoys.txt; gentrome.fa; ```. And then if I try to build an index with:. ```; salmon index \; -t /no_backup/indexes/salmon/gencode_mm10/gentrome.fa \; -i /no_backup/indexes/salmon/gencode_mm10 \; -d /no_backup/indexes/salmon/gencode_mm10/decoys.txt \; -k 28 --threads 8; ```. the job starts running but dies immediately for the same error:. ```; tail -n 4 /no_backup/indexes/salmon/gencode_mm10/ref_indexing.log . [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ````. I do believe that the names in `decoys.txt` match some fasta headers in `gentrome.fa` as show here:. ```; head -n 1 decoys.txt; GL456210.1. zgrep ""GL456210.1"" gentrome.fa ; >GL456210.1; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1001151467
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002286946:47,Availability,ping,pinging,47,"Hi @Ni-Ar,. We're happy to look into this. I'm pinging @k3yavi here to take a look as well. In the meantime, I'm actually surprised indexing worked when you passed `-k 28`. In general, the code should stop and refuse if k is not odd, since we need a k-mer to be odd to be able to distinguish between a k-mer and its reverse complement.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002286946
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002804831:301,Availability,error,error,301,"Hi @Ni-Ar ,. Thanks for reporting this. I checked the salmon index from the files [here](http://refgenomes.databio.org/v3/assets/archive/0f10d83b1050c08dd53189986f60970b92a315aa7a16a6f1/salmon_partial_sa_index?tag=default) build on gencode reference but with k 28 salmon complaints with the following error:. > [2021-12-29 17:04:14.809] [puff::index::jointLog] [info] ntHll estimated 144189454 distinct k-mers, setting filter size to 2^32; > PARSE ERROR: Argument: -k (--kvalue); > Value '28' does not meet constraint: value of K must be odd. Is it possible to double check the salmon version and the files which you are using are with the right path ? Because salmon should definitely not be able to go through the process of indexing with k 28. If the error persists can you please share the files so that we can double check them on our end ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002804831
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002804831:448,Availability,ERROR,ERROR,448,"Hi @Ni-Ar ,. Thanks for reporting this. I checked the salmon index from the files [here](http://refgenomes.databio.org/v3/assets/archive/0f10d83b1050c08dd53189986f60970b92a315aa7a16a6f1/salmon_partial_sa_index?tag=default) build on gencode reference but with k 28 salmon complaints with the following error:. > [2021-12-29 17:04:14.809] [puff::index::jointLog] [info] ntHll estimated 144189454 distinct k-mers, setting filter size to 2^32; > PARSE ERROR: Argument: -k (--kvalue); > Value '28' does not meet constraint: value of K must be odd. Is it possible to double check the salmon version and the files which you are using are with the right path ? Because salmon should definitely not be able to go through the process of indexing with k 28. If the error persists can you please share the files so that we can double check them on our end ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002804831
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002804831:754,Availability,error,error,754,"Hi @Ni-Ar ,. Thanks for reporting this. I checked the salmon index from the files [here](http://refgenomes.databio.org/v3/assets/archive/0f10d83b1050c08dd53189986f60970b92a315aa7a16a6f1/salmon_partial_sa_index?tag=default) build on gencode reference but with k 28 salmon complaints with the following error:. > [2021-12-29 17:04:14.809] [puff::index::jointLog] [info] ntHll estimated 144189454 distinct k-mers, setting filter size to 2^32; > PARSE ERROR: Argument: -k (--kvalue); > Value '28' does not meet constraint: value of K must be odd. Is it possible to double check the salmon version and the files which you are using are with the right path ? Because salmon should definitely not be able to go through the process of indexing with k 28. If the error persists can you please share the files so that we can double check them on our end ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002804831
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868:1919,Availability,error,errors,1919,"[2021-12-30 00:46:19.915] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000229312.1|ENSMUSG00000056486.18|OTTMUSG00000013428.7|OTTMUST00000171565.1|Chn1-211|Chn1|20|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping). # [omissis]. [2021-12-30 00:46:27.227] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000226172.1|ENSMUSG00000002249.21|OTTMUSG00000024245.6|OTTMUST00000167695.2|Tead3-208|Tead3|11|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping); [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-12-30 00:46:28.327] [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-12-30 00:46:28.449] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```. I believe; - [x] version is correct; - [x] file paths exists; - [x] It is not the k-mer number as now I get the same error with `-k 29`. For the files I've used:; The GTF file I've used for `./generateDecoyTranscriptome.sh ` I've downloaded it from ensembl with:. ```; wget http://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.gtf.gz; ```; and just renamed it with `mv`. While the genome file I something already present in my lab folder structure. I could download a new original genome fasta file for mm10 if you think it's worth for the troubleshooting. Otherwise I'm not really sure how I could share this file with you. Thanks a lot for your support, ; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868:1926,Availability,down,downstream,1926,"[2021-12-30 00:46:19.915] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000229312.1|ENSMUSG00000056486.18|OTTMUSG00000013428.7|OTTMUST00000171565.1|Chn1-211|Chn1|20|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping). # [omissis]. [2021-12-30 00:46:27.227] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000226172.1|ENSMUSG00000002249.21|OTTMUSG00000024245.6|OTTMUST00000167695.2|Tead3-208|Tead3|11|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping); [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-12-30 00:46:28.327] [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-12-30 00:46:28.449] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```. I believe; - [x] version is correct; - [x] file paths exists; - [x] It is not the k-mer number as now I get the same error with `-k 29`. For the files I've used:; The GTF file I've used for `./generateDecoyTranscriptome.sh ` I've downloaded it from ensembl with:. ```; wget http://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.gtf.gz; ```; and just renamed it with `mv`. While the genome file I something already present in my lab folder structure. I could download a new original genome fasta file for mm10 if you think it's worth for the troubleshooting. Otherwise I'm not really sure how I could share this file with you. Thanks a lot for your support, ; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868:2083,Availability,error,error,2083,"[2021-12-30 00:46:19.915] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000229312.1|ENSMUSG00000056486.18|OTTMUSG00000013428.7|OTTMUST00000171565.1|Chn1-211|Chn1|20|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping). # [omissis]. [2021-12-30 00:46:27.227] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000226172.1|ENSMUSG00000002249.21|OTTMUSG00000024245.6|OTTMUST00000167695.2|Tead3-208|Tead3|11|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping); [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-12-30 00:46:28.327] [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-12-30 00:46:28.449] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```. I believe; - [x] version is correct; - [x] file paths exists; - [x] It is not the k-mer number as now I get the same error with `-k 29`. For the files I've used:; The GTF file I've used for `./generateDecoyTranscriptome.sh ` I've downloaded it from ensembl with:. ```; wget http://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.gtf.gz; ```; and just renamed it with `mv`. While the genome file I something already present in my lab folder structure. I could download a new original genome fasta file for mm10 if you think it's worth for the troubleshooting. Otherwise I'm not really sure how I could share this file with you. Thanks a lot for your support, ; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868:2256,Availability,error,error,2256,"[2021-12-30 00:46:19.915] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000229312.1|ENSMUSG00000056486.18|OTTMUSG00000013428.7|OTTMUST00000171565.1|Chn1-211|Chn1|20|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping). # [omissis]. [2021-12-30 00:46:27.227] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000226172.1|ENSMUSG00000002249.21|OTTMUSG00000024245.6|OTTMUST00000167695.2|Tead3-208|Tead3|11|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping); [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-12-30 00:46:28.327] [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-12-30 00:46:28.449] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```. I believe; - [x] version is correct; - [x] file paths exists; - [x] It is not the k-mer number as now I get the same error with `-k 29`. For the files I've used:; The GTF file I've used for `./generateDecoyTranscriptome.sh ` I've downloaded it from ensembl with:. ```; wget http://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.gtf.gz; ```; and just renamed it with `mv`. While the genome file I something already present in my lab folder structure. I could download a new original genome fasta file for mm10 if you think it's worth for the troubleshooting. Otherwise I'm not really sure how I could share this file with you. Thanks a lot for your support, ; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868:2369,Availability,down,downloaded,2369,"[2021-12-30 00:46:19.915] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000229312.1|ENSMUSG00000056486.18|OTTMUSG00000013428.7|OTTMUST00000171565.1|Chn1-211|Chn1|20|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping). # [omissis]. [2021-12-30 00:46:27.227] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000226172.1|ENSMUSG00000002249.21|OTTMUSG00000024245.6|OTTMUST00000167695.2|Tead3-208|Tead3|11|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping); [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-12-30 00:46:28.327] [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-12-30 00:46:28.449] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```. I believe; - [x] version is correct; - [x] file paths exists; - [x] It is not the k-mer number as now I get the same error with `-k 29`. For the files I've used:; The GTF file I've used for `./generateDecoyTranscriptome.sh ` I've downloaded it from ensembl with:. ```; wget http://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.gtf.gz; ```; and just renamed it with `mv`. While the genome file I something already present in my lab folder structure. I could download a new original genome fasta file for mm10 if you think it's worth for the troubleshooting. Otherwise I'm not really sure how I could share this file with you. Thanks a lot for your support, ; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868:2623,Availability,down,download,2623,"[2021-12-30 00:46:19.915] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000229312.1|ENSMUSG00000056486.18|OTTMUSG00000013428.7|OTTMUST00000171565.1|Chn1-211|Chn1|20|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping). # [omissis]. [2021-12-30 00:46:27.227] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000226172.1|ENSMUSG00000002249.21|OTTMUSG00000024245.6|OTTMUST00000167695.2|Tead3-208|Tead3|11|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping); [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-12-30 00:46:28.327] [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-12-30 00:46:28.449] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```. I believe; - [x] version is correct; - [x] file paths exists; - [x] It is not the k-mer number as now I get the same error with `-k 29`. For the files I've used:; The GTF file I've used for `./generateDecoyTranscriptome.sh ` I've downloaded it from ensembl with:. ```; wget http://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.gtf.gz; ```; and just renamed it with `mv`. While the genome file I something already present in my lab folder structure. I could download a new original genome fasta file for mm10 if you think it's worth for the troubleshooting. Otherwise I'm not really sure how I could share this file with you. Thanks a lot for your support, ; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868:2440,Deployability,release,release-,2440,"[2021-12-30 00:46:19.915] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000229312.1|ENSMUSG00000056486.18|OTTMUSG00000013428.7|OTTMUST00000171565.1|Chn1-211|Chn1|20|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping). # [omissis]. [2021-12-30 00:46:27.227] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000226172.1|ENSMUSG00000002249.21|OTTMUSG00000024245.6|OTTMUST00000167695.2|Tead3-208|Tead3|11|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping); [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-12-30 00:46:28.327] [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2021-12-30 00:46:28.449] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```. I believe; - [x] version is correct; - [x] file paths exists; - [x] It is not the k-mer number as now I get the same error with `-k 29`. For the files I've used:; The GTF file I've used for `./generateDecoyTranscriptome.sh ` I've downloaded it from ensembl with:. ```; wget http://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.gtf.gz; ```; and just renamed it with `mv`. While the genome file I something already present in my lab folder structure. I could download a new original genome fasta file for mm10 if you think it's worth for the troubleshooting. Otherwise I'm not really sure how I could share this file with you. Thanks a lot for your support, ; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868:226,Testability,log,log,226,"Hi. I just did:. ```; salmon index -t /no_backup/indexes/salmon/gencode_mm10/gentrome.fa \; -i /no_backup/indexes/salmon/gencode_mm10 \; -d /no_backup/indexes/salmon/gencode_mm10/decoys.txt \; -k 29 --threads 8; ````. And the log file says:. ```; Version Info: This is the most recent version of salmon.; [2021-12-30 00:46:18.878] [jLog] [info] building index; out : /no_backup/indexes/salmon/gencode_mm10; [2021-12-30 00:46:18.881] [puff::index::jointLog] [info] Running fixFasta. [Step 1 of 4] : counting k-mers; [2021-12-30 00:46:18.914] [puff::index::jointLog] [warning] It appears that this may be a GENCODE transcriptome (from analyzing the separators in the FASTA header). However, you have not set '|' as a header separator. If this is a GENCODE transcriptome, consider passing --gencode to the pufferfish index command. [2021-12-30 00:46:19.915] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000229312.1|ENSMUSG00000056486.18|OTTMUSG00000013428.7|OTTMUST00000171565.1|Chn1-211|Chn1|20|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping). # [omissis]. [2021-12-30 00:46:27.227] [puff::index::jointLog] [warning] Entry with header [ENSMUST00000226172.1|ENSMUSG00000002249.21|OTTMUSG00000024245.6|OTTMUST00000167695.2|Tead3-208|Tead3|11|processed_transcript|], had length less than equal to the k-mer length of 29 (perhaps after poly-A clipping); [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] Removed 1612 transcripts that were sequence duplicates of indexed transcripts.; [2021-12-30 00:46:28.327] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2021-12-30 00:46:28.327] [puff::index::jointLog] [critical] The decoy file contained the names of 55 decoy sequences, but 0 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fast",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1002817868
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003091513:327,Availability,down,downloaded,327,"Hey @Ni-Ar ,; I can confirm that I can replicate the issue on my end. While we are working on this there are a couple of workarounds.; (1) if RAM is not a limitation you can basically index the full genome as decoy i.e. with running the script.; (2) if you are using standard model organism, the decoy salmon index can also be downloaded from refgenie website.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003091513
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003335885:457,Availability,Mask,Masking,457,"This gave me:; ```; ****************; *** getDecoy ***; ****************; -a <Annotation GTF file> = /nfs/no_backup/genome_annots/Mus_musculus_ENSEMBL_v_102_GRCm38.gtf; -g <Genome fasta> = /nfs/no_backup/genome_seqs/Mmu10_gDNA.fasta; -t <Transcriptome fasta> = /nfs/no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa; -o <Output files Path> = /no_backup/indexes/salmon/gencode_mm10_unzip; [1/10] Extracting exonic features from the gtf; [2/10] Masking the genome fasta; [3/10] Aligning transcriptome to genome; >>>>>>>>>>>>>>>>>>; Reference = [reference.masked.genome.fa]; Query = [/nfs/no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa]; Kmer size = 16; Window size = 5; Segment length = 500 (read split allowed); Alphabet = DNA; Percentage identity threshold = 80%; Mapping output file = mashmap.out; Filter mode = 1 (1 = map, 2 = one-to-one, 3 = none); Execution threads = 8; >>>>>>>>>>>>>>>>>>; INFO, skch::Sketch::build, minimizers picked from reference = 843543544; INFO, skch::Sketch::index, unique minimizers = 276648625; INFO, skch::Sketch::computeFreqHist, Frequency histogram of minimizers = (1, 141685574) ... (2606547, 1); INFO, skch::Sketch::computeFreqHist, With threshold 0.001%, ignore minimizers occurring >= 7361 times during lookup.; INFO, skch::main, Time spent computing the reference index: 549.706 sec; INFO, skch::Map::mapQuery, [count of mapped reads, reads qualified for mapping, total input reads] = [111965, 112131, 142604]; INFO, skch::main, Time spent mapping the query : 17487.5 sec; INFO, skch::main, mapping results saved in : mashmap.out; [4/10] Extracting intervals from mashmap alignments; [5/10] Merging the intervals; [6/10] Extracting sequences from the genome; [7/10] Concatenating to get decoy sequences; [8/10] Making gentrome; [9/10] Extracting decoy sequence ids; [10/10] Removing temporary files; ```. Check the decoys:; ```; head /no_backup/indexes/salmon/gencode_mm10_unzip/decoys.txt ; GL456210.1; GL456367.1; chrX; chrY; GL4",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003335885
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003335885:567,Availability,mask,masked,567,"This gave me:; ```; ****************; *** getDecoy ***; ****************; -a <Annotation GTF file> = /nfs/no_backup/genome_annots/Mus_musculus_ENSEMBL_v_102_GRCm38.gtf; -g <Genome fasta> = /nfs/no_backup/genome_seqs/Mmu10_gDNA.fasta; -t <Transcriptome fasta> = /nfs/no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa; -o <Output files Path> = /no_backup/indexes/salmon/gencode_mm10_unzip; [1/10] Extracting exonic features from the gtf; [2/10] Masking the genome fasta; [3/10] Aligning transcriptome to genome; >>>>>>>>>>>>>>>>>>; Reference = [reference.masked.genome.fa]; Query = [/nfs/no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa]; Kmer size = 16; Window size = 5; Segment length = 500 (read split allowed); Alphabet = DNA; Percentage identity threshold = 80%; Mapping output file = mashmap.out; Filter mode = 1 (1 = map, 2 = one-to-one, 3 = none); Execution threads = 8; >>>>>>>>>>>>>>>>>>; INFO, skch::Sketch::build, minimizers picked from reference = 843543544; INFO, skch::Sketch::index, unique minimizers = 276648625; INFO, skch::Sketch::computeFreqHist, Frequency histogram of minimizers = (1, 141685574) ... (2606547, 1); INFO, skch::Sketch::computeFreqHist, With threshold 0.001%, ignore minimizers occurring >= 7361 times during lookup.; INFO, skch::main, Time spent computing the reference index: 549.706 sec; INFO, skch::Map::mapQuery, [count of mapped reads, reads qualified for mapping, total input reads] = [111965, 112131, 142604]; INFO, skch::main, Time spent mapping the query : 17487.5 sec; INFO, skch::main, mapping results saved in : mashmap.out; [4/10] Extracting intervals from mashmap alignments; [5/10] Merging the intervals; [6/10] Extracting sequences from the genome; [7/10] Concatenating to get decoy sequences; [8/10] Making gentrome; [9/10] Extracting decoy sequence ids; [10/10] Removing temporary files; ```. Check the decoys:; ```; head /no_backup/indexes/salmon/gencode_mm10_unzip/decoys.txt ; GL456210.1; GL456367.1; chrX; chrY; GL4",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003335885
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003335885:2494,Performance,queue,queue,2494,">>>>>>>>>>>>>>>>; Reference = [reference.masked.genome.fa]; Query = [/nfs/no_backup/transcriptome_seqs/Mus_musculus_GENCODE_v25_GRCm38.fa]; Kmer size = 16; Window size = 5; Segment length = 500 (read split allowed); Alphabet = DNA; Percentage identity threshold = 80%; Mapping output file = mashmap.out; Filter mode = 1 (1 = map, 2 = one-to-one, 3 = none); Execution threads = 8; >>>>>>>>>>>>>>>>>>; INFO, skch::Sketch::build, minimizers picked from reference = 843543544; INFO, skch::Sketch::index, unique minimizers = 276648625; INFO, skch::Sketch::computeFreqHist, Frequency histogram of minimizers = (1, 141685574) ... (2606547, 1); INFO, skch::Sketch::computeFreqHist, With threshold 0.001%, ignore minimizers occurring >= 7361 times during lookup.; INFO, skch::main, Time spent computing the reference index: 549.706 sec; INFO, skch::Map::mapQuery, [count of mapped reads, reads qualified for mapping, total input reads] = [111965, 112131, 142604]; INFO, skch::main, Time spent mapping the query : 17487.5 sec; INFO, skch::main, mapping results saved in : mashmap.out; [4/10] Extracting intervals from mashmap alignments; [5/10] Merging the intervals; [6/10] Extracting sequences from the genome; [7/10] Concatenating to get decoy sequences; [8/10] Making gentrome; [9/10] Extracting decoy sequence ids; [10/10] Removing temporary files; ```. Check the decoys:; ```; head /no_backup/indexes/salmon/gencode_mm10_unzip/decoys.txt ; GL456210.1; GL456367.1; chrX; chrY; GL456221.1; JH584304.1; GL456378.1; GL456211.1; JH584296.1; JH584300.1; ```. Check that the decoys are there in the `gentrome.fa`; ```; zgrep "">GL456210.1"" /no_backup/indexes/salmon/gencode_mm10_unzip/gentrome.fa ; >GL456210.1; ```. and then index with :; ```; salmon index -t /no_backup/indexes/salmon/gencode_mm10_unzip/gentrome.fa \; -i /no_backup/indexes/salmon/mm10_gencode \; -d /no_backup/indexes/salmon/gencode_mm10_unzip/decoys.txt \; -k 29 --threads 8 --gencode; ```; the job is in the queue now, I'll keep you posted.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003335885
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:2840,Integrability,wrap,wrapping,2840,"12; approximateContigTotalLength: 132160289; counters for complex kmers:; (prec>1 & succ>1)=181344 | (succ>1 & isStart)=714 | (prec>1 & isEnd)=800 | (isStart & isEnd)=42; contig count: 2077595 element count: 297242564 complex nodes: 182900; # of ones in rank vector: 2077594; [2021-12-31 11:28:32.554] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file.; [2021-12-31 11:28:32.554] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory /no_backup/indexes/salmon/mm10_gencode; size = 297242564; -----------------------------------------; | Loading contigs | Time = 135.18 ms; -----------------------------------------; size = 297242564; -----------------------------------------; | Loading contig boundaries | Time = 61.18 ms; -----------------------------------------; Number of ones: 2077594; Number of ones per inventory item: 512; Inventory entries filled: 4058; 2077594; [2021-12-31 11:28:33.532] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2021-12-31 11:28:33.566] [puff::index::jointLog] [info] contig count for validation: 2,077,594; [2021-12-31 11:28:34.693] [puff::index::jointLog] [info] Total # of Contigs : 2,077,594; [2021-12-31 11:28:34.693] [puff::index::jointLog] [info] Total # of numerical Contigs : 2,077,594; [2021-12-31 11:28:34.787] [puff::index::jointLog] [info] Total # of contig vec entries: 13,003,859; [2021-12-31 11:28:34.787] [puff::index::jointLog] [info] bits per offset entry 24; [2021-12-31 11:28:35.409] [puff::index::jointLog] [info] Done constructing the contig vector. 2077595; [2021-12-31 11:28:36.870] [puff::index::jointLog] [info] # segments = 2,077,594; [2021-12-31 11:28:36.870] [puff::index::jointLog] [info] total length = 297,242,564; [2021-12-31 11:28:36.999] [puff::index::jointLog] [info] Reading the reference files ...; [2021-12-31 11:28:38.719] [puff::index::jointLog] [info] positional integer width = 29; [2021-12-31 11:28:38.719] [puff::inde",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:2442,Performance,Load,Loading,2442,"ides; [2021-12-31 11:26:33.218] [puff::index::jointLog] [info] Clipped poly-A tails from 758 transcripts; wrote 141009 cleaned references; [2021-12-31 11:26:34.700] [puff::index::jointLog] [info] Filter size not provided; estimating from number of distinct k-mers; [2021-12-31 11:26:38.852] [puff::index::jointLog] [info] ntHll estimated 239287090 distinct k-mers, setting filter size to 2^32; allowedIn: 21; Max Junction ID: 1394611; seen.size():11156897 kmerInfo.size():1394612; approximateContigTotalLength: 132160289; counters for complex kmers:; (prec>1 & succ>1)=181344 | (succ>1 & isStart)=714 | (prec>1 & isEnd)=800 | (isStart & isEnd)=42; contig count: 2077595 element count: 297242564 complex nodes: 182900; # of ones in rank vector: 2077594; [2021-12-31 11:28:32.554] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file.; [2021-12-31 11:28:32.554] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory /no_backup/indexes/salmon/mm10_gencode; size = 297242564; -----------------------------------------; | Loading contigs | Time = 135.18 ms; -----------------------------------------; size = 297242564; -----------------------------------------; | Loading contig boundaries | Time = 61.18 ms; -----------------------------------------; Number of ones: 2077594; Number of ones per inventory item: 512; Inventory entries filled: 4058; 2077594; [2021-12-31 11:28:33.532] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2021-12-31 11:28:33.566] [puff::index::jointLog] [info] contig count for validation: 2,077,594; [2021-12-31 11:28:34.693] [puff::index::jointLog] [info] Total # of Contigs : 2,077,594; [2021-12-31 11:28:34.693] [puff::index::jointLog] [info] Total # of numerical Contigs : 2,077,594; [2021-12-31 11:28:34.787] [puff::index::jointLog] [info] Total # of contig vec entries: 13,003,859; [2021-12-31 11:28:34.787] [puff::index::jointLog] [info] bits per offset entry 24; [2021-1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:2584,Performance,Load,Loading,2584,":jointLog] [info] Filter size not provided; estimating from number of distinct k-mers; [2021-12-31 11:26:38.852] [puff::index::jointLog] [info] ntHll estimated 239287090 distinct k-mers, setting filter size to 2^32; allowedIn: 21; Max Junction ID: 1394611; seen.size():11156897 kmerInfo.size():1394612; approximateContigTotalLength: 132160289; counters for complex kmers:; (prec>1 & succ>1)=181344 | (succ>1 & isStart)=714 | (prec>1 & isEnd)=800 | (isStart & isEnd)=42; contig count: 2077595 element count: 297242564 complex nodes: 182900; # of ones in rank vector: 2077594; [2021-12-31 11:28:32.554] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file.; [2021-12-31 11:28:32.554] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory /no_backup/indexes/salmon/mm10_gencode; size = 297242564; -----------------------------------------; | Loading contigs | Time = 135.18 ms; -----------------------------------------; size = 297242564; -----------------------------------------; | Loading contig boundaries | Time = 61.18 ms; -----------------------------------------; Number of ones: 2077594; Number of ones per inventory item: 512; Inventory entries filled: 4058; 2077594; [2021-12-31 11:28:33.532] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2021-12-31 11:28:33.566] [puff::index::jointLog] [info] contig count for validation: 2,077,594; [2021-12-31 11:28:34.693] [puff::index::jointLog] [info] Total # of Contigs : 2,077,594; [2021-12-31 11:28:34.693] [puff::index::jointLog] [info] Total # of numerical Contigs : 2,077,594; [2021-12-31 11:28:34.787] [puff::index::jointLog] [info] Total # of contig vec entries: 13,003,859; [2021-12-31 11:28:34.787] [puff::index::jointLog] [info] bits per offset entry 24; [2021-12-31 11:28:35.409] [puff::index::jointLog] [info] Done constructing the contig vector. 2077595; [2021-12-31 11:28:36.870] [puff::index::jointLog] [info] # segments = 2,077,594; [2",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:2967,Security,validat,validation,2967,"t)=714 | (prec>1 & isEnd)=800 | (isStart & isEnd)=42; contig count: 2077595 element count: 297242564 complex nodes: 182900; # of ones in rank vector: 2077594; [2021-12-31 11:28:32.554] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file.; [2021-12-31 11:28:32.554] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory /no_backup/indexes/salmon/mm10_gencode; size = 297242564; -----------------------------------------; | Loading contigs | Time = 135.18 ms; -----------------------------------------; size = 297242564; -----------------------------------------; | Loading contig boundaries | Time = 61.18 ms; -----------------------------------------; Number of ones: 2077594; Number of ones per inventory item: 512; Inventory entries filled: 4058; 2077594; [2021-12-31 11:28:33.532] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2021-12-31 11:28:33.566] [puff::index::jointLog] [info] contig count for validation: 2,077,594; [2021-12-31 11:28:34.693] [puff::index::jointLog] [info] Total # of Contigs : 2,077,594; [2021-12-31 11:28:34.693] [puff::index::jointLog] [info] Total # of numerical Contigs : 2,077,594; [2021-12-31 11:28:34.787] [puff::index::jointLog] [info] Total # of contig vec entries: 13,003,859; [2021-12-31 11:28:34.787] [puff::index::jointLog] [info] bits per offset entry 24; [2021-12-31 11:28:35.409] [puff::index::jointLog] [info] Done constructing the contig vector. 2077595; [2021-12-31 11:28:36.870] [puff::index::jointLog] [info] # segments = 2,077,594; [2021-12-31 11:28:36.870] [puff::index::jointLog] [info] total length = 297,242,564; [2021-12-31 11:28:36.999] [puff::index::jointLog] [info] Reading the reference files ...; [2021-12-31 11:28:38.719] [puff::index::jointLog] [info] positional integer width = 29; [2021-12-31 11:28:38.719] [puff::index::jointLog] [info] seqSize = 297,242,564; [2021-12-31 11:28:38.719] [puff::index::jointLog] [info] rankSize = 297,242",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:5466,Security,Hash,Hash,5466,"[info] chunk 1 = [37,155,321, 74,310,642); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 2 = [74,310,642, 111,465,963); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 3 = [111,465,963, 148,621,284); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 4 = [148,621,284, 185,776,605); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 5 = [185,776,605, 222,931,953); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 6 = [222,931,953, 260,087,274); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 7 = [260,087,274, 297,242,536); [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] finished populating pos vector; [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] writing index components; [2021-12-31 11:28:59.670] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2021-12-31 11:28:59.944] [jLog] [info] done building index; Threads = 8; Vertex length = 29; Hash functions = 5; Filter size = 4294967296; Capacity = 2; Files: ; /no_backup/indexes/salmon/mm10_gencode/ref_k29_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:4294967296; Pass	Filling	Filtering; 1	22	34	; 2	9	0; True junctions count = 1275494; False junctions count = 1606379; Hash table size = 2881873; Candidate marks count = 14783512; --------------------------------------------------------------------------------; Reallocating bifurcations time: 0; True marks count: 12564712; Edges construction time: 10; --------------------------------------------------------------------------------; Distinct junctions = 1275494. for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000 ; Bitarray 1252655360 bits (100.00 %) (array + ranks ); final hash 0 bits (0.00 %) (nb in final hash 0); ```. And these files are present in the index folder:; ```; ls -1 /no_backup/indexes/salmon/mm10_gencode; complete_ref_lens.bin; ctabl",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:5803,Security,Hash,Hash,5803,"og] [info] chunk 5 = [185,776,605, 222,931,953); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 6 = [222,931,953, 260,087,274); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 7 = [260,087,274, 297,242,536); [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] finished populating pos vector; [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] writing index components; [2021-12-31 11:28:59.670] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2021-12-31 11:28:59.944] [jLog] [info] done building index; Threads = 8; Vertex length = 29; Hash functions = 5; Filter size = 4294967296; Capacity = 2; Files: ; /no_backup/indexes/salmon/mm10_gencode/ref_k29_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:4294967296; Pass	Filling	Filtering; 1	22	34	; 2	9	0; True junctions count = 1275494; False junctions count = 1606379; Hash table size = 2881873; Candidate marks count = 14783512; --------------------------------------------------------------------------------; Reallocating bifurcations time: 0; True marks count: 12564712; Edges construction time: 10; --------------------------------------------------------------------------------; Distinct junctions = 1275494. for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000 ; Bitarray 1252655360 bits (100.00 %) (array + ranks ); final hash 0 bits (0.00 %) (nb in final hash 0); ```. And these files are present in the index folder:; ```; ls -1 /no_backup/indexes/salmon/mm10_gencode; complete_ref_lens.bin; ctable.bin; ctg_offsets.bin; duplicate_clusters.tsv; info.json; mphf.bin; pos.bin; pre_indexing.log; rank.bin; refAccumLengths.bin; ref_indexing.log; reflengths.bin; refseq.bin; seq.bin; versionInfo.json; ```. So the problem was that the transcript file I provided to the `generateDecoyTranscriptome.sh` was gzipped and failed with `cat`.... 🤦‍♂️. Thanks a lot the help!; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:6314,Security,hash,hash,6314,"og] [info] chunk 5 = [185,776,605, 222,931,953); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 6 = [222,931,953, 260,087,274); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 7 = [260,087,274, 297,242,536); [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] finished populating pos vector; [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] writing index components; [2021-12-31 11:28:59.670] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2021-12-31 11:28:59.944] [jLog] [info] done building index; Threads = 8; Vertex length = 29; Hash functions = 5; Filter size = 4294967296; Capacity = 2; Files: ; /no_backup/indexes/salmon/mm10_gencode/ref_k29_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:4294967296; Pass	Filling	Filtering; 1	22	34	; 2	9	0; True junctions count = 1275494; False junctions count = 1606379; Hash table size = 2881873; Candidate marks count = 14783512; --------------------------------------------------------------------------------; Reallocating bifurcations time: 0; True marks count: 12564712; Edges construction time: 10; --------------------------------------------------------------------------------; Distinct junctions = 1275494. for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000 ; Bitarray 1252655360 bits (100.00 %) (array + ranks ); final hash 0 bits (0.00 %) (nb in final hash 0); ```. And these files are present in the index folder:; ```; ls -1 /no_backup/indexes/salmon/mm10_gencode; complete_ref_lens.bin; ctable.bin; ctg_offsets.bin; duplicate_clusters.tsv; info.json; mphf.bin; pos.bin; pre_indexing.log; rank.bin; refAccumLengths.bin; ref_indexing.log; reflengths.bin; refseq.bin; seq.bin; versionInfo.json; ```. So the problem was that the transcript file I provided to the `generateDecoyTranscriptome.sh` was gzipped and failed with `cat`.... 🤦‍♂️. Thanks a lot the help!; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:6348,Security,hash,hash,6348,"og] [info] chunk 5 = [185,776,605, 222,931,953); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 6 = [222,931,953, 260,087,274); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 7 = [260,087,274, 297,242,536); [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] finished populating pos vector; [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] writing index components; [2021-12-31 11:28:59.670] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2021-12-31 11:28:59.944] [jLog] [info] done building index; Threads = 8; Vertex length = 29; Hash functions = 5; Filter size = 4294967296; Capacity = 2; Files: ; /no_backup/indexes/salmon/mm10_gencode/ref_k29_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:4294967296; Pass	Filling	Filtering; 1	22	34	; 2	9	0; True junctions count = 1275494; False junctions count = 1606379; Hash table size = 2881873; Candidate marks count = 14783512; --------------------------------------------------------------------------------; Reallocating bifurcations time: 0; True marks count: 12564712; Edges construction time: 10; --------------------------------------------------------------------------------; Distinct junctions = 1275494. for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000 ; Bitarray 1252655360 bits (100.00 %) (array + ranks ); final hash 0 bits (0.00 %) (nb in final hash 0); ```. And these files are present in the index folder:; ```; ls -1 /no_backup/indexes/salmon/mm10_gencode; complete_ref_lens.bin; ctable.bin; ctg_offsets.bin; duplicate_clusters.tsv; info.json; mphf.bin; pos.bin; pre_indexing.log; rank.bin; refAccumLengths.bin; ref_indexing.log; reflengths.bin; refseq.bin; seq.bin; versionInfo.json; ```. So the problem was that the transcript file I provided to the `generateDecoyTranscriptome.sh` was gzipped and failed with `cat`.... 🤦‍♂️. Thanks a lot the help!; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:6582,Testability,log,log,6582,"og] [info] chunk 5 = [185,776,605, 222,931,953); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 6 = [222,931,953, 260,087,274); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 7 = [260,087,274, 297,242,536); [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] finished populating pos vector; [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] writing index components; [2021-12-31 11:28:59.670] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2021-12-31 11:28:59.944] [jLog] [info] done building index; Threads = 8; Vertex length = 29; Hash functions = 5; Filter size = 4294967296; Capacity = 2; Files: ; /no_backup/indexes/salmon/mm10_gencode/ref_k29_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:4294967296; Pass	Filling	Filtering; 1	22	34	; 2	9	0; True junctions count = 1275494; False junctions count = 1606379; Hash table size = 2881873; Candidate marks count = 14783512; --------------------------------------------------------------------------------; Reallocating bifurcations time: 0; True marks count: 12564712; Edges construction time: 10; --------------------------------------------------------------------------------; Distinct junctions = 1275494. for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000 ; Bitarray 1252655360 bits (100.00 %) (array + ranks ); final hash 0 bits (0.00 %) (nb in final hash 0); ```. And these files are present in the index folder:; ```; ls -1 /no_backup/indexes/salmon/mm10_gencode; complete_ref_lens.bin; ctable.bin; ctg_offsets.bin; duplicate_clusters.tsv; info.json; mphf.bin; pos.bin; pre_indexing.log; rank.bin; refAccumLengths.bin; ref_indexing.log; reflengths.bin; refseq.bin; seq.bin; versionInfo.json; ```. So the problem was that the transcript file I provided to the `generateDecoyTranscriptome.sh` was gzipped and failed with `cat`.... 🤦‍♂️. Thanks a lot the help!; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883:6631,Testability,log,log,6631,"og] [info] chunk 5 = [185,776,605, 222,931,953); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 6 = [222,931,953, 260,087,274); [2021-12-31 11:28:47.220] [puff::index::jointLog] [info] chunk 7 = [260,087,274, 297,242,536); [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] finished populating pos vector; [2021-12-31 11:28:57.275] [puff::index::jointLog] [info] writing index components; [2021-12-31 11:28:59.670] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2021-12-31 11:28:59.944] [jLog] [info] done building index; Threads = 8; Vertex length = 29; Hash functions = 5; Filter size = 4294967296; Capacity = 2; Files: ; /no_backup/indexes/salmon/mm10_gencode/ref_k29_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:4294967296; Pass	Filling	Filtering; 1	22	34	; 2	9	0; True junctions count = 1275494; False junctions count = 1606379; Hash table size = 2881873; Candidate marks count = 14783512; --------------------------------------------------------------------------------; Reallocating bifurcations time: 0; True marks count: 12564712; Edges construction time: 10; --------------------------------------------------------------------------------; Distinct junctions = 1275494. for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000 ; Bitarray 1252655360 bits (100.00 %) (array + ranks ); final hash 0 bits (0.00 %) (nb in final hash 0); ```. And these files are present in the index folder:; ```; ls -1 /no_backup/indexes/salmon/mm10_gencode; complete_ref_lens.bin; ctable.bin; ctg_offsets.bin; duplicate_clusters.tsv; info.json; mphf.bin; pos.bin; pre_indexing.log; rank.bin; refAccumLengths.bin; ref_indexing.log; reflengths.bin; refseq.bin; seq.bin; versionInfo.json; ```. So the problem was that the transcript file I provided to the `generateDecoyTranscriptome.sh` was gzipped and failed with `cat`.... 🤦‍♂️. Thanks a lot the help!; Nicco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/731#issuecomment-1003342883
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013326966:317,Integrability,protocol,protocol,317,"Great suggestion, thanks @rob-p and @gmarcais . Somehow, I missed it. I added it in the latest commit. Speed now from 3 runs:; ```; real 1m19.884s 1m15.891s 1m21.462s ; user 8m9.189s 9m1.100s 9m48.764s ; sys 0m5.079s 0m5.170s 0m3.477s; ```; 50% improvement over the past results, i.e., about 33% slower than specific protocol flag now. Although, ideally I should have ran the earlier tests thrice but the sd is small so results should be valid. Nonetheless, I'll do more speed tests with versions in the future. . Let me know what other thoughts you have and what else have I missed. I have some minor improvements in mind too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013326966
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013326966:384,Testability,test,tests,384,"Great suggestion, thanks @rob-p and @gmarcais . Somehow, I missed it. I added it in the latest commit. Speed now from 3 runs:; ```; real 1m19.884s 1m15.891s 1m21.462s ; user 8m9.189s 9m1.100s 9m48.764s ; sys 0m5.079s 0m5.170s 0m3.477s; ```; 50% improvement over the past results, i.e., about 33% slower than specific protocol flag now. Although, ideally I should have ran the earlier tests thrice but the sd is small so results should be valid. Nonetheless, I'll do more speed tests with versions in the future. . Let me know what other thoughts you have and what else have I missed. I have some minor improvements in mind too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013326966
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013326966:477,Testability,test,tests,477,"Great suggestion, thanks @rob-p and @gmarcais . Somehow, I missed it. I added it in the latest commit. Speed now from 3 runs:; ```; real 1m19.884s 1m15.891s 1m21.462s ; user 8m9.189s 9m1.100s 9m48.764s ; sys 0m5.079s 0m5.170s 0m3.477s; ```; 50% improvement over the past results, i.e., about 33% slower than specific protocol flag now. Although, ideally I should have ran the earlier tests thrice but the sd is small so results should be valid. Nonetheless, I'll do more speed tests with versions in the future. . Let me know what other thoughts you have and what else have I missed. I have some minor improvements in mind too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013326966
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013343046:136,Testability,test,tested,136,"Does it make sense to have '^' and/or '$' around the regex? Having anchors usually speeds up a regex. Otherwise, I am not sure. Have we tested other libraries than boost::regexp?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013343046
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013348732:44,Testability,test,test,44,"I can try '^' and '$' after lunch. I didn't test any other library, since boost was already a pre-requisite for salmon and my focus was on getting it to work first. But now that it is done, other libraries can be tried. However, at this moment, I'm not clear about the effort and speed-up ratio.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013348732
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013348732:253,Usability,clear,clear,253,"I can try '^' and '$' after lunch. I didn't test any other library, since boost was already a pre-requisite for salmon and my focus was on getting it to work first. But now that it is done, other libraries can be tried. However, at this moment, I'm not clear about the effort and speed-up ratio.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013348732
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013381545:95,Integrability,protocol,protocol,95,"Including '^' didn't change the speed, and '$' can't be added as there are extra bases (not in protocol spec) at end which can vary in number.; ```; real 1m19.392s; user 9m38.357s; sys 0m4.176s; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013381545
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023219391:99,Testability,test,tested,99,"Hi @rob-p! Thanks for all the great suggestions and comments. I have addressed all of them. I also tested the speed after the changes. The test was done 3 times in the same way as done earlier. . 1. `--sciseq3` ; ```; real 0m58.463s 0m57.884s 0m57.413s; user 7m0.652s 7m1.731s 7m1.278s; sys 0m3.305s 0m3.078s 0m2.665s; ```. 2. `--custom-geo`; ```; real 1m7.411s 1m14.988s 1m3.868s; user 8m8.795s 8m40.302s 7m49.107s; sys 0m4.194s 0m6.412s 0m2.969s; ```. The real time in case 1. was 57.92 ± 0.57s and for case 2. it was 68.75 ± 5.68s, which is about 19% slower.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023219391
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023219391:139,Testability,test,test,139,"Hi @rob-p! Thanks for all the great suggestions and comments. I have addressed all of them. I also tested the speed after the changes. The test was done 3 times in the same way as done earlier. . 1. `--sciseq3` ; ```; real 0m58.463s 0m57.884s 0m57.413s; user 7m0.652s 7m1.731s 7m1.278s; sys 0m3.305s 0m3.078s 0m2.665s; ```. 2. `--custom-geo`; ```; real 1m7.411s 1m14.988s 1m3.868s; user 8m8.795s 8m40.302s 7m49.107s; sys 0m4.194s 0m6.412s 0m2.969s; ```. The real time in case 1. was 57.92 ± 0.57s and for case 2. it was 68.75 ± 5.68s, which is about 19% slower.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023219391
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721:12,Performance,perform,performance,12,"1/3 loss in performance seems significant, given that presumably the code does something else than just parsing UMIs. I am looking at Boost own comparison and benchmarks, and on long inputs (20MB) it is competitive with PCRE2. But with short inputs (20-30 characters) PCRE2 is consistently faster (by about 30% :thinking: ). And if PCRE2 is feature full, not sure it is the fastest either, especially for simple regexp.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721:159,Testability,benchmark,benchmarks,159,"1/3 loss in performance seems significant, given that presumably the code does something else than just parsing UMIs. I am looking at Boost own comparison and benchmarks, and on long inputs (20MB) it is competitive with PCRE2. But with short inputs (20-30 characters) PCRE2 is consistently faster (by about 30% :thinking: ). And if PCRE2 is feature full, not sure it is the fastest either, especially for simple regexp.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721:405,Usability,simpl,simple,405,"1/3 loss in performance seems significant, given that presumably the code does something else than just parsing UMIs. I am looking at Boost own comparison and benchmarks, and on long inputs (20MB) it is competitive with PCRE2. But with short inputs (20-30 characters) PCRE2 is consistently faster (by about 30% :thinking: ). And if PCRE2 is feature full, not sure it is the fastest either, especially for simple regexp.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023346183:105,Integrability,protocol,protocol,105,"@Gaura So with the changes implemented, custom geometry is ~19% slower here than the hand-coded sci-seq3 protocol (improved from ~1/3 slower); is that correct? That's a nice improvement. @gmarcais — do you think it's worth testing out PCRE2? Most of these regexes are *very* short — and if boost is ~20% slower than PCRE2 and we are ~20% slower than the custom parsing code .... maybe that's the whole gap? Any idea how difficult this would be to try?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023346183
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023346183:223,Testability,test,testing,223,"@Gaura So with the changes implemented, custom geometry is ~19% slower here than the hand-coded sci-seq3 protocol (improved from ~1/3 slower); is that correct? That's a nice improvement. @gmarcais — do you think it's worth testing out PCRE2? Most of these regexes are *very* short — and if boost is ~20% slower than PCRE2 and we are ~20% slower than the custom parsing code .... maybe that's the whole gap? Any idea how difficult this would be to try?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023346183
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023351185:98,Testability,test,testing,98,"Yes, @rob-p! It's an improvement from ~33% to ~19% slower (I'm not sure why sd is high for custom testing though). Yeah, I'm eager to hear your views too, @gmarcais.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023351185
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023366519:111,Integrability,wrap,wrapper,111,"I don't know how difficult it would be to use PCRE2, I have never used it directly. Can we write a thin struct wrapper structure that is defined as the boost object by default, but with` #ifdef ` can be implemented by PCRE2 if we wish? (well, if cmake finds pcre2). Keeping with boost, we should also try https://www.boost.org/doc/libs/1_78_0/doc/html/xpressive.html . That does not add a new dependency. Don't know about the speed.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023366519
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023366519:393,Integrability,depend,dependency,393,"I don't know how difficult it would be to use PCRE2, I have never used it directly. Can we write a thin struct wrapper structure that is defined as the boost object by default, but with` #ifdef ` can be implemented by PCRE2 if we wish? (well, if cmake finds pcre2). Keeping with boost, we should also try https://www.boost.org/doc/libs/1_78_0/doc/html/xpressive.html . That does not add a new dependency. Don't know about the speed.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023366519
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1024181953:10,Testability,test,testing,10,"My little testing: https://github.com/OceanGenomics/RegexBench. Some results on matching 1 million short strings with ~90% positive match and ~10% random strings:; ```; time-manual:	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.06; time-boostregex:	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.28; time-pcre2:	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.23; time-retwo:	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.63; time-boostxpressive:	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.76; time-grep:	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.17; ```. retwo is RE2. It is surprisingly highly affected by the number of captures. With 4 captures as above, it is the slowest. Without any it is as fast as grep. And xpressive is very slow, while I expected it to be the fastest!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1024181953
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1024188974:103,Energy Efficiency,allocate,allocated,103,"Very interesting @gmarcais. I wonder if/how allocations have an effect here. Are all methods using pre-allocated space to store their captures? Also, is xpressive drunk?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1024188974
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1024206420:287,Performance,optimiz,optimize,287,"I explicitly preallocate the output array/vector for pcre and re2. Boost regex doesn't seem to offer that (at least, I don't know). Regarding xpressive: yeah, what a disappointment. And I don't actually save the capture with xpressive. I thought the automaton was entirely generated and optimize at compile time. Apparently creating an automaton with C++ template system must be really hard because the generated code is garbage. Or I am using it wrong. In any case, xpressive as I use it is entirely static (I haven't tested the dynamic version). So it is not useful in our case. I was just curious if it could match hand crafted code. What was I thinking!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1024206420
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1024206420:519,Testability,test,tested,519,"I explicitly preallocate the output array/vector for pcre and re2. Boost regex doesn't seem to offer that (at least, I don't know). Regarding xpressive: yeah, what a disappointment. And I don't actually save the capture with xpressive. I thought the automaton was entirely generated and optimize at compile time. Apparently creating an automaton with C++ template system must be really hard because the generated code is garbage. Or I am using it wrong. In any case, xpressive as I use it is entirely static (I haven't tested the dynamic version). So it is not useful in our case. I was just curious if it could match hand crafted code. What was I thinking!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1024206420
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961:1049,Usability,simpl,simple,1049,"Hi all (@Gaura / @gmarcais),. While this has languished somewhat as we try to figure out what regex engine to include and how best to package it, I wanted to mention that I am attempting something similar in rust (which has a canonical regex crate which, I believe, is supposed to be among the fast ones). That repo is over on [seq_geom_xform](https://github.com/COMBINE-lab/seq_geom_xform) and it relies on [seq_geom_parser](https://github.com/COMBINE-lab/seq_geom_parser). I like @Gaura's geometry string specification, so we're going with that for time time being. If you want to chime in or start a discussion on either of those repos, please do let me know if you have any other thoughts on this generalized scheme. The purpose of the `seq_geom_xform` crate is actually that it will be *both* a rust library (to allow parsing complex geometry descriptions as a regex and extract the relevant sequence) *and* a stand-alone executable that can do streaming sequence transformation from a ""complex"" barcode geometry (e.g. the sciseq3 above) to a ""simple"" geometry (fixed position and fixed length barcode and UMI). Thus, one could imagine (at the cost of sticking a rust executable in the invocation here) replacing this feature by a streaming invocation to `seq_geom_xform` that would take the compressed fastq files as input along with the geometry specification, and which would output two streams (one for each read) with a simplified geometry that could be parsed in the simpler format. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961:1430,Usability,simpl,simplified,1430,"Hi all (@Gaura / @gmarcais),. While this has languished somewhat as we try to figure out what regex engine to include and how best to package it, I wanted to mention that I am attempting something similar in rust (which has a canonical regex crate which, I believe, is supposed to be among the fast ones). That repo is over on [seq_geom_xform](https://github.com/COMBINE-lab/seq_geom_xform) and it relies on [seq_geom_parser](https://github.com/COMBINE-lab/seq_geom_parser). I like @Gaura's geometry string specification, so we're going with that for time time being. If you want to chime in or start a discussion on either of those repos, please do let me know if you have any other thoughts on this generalized scheme. The purpose of the `seq_geom_xform` crate is actually that it will be *both* a rust library (to allow parsing complex geometry descriptions as a regex and extract the relevant sequence) *and* a stand-alone executable that can do streaming sequence transformation from a ""complex"" barcode geometry (e.g. the sciseq3 above) to a ""simple"" geometry (fixed position and fixed length barcode and UMI). Thus, one could imagine (at the cost of sticking a rust executable in the invocation here) replacing this feature by a streaming invocation to `seq_geom_xform` that would take the compressed fastq files as input along with the geometry specification, and which would output two streams (one for each read) with a simplified geometry that could be parsed in the simpler format. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961:1478,Usability,simpl,simpler,1478,"Hi all (@Gaura / @gmarcais),. While this has languished somewhat as we try to figure out what regex engine to include and how best to package it, I wanted to mention that I am attempting something similar in rust (which has a canonical regex crate which, I believe, is supposed to be among the fast ones). That repo is over on [seq_geom_xform](https://github.com/COMBINE-lab/seq_geom_xform) and it relies on [seq_geom_parser](https://github.com/COMBINE-lab/seq_geom_parser). I like @Gaura's geometry string specification, so we're going with that for time time being. If you want to chime in or start a discussion on either of those repos, please do let me know if you have any other thoughts on this generalized scheme. The purpose of the `seq_geom_xform` crate is actually that it will be *both* a rust library (to allow parsing complex geometry descriptions as a regex and extract the relevant sequence) *and* a stand-alone executable that can do streaming sequence transformation from a ""complex"" barcode geometry (e.g. the sciseq3 above) to a ""simple"" geometry (fixed position and fixed length barcode and UMI). Thus, one could imagine (at the cost of sticking a rust executable in the invocation here) replacing this feature by a streaming invocation to `seq_geom_xform` that would take the compressed fastq files as input along with the geometry specification, and which would output two streams (one for each read) with a simplified geometry that could be parsed in the simpler format. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1460840281:34,Deployability,update,update,34,"Hi @Gaura and @gmarcais,. Just an update, this approach currently seems to be working. We can stream sciseq3 data through our `seq_geom_xform` tool to convert it to a standard ""normalized"" geometry, and the quantify using `piscem` + `alevin-fry` and get highly-concordant results to what we get with the builtin `--sciseq3` flag! In case you are interested, the current grammar that is supported is described [here](https://hackmd.io/kfRWvfjQTua42PxD_v5Vzg). The syntax is that of the [pest](pest.rs) parser. In fact, if you copy that grammar into [https://pest.rs/#editor](https://pest.rs/#editor) you can try out geometry strings live and see how it's parsed!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1460840281
https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606:57,Availability,error,error,57,"Hi @seanken,. Thank you for reporting this. I agree this error message should always show up. My guess is that this is related to the fact that the error is reported through the asynchronous logger, which is notoriously picky about how it must be torn down to avoid dropping messages on atypical (non-zero) program exit. I'll see if I can make this one show up reliably. By the way, do you have a small pair of FASTQ files that will trigger this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606
https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606:148,Availability,error,error,148,"Hi @seanken,. Thank you for reporting this. I agree this error message should always show up. My guess is that this is related to the fact that the error is reported through the asynchronous logger, which is notoriously picky about how it must be torn down to avoid dropping messages on atypical (non-zero) program exit. I'll see if I can make this one show up reliably. By the way, do you have a small pair of FASTQ files that will trigger this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606
https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606:252,Availability,down,down,252,"Hi @seanken,. Thank you for reporting this. I agree this error message should always show up. My guess is that this is related to the fact that the error is reported through the asynchronous logger, which is notoriously picky about how it must be torn down to avoid dropping messages on atypical (non-zero) program exit. I'll see if I can make this one show up reliably. By the way, do you have a small pair of FASTQ files that will trigger this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606
https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606:361,Availability,reliab,reliably,361,"Hi @seanken,. Thank you for reporting this. I agree this error message should always show up. My guess is that this is related to the fact that the error is reported through the asynchronous logger, which is notoriously picky about how it must be torn down to avoid dropping messages on atypical (non-zero) program exit. I'll see if I can make this one show up reliably. By the way, do you have a small pair of FASTQ files that will trigger this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606
https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606:446,Availability,error,error,446,"Hi @seanken,. Thank you for reporting this. I agree this error message should always show up. My guess is that this is related to the fact that the error is reported through the asynchronous logger, which is notoriously picky about how it must be torn down to avoid dropping messages on atypical (non-zero) program exit. I'll see if I can make this one show up reliably. By the way, do you have a small pair of FASTQ files that will trigger this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606
https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606:63,Integrability,message,message,63,"Hi @seanken,. Thank you for reporting this. I agree this error message should always show up. My guess is that this is related to the fact that the error is reported through the asynchronous logger, which is notoriously picky about how it must be torn down to avoid dropping messages on atypical (non-zero) program exit. I'll see if I can make this one show up reliably. By the way, do you have a small pair of FASTQ files that will trigger this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606
https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606:275,Integrability,message,messages,275,"Hi @seanken,. Thank you for reporting this. I agree this error message should always show up. My guess is that this is related to the fact that the error is reported through the asynchronous logger, which is notoriously picky about how it must be torn down to avoid dropping messages on atypical (non-zero) program exit. I'll see if I can make this one show up reliably. By the way, do you have a small pair of FASTQ files that will trigger this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606
https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606:260,Safety,avoid,avoid,260,"Hi @seanken,. Thank you for reporting this. I agree this error message should always show up. My guess is that this is related to the fact that the error is reported through the asynchronous logger, which is notoriously picky about how it must be torn down to avoid dropping messages on atypical (non-zero) program exit. I'll see if I can make this one show up reliably. By the way, do you have a small pair of FASTQ files that will trigger this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606
https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606:191,Testability,log,logger,191,"Hi @seanken,. Thank you for reporting this. I agree this error message should always show up. My guess is that this is related to the fact that the error is reported through the asynchronous logger, which is notoriously picky about how it must be torn down to avoid dropping messages on atypical (non-zero) program exit. I'll see if I can make this one show up reliably. By the way, do you have a small pair of FASTQ files that will trigger this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/736#issuecomment-1018119606
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1019983461:676,Security,validat,validateMappings,676,"> Hi @amitpande74 ,; > ; > Please check the path of the files like `V300012057_L3_HK500HUMpybEAAKRAAPEI-530_2.fq` since you provided the full path of other files like `/media/amit/Amit/Usr/DNA12/fastq/V300012057_L3_HK500HUMpybEAAKRAAPEI-530_1.fq`. @k3yavi it should be like this ?. `./bin/salmon quant -i /media/amit/Amit/Usr/new_salmon_index/ -l IU -1 /media/amit/Amit/Usr/DNA12/fastq/V300012057_L3_HK500HUMpybEAAKRAAPEI-530_1.fq /media/amit/Amit/Usr/DNA12/fastq V300012057_L4_HK500HUMpybEAAKRAAPEI-530_1.fq -2 /media/amit/Amit/Usr/DNA12/fastq/V300012057_L3_HK500HUMpybEAAKRAAPEI-530_2.fq /media/amit/Amit/Usr/DNA12/fastq /V300012057_L4_HK500HUMpybEAAKRAAPEI-530_2.fq -p 8 --validateMappings -o /media/amit/Amit/Usr/DNA12/fastq/DNA12.quant`. Regards.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1019983461
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1021830675:547,Security,validat,validateMappings,547,"Hi @amitpande74,. Almost, but I think you are missing a `/` or so. Assuming uniform paths for the input, I think the command should look like:. ```{bash}. ./bin/salmon quant -i /media/amit/Amit/Usr/new_salmon_index/ -l IU -1 /media/amit/Amit/Usr/DNA12/fastq/V300012057_L3_HK500HUMpybEAAKRAAPEI-530_1.fq /media/amit/Amit/Usr/DNA12/fastq/V300012057_L4_HK500HUMpybEAAKRAAPEI-530_1.fq -2 /media/amit/Amit/Usr/DNA12/fastq/V300012057_L3_HK500HUMpybEAAKRAAPEI-530_2.fq /media/amit/Amit/Usr/DNA12/fastq/V300012057_L4_HK500HUMpybEAAKRAAPEI-530_2.fq -p 8 --validateMappings -o /media/amit/Amit/Usr/DNA12/fastq/DNA12.quant. ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1021830675
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973:581,Availability,mask,masked,581,"Thanks a lot @rob-p and @k3yavi .; I wanted your advise on something more intricate. Asking since you people are the pioneers in this aspect of problem solving.; I have to determine the expression of GFP and Transposon sequence in the transcriptome.; I read the material posted on the link https://salmon.readthedocs.io/en/latest/salmon.html.; It instructs this to be done in 2 different ways-; There are two options for generating a decoy-aware transcriptome:. The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README. The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index. I tried the 2nd approach. Combined the GFP,Transposon and the genome FASTA files, indexed it , constructed the decoy according to the given instructions given here https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/.; When I ran Salmon (version 1.2.1_linux_x86_64) it did not report anything in the quant files (I know that these samples have high GFP and Transposon expression in these samples). The 1st approach is giving me problems to the construction of the GTF file and then memory usage. The instructions say - generateDecoyTranscriptome.sh — Located in the scripts/ directory, this is a preprocessing script for creating augmented hybrid fasta file for salmon index. It cons",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973:1156,Availability,avail,available,1156,"n this aspect of problem solving.; I have to determine the expression of GFP and Transposon sequence in the transcriptome.; I read the material posted on the link https://salmon.readthedocs.io/en/latest/salmon.html.; It instructs this to be done in 2 different ways-; There are two options for generating a decoy-aware transcriptome:. The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README. The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index. I tried the 2nd approach. Combined the GFP,Transposon and the genome FASTA files, indexed it , constructed the decoy according to the given instructions given here https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/.; When I ran Salmon (version 1.2.1_linux_x86_64) it did not report anything in the quant files (I know that these samples have high GFP and Transposon expression in these samples). The 1st approach is giving me problems to the construction of the GTF file and then memory usage. The instructions say - generateDecoyTranscriptome.sh — Located in the scripts/ directory, this is a preprocessing script for creating augmented hybrid fasta file for salmon index. It consumes genome fasta (one file given through -g), transcriptome fasta (-t) and the annotation (GTF file given through -a) to creat",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973:679,Usability,simpl,simple,679,"Thanks a lot @rob-p and @k3yavi .; I wanted your advise on something more intricate. Asking since you people are the pioneers in this aspect of problem solving.; I have to determine the expression of GFP and Transposon sequence in the transcriptome.; I read the material posted on the link https://salmon.readthedocs.io/en/latest/salmon.html.; It instructs this to be done in 2 different ways-; There are two options for generating a decoy-aware transcriptome:. The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README. The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index. I tried the 2nd approach. Combined the GFP,Transposon and the genome FASTA files, indexed it , constructed the decoy according to the given instructions given here https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/.; When I ran Salmon (version 1.2.1_linux_x86_64) it did not report anything in the quant files (I know that these samples have high GFP and Transposon expression in these samples). The 1st approach is giving me problems to the construction of the GTF file and then memory usage. The instructions say - generateDecoyTranscriptome.sh — Located in the scripts/ directory, this is a preprocessing script for creating augmented hybrid fasta file for salmon index. It cons",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973:705,Usability,simpl,simplify,705,"Thanks a lot @rob-p and @k3yavi .; I wanted your advise on something more intricate. Asking since you people are the pioneers in this aspect of problem solving.; I have to determine the expression of GFP and Transposon sequence in the transcriptome.; I read the material posted on the link https://salmon.readthedocs.io/en/latest/salmon.html.; It instructs this to be done in 2 different ways-; There are two options for generating a decoy-aware transcriptome:. The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README. The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index. I tried the 2nd approach. Combined the GFP,Transposon and the genome FASTA files, indexed it , constructed the decoy according to the given instructions given here https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/.; When I ran Salmon (version 1.2.1_linux_x86_64) it did not report anything in the quant files (I know that these samples have high GFP and Transposon expression in these samples). The 1st approach is giving me problems to the construction of the GTF file and then memory usage. The instructions say - generateDecoyTranscriptome.sh — Located in the scripts/ directory, this is a preprocessing script for creating augmented hybrid fasta file for salmon index. It cons",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1024668469:387,Deployability,pipeline,pipelines,387,"Hi @amitpande74 ,. May be you are already aware of this but just to make it clear, the idea behind decoy indexing is to ""exclude"" those sequences from the transcriptome quantification and that's why you don't see the indexed transposon sequence in the salmon output. The motivation behind such indexing is to remove false mapping reads i.e. exclude RNA-seq reads from the quantification pipelines which maps better to the decoy sequences compared to the provided transcriptome. . I hope that clarifies your doubt and if you wan't to quantify GFP sequences then you have to concatenate them with the transcriptome sequence ""not"" the decoy sequence, although I just wanted to give you heads up that this analysis goes into an unexplored territory as we personally have not explored such use cases. Unless Rob have more thoughts on it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1024668469
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1024668469:76,Usability,clear,clear,76,"Hi @amitpande74 ,. May be you are already aware of this but just to make it clear, the idea behind decoy indexing is to ""exclude"" those sequences from the transcriptome quantification and that's why you don't see the indexed transposon sequence in the salmon output. The motivation behind such indexing is to remove false mapping reads i.e. exclude RNA-seq reads from the quantification pipelines which maps better to the decoy sequences compared to the provided transcriptome. . I hope that clarifies your doubt and if you wan't to quantify GFP sequences then you have to concatenate them with the transcriptome sequence ""not"" the decoy sequence, although I just wanted to give you heads up that this analysis goes into an unexplored territory as we personally have not explored such use cases. Unless Rob have more thoughts on it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1024668469
https://github.com/COMBINE-lab/salmon/issues/738#issuecomment-1019515038:242,Usability,clear,clear,242,"@k3yavi — might it be worthwhile exploring the effect of changing the min score fraction here, or enabling softclipping? I do recall that this seems in the ballpark of drop-seq data mapping to the annotated (spliced) transcriptome, but is it clear _why_ the mapping rates for this technology are so low?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/738#issuecomment-1019515038
https://github.com/COMBINE-lab/salmon/issues/738#issuecomment-1024672260:320,Deployability,pipeline,pipeline,320,"Unfortunately I don't have much thoughts about the low mapping rates of Drop-seq technology. ; Although, it doesn't necessarily have to be associated with the mapping rate, from what I remember a lot CB are more than one-edit distance away from the high quality barcodes which are excluded from the mapping phase of the pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/738#issuecomment-1024672260
https://github.com/COMBINE-lab/salmon/issues/738#issuecomment-1027231550:188,Security,validat,validateMappings,188,"I ran salmon alevin with the additional ``--softclip`` flag as @rob-p suggested. The mapping rate was almost identical compared to results without that flag. . As I didn't include the ``--validateMappings`` flag, changing the ``--minScoreFraction`` wouldn't matter (if I understand the manual correctly). . If these mapping rates are what you have seen for similar data, that is fine for me.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/738#issuecomment-1027231550
https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1019516869:23,Testability,log,log,23,Can you share the full log @tmms1 ?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1019516869
https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628:34,Testability,log,log,34,"Hi @k3yavi . I have added all the log files I could find of one sample. If you need anything else, please let me know. [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/7943404/salmon_quant.log); [alevin.log](https://github.com/COMBINE-lab/salmon/files/7943405/alevin.log). Thanks in advance for looking into this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628
https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628:133,Testability,log,log,133,"Hi @k3yavi . I have added all the log files I could find of one sample. If you need anything else, please let me know. [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/7943404/salmon_quant.log); [alevin.log](https://github.com/COMBINE-lab/salmon/files/7943405/alevin.log). Thanks in advance for looking into this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628
https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628:203,Testability,log,log,203,"Hi @k3yavi . I have added all the log files I could find of one sample. If you need anything else, please let me know. [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/7943404/salmon_quant.log); [alevin.log](https://github.com/COMBINE-lab/salmon/files/7943405/alevin.log). Thanks in advance for looking into this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628
https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628:217,Testability,log,log,217,"Hi @k3yavi . I have added all the log files I could find of one sample. If you need anything else, please let me know. [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/7943404/salmon_quant.log); [alevin.log](https://github.com/COMBINE-lab/salmon/files/7943405/alevin.log). Thanks in advance for looking into this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628
https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628:281,Testability,log,log,281,"Hi @k3yavi . I have added all the log files I could find of one sample. If you need anything else, please let me know. [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/7943404/salmon_quant.log); [alevin.log](https://github.com/COMBINE-lab/salmon/files/7943405/alevin.log). Thanks in advance for looking into this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022373628
https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022395566:88,Availability,down,downstream,88,Ah so what's happening is alevin is keeping around 500 low confidence CB for performing downstream whitelisting.; `[2021-12-10 15:28:09.434] [alevinLog] [info] Total 1501(has 500 low confidence) barcodes`,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022395566
https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022395566:77,Performance,perform,performing,77,Ah so what's happening is alevin is keeping around 500 low confidence CB for performing downstream whitelisting.; `[2021-12-10 15:28:09.434] [alevinLog] [info] Total 1501(has 500 low confidence) barcodes`,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1022395566
https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1024658484:43,Deployability,pipeline,pipeline,43,"Right, so at the end of the quantification pipeline you should have the file `whitelist.txt` which you can use as a high confidence barcodes and filter the full matrix file.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/739#issuecomment-1024658484
https://github.com/COMBINE-lab/salmon/issues/740#issuecomment-1139144572:434,Testability,benchmark,benchmark,434,"You do not need a different transcriptome for each readset. Your general workflow should be... 1. prepare a transcriptome in fasta format; 2. For each sample, align that sample to the transcriptome to create a bam file using hisat2 and samtools; 3. For each bam file, run salmon quant using that bam file and the transcriptome as input; 4. merge output using salmon quantmerge; . ...I'm curious why you are using hisat2 though. Every benchmark I've seen suggests that just using salmons psuedo-mapper works just as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/740#issuecomment-1139144572
https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1024655023:214,Testability,log,log,214,"Hi @bzmby ,. I am sure you are aware of this but just wanted to clear that salmon is primarily designed for transcriptome quantification.; Ideally, there should not be a problem with indexing genome, also from the log you shared it looks like a warning. ; Having said that if you will index the genome then at the end of the day you will get quantification of the chromosomes, is that what you wan't ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1024655023
https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1024655023:64,Usability,clear,clear,64,"Hi @bzmby ,. I am sure you are aware of this but just wanted to clear that salmon is primarily designed for transcriptome quantification.; Ideally, there should not be a problem with indexing genome, also from the log you shared it looks like a warning. ; Having said that if you will index the genome then at the end of the day you will get quantification of the chromosomes, is that what you wan't ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1024655023
https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1138617086:72,Deployability,update,updates,72,"Closing this for lack of activity, but feel free to reopen if there are updates.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1138617086
https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1098426758:1189,Testability,test,test,1189,"Hi @taylorreiter,. First thing first — this sat for way too long before I got to it, so apologies for that!. So, the reason that they don’t show up in the unmapped.txt file is that is that reads mapped to decoys occupy a sort of “no man’s land” with respect to their mapping status. That is, they *do* map to the index, but just not to a valid target within the index. In other words, if you write a BAM output from `salmon`, the decoy aligned reads will actually show up there, with the information about the decoy to which they are aligned. This is because they are mapped to something in the index, it just happens to be a decoy rather than a “valid target”. However, the output BAM files are big, so I absolutely understand the desire to have them appear in the unmapped names list as well — it's a much smaller and easier thing to go through. I think the right thing to handle this would be to add a specific code/category to the set of unmapped codes used in the `unmapped.txt` file, to designate this is read best mapped to a decoy (rather than that this read is completely unmapped to the index). This shouldn't be too hard to do — I will try to find a few cycles to implement and test it. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1098426758
https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1098429513:235,Availability,down,downstream,235,"Ah that makes sense, thank you for the explanation! Having a specific code/category to the set of unmapped codes used in the unmapped.txt file would be absolutely wonderful, and would give me all of the information I need to pursue my downstream use cases.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1098429513
https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1146205498:187,Deployability,release,release,187,"Hi @taylorreiter,. I was wrong — there was simply a bug that, in single end mode, everything was being written out with the `u` flag. This is now fixed in develop. It will be in the next release. Sorry about that!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1146205498
https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1146205498:43,Usability,simpl,simply,43,"Hi @taylorreiter,. I was wrong — there was simply a bug that, in single end mode, everything was being written out with the `u` flag. This is now fixed in develop. It will be in the next release. Sorry about that!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1146205498
https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1164760864:69,Deployability,release,released,69,"Hi @taylorreiter,. This should now be fixed in v1.9.0 which was just released 🎉 . Let us know if it works for you. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1164760864
https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742:64,Deployability,update,update,64,"Hi @yagam-fluent,. Thanks for the excellent question. We should update the documentation regarding this option. Basically, in `salmon alevin`, we assumptions about expected read orientation are applied as ""hard filters"". That is, the behavior is equivalent to `--incompatPrior 0`, so that aligninments not in the prescribed orientation are simply not considered as invalid alignments. This is because in the case of single-cell processing, we (the community in general) currently do not have as sophisticated of probabilistic models for resolving UMI origins and gene abundances, and so algorithms typically do not take into account a ""wrong orientation"" probability. So, in `salmon alevin` if you are using alevin itself for the quantification, then hard filtering will be applied based on the expectations of `--libType`. On the other hand, if you are using `salmon alevin` to simply map the reads for subsequent processing with [`alevin-fry`](https://github.com/COMBINE-lab/alevin-fry) (i.e. `salmon alevin .... --rad` or `salmon alevin ... --sketch`), then *no* filtering is applied to mapping orientation, and instead you filter reads by orientation later in `alevin-fry`'s `generate-permit-list` step. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742
https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742:340,Usability,simpl,simply,340,"Hi @yagam-fluent,. Thanks for the excellent question. We should update the documentation regarding this option. Basically, in `salmon alevin`, we assumptions about expected read orientation are applied as ""hard filters"". That is, the behavior is equivalent to `--incompatPrior 0`, so that aligninments not in the prescribed orientation are simply not considered as invalid alignments. This is because in the case of single-cell processing, we (the community in general) currently do not have as sophisticated of probabilistic models for resolving UMI origins and gene abundances, and so algorithms typically do not take into account a ""wrong orientation"" probability. So, in `salmon alevin` if you are using alevin itself for the quantification, then hard filtering will be applied based on the expectations of `--libType`. On the other hand, if you are using `salmon alevin` to simply map the reads for subsequent processing with [`alevin-fry`](https://github.com/COMBINE-lab/alevin-fry) (i.e. `salmon alevin .... --rad` or `salmon alevin ... --sketch`), then *no* filtering is applied to mapping orientation, and instead you filter reads by orientation later in `alevin-fry`'s `generate-permit-list` step. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742
https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742:879,Usability,simpl,simply,879,"Hi @yagam-fluent,. Thanks for the excellent question. We should update the documentation regarding this option. Basically, in `salmon alevin`, we assumptions about expected read orientation are applied as ""hard filters"". That is, the behavior is equivalent to `--incompatPrior 0`, so that aligninments not in the prescribed orientation are simply not considered as invalid alignments. This is because in the case of single-cell processing, we (the community in general) currently do not have as sophisticated of probabilistic models for resolving UMI origins and gene abundances, and so algorithms typically do not take into account a ""wrong orientation"" probability. So, in `salmon alevin` if you are using alevin itself for the quantification, then hard filtering will be applied based on the expectations of `--libType`. On the other hand, if you are using `salmon alevin` to simply map the reads for subsequent processing with [`alevin-fry`](https://github.com/COMBINE-lab/alevin-fry) (i.e. `salmon alevin .... --rad` or `salmon alevin ... --sketch`), then *no* filtering is applied to mapping orientation, and instead you filter reads by orientation later in `alevin-fry`'s `generate-permit-list` step. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742
https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038381121:282,Usability,clear,clear,282,"Thanks Rob. I am using alevin with libType of ISR. When I looked at the SAM file after setting the writeMapping option, I observed that ~1% had the ""reverse alignment"" flag set to 1 (the only flags field that had this flag set was 341, all the rest had that bit set to 0). It's not clear to me whether alevin uses those reads for UMI counting, and if so, what's the best way to turn off this behavior.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038381121
https://github.com/COMBINE-lab/salmon/issues/753#issuecomment-1045150809:35,Availability,ping,ping,35,"Hi @sjroth,. This is strange. I'll ping @k3yavi here to see if he has any thoughts about what might be going on.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/753#issuecomment-1045150809
https://github.com/COMBINE-lab/salmon/issues/753#issuecomment-1045154961:41,Availability,error,error,41,"Hi @sjroth , I think it's a warning (not error) with unstranded library, can you share the file `salmon-out/lib_format_counts.json` ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/753#issuecomment-1045154961
https://github.com/COMBINE-lab/salmon/issues/754#issuecomment-1050469681:265,Energy Efficiency,reduce,reduce,265,"Hi @diyang1354,. To answer your questions as directly as possible:. 1.) Yes, you cannot align to a transcript of less than the k-mer length.; 2.) If you are processing bulk RNA-seq data, and therefore using `selective-alignment`, using a smaller `k` is unlikely to reduce your alignment accuracy, but it *can* increase the runtime of mapping if you make it too small. If you are processing single-cell RNA-seq data and using `--sketch` mode, then a reduction in the value of `k` can negatively impact accuracy. Finally, I'll mention that RNA-seq, in general, isn't a great assay for measuring very small molecules, so you probably want to be somewhat suspicious of the quantification results for *very short* transcripts anyway. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/754#issuecomment-1050469681
https://github.com/COMBINE-lab/salmon/issues/755#issuecomment-1050468212:18,Integrability,message,message,18,"Hi @dhy2002,. The message at the beginning is just a result of salmon not being able to complete the version check — that is not related to any issues building the index. What is at the end of the log file?. Also, I'll note that we've seen before some issues related to building the index directly on a network file system mounted partition — the tool we use for compacted de Bruijn graph construction, TwoPaCo, can create many small intermediate files that causes issues for NFS. If this is the problem, I might suggest building the index on the local scratch disk of a node, and then copying over the completed index when it's finished. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/755#issuecomment-1050468212
https://github.com/COMBINE-lab/salmon/issues/755#issuecomment-1050468212:197,Testability,log,log,197,"Hi @dhy2002,. The message at the beginning is just a result of salmon not being able to complete the version check — that is not related to any issues building the index. What is at the end of the log file?. Also, I'll note that we've seen before some issues related to building the index directly on a network file system mounted partition — the tool we use for compacted de Bruijn graph construction, TwoPaCo, can create many small intermediate files that causes issues for NFS. If this is the problem, I might suggest building the index on the local scratch disk of a node, and then copying over the completed index when it's finished. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/755#issuecomment-1050468212
https://github.com/COMBINE-lab/salmon/issues/756#issuecomment-1068650251:172,Modifiability,enhance,enhancement,172,"Hi George,. This will require some upstream changes to the SAM writing code, but I don't think it should be too hard. We could add this to the roadmap. I'll tag this as an enhancement. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/756#issuecomment-1068650251
https://github.com/COMBINE-lab/salmon/issues/756#issuecomment-1164760347:0,Deployability,Release,Released,0,Released in v1.9.0 🎉 ! Let us know if you have any questions about or trouble with the feature.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/756#issuecomment-1164760347
https://github.com/COMBINE-lab/salmon/issues/757#issuecomment-1068649915:94,Availability,ping,ping,94,"Hi @knokknok,. That's a good point. I don't know if there's a way to get at this easily. I'll ping @k3yavi here for his input. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/757#issuecomment-1068649915
https://github.com/COMBINE-lab/salmon/issues/757#issuecomment-1138604971:0,Availability,Ping,Pinging,0,Pinging @k3yavi here again and moving this over to a discussion where convo might better take place. This now lives [here](https://github.com/COMBINE-lab/salmon/discussions/780).,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/757#issuecomment-1138604971
https://github.com/COMBINE-lab/salmon/issues/760#issuecomment-1064350509:55,Deployability,release,release,55,"Alas, I'd just forgotten to do it :). That part of the release process is manual, and requires me grabbing files from the CI, changing the folder name to reflect the release, and uploading them to GitHub. One of these days I'll automate that, but for now it's a manual process. Just added the 1.8.0 binary tarball. Thanks for the reminder.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/760#issuecomment-1064350509
https://github.com/COMBINE-lab/salmon/issues/760#issuecomment-1064350509:166,Deployability,release,release,166,"Alas, I'd just forgotten to do it :). That part of the release process is manual, and requires me grabbing files from the CI, changing the folder name to reflect the release, and uploading them to GitHub. One of these days I'll automate that, but for now it's a manual process. Just added the 1.8.0 binary tarball. Thanks for the reminder.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/760#issuecomment-1064350509
https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067229426:87,Deployability,install,install,87,"Oh, I also missed that `$prefix/bin/salmon` is marked executable, even for the use who install it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067229426
https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067239060:41,Deployability,update,update,41,"@HenrikBengtsson,. Good catch. We had to update the continuous integration image used because we bumped some libraries (and the compiler version). Apparently the permissions are not set up in the same way by default. Can you share what you think the permissions should be for the relevant files / folders?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067239060
https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067239060:52,Deployability,continuous,continuous,52,"@HenrikBengtsson,. Good catch. We had to update the continuous integration image used because we bumped some libraries (and the compiler version). Apparently the permissions are not set up in the same way by default. Can you share what you think the permissions should be for the relevant files / folders?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067239060
https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067239060:63,Deployability,integrat,integration,63,"@HenrikBengtsson,. Good catch. We had to update the continuous integration image used because we bumped some libraries (and the compiler version). Apparently the permissions are not set up in the same way by default. Can you share what you think the permissions should be for the relevant files / folders?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067239060
https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067239060:63,Integrability,integrat,integration,63,"@HenrikBengtsson,. Good catch. We had to update the continuous integration image used because we bumped some libraries (and the compiler version). Apparently the permissions are not set up in the same way by default. Can you share what you think the permissions should be for the relevant files / folders?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067239060
https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067244078:79,Deployability,patch,patched,79,"This was caught by an end-user, reporting that `salmon` was not found. I just [patched our installation script](https://github.com/HenrikBengtsson/CBI-software/blob/c0ed7c62446b8b26559bb71ff5f55c7d8e751296/CBI/salmon/Makefile#L24-L29) to do:. ```sh; chmod -R go+r $(PREFIX); chmod ugo+x $(PREFIX)/{bin,lib,bin/salmon}; ```. which I decided on looking at what salmon 1.7.0 had, except I skipped setting executable on the `lib/*.so` files (which 1.7.0 has for some of them).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067244078
https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067244078:91,Deployability,install,installation,91,"This was caught by an end-user, reporting that `salmon` was not found. I just [patched our installation script](https://github.com/HenrikBengtsson/CBI-software/blob/c0ed7c62446b8b26559bb71ff5f55c7d8e751296/CBI/salmon/Makefile#L24-L29) to do:. ```sh; chmod -R go+r $(PREFIX); chmod ugo+x $(PREFIX)/{bin,lib,bin/salmon}; ```. which I decided on looking at what salmon 1.7.0 had, except I skipped setting executable on the `lib/*.so` files (which 1.7.0 has for some of them).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067244078
https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067481658:27,Deployability,update,updated,27,"Hi @HenrikBengtsson,. I've updated the script that builds the tarball of the CI server to apply the above permissions explicitly. It seems to be working (and I've uploaded the modified tarball under the release page). Thanks again for the issue!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067481658
https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067481658:203,Deployability,release,release,203,"Hi @HenrikBengtsson,. I've updated the script that builds the tarball of the CI server to apply the above permissions explicitly. It seems to be working (and I've uploaded the modified tarball under the release page). Thanks again for the issue!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/761#issuecomment-1067481658
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073004001:46,Availability,ping,pinging,46,"Hi @SeBaBInf,. Thanks for reporting this. I'm pinging @k3yavi for his thoughts here. Two quick thoughts though -- the first is that the abstract for this paper mentions 5' tagged end sequencing, thus it might be necessary to swap the reads so that the biological and technical reads are in the expected order. Second, it's likely also worth seeing if and how the data look different if you process with alevin-fry rather than alevin. I'll let @k3yavi provide more detailed guidance here. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073004001
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073004001:473,Usability,guid,guidance,473,"Hi @SeBaBInf,. Thanks for reporting this. I'm pinging @k3yavi for his thoughts here. Two quick thoughts though -- the first is that the abstract for this paper mentions 5' tagged end sequencing, thus it might be necessary to swap the reads so that the biological and technical reads are in the expected order. Second, it's likely also worth seeing if and how the data look different if you process with alevin-fry rather than alevin. I'll let @k3yavi provide more detailed guidance here. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073004001
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073009028:66,Testability,log,logs,66,"I guess there can be many things and it's hard to say without the logs, can you share the alevin and the salmon logs ?; Like @rob-p mentioned you can try changing ISR to ISF and/or using alevin-fry and check if that makes a difference, although if it's about mapping then probably it won't matter. I did notice one strange thing in the `umi extract` command though, please recheck this through the UMI tools package but you used `--stdout GSE140511/fastq_files/SRR10480618_BC_1.fastq.gz` and `--read2-out GSE140511/fastq_files/SRR10480618_BC_2.fastq.gz` in the umi tools command, which are the two files that probably should be provided as `-1` and `-2` flags to alevin. I am not sure why you are using `GSE140511/fastq_files/SRR10480618_1.fastq.gz` as the `-2` flag, which looks wrong to me, may be that will solve the issue. . Hope it helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073009028
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073009028:112,Testability,log,logs,112,"I guess there can be many things and it's hard to say without the logs, can you share the alevin and the salmon logs ?; Like @rob-p mentioned you can try changing ISR to ISF and/or using alevin-fry and check if that makes a difference, although if it's about mapping then probably it won't matter. I did notice one strange thing in the `umi extract` command though, please recheck this through the UMI tools package but you used `--stdout GSE140511/fastq_files/SRR10480618_BC_1.fastq.gz` and `--read2-out GSE140511/fastq_files/SRR10480618_BC_2.fastq.gz` in the umi tools command, which are the two files that probably should be provided as `-1` and `-2` flags to alevin. I am not sure why you are using `GSE140511/fastq_files/SRR10480618_1.fastq.gz` as the `-2` flag, which looks wrong to me, may be that will solve the issue. . Hope it helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073009028
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:3082,Performance,optimiz,optimizer,3082,77869[0m.; [2022-03-27 05:34:04.367] [alevinLog] [info] Throwing 0 barcodes with < 10 reads; [2022-03-27 05:34:05.069] [alevinLog] [info] Total [32m4000[0m(has [32m999[0m low confidence) barcodes; [2022-03-27 05:34:07.956] [alevinLog] [info] Done True Barcode Sampling; [2022-03-27 05:34:25.703] [alevinLog] [warning] Total 91.5531% reads will be thrown away because of noisy Cellular barcodes.; [2022-03-27 05:34:26.221] [alevinLog] [info] Done populating Z matrix; [2022-03-27 05:34:26.232] [alevinLog] [info] Total 60208 CB got sequence corrected; [2022-03-27 05:34:26.234] [alevinLog] [info] Done indexing Barcodes; [2022-03-27 05:34:26.234] [alevinLog] [info] Total Unique barcodes found: 127233006; [2022-03-27 05:34:26.234] [alevinLog] [info] Used Barcodes except Whitelist: 50131; [2022-03-27 05:34:26.966] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2022-03-27 05:34:26.966] [alevinLog] [info] parsing read library format; [2022-03-27 05:46:41.876] [alevinLog] [info] Starting optimizer. [2022-03-27 05:46:42.064] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2022-03-27 05:46:42.064] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 535438.00 UMI after deduplicating.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 2317116 BiDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 867878 UniDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [warning] Skipped 955 barcodes due to No mapped read; [2022-03-27 05:49:49.766] [alevinLog] [info] Clearing EqMap; Might take some time.; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting white listing of 3044 cells; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting to make feature Matrix; [2022-03-27 05:49:50.014] [alevinLog] [info] Done making feature Matrix; [2022-03-27 05:49:50.717] [alevinLog] [info] Finished white listing; [2022-03-27 05:49:51.422] [a,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:4097,Performance,optimiz,optimizer,4097,"zer. [2022-03-27 05:46:42.064] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2022-03-27 05:46:42.064] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 535438.00 UMI after deduplicating.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 2317116 BiDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 867878 UniDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [warning] Skipped 955 barcodes due to No mapped read; [2022-03-27 05:49:49.766] [alevinLog] [info] Clearing EqMap; Might take some time.; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting white listing of 3044 cells; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting to make feature Matrix; [2022-03-27 05:49:50.014] [alevinLog] [info] Done making feature Matrix; [2022-03-27 05:49:50.717] [alevinLog] [info] Finished white listing; [2022-03-27 05:49:51.422] [alevinLog] [info] Finished optimizer; ````. and the `salmon_quant.log` looks like:; ```; [2022-03-27 05:24:09.395] [jointLog] [info] setting maxHashResizeThreads to 32; [2022-03-27 05:24:09.395] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2022-03-27 05:24:09.395] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2022-03-27 05:24:09.395] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2022-03-27 05:24:09.395] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; [2022-03-27 05:24:09.395] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.; [2022-03-27 05:24:09.395] [jointLog] [info] Using default value of 0.87 fo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:5285,Performance,Load,Loading,5285,"patibility prior below threshold. Incompatible fragments will be ignored.; [2022-03-27 05:24:09.395] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2022-03-27 05:24:09.395] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2022-03-27 05:24:09.395] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; [2022-03-27 05:24:09.395] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.; [2022-03-27 05:24:09.395] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2022-03-27 05:34:26.966] [jointLog] [info] There is 1 library.; [2022-03-27 05:34:26.967] [jointLog] [info] Loading pufferfish index; [2022-03-27 05:34:26.967] [jointLog] [info] Loading dense pufferfish index.; [2022-03-27 05:34:27.433] [jointLog] [info] done; [2022-03-27 05:34:27.504] [jointLog] [info] Index contained 116,755 targets; [2022-03-27 05:34:27.540] [jointLog] [info] Number of decoys : 0; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 10.50% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 7.74% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 23.62% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 9.60% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 15.40% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 25.48% zero probability fragments; [202",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:5355,Performance,Load,Loading,5355,"be ignored.; [2022-03-27 05:24:09.395] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2022-03-27 05:24:09.395] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2022-03-27 05:24:09.395] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; [2022-03-27 05:24:09.395] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.; [2022-03-27 05:24:09.395] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2022-03-27 05:34:26.966] [jointLog] [info] There is 1 library.; [2022-03-27 05:34:26.967] [jointLog] [info] Loading pufferfish index; [2022-03-27 05:34:26.967] [jointLog] [info] Loading dense pufferfish index.; [2022-03-27 05:34:27.433] [jointLog] [info] done; [2022-03-27 05:34:27.504] [jointLog] [info] Index contained 116,755 targets; [2022-03-27 05:34:27.540] [jointLog] [info] Number of decoys : 0; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 10.50% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 7.74% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 23.62% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 9.60% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 15.40% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch with a maximum of 25.48% zero probability fragments; [2022-03-27 05:46:41.460] [jointLog] [info] Thread saw mini-batch ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:4486,Security,validat,validation,4486,"tal 2317116 BiDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 867878 UniDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [warning] Skipped 955 barcodes due to No mapped read; [2022-03-27 05:49:49.766] [alevinLog] [info] Clearing EqMap; Might take some time.; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting white listing of 3044 cells; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting to make feature Matrix; [2022-03-27 05:49:50.014] [alevinLog] [info] Done making feature Matrix; [2022-03-27 05:49:50.717] [alevinLog] [info] Finished white listing; [2022-03-27 05:49:51.422] [alevinLog] [info] Finished optimizer; ````. and the `salmon_quant.log` looks like:; ```; [2022-03-27 05:24:09.395] [jointLog] [info] setting maxHashResizeThreads to 32; [2022-03-27 05:24:09.395] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2022-03-27 05:24:09.395] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2022-03-27 05:24:09.395] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2022-03-27 05:24:09.395] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; [2022-03-27 05:24:09.395] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.; [2022-03-27 05:24:09.395] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2022-03-27 05:34:26.966] [jointLog] [info] There is 1 library.; [2022-03-27 05:34:26.967] [jointLog] [info] Loading pufferfish index; [2022-03-27 05:34:26.967] [jointLog] [info] Loading dense pufferfish index.; [2022-03-27 05:34:27.433] [jointLog] [info] done; [2022-03-27 05:34:",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:4500,Security,validat,validateMappings,4500,"tal 2317116 BiDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 867878 UniDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [warning] Skipped 955 barcodes due to No mapped read; [2022-03-27 05:49:49.766] [alevinLog] [info] Clearing EqMap; Might take some time.; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting white listing of 3044 cells; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting to make feature Matrix; [2022-03-27 05:49:50.014] [alevinLog] [info] Done making feature Matrix; [2022-03-27 05:49:50.717] [alevinLog] [info] Finished white listing; [2022-03-27 05:49:51.422] [alevinLog] [info] Finished optimizer; ````. and the `salmon_quant.log` looks like:; ```; [2022-03-27 05:24:09.395] [jointLog] [info] setting maxHashResizeThreads to 32; [2022-03-27 05:24:09.395] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2022-03-27 05:24:09.395] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2022-03-27 05:24:09.395] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2022-03-27 05:24:09.395] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; [2022-03-27 05:24:09.395] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.; [2022-03-27 05:24:09.395] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2022-03-27 05:34:26.966] [jointLog] [info] There is 1 library.; [2022-03-27 05:34:26.967] [jointLog] [info] Loading pufferfish index; [2022-03-27 05:34:26.967] [jointLog] [info] Loading dense pufferfish index.; [2022-03-27 05:34:27.433] [jointLog] [info] done; [2022-03-27 05:34:",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:4536,Security,validat,validation,4536,"otal 867878 UniDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [warning] Skipped 955 barcodes due to No mapped read; [2022-03-27 05:49:49.766] [alevinLog] [info] Clearing EqMap; Might take some time.; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting white listing of 3044 cells; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting to make feature Matrix; [2022-03-27 05:49:50.014] [alevinLog] [info] Done making feature Matrix; [2022-03-27 05:49:50.717] [alevinLog] [info] Finished white listing; [2022-03-27 05:49:51.422] [alevinLog] [info] Finished optimizer; ````. and the `salmon_quant.log` looks like:; ```; [2022-03-27 05:24:09.395] [jointLog] [info] setting maxHashResizeThreads to 32; [2022-03-27 05:24:09.395] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2022-03-27 05:24:09.395] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2022-03-27 05:24:09.395] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2022-03-27 05:24:09.395] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; [2022-03-27 05:24:09.395] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.; [2022-03-27 05:24:09.395] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2022-03-27 05:34:26.966] [jointLog] [info] There is 1 library.; [2022-03-27 05:34:26.967] [jointLog] [info] Loading pufferfish index; [2022-03-27 05:34:26.967] [jointLog] [info] Loading dense pufferfish index.; [2022-03-27 05:34:27.433] [jointLog] [info] done; [2022-03-27 05:34:27.504] [jointLog] [info] Index contained 116,755 targets; [2022-03-27 05:34:",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:4604,Security,validat,validateMappings,4604," Skipped 955 barcodes due to No mapped read; [2022-03-27 05:49:49.766] [alevinLog] [info] Clearing EqMap; Might take some time.; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting white listing of 3044 cells; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting to make feature Matrix; [2022-03-27 05:49:50.014] [alevinLog] [info] Done making feature Matrix; [2022-03-27 05:49:50.717] [alevinLog] [info] Finished white listing; [2022-03-27 05:49:51.422] [alevinLog] [info] Finished optimizer; ````. and the `salmon_quant.log` looks like:; ```; [2022-03-27 05:24:09.395] [jointLog] [info] setting maxHashResizeThreads to 32; [2022-03-27 05:24:09.395] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2022-03-27 05:24:09.395] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2022-03-27 05:24:09.395] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2022-03-27 05:24:09.395] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; [2022-03-27 05:24:09.395] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.; [2022-03-27 05:24:09.395] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2022-03-27 05:34:26.966] [jointLog] [info] There is 1 library.; [2022-03-27 05:34:26.967] [jointLog] [info] Loading pufferfish index; [2022-03-27 05:34:26.967] [jointLog] [info] Loading dense pufferfish index.; [2022-03-27 05:34:27.433] [jointLog] [info] done; [2022-03-27 05:34:27.504] [jointLog] [info] Index contained 116,755 targets; [2022-03-27 05:34:27.540] [jointLog] [info] Number of decoys : 0; [2022-03-27 05:46:41.460] [join",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:175,Testability,log,log,175,"so, I did fix the `umi_tools` commands as; ```; umi_tools whitelist \; -I /home/GSE140511/fastq_files/SRR10480618_1.fastq.gz \; --bc-pattern=CCCCCCCCCCCCCCCCNNNNNNNNNNNN \; --log=/home/GSE140511/SRR10480618_fq/SRR10480618_log_whitelist.log \; --log2stderr > /home/GSE140511/SRR10480618_fq/SRR10480618_whitelist.txt. umi_tools extract \; --bc-pattern=CCCCCCCCCCCCCCCCNNNNNNNNNNNN \; --stdin /home/GSE140511/fastq_files/SRR10480618_1.fastq.gz \; --stdout /home/GSE140511/SRR10480618_fq/SRR10480618_BC_1.fastq.gz \; --read2-in /home/GSE140511/fastq_files/SRR10480618_2.fastq.gz \; --read2-out=/home/GSE140511/SRR10480618_fq/SRR10480618_BC_2.fastq.gz \; --whitelist=/home/GSE140511/SRR10480618_fq/SRR10480618_whitelist.txt; ```. I swapped the reads as:; ```; /salmon-1.8.0_linux_x86_64/bin/salmon alevin \; -l ISR \; -2 /home/GSE140511/SRR10480618_fq/SRR10480618_BC_trimmed_1.fastq.gz \; -1 /home/GSE140511/SRR10480618_fq/SRR10480618_BC_trimmed_2.fastq.gz \; --chromiumV3 \; -i /data/ref_genomes/Mmus_GrCm39 \; -p 32 \; -o /home/GSE140511/salmon_alevin_output/SRR10480618_rev2 \; --expectCells 3000 --forceCells 3000 \; --tgMap /home/txp2gene_SB.tsv; ```; I tried both `ISR` and `ISF` (just in case)...mapping rate ranged from zero point something to one point something.; I also tried with and without `--expectCells 3000 --forceCells 3000` looking at a few suggestions [here](https://github.com/COMBINE-lab/salmon/discussions/506) but it didn't really make any difference. `Alevin.log` from the last run is:; ````; [2022-03-27 05:24:09.430] [alevinLog] [info] Found 116716 transcripts(+0 decoys, +39 short and +0 duplicate names in the index); [2022-03-27 05:24:09.478] [alevinLog] [info] Filled with 116755 txp to gene entries ; [2022-03-27 05:24:09.484] [alevinLog] [info] Found all transcripts to gene mappings; [2022-03-27 05:24:09.495] [alevinLog] [info] Processing barcodes files (if Present) . ; [2022-03-27 05:33:37.411] [alevinLog] [info] Done barcode density calculation.; [2022-03-27 05:33:37",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:236,Testability,log,log,236,"so, I did fix the `umi_tools` commands as; ```; umi_tools whitelist \; -I /home/GSE140511/fastq_files/SRR10480618_1.fastq.gz \; --bc-pattern=CCCCCCCCCCCCCCCCNNNNNNNNNNNN \; --log=/home/GSE140511/SRR10480618_fq/SRR10480618_log_whitelist.log \; --log2stderr > /home/GSE140511/SRR10480618_fq/SRR10480618_whitelist.txt. umi_tools extract \; --bc-pattern=CCCCCCCCCCCCCCCCNNNNNNNNNNNN \; --stdin /home/GSE140511/fastq_files/SRR10480618_1.fastq.gz \; --stdout /home/GSE140511/SRR10480618_fq/SRR10480618_BC_1.fastq.gz \; --read2-in /home/GSE140511/fastq_files/SRR10480618_2.fastq.gz \; --read2-out=/home/GSE140511/SRR10480618_fq/SRR10480618_BC_2.fastq.gz \; --whitelist=/home/GSE140511/SRR10480618_fq/SRR10480618_whitelist.txt; ```. I swapped the reads as:; ```; /salmon-1.8.0_linux_x86_64/bin/salmon alevin \; -l ISR \; -2 /home/GSE140511/SRR10480618_fq/SRR10480618_BC_trimmed_1.fastq.gz \; -1 /home/GSE140511/SRR10480618_fq/SRR10480618_BC_trimmed_2.fastq.gz \; --chromiumV3 \; -i /data/ref_genomes/Mmus_GrCm39 \; -p 32 \; -o /home/GSE140511/salmon_alevin_output/SRR10480618_rev2 \; --expectCells 3000 --forceCells 3000 \; --tgMap /home/txp2gene_SB.tsv; ```; I tried both `ISR` and `ISF` (just in case)...mapping rate ranged from zero point something to one point something.; I also tried with and without `--expectCells 3000 --forceCells 3000` looking at a few suggestions [here](https://github.com/COMBINE-lab/salmon/discussions/506) but it didn't really make any difference. `Alevin.log` from the last run is:; ````; [2022-03-27 05:24:09.430] [alevinLog] [info] Found 116716 transcripts(+0 decoys, +39 short and +0 duplicate names in the index); [2022-03-27 05:24:09.478] [alevinLog] [info] Filled with 116755 txp to gene entries ; [2022-03-27 05:24:09.484] [alevinLog] [info] Found all transcripts to gene mappings; [2022-03-27 05:24:09.495] [alevinLog] [info] Processing barcodes files (if Present) . ; [2022-03-27 05:33:37.411] [alevinLog] [info] Done barcode density calculation.; [2022-03-27 05:33:37",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:1479,Testability,log,log,1479,".gz \; --read2-in /home/GSE140511/fastq_files/SRR10480618_2.fastq.gz \; --read2-out=/home/GSE140511/SRR10480618_fq/SRR10480618_BC_2.fastq.gz \; --whitelist=/home/GSE140511/SRR10480618_fq/SRR10480618_whitelist.txt; ```. I swapped the reads as:; ```; /salmon-1.8.0_linux_x86_64/bin/salmon alevin \; -l ISR \; -2 /home/GSE140511/SRR10480618_fq/SRR10480618_BC_trimmed_1.fastq.gz \; -1 /home/GSE140511/SRR10480618_fq/SRR10480618_BC_trimmed_2.fastq.gz \; --chromiumV3 \; -i /data/ref_genomes/Mmus_GrCm39 \; -p 32 \; -o /home/GSE140511/salmon_alevin_output/SRR10480618_rev2 \; --expectCells 3000 --forceCells 3000 \; --tgMap /home/txp2gene_SB.tsv; ```; I tried both `ISR` and `ISF` (just in case)...mapping rate ranged from zero point something to one point something.; I also tried with and without `--expectCells 3000 --forceCells 3000` looking at a few suggestions [here](https://github.com/COMBINE-lab/salmon/discussions/506) but it didn't really make any difference. `Alevin.log` from the last run is:; ````; [2022-03-27 05:24:09.430] [alevinLog] [info] Found 116716 transcripts(+0 decoys, +39 short and +0 duplicate names in the index); [2022-03-27 05:24:09.478] [alevinLog] [info] Filled with 116755 txp to gene entries ; [2022-03-27 05:24:09.484] [alevinLog] [info] Found all transcripts to gene mappings; [2022-03-27 05:24:09.495] [alevinLog] [info] Processing barcodes files (if Present) . ; [2022-03-27 05:33:37.411] [alevinLog] [info] Done barcode density calculation.; [2022-03-27 05:33:37.411] [alevinLog] [info] # Barcodes Used: [32m359273127[0m / [31m359277869[0m.; [2022-03-27 05:34:04.367] [alevinLog] [info] Throwing 0 barcodes with < 10 reads; [2022-03-27 05:34:05.069] [alevinLog] [info] Total [32m4000[0m(has [32m999[0m low confidence) barcodes; [2022-03-27 05:34:07.956] [alevinLog] [info] Done True Barcode Sampling; [2022-03-27 05:34:25.703] [alevinLog] [warning] Total 91.5531% reads will be thrown away because of noisy Cellular barcodes.; [2022-03-27 05:34:26.221] [alevinLog] [in",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:4136,Testability,log,log,4136,"vided; using is 1 less feature for whitelisting; [2022-03-27 05:46:42.064] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 535438.00 UMI after deduplicating.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 2317116 BiDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 867878 UniDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [warning] Skipped 955 barcodes due to No mapped read; [2022-03-27 05:49:49.766] [alevinLog] [info] Clearing EqMap; Might take some time.; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting white listing of 3044 cells; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting to make feature Matrix; [2022-03-27 05:49:50.014] [alevinLog] [info] Done making feature Matrix; [2022-03-27 05:49:50.717] [alevinLog] [info] Finished white listing; [2022-03-27 05:49:51.422] [alevinLog] [info] Finished optimizer; ````. and the `salmon_quant.log` looks like:; ```; [2022-03-27 05:24:09.395] [jointLog] [info] setting maxHashResizeThreads to 32; [2022-03-27 05:24:09.395] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2022-03-27 05:24:09.395] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2022-03-27 05:24:09.395] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2022-03-27 05:24:09.395] [jointLog] [info] The use of range-factorized equivalence classes does not make sense in conjunction with --hardFilter. Disabling range-factorized equivalence classes. ; [2022-03-27 05:24:09.395] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35.; [2022-03-27 05:24:09.395] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensu",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942:3701,Usability,Clear,Clearing,3701,":26.234] [alevinLog] [info] Total Unique barcodes found: 127233006; [2022-03-27 05:34:26.234] [alevinLog] [info] Used Barcodes except Whitelist: 50131; [2022-03-27 05:34:26.966] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2022-03-27 05:34:26.966] [alevinLog] [info] parsing read library format; [2022-03-27 05:46:41.876] [alevinLog] [info] Starting optimizer. [2022-03-27 05:46:42.064] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2022-03-27 05:46:42.064] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 535438.00 UMI after deduplicating.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 2317116 BiDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [info] Total 867878 UniDirected Edges.; [2022-03-27 05:49:49.761] [alevinLog] [warning] Skipped 955 barcodes due to No mapped read; [2022-03-27 05:49:49.766] [alevinLog] [info] Clearing EqMap; Might take some time.; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting white listing of 3044 cells; [2022-03-27 05:49:50.011] [alevinLog] [info] Starting to make feature Matrix; [2022-03-27 05:49:50.014] [alevinLog] [info] Done making feature Matrix; [2022-03-27 05:49:50.717] [alevinLog] [info] Finished white listing; [2022-03-27 05:49:51.422] [alevinLog] [info] Finished optimizer; ````. and the `salmon_quant.log` looks like:; ```; [2022-03-27 05:24:09.395] [jointLog] [info] setting maxHashResizeThreads to 32; [2022-03-27 05:24:09.395] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2022-03-27 05:24:09.395] [jointLog] [info] The --mimicBT2, --mimicStrictBT2 and --hardFilter flags imply mapping validation (--validateMappings). Enabling mapping validation.; [2022-03-27 05:24:09.395] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.6",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1082597942
https://github.com/COMBINE-lab/salmon/issues/764#issuecomment-1073155343:123,Availability,error,error,123,"Hi @Klud112,. The command itself looks OK. However, given the formatting, I wonder if the line breaks are intentional. The error you get is what I might imagine if the command is actually spread over multiple lines and therefore not properly parsed. When there is a new line in your command above, is that just for formatting, or actually present in the command you issue on the command line? If the latter, you need to escape it with `\` i.e. ```. salmon quant -t References/Ensembl/Homo_sapiens.GRCh37.cdna.all.fa \; -l A \; -a bam_files/RNA_007_7669.Aligned.out.sort.com.bam \; -o salmon_quant_trial; ```. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/764#issuecomment-1073155343
https://github.com/COMBINE-lab/salmon/issues/766#issuecomment-1082558366:168,Testability,test,test,168,"Hi @fweghorst,. My guess is that a large fraction of this high number come from `N` bases in the underlying genome assembly (since you are making a gentrome index). To test this hypothesis, you could also build an index with just the cdna (or edna + ncrna) and see what that number is. At the end of the day, of course, it makes sense to use the gentrome index anyway, but this will at least give you an idea of what fraction of nucleotides are being replaced from the decoy (genome) versus the target transcript sequences. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/766#issuecomment-1082558366
https://github.com/COMBINE-lab/salmon/issues/767#issuecomment-1094372240:674,Security,access,access,674,"Hi @benadikt,. This indicates a system level problem with the underlying filesystem during the period that salmon calls out to TwoPaCo to create the compacted de Bruijn graph (or when attempting to clean up the intermediate files it makes after execution). Please make sure that the filesystem has sufficient free space, and that you have sufficient permissions for the salmon output directory. Finally, there have been intermittent issues in the past with the behavior of Twopaco on NFS mounted filesystems. If you are on a networkes filesystem, you should try building the index on a loc scratch partition, and then copying it over to a shared location if you need shared access to it. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/767#issuecomment-1094372240
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:12073,Integrability,wrap,wrapping,12073,":1458040 approximateContigTotalLength: 96596288 ; counters for complex kmers: ; (prec>1 & succ>1)=163493 | (succ>1 & isStart)=1600 | (prec>1 & isEnd)=1705 | (isStart & isEnd)=136 contig count: 2046804 element count: 189087548 complex nodes: 166934 ; number of ones in rank vector: 2046803 ; [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file. [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory salmon_index_23 ; size = 189087548 ; ----------------------------------------- ; | Loading contigs | Time = 43.37 ms ----------------------------------------- ; size = 189087548 ; ----------------------------------------- ; | Loading contig boundaries | Time = 19.565 ms ----------------------------------------- ; Number of ones: 2046803 ; Number of ones per inventory item: 512 ; Inventory entries filled: 3998 ; 2046803 ; [2022-04-16 11:19:37.638] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure. [2022-04-16 11:19:37.687] [puff::index::jointLog] [info] contig count for validation: 2,046,803 ; [2022-04-16 11:19:38.556] [puff::index::jointLog] [info] Total # of Contigs : 2,046,803 ; [2022-04-16 11:19:38.556] [puff::index::jointLog] [info] Total # of numerical Contigs : 2,046,803 ; [2022-04-16 11:19:38.774] [puff::index::jointLog] [info] Total # of contig vec entries: 15,036,896 ; [2022-04-16 11:19:38.774] [puff::index::jointLog] [info] bits per offset entry 24 ; [2022-04-16 11:19:39.637] [puff::index::jointLog] [info] Done constructing the contig vector. 2046804 [2022-04-16 11:19:40.720] [puff::index::jointLog] [info] # segments = 2,046,803 ; [2022-04-16 11:19:40.720] [puff::index::jointLog] [info] total length = 189,087,548 ; [2022-04-16 11:19:40.878] [puff::index::jointLog] [info] Reading the reference files ... ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] positional integer width = 28 ; [2022-04-16 11:19:42.562] [puf",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:11669,Performance,Load,Loading,11669,"t = 1307919 ; False junctions count = 233850 ; Hash table size = 1541769 ; Candidate marks count = 14841235 -------------------------------------------------------------------------------- ; Reallocating bifurcations time: 0 ; True marks count: 14610695 ; Edges construction time: 9 -------------------------------------------------------------------------------- ; Distinct junctions = 1307919 allowedIn: 18 ; Max Junction ID: 1458039 ; seen.size():11664321 kmerInfo.size():1458040 approximateContigTotalLength: 96596288 ; counters for complex kmers: ; (prec>1 & succ>1)=163493 | (succ>1 & isStart)=1600 | (prec>1 & isEnd)=1705 | (isStart & isEnd)=136 contig count: 2046804 element count: 189087548 complex nodes: 166934 ; number of ones in rank vector: 2046803 ; [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file. [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory salmon_index_23 ; size = 189087548 ; ----------------------------------------- ; | Loading contigs | Time = 43.37 ms ----------------------------------------- ; size = 189087548 ; ----------------------------------------- ; | Loading contig boundaries | Time = 19.565 ms ----------------------------------------- ; Number of ones: 2046803 ; Number of ones per inventory item: 512 ; Inventory entries filled: 3998 ; 2046803 ; [2022-04-16 11:19:37.638] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure. [2022-04-16 11:19:37.687] [puff::index::jointLog] [info] contig count for validation: 2,046,803 ; [2022-04-16 11:19:38.556] [puff::index::jointLog] [info] Total # of Contigs : 2,046,803 ; [2022-04-16 11:19:38.556] [puff::index::jointLog] [info] Total # of numerical Contigs : 2,046,803 ; [2022-04-16 11:19:38.774] [puff::index::jointLog] [info] Total # of contig vec entries: 15,036,896 ; [2022-04-16 11:19:38.774] [puff::index::jointLog] [info] bits per offset entry 24 ; [2022-0",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:11812,Performance,Load,Loading,11812,"------------------- ; Reallocating bifurcations time: 0 ; True marks count: 14610695 ; Edges construction time: 9 -------------------------------------------------------------------------------- ; Distinct junctions = 1307919 allowedIn: 18 ; Max Junction ID: 1458039 ; seen.size():11664321 kmerInfo.size():1458040 approximateContigTotalLength: 96596288 ; counters for complex kmers: ; (prec>1 & succ>1)=163493 | (succ>1 & isStart)=1600 | (prec>1 & isEnd)=1705 | (isStart & isEnd)=136 contig count: 2046804 element count: 189087548 complex nodes: 166934 ; number of ones in rank vector: 2046803 ; [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file. [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory salmon_index_23 ; size = 189087548 ; ----------------------------------------- ; | Loading contigs | Time = 43.37 ms ----------------------------------------- ; size = 189087548 ; ----------------------------------------- ; | Loading contig boundaries | Time = 19.565 ms ----------------------------------------- ; Number of ones: 2046803 ; Number of ones per inventory item: 512 ; Inventory entries filled: 3998 ; 2046803 ; [2022-04-16 11:19:37.638] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure. [2022-04-16 11:19:37.687] [puff::index::jointLog] [info] contig count for validation: 2,046,803 ; [2022-04-16 11:19:38.556] [puff::index::jointLog] [info] Total # of Contigs : 2,046,803 ; [2022-04-16 11:19:38.556] [puff::index::jointLog] [info] Total # of numerical Contigs : 2,046,803 ; [2022-04-16 11:19:38.774] [puff::index::jointLog] [info] Total # of contig vec entries: 15,036,896 ; [2022-04-16 11:19:38.774] [puff::index::jointLog] [info] bits per offset entry 24 ; [2022-04-16 11:19:39.637] [puff::index::jointLog] [info] Done constructing the contig vector. 2046804 [2022-04-16 11:19:40.720] [puff::index::jointLog] [info] # segments = 2,04",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:15981,Performance,Load,Loading,15981,"[info] finished populating pos vector ; [2022-04-16 11:19:53.442] [puff::index::jointLog] [info] writing index components ; [2022-04-16 11:19:55.117] [puff::index::jointLog] [info] finished writing dense pufferfish index ; [2022-04-16 11:19:55.401] [jLog] [info] done building index. and the log for quantification:. > [2022-04-16 11:23:51.572] [jointLog] [info] setting maxHashResizeThreads to 48 ; [2022-04-16 11:23:51.572] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored. ; [2022-04-16 11:23:51.572] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65 ; [2022-04-16 11:23:51.572] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35. [2022-04-16 11:23:51.572] [jointLog] [info] parsing read library format ; [2022-04-16 11:23:51.572] [jointLog] [info] There is 1 library. ; [2022-04-16 11:23:51.694] [jointLog] [info] Loading pufferfish index ; [2022-04-16 11:23:51.695] [jointLog] [info] Loading dense pufferfish index. ; [2022-04-16 11:23:53.681] [jointLog] [info] done ; [2022-04-16 11:23:53.681] [jointLog] [info] Index contained 245,261 targets ; [2022-04-16 11:23:53.776] [jointLog] [info] Number of decoys : 0 ; [2022-04-16 11:24:42.358] [jointLog] [info] Computed 960,194 rich equivalence classes for further processing [2022-04-16 11:24:42.358] [jointLog] [info] Counted 23,784,776 total reads in the equivalence classes [2022-04-16 11:24:42.426] [jointLog] [info] Number of mappings discarded because of alignment score : 3,206,484 [2022-04-16 11:24:42.426] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 170,372 [2022-04-16 11:24:42.426] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0 [2022-04-16 11:24:42.426] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 154,1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:16052,Performance,Load,Loading,16052,"2] [puff::index::jointLog] [info] writing index components ; [2022-04-16 11:19:55.117] [puff::index::jointLog] [info] finished writing dense pufferfish index ; [2022-04-16 11:19:55.401] [jLog] [info] done building index. and the log for quantification:. > [2022-04-16 11:23:51.572] [jointLog] [info] setting maxHashResizeThreads to 48 ; [2022-04-16 11:23:51.572] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored. ; [2022-04-16 11:23:51.572] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65 ; [2022-04-16 11:23:51.572] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35. [2022-04-16 11:23:51.572] [jointLog] [info] parsing read library format ; [2022-04-16 11:23:51.572] [jointLog] [info] There is 1 library. ; [2022-04-16 11:23:51.694] [jointLog] [info] Loading pufferfish index ; [2022-04-16 11:23:51.695] [jointLog] [info] Loading dense pufferfish index. ; [2022-04-16 11:23:53.681] [jointLog] [info] done ; [2022-04-16 11:23:53.681] [jointLog] [info] Index contained 245,261 targets ; [2022-04-16 11:23:53.776] [jointLog] [info] Number of decoys : 0 ; [2022-04-16 11:24:42.358] [jointLog] [info] Computed 960,194 rich equivalence classes for further processing [2022-04-16 11:24:42.358] [jointLog] [info] Counted 23,784,776 total reads in the equivalence classes [2022-04-16 11:24:42.426] [jointLog] [info] Number of mappings discarded because of alignment score : 3,206,484 [2022-04-16 11:24:42.426] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 170,372 [2022-04-16 11:24:42.426] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0 [2022-04-16 11:24:42.426] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 154,144 ; [2022-04-16 11:24:42.426] [jointLog] [info] Mapping rate ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:17193,Performance,optimiz,optimizer,17193,"ned 245,261 targets ; [2022-04-16 11:23:53.776] [jointLog] [info] Number of decoys : 0 ; [2022-04-16 11:24:42.358] [jointLog] [info] Computed 960,194 rich equivalence classes for further processing [2022-04-16 11:24:42.358] [jointLog] [info] Counted 23,784,776 total reads in the equivalence classes [2022-04-16 11:24:42.426] [jointLog] [info] Number of mappings discarded because of alignment score : 3,206,484 [2022-04-16 11:24:42.426] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 170,372 [2022-04-16 11:24:42.426] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0 [2022-04-16 11:24:42.426] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 154,144 ; [2022-04-16 11:24:42.426] [jointLog] [info] Mapping rate = 95.4075% [2022-04-16 11:24:42.426] [jointLog] [info] finished quantifyLibrary() ; [2022-04-16 11:24:42.494] [jointLog] [info] Starting optimizer ; [2022-04-16 11:24:42.359] [fileLog] [info] ; At end of round 0 ; ================== ; Observed 24929662 total fragments (24929662 in most recent round) [2022-04-16 11:24:43.294] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate [2022-04-16 11:24:43.366] [jointLog] [info] iteration = 0 | max rel diff. = 5078.09 ; [2022-04-16 11:24:50.352] [jointLog] [info] iteration = 100 | max rel diff. = 20.8492 ; [2022-04-16 11:24:57.458] [jointLog] [info] iteration = 200 | max rel diff. = 18.848 ; [2022-04-16 11:25:04.256] [jointLog] [info] iteration = 300 | max rel diff. = 4.55549 ; [2022-04-16 11:25:09.015] [jointLog] [info] iteration = 400 | max rel diff. = 2.20112 ; [2022-04-16 11:25:15.019] [jointLog] [info] iteration = 500 | max rel diff. = 8.9451 ; [2022-04-16 11:25:20.936] [jointLog] [info] iteration = 600 | max rel diff. = 8.80249 ; [2022-04-16 11:25:26.808] [jointLog] [info] iteration = 700 | max rel diff. = 0.955605 ; [2022-04-16 11:25:32.739] [jointLog] [in",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:19756,Performance,optimiz,optimizer,19756,"] [jointLog] [info] iteration = 400 | max rel diff. = 2.20112 ; [2022-04-16 11:25:15.019] [jointLog] [info] iteration = 500 | max rel diff. = 8.9451 ; [2022-04-16 11:25:20.936] [jointLog] [info] iteration = 600 | max rel diff. = 8.80249 ; [2022-04-16 11:25:26.808] [jointLog] [info] iteration = 700 | max rel diff. = 0.955605 ; [2022-04-16 11:25:32.739] [jointLog] [info] iteration = 800 | max rel diff. = 0.783506 ; [2022-04-16 11:25:38.614] [jointLog] [info] iteration = 900 | max rel diff. = 0.315252 ; [2022-04-16 11:25:44.475] [jointLog] [info] iteration = 1,000 | max rel diff. = 0.345064 ; [2022-04-16 11:25:50.254] [jointLog] [info] iteration = 1,100 | max rel diff. = 0.277546 ; [2022-04-16 11:25:56.156] [jointLog] [info] iteration = 1,200 | max rel diff. = 0.350471 ; [2022-04-16 11:26:02.062] [jointLog] [info] iteration = 1,300 | max rel diff. = 0.0385924 ; [2022-04-16 11:26:08.103] [jointLog] [info] iteration = 1,400 | max rel diff. = 10.4871 ; [2022-04-16 11:26:14.033] [jointLog] [info] iteration = 1,500 | max rel diff. = 0.0571527 ; [2022-04-16 11:26:20.066] [jointLog] [info] iteration = 1,600 | max rel diff. = 0.260293 ; [2022-04-16 11:26:26.129] [jointLog] [info] iteration = 1,700 | max rel diff. = 0.0360025 ; [2022-04-16 11:26:32.099] [jointLog] [info] iteration = 1,800 | max rel diff. = 0.0550004 ; [2022-04-16 11:26:38.060] [jointLog] [info] iteration = 1,900 | max rel diff. = 1.52554 ; [2022-04-16 11:26:45.086] [jointLog] [info] iteration = 2,000 | max rel diff. = 0.0264604 ; [2022-04-16 11:26:54.108] [jointLog] [info] iteration = 2,100 | max rel diff. = 0.0825479 ; [2022-04-16 11:27:03.466] [jointLog] [info] iteration = 2,200 | max rel diff. = 0.0842979 ; [2022-04-16 11:27:12.895] [jointLog] [info] iteration = 2,300 | max rel diff. = 0.441117 ; [2022-04-16 11:27:21.476] [jointLog] [info] iteration = 2,389 | max rel diff. = 0.0091923 ; [2022-04-16 11:27:21.658] [jointLog] [info] Finished optimizer ; [2022-04-16 11:27:21.658] [jointLog] [info] writing output",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:821,Security,validat,validateMappings,821,"Hi @rob-p ,. It works! Thank you so much!; I tried all of the k-mer values in your advice (19, 21, 23, 25) for building indices and set the `--minAssignedFrags` parameter rather small to 3 and got a pretty nice mapping rate. And among them `-k` of 19 seemed to have the highest mapping rate. . Please let me know if anything looks abnormal!. Here is the command I used for indexing (same for `-k` = 19, 21, 23, 25):. `salmon index -t gencode.v40.transcripts.fa.gz -k 19 -p 12 -i salmon_index_19 --gencode`. And here is my command for quantification:. `salmon quant -i ../ref/salmon_index_19 -l IU -1 SRR493372_1.fastq SRR493373_1.fastq SRR493374_1.fastq SRR493375_1.fastq SRR493376_1.fastq SRR493377_1.fastq -2 SRR493372_2.fastq SRR493373_2.fastq SRR493374_2.fastq SRR493375_2.fastq SRR493376_2.fastq SRR493377_2.fastq --validateMappings --minAssignedFrags 3 -o transcripts_quant_19`. And the log file for indexing:. > [2022-04-16 11:15:45.756] [jLog] [info] building index ; out : salmon_index_23 ; [2022-04-16 11:15:45.778] [puff::index::jointLog] [info] Running fixFasta [Step 1 of 4] : counting k-mers ; [2022-04-16 11:15:46.377] [puff::index::jointLog] [warning] Entry with header [ENST00000682202.1|ENSG00000243480.8|OTTHUMG00000011023.3|-|AMY2A-204|AMY2A|19|processed_transcript|], had length less than equal to the k-mer length of 23 (perhaps after poly-A clipping) [2022-04-16 11:15:49.574] [puff::index::jointLog] [warning] Entry with header [ENST00000603775.1|ENSG00000271544.1|OTTHUMG00000184300.1|OTTHUMT00000468575.1|ENST00000603775|ENSG00000271544|23|processed_pseudogene|], had length less than equal to the k-mer length of 23 (perhaps after poly-A clipping) [2022-04-16 11:15:52.071] [puff::index::jointLog] [warning] Entry with header [ENST00000632684.1|ENSG00000282431.1|OTTHUMG00000190602.2|OTTHUMT00000485301.2|TRBD1-202|TRBD1|12|TR_D_gene|], had length less than equal to the k-mer length of 23 (perhaps after poly-A clipping) [2022-04-16 11:15:55.682] [puff::index::jointLog] [w",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:10329,Security,Hash,Hash,10329,"h header [ENST00000604838.1|ENSG00000270185.1|OTTHUMG00000184585.2|OTTHUMT00000468915.2|IGHD1OR15-1B-201|IGHD1OR15-1B|17|IG_D_gene|], had length less than equal to the k-mer length of 23 (perhaps after poly-A clipping) [2022-04-16 11:16:00.515] [puff::index::jointLog] [warning] Removed 1363 transcripts that were sequence duplicates of indexed transcripts. ; [2022-04-16 11:16:00.515] [puff::index::jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag ; [2022-04-16 11:16:00.541] [puff::index::jointLog] [info] Replaced 4 non-ATCG nucleotides ; [2022-04-16 11:16:00.541] [puff::index::jointLog] [info] Clipped poly-A tails from 1,961 transcripts wrote 245236 cleaned references ; [2022-04-16 11:16:02.811] [puff::index::jointLog] [info] Filter size not provided; estimating from number of distinct k-mers [2022-04-16 11:16:07.979] [puff::index::jointLog] [info] ntHll estimated 143558277 distinct k-mers, setting filter size to 2^32 Threads = 12 ; Vertex length = 23 ; Hash functions = 5 ; Filter size = 4294967296 ; Capacity = 1 ; Files: ; salmon_index_23/ref_k23_fixed.fa -------------------------------------------------------------------------------- ; Round 0, 0:4294967296 ; Pass Filling Filtering ; 1 13 148 ; 2 9 0 ; True junctions count = 1307919 ; False junctions count = 233850 ; Hash table size = 1541769 ; Candidate marks count = 14841235 -------------------------------------------------------------------------------- ; Reallocating bifurcations time: 0 ; True marks count: 14610695 ; Edges construction time: 9 -------------------------------------------------------------------------------- ; Distinct junctions = 1307919 allowedIn: 18 ; Max Junction ID: 1458039 ; seen.size():11664321 kmerInfo.size():1458040 approximateContigTotalLength: 96596288 ; counters for complex kmers: ; (prec>1 & succ>1)=163493 | (succ>1 & isStart)=1600 | (prec>1 & isEnd)=1705 | (isStart & isEnd)=136 contig count: 2046804 element count: 189087548 compl",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:10651,Security,Hash,Hash,10651," to retain duplicate transcripts, please use the `--keepDuplicates` flag ; [2022-04-16 11:16:00.541] [puff::index::jointLog] [info] Replaced 4 non-ATCG nucleotides ; [2022-04-16 11:16:00.541] [puff::index::jointLog] [info] Clipped poly-A tails from 1,961 transcripts wrote 245236 cleaned references ; [2022-04-16 11:16:02.811] [puff::index::jointLog] [info] Filter size not provided; estimating from number of distinct k-mers [2022-04-16 11:16:07.979] [puff::index::jointLog] [info] ntHll estimated 143558277 distinct k-mers, setting filter size to 2^32 Threads = 12 ; Vertex length = 23 ; Hash functions = 5 ; Filter size = 4294967296 ; Capacity = 1 ; Files: ; salmon_index_23/ref_k23_fixed.fa -------------------------------------------------------------------------------- ; Round 0, 0:4294967296 ; Pass Filling Filtering ; 1 13 148 ; 2 9 0 ; True junctions count = 1307919 ; False junctions count = 233850 ; Hash table size = 1541769 ; Candidate marks count = 14841235 -------------------------------------------------------------------------------- ; Reallocating bifurcations time: 0 ; True marks count: 14610695 ; Edges construction time: 9 -------------------------------------------------------------------------------- ; Distinct junctions = 1307919 allowedIn: 18 ; Max Junction ID: 1458039 ; seen.size():11664321 kmerInfo.size():1458040 approximateContigTotalLength: 96596288 ; counters for complex kmers: ; (prec>1 & succ>1)=163493 | (succ>1 & isStart)=1600 | (prec>1 & isEnd)=1705 | (isStart & isEnd)=136 contig count: 2046804 element count: 189087548 complex nodes: 166934 ; number of ones in rank vector: 2046803 ; [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file. [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory salmon_index_23 ; size = 189087548 ; ----------------------------------------- ; | Loading contigs | Time = 43.37 ms ------------------------------------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:12199,Security,validat,validation,12199,"isStart)=1600 | (prec>1 & isEnd)=1705 | (isStart & isEnd)=136 contig count: 2046804 element count: 189087548 complex nodes: 166934 ; number of ones in rank vector: 2046803 ; [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file. [2022-04-16 11:19:37.060] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory salmon_index_23 ; size = 189087548 ; ----------------------------------------- ; | Loading contigs | Time = 43.37 ms ----------------------------------------- ; size = 189087548 ; ----------------------------------------- ; | Loading contig boundaries | Time = 19.565 ms ----------------------------------------- ; Number of ones: 2046803 ; Number of ones per inventory item: 512 ; Inventory entries filled: 3998 ; 2046803 ; [2022-04-16 11:19:37.638] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure. [2022-04-16 11:19:37.687] [puff::index::jointLog] [info] contig count for validation: 2,046,803 ; [2022-04-16 11:19:38.556] [puff::index::jointLog] [info] Total # of Contigs : 2,046,803 ; [2022-04-16 11:19:38.556] [puff::index::jointLog] [info] Total # of numerical Contigs : 2,046,803 ; [2022-04-16 11:19:38.774] [puff::index::jointLog] [info] Total # of contig vec entries: 15,036,896 ; [2022-04-16 11:19:38.774] [puff::index::jointLog] [info] bits per offset entry 24 ; [2022-04-16 11:19:39.637] [puff::index::jointLog] [info] Done constructing the contig vector. 2046804 [2022-04-16 11:19:40.720] [puff::index::jointLog] [info] # segments = 2,046,803 ; [2022-04-16 11:19:40.720] [puff::index::jointLog] [info] total length = 189,087,548 ; [2022-04-16 11:19:40.878] [puff::index::jointLog] [info] Reading the reference files ... ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] positional integer width = 28 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] seqSize = 189,087,548 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] rankSize ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:13602,Security,hash,hash,13602,"4 ; [2022-04-16 11:19:39.637] [puff::index::jointLog] [info] Done constructing the contig vector. 2046804 [2022-04-16 11:19:40.720] [puff::index::jointLog] [info] # segments = 2,046,803 ; [2022-04-16 11:19:40.720] [puff::index::jointLog] [info] total length = 189,087,548 ; [2022-04-16 11:19:40.878] [puff::index::jointLog] [info] Reading the reference files ... ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] positional integer width = 28 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] seqSize = 189,087,548 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] rankSize = 189,087,548 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] edgeVecSize = 0 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] num keys = 144,057,882 ; for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000 ; [Building BooPHF] 100 % elapsed: 0 min 6 sec remaining: 0 min 0 sec ; Bitarray 754822720 bits (100.00 %) (array + ranks ) ; final hash 0 bits (0.00 %) (nb in final hash 0) ; [2022-04-16 11:19:48.362] [puff::index::jointLog] [info] mphf size = 89.9819 MB ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk size = 15,757,296 ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 0 = [0, 15,757,296) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 1 = [15,757,296, 31,514,592) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 2 = [31,514,592, 47,271,888) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 3 = [47,271,888, 63,029,184) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 4 = [63,029,184, 78,786,480) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 5 = [78,786,480, 94,543,776) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 6 = [94,543,776, 110,301,072) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 7 = [110,301,072, 126,058,368) ; [2022-04-16 11:19:48.638] [puff::inde",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:13636,Security,hash,hash,13636,"tLog] [info] Done constructing the contig vector. 2046804 [2022-04-16 11:19:40.720] [puff::index::jointLog] [info] # segments = 2,046,803 ; [2022-04-16 11:19:40.720] [puff::index::jointLog] [info] total length = 189,087,548 ; [2022-04-16 11:19:40.878] [puff::index::jointLog] [info] Reading the reference files ... ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] positional integer width = 28 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] seqSize = 189,087,548 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] rankSize = 189,087,548 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] edgeVecSize = 0 ; [2022-04-16 11:19:42.562] [puff::index::jointLog] [info] num keys = 144,057,882 ; for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000 ; [Building BooPHF] 100 % elapsed: 0 min 6 sec remaining: 0 min 0 sec ; Bitarray 754822720 bits (100.00 %) (array + ranks ) ; final hash 0 bits (0.00 %) (nb in final hash 0) ; [2022-04-16 11:19:48.362] [puff::index::jointLog] [info] mphf size = 89.9819 MB ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk size = 15,757,296 ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 0 = [0, 15,757,296) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 1 = [15,757,296, 31,514,592) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 2 = [31,514,592, 47,271,888) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 3 = [47,271,888, 63,029,184) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 4 = [63,029,184, 78,786,480) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 5 = [78,786,480, 94,543,776) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 6 = [94,543,776, 110,301,072) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 7 = [110,301,072, 126,058,368) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 8 = [126,058,368, 141,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:15582,Security,validat,validateMappings,15582,":index::jointLog] [info] chunk 8 = [126,058,368, 141,815,664) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 9 = [141,815,664, 157,572,960) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 10 = [157,572,960, 173,330,256) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 11 = [173,330,256, 189,087,526) ; [2022-04-16 11:19:53.442] [puff::index::jointLog] [info] finished populating pos vector ; [2022-04-16 11:19:53.442] [puff::index::jointLog] [info] writing index components ; [2022-04-16 11:19:55.117] [puff::index::jointLog] [info] finished writing dense pufferfish index ; [2022-04-16 11:19:55.401] [jLog] [info] done building index. and the log for quantification:. > [2022-04-16 11:23:51.572] [jointLog] [info] setting maxHashResizeThreads to 48 ; [2022-04-16 11:23:51.572] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored. ; [2022-04-16 11:23:51.572] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65 ; [2022-04-16 11:23:51.572] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35. [2022-04-16 11:23:51.572] [jointLog] [info] parsing read library format ; [2022-04-16 11:23:51.572] [jointLog] [info] There is 1 library. ; [2022-04-16 11:23:51.694] [jointLog] [info] Loading pufferfish index ; [2022-04-16 11:23:51.695] [jointLog] [info] Loading dense pufferfish index. ; [2022-04-16 11:23:53.681] [jointLog] [info] done ; [2022-04-16 11:23:53.681] [jointLog] [info] Index contained 245,261 targets ; [2022-04-16 11:23:53.776] [jointLog] [info] Number of decoys : 0 ; [2022-04-16 11:24:42.358] [jointLog] [info] Computed 960,194 rich equivalence classes for further processing [2022-04-16 11:24:42.358] [jointLog] [info] Counted 23,784,776 total reads in the equivalence classes [2022-04-16 11:24:42.426] [jointLog] [info] Number of mappings discarded because of alignment sco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:893,Testability,log,log,893,"Hi @rob-p ,. It works! Thank you so much!; I tried all of the k-mer values in your advice (19, 21, 23, 25) for building indices and set the `--minAssignedFrags` parameter rather small to 3 and got a pretty nice mapping rate. And among them `-k` of 19 seemed to have the highest mapping rate. . Please let me know if anything looks abnormal!. Here is the command I used for indexing (same for `-k` = 19, 21, 23, 25):. `salmon index -t gencode.v40.transcripts.fa.gz -k 19 -p 12 -i salmon_index_19 --gencode`. And here is my command for quantification:. `salmon quant -i ../ref/salmon_index_19 -l IU -1 SRR493372_1.fastq SRR493373_1.fastq SRR493374_1.fastq SRR493375_1.fastq SRR493376_1.fastq SRR493377_1.fastq -2 SRR493372_2.fastq SRR493373_2.fastq SRR493374_2.fastq SRR493375_2.fastq SRR493376_2.fastq SRR493377_2.fastq --validateMappings --minAssignedFrags 3 -o transcripts_quant_19`. And the log file for indexing:. > [2022-04-16 11:15:45.756] [jLog] [info] building index ; out : salmon_index_23 ; [2022-04-16 11:15:45.778] [puff::index::jointLog] [info] Running fixFasta [Step 1 of 4] : counting k-mers ; [2022-04-16 11:15:46.377] [puff::index::jointLog] [warning] Entry with header [ENST00000682202.1|ENSG00000243480.8|OTTHUMG00000011023.3|-|AMY2A-204|AMY2A|19|processed_transcript|], had length less than equal to the k-mer length of 23 (perhaps after poly-A clipping) [2022-04-16 11:15:49.574] [puff::index::jointLog] [warning] Entry with header [ENST00000603775.1|ENSG00000271544.1|OTTHUMG00000184300.1|OTTHUMT00000468575.1|ENST00000603775|ENSG00000271544|23|processed_pseudogene|], had length less than equal to the k-mer length of 23 (perhaps after poly-A clipping) [2022-04-16 11:15:52.071] [puff::index::jointLog] [warning] Entry with header [ENST00000632684.1|ENSG00000282431.1|OTTHUMG00000190602.2|OTTHUMT00000485301.2|TRBD1-202|TRBD1|12|TR_D_gene|], had length less than equal to the k-mer length of 23 (perhaps after poly-A clipping) [2022-04-16 11:15:55.682] [puff::index::jointLog] [w",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317:15285,Testability,log,log,15285,".637] [puff::index::jointLog] [info] chunk 5 = [78,786,480, 94,543,776) ; [2022-04-16 11:19:48.637] [puff::index::jointLog] [info] chunk 6 = [94,543,776, 110,301,072) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 7 = [110,301,072, 126,058,368) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 8 = [126,058,368, 141,815,664) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 9 = [141,815,664, 157,572,960) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 10 = [157,572,960, 173,330,256) ; [2022-04-16 11:19:48.638] [puff::index::jointLog] [info] chunk 11 = [173,330,256, 189,087,526) ; [2022-04-16 11:19:53.442] [puff::index::jointLog] [info] finished populating pos vector ; [2022-04-16 11:19:53.442] [puff::index::jointLog] [info] writing index components ; [2022-04-16 11:19:55.117] [puff::index::jointLog] [info] finished writing dense pufferfish index ; [2022-04-16 11:19:55.401] [jLog] [info] done building index. and the log for quantification:. > [2022-04-16 11:23:51.572] [jointLog] [info] setting maxHashResizeThreads to 48 ; [2022-04-16 11:23:51.572] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored. ; [2022-04-16 11:23:51.572] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65 ; [2022-04-16 11:23:51.572] [jointLog] [info] Setting consensusSlack to selective-alignment default of 0.35. [2022-04-16 11:23:51.572] [jointLog] [info] parsing read library format ; [2022-04-16 11:23:51.572] [jointLog] [info] There is 1 library. ; [2022-04-16 11:23:51.694] [jointLog] [info] Loading pufferfish index ; [2022-04-16 11:23:51.695] [jointLog] [info] Loading dense pufferfish index. ; [2022-04-16 11:23:53.681] [jointLog] [info] done ; [2022-04-16 11:23:53.681] [jointLog] [info] Index contained 245,261 targets ; [2022-04-16 11:23:53.776] [jointLog] [info] Number of decoys : 0 ; [2022-04-16",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/768#issuecomment-1100734317
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107467832:61,Integrability,protocol,protocol,61,"Hi @BW15061999,. I’m not aware of any tagged-end single-cell protocol that uses only 1 read. The most common data types place the UMI and Barcode on one of the reads, while the other “biological” reads are drawn from the transcriptome. This is the case with the Chromimum protocol. The reason you are seeing 0 assigned reads is that no barcodes can be extracted, because the second read is missing. Therefore, no reads can be assigned to any cell. What specific protocol are you using? Do you not have the full read pairs for each sample? Cc @k3yavi as the resident protocol guru. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107467832
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107467832:272,Integrability,protocol,protocol,272,"Hi @BW15061999,. I’m not aware of any tagged-end single-cell protocol that uses only 1 read. The most common data types place the UMI and Barcode on one of the reads, while the other “biological” reads are drawn from the transcriptome. This is the case with the Chromimum protocol. The reason you are seeing 0 assigned reads is that no barcodes can be extracted, because the second read is missing. Therefore, no reads can be assigned to any cell. What specific protocol are you using? Do you not have the full read pairs for each sample? Cc @k3yavi as the resident protocol guru. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107467832
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107467832:462,Integrability,protocol,protocol,462,"Hi @BW15061999,. I’m not aware of any tagged-end single-cell protocol that uses only 1 read. The most common data types place the UMI and Barcode on one of the reads, while the other “biological” reads are drawn from the transcriptome. This is the case with the Chromimum protocol. The reason you are seeing 0 assigned reads is that no barcodes can be extracted, because the second read is missing. Therefore, no reads can be assigned to any cell. What specific protocol are you using? Do you not have the full read pairs for each sample? Cc @k3yavi as the resident protocol guru. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107467832
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107467832:566,Integrability,protocol,protocol,566,"Hi @BW15061999,. I’m not aware of any tagged-end single-cell protocol that uses only 1 read. The most common data types place the UMI and Barcode on one of the reads, while the other “biological” reads are drawn from the transcriptome. This is the case with the Chromimum protocol. The reason you are seeing 0 assigned reads is that no barcodes can be extracted, because the second read is missing. Therefore, no reads can be assigned to any cell. What specific protocol are you using? Do you not have the full read pairs for each sample? Cc @k3yavi as the resident protocol guru. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107467832
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107684572:22,Availability,down,downloaded,22,"Hi @rob-p ,. The data downloaded from sra database and use `fastq-dump` to split it only generate one fastq file, and EBI database only show one fastq file per sample. I am not sure if I process the file correctly. And here is a part of the description of the file on the sra database, and the link of one of the file <br class=""Apple-interchange-newline"">[SRR8453531](https://www.ncbi.nlm.nih.gov/sra/SRX5260234[accn]). ```; Instrument: Illumina HiSeq 3000; Strategy: RNA-Seq; Source: TRANSCRIPTOMIC; Selection: cDNA; Layout: SINGLE; Construction protocol: The scRNA-seq libraries were generated using Chromium Single Cell 3' Library & Gel Bead Kit v2 (10X Genomic) according to manufacturer's protocol. Briefly, 10,000-15,000 live cells were FACS-sorted and used to generate single-cell gel-bead in emulsion (GEM). After reverse transcription, GEMs were disrupted. Barcoded cDNA was isolated and amplified by PCR (12 cycles). Following fragmentation, end repair, and A-tailing, sample indexes were added during index PCR (8 cycles). Indexed libraries were multiplexed and sequenced on Illumina HiSeq 3000 instruments according to the manufacturer's instructions (26 cycles of Read 1, 8 cycles of i7 Index, and 98 cycles of Read2).; ```. Best",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107684572
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107684572:957,Availability,repair,repair,957,"Hi @rob-p ,. The data downloaded from sra database and use `fastq-dump` to split it only generate one fastq file, and EBI database only show one fastq file per sample. I am not sure if I process the file correctly. And here is a part of the description of the file on the sra database, and the link of one of the file <br class=""Apple-interchange-newline"">[SRR8453531](https://www.ncbi.nlm.nih.gov/sra/SRX5260234[accn]). ```; Instrument: Illumina HiSeq 3000; Strategy: RNA-Seq; Source: TRANSCRIPTOMIC; Selection: cDNA; Layout: SINGLE; Construction protocol: The scRNA-seq libraries were generated using Chromium Single Cell 3' Library & Gel Bead Kit v2 (10X Genomic) according to manufacturer's protocol. Briefly, 10,000-15,000 live cells were FACS-sorted and used to generate single-cell gel-bead in emulsion (GEM). After reverse transcription, GEMs were disrupted. Barcoded cDNA was isolated and amplified by PCR (12 cycles). Following fragmentation, end repair, and A-tailing, sample indexes were added during index PCR (8 cycles). Indexed libraries were multiplexed and sequenced on Illumina HiSeq 3000 instruments according to the manufacturer's instructions (26 cycles of Read 1, 8 cycles of i7 Index, and 98 cycles of Read2).; ```. Best",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107684572
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107684572:548,Integrability,protocol,protocol,548,"Hi @rob-p ,. The data downloaded from sra database and use `fastq-dump` to split it only generate one fastq file, and EBI database only show one fastq file per sample. I am not sure if I process the file correctly. And here is a part of the description of the file on the sra database, and the link of one of the file <br class=""Apple-interchange-newline"">[SRR8453531](https://www.ncbi.nlm.nih.gov/sra/SRX5260234[accn]). ```; Instrument: Illumina HiSeq 3000; Strategy: RNA-Seq; Source: TRANSCRIPTOMIC; Selection: cDNA; Layout: SINGLE; Construction protocol: The scRNA-seq libraries were generated using Chromium Single Cell 3' Library & Gel Bead Kit v2 (10X Genomic) according to manufacturer's protocol. Briefly, 10,000-15,000 live cells were FACS-sorted and used to generate single-cell gel-bead in emulsion (GEM). After reverse transcription, GEMs were disrupted. Barcoded cDNA was isolated and amplified by PCR (12 cycles). Following fragmentation, end repair, and A-tailing, sample indexes were added during index PCR (8 cycles). Indexed libraries were multiplexed and sequenced on Illumina HiSeq 3000 instruments according to the manufacturer's instructions (26 cycles of Read 1, 8 cycles of i7 Index, and 98 cycles of Read2).; ```. Best",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107684572
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107684572:695,Integrability,protocol,protocol,695,"Hi @rob-p ,. The data downloaded from sra database and use `fastq-dump` to split it only generate one fastq file, and EBI database only show one fastq file per sample. I am not sure if I process the file correctly. And here is a part of the description of the file on the sra database, and the link of one of the file <br class=""Apple-interchange-newline"">[SRR8453531](https://www.ncbi.nlm.nih.gov/sra/SRX5260234[accn]). ```; Instrument: Illumina HiSeq 3000; Strategy: RNA-Seq; Source: TRANSCRIPTOMIC; Selection: cDNA; Layout: SINGLE; Construction protocol: The scRNA-seq libraries were generated using Chromium Single Cell 3' Library & Gel Bead Kit v2 (10X Genomic) according to manufacturer's protocol. Briefly, 10,000-15,000 live cells were FACS-sorted and used to generate single-cell gel-bead in emulsion (GEM). After reverse transcription, GEMs were disrupted. Barcoded cDNA was isolated and amplified by PCR (12 cycles). Following fragmentation, end repair, and A-tailing, sample indexes were added during index PCR (8 cycles). Indexed libraries were multiplexed and sequenced on Illumina HiSeq 3000 instruments according to the manufacturer's instructions (26 cycles of Read 1, 8 cycles of i7 Index, and 98 cycles of Read2).; ```. Best",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107684572
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107843131:102,Availability,down,download,102,"Hi @BW15061999 , ; Yes, this is a known problem for single-cell data uploaded on NCBI. The idea is to download the BAM files of the data (yours should be [here](https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR8453531) under data access section) and then use tools like [these](https://github.com/10XGenomics/bamtofastq) to generate paired-end FASTQ files from the BAM file before running alevin. The one downloaded directly from NCBI/EBI doesn't has the CB/UMI components of the paired-reads. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107843131
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107843131:406,Availability,down,downloaded,406,"Hi @BW15061999 , ; Yes, this is a known problem for single-cell data uploaded on NCBI. The idea is to download the BAM files of the data (yours should be [here](https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR8453531) under data access section) and then use tools like [these](https://github.com/10XGenomics/bamtofastq) to generate paired-end FASTQ files from the BAM file before running alevin. The one downloaded directly from NCBI/EBI doesn't has the CB/UMI components of the paired-reads. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107843131
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107843131:231,Security,access,access,231,"Hi @BW15061999 , ; Yes, this is a known problem for single-cell data uploaded on NCBI. The idea is to download the BAM files of the data (yours should be [here](https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR8453531) under data access section) and then use tools like [these](https://github.com/10XGenomics/bamtofastq) to generate paired-end FASTQ files from the BAM file before running alevin. The one downloaded directly from NCBI/EBI doesn't has the CB/UMI components of the paired-reads. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107843131
https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107843588:376,Availability,down,download,376,"@k3yavi beat me to it! It is, unfortunately, a recurring problem. The SRA file itself only contains one of the reads and is therefore essentially useless in analyzing the single-cell data. This is an ongoing problem that I've mentioned several times, but I don't know if the SRA has a plan in place to address it. The proper solution at this point is exactly as Avi suggests; download the bam file (what the SRA calls the original TenX format data), and run it through 10x's bamtofastq to get back the original fastq files (this time paired-end) that you can process. Let us know if you have success with this. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/769#issuecomment-1107843588
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126077334:55,Availability,ping,pinging,55,"Hi @Liripo,. Thanks for reporting this issue. I'm also pinging @Gaura and @jeremymsimon here as our resident split-seq experts. One other note though, is that we'd generally recommend using `alevin-fry`. Here, it seems you're utilizing a _splici_ index but then using `alevin` rather than `alevin-fry` for quantification (given there is no `--sketch` or `--rad` flag in your command line invocation). Dealing with the structure of the _splici_ index is an alevin-fry specific capability. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126077334
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126090795:197,Testability,test,test,197,"Hi @rob-p ,; thank you for your help. I got my hands on this kind of data for the first time today and followed the tutorial <https://combine-lab.github.io/alevin-fry-tutorials/2022/split-seq/> to test my data, but following the tutorial I ended up with a matrix file, doesn't seem to generate the file alevin_out/aux_info/alevin_meta_info.json, I actually want to get a report like 10X cellranger summary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126090795
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126137203:78,Testability,log,log,78,"Hi @Gaura :; ```shell; ./; ├── alevin_out; │   ├── alevin; │   │   └── alevin.log; │   ├── aux_info; │   │   └── meta_info.json; │   ├── cmd_info.json; │   ├── libParams; │   ├── logs; │   │   └── salmon_quant.log; │   ├── map.rad; │   └── unmapped_bc_count.bin; ├── count; │   ├── alevin; │   │   ├── quants_mat_cols.txt; │   │   ├── quants_mat.mtx; │   │   └── quants_mat_rows.txt; │   ├── featureDump.txt; │   └── quant.json; └── permit_knee_out; ├── all_freq.bin; ├── collate.json; ├── generate_permit_list.json; ├── map.collated.rad; ├── permit_freq.bin; ├── permit_map.bin; └── unmapped_bc_count_collated.bin; ```; I follow the output of the tutorial, it seems that there is no file `alevin_out/alevin/raw_cb_frequency.txt`, it may be that the parameter --dumpFeatures is not added during runtime, I add the --dumpFeatures parameter to try",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126137203
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126137203:179,Testability,log,logs,179,"Hi @Gaura :; ```shell; ./; ├── alevin_out; │   ├── alevin; │   │   └── alevin.log; │   ├── aux_info; │   │   └── meta_info.json; │   ├── cmd_info.json; │   ├── libParams; │   ├── logs; │   │   └── salmon_quant.log; │   ├── map.rad; │   └── unmapped_bc_count.bin; ├── count; │   ├── alevin; │   │   ├── quants_mat_cols.txt; │   │   ├── quants_mat.mtx; │   │   └── quants_mat_rows.txt; │   ├── featureDump.txt; │   └── quant.json; └── permit_knee_out; ├── all_freq.bin; ├── collate.json; ├── generate_permit_list.json; ├── map.collated.rad; ├── permit_freq.bin; ├── permit_map.bin; └── unmapped_bc_count_collated.bin; ```; I follow the output of the tutorial, it seems that there is no file `alevin_out/alevin/raw_cb_frequency.txt`, it may be that the parameter --dumpFeatures is not added during runtime, I add the --dumpFeatures parameter to try",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126137203
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126137203:210,Testability,log,log,210,"Hi @Gaura :; ```shell; ./; ├── alevin_out; │   ├── alevin; │   │   └── alevin.log; │   ├── aux_info; │   │   └── meta_info.json; │   ├── cmd_info.json; │   ├── libParams; │   ├── logs; │   │   └── salmon_quant.log; │   ├── map.rad; │   └── unmapped_bc_count.bin; ├── count; │   ├── alevin; │   │   ├── quants_mat_cols.txt; │   │   ├── quants_mat.mtx; │   │   └── quants_mat_rows.txt; │   ├── featureDump.txt; │   └── quant.json; └── permit_knee_out; ├── all_freq.bin; ├── collate.json; ├── generate_permit_list.json; ├── map.collated.rad; ├── permit_freq.bin; ├── permit_map.bin; └── unmapped_bc_count_collated.bin; ```; I follow the output of the tutorial, it seems that there is no file `alevin_out/alevin/raw_cb_frequency.txt`, it may be that the parameter --dumpFeatures is not added during runtime, I add the --dumpFeatures parameter to try",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1126137203
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1127073879:164,Usability,simpl,simply,164,"Hi @Liripo - if I'm understanding correctly, your UMI/barcodes are on R2 but `alevin` is incorrectly extracting them from the R1 file? If so, you should be able to simply reverse your inputs, e.g.:. ```; salmon alevin -i ../../GRch38_splici_idx \; -l A \; -1 2.fq.gz \; -2 1.fq.gz \; -p 32 \; --splitseqV1 \; -o alevin_out \; --tgMap ../../transcriptome_splici_fl86/transcriptome_splici_fl86_t2g.tsv \; --dumpFeatures --whitelist ../white_barcode.txt; ```; I've needed to do this for my own projects sometimes, since our cDNA/barcode reads are opposite that of the original Rosenberg paper, and it works fine. Can you give that a try and see if it solves your issue? Or if I'm misunderstanding, can you elaborate more on what you expected the output to look like?. And seconding @rob-p's suggestion above - you should be using `alevin` -> `alevin-fry`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1127073879
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1127121152:55,Deployability,update,updates,55,"Great to hear @Liripo. Note that we're working on some updates to the SPLiT-seq vignette, so be sure to check back there soon for a new version!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1127121152
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1181068459:780,Integrability,protocol,protocol-specific,780,"Hi @rob-p @Gaura and @k3yavi - I actually just had this problem myself, and would now like to confirm what `--splitseqV1` and `--splitseqV2` are doing. I can't seem to find the UMI or barcode geometries these settings assume, can one of you clarify that here? Usually the cDNA is on R1 and UMI + barcodes are on R2, but it seems as though `--splitseqV1` may be assuming the opposite. It's okay and easy enough to reverse the input files like recommended above, but many users may face a similar issue if so. On a related note, it would be great to add these presets to the `alevin` docs, and for these and other geometry presets, spell out exactly which nucleotides on which reads are being extracted. `--splitseqV1` and `--splitseqV2` are not currently mentioned among the other protocol-specific setups in the `alevin` docs. . Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1181068459
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1184412691:643,Deployability,pipeline,pipeline,643,"Hi @jeremymsimon,. Both `--splitseqV1` and `--splitseqV2` assume that cDNA is on R1 and UMI + barcodes are on R2. The geometries used are:; - For `--splitseqV1`:; `NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGAGGCCAGAGCATTCGIIIIIIII`, or; UMI-BC1-FixedSeq-BC2-FixedSeq-BC3; Barcode positions (0-based index): 10, 48, 86; - For `--splitseqV2`: ; `NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGACTGTGGIIIIIIII` or ; UMI-BC1-FixedSeq-BC2-FixedSeq-BC3; Barcode positions (0-based index): 10, 48, 78. where the `IIIIIIII` sequence corresponds to barcode and `NNNNNNNNNN` to UMI. This is from the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py) used in [this paper](https://www.nature.com/articles/s41593-021-00872-y) co-authored by the lab that developed the protocol. I previously mentioned this in our initial discussions [here](https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951080577). I think adding this to the documentation is a good idea. I will discuss with Rob and add the information. Thank you. :). Now it seems like it's not behaving as expected for `splitseqV1`? Can you share the command you used and the salmon version?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1184412691
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1184412691:695,Deployability,pipeline,pipeline,695,"Hi @jeremymsimon,. Both `--splitseqV1` and `--splitseqV2` assume that cDNA is on R1 and UMI + barcodes are on R2. The geometries used are:; - For `--splitseqV1`:; `NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGAGGCCAGAGCATTCGIIIIIIII`, or; UMI-BC1-FixedSeq-BC2-FixedSeq-BC3; Barcode positions (0-based index): 10, 48, 86; - For `--splitseqV2`: ; `NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGACTGTGGIIIIIIII` or ; UMI-BC1-FixedSeq-BC2-FixedSeq-BC3; Barcode positions (0-based index): 10, 48, 78. where the `IIIIIIII` sequence corresponds to barcode and `NNNNNNNNNN` to UMI. This is from the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py) used in [this paper](https://www.nature.com/articles/s41593-021-00872-y) co-authored by the lab that developed the protocol. I previously mentioned this in our initial discussions [here](https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951080577). I think adding this to the documentation is a good idea. I will discuss with Rob and add the information. Thank you. :). Now it seems like it's not behaving as expected for `splitseqV1`? Can you share the command you used and the salmon version?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1184412691
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1184412691:851,Integrability,protocol,protocol,851,"Hi @jeremymsimon,. Both `--splitseqV1` and `--splitseqV2` assume that cDNA is on R1 and UMI + barcodes are on R2. The geometries used are:; - For `--splitseqV1`:; `NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGAGGCCAGAGCATTCGIIIIIIII`, or; UMI-BC1-FixedSeq-BC2-FixedSeq-BC3; Barcode positions (0-based index): 10, 48, 86; - For `--splitseqV2`: ; `NNNNNNNNNNIIIIIIIIGTGGCCGATGTTTCGCATCGGCGTACGACTIIIIIIIIATCCACGTGCTTGAGACTGTGGIIIIIIII` or ; UMI-BC1-FixedSeq-BC2-FixedSeq-BC3; Barcode positions (0-based index): 10, 48, 78. where the `IIIIIIII` sequence corresponds to barcode and `NNNNNNNNNN` to UMI. This is from the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py) used in [this paper](https://www.nature.com/articles/s41593-021-00872-y) co-authored by the lab that developed the protocol. I previously mentioned this in our initial discussions [here](https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-951080577). I think adding this to the documentation is a good idea. I will discuss with Rob and add the information. Thank you. :). Now it seems like it's not behaving as expected for `splitseqV1`? Can you share the command you used and the salmon version?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1184412691
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1185098089:72,Availability,fault,fault,72,"Thanks @Gaura! This is helpful. I think I figured it out, and it was my fault indeed. Everything seems to be working as expected based on this info!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1185098089
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1185670648:9,Usability,learn,learn,9,Great to learn that. Let us know if you have any other issue. :),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1185670648
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126361719:1778,Performance,perform,performing,1778,"al `1,099,008` are being discarded because they have only dovetail mappings. . We discard dovetail mappings by default, but you can admit them with `--allowDovetail`. The other `2,776,678` fragments are discarded because, though there are seeds for mapping that match, they do not have sufficiently high alignment score to be allowed for mapping. This default behavior, too, can be modified. The main flags that affect the behavior here are `--minScoreFraction`, where a lower number allows lower-quality alignments through and also the `--softclipOverhangs` flag which will decrease the penalty on alignments that overhang the end of an annotated transcript. However, it's worth noting that this is up to `3,875,686` more reads that might be mappable. This number is non-trivial, but quite far from the 90% rate of STAR. The rest of the reads, however, simply don't have support for alignment against the annotated transcriptome. **This suggests to me that STAR is probably aligning a lot of reads outside of annotated genes**. If you build the salmon index [using a full decoy of the genome](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/), then you might be able to evaluate intergenic mapping in the output in terms of `Number of fragments discarded because they are best-mapped to decoys`. However, in that case, these reads still won't contribute to transcript expression, as they do not align to annotated transcripts. Finally, if you suspect these reads might be coming from genes expressed in your sample but not present in the annotation, you might consider performing a transcript assembly on your data, using a tool like [scallop2](https://github.com/Shao-Group/scallop2) or [stringtie](https://github.com/gpertea/stringtie). Best,; Rob. P.S. I'm closing the thread, since I think the above answers your direct question, but please feel free to continue commenting here (for discussion) or to open up another issue if there are follow-ups that are salmon-related.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126361719
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126361719:1039,Usability,simpl,simply,1039,"re being discarded because they have no alignment to annotated transcripts above the minimum allowable score, and an additional `1,099,008` are being discarded because they have only dovetail mappings. . We discard dovetail mappings by default, but you can admit them with `--allowDovetail`. The other `2,776,678` fragments are discarded because, though there are seeds for mapping that match, they do not have sufficiently high alignment score to be allowed for mapping. This default behavior, too, can be modified. The main flags that affect the behavior here are `--minScoreFraction`, where a lower number allows lower-quality alignments through and also the `--softclipOverhangs` flag which will decrease the penalty on alignments that overhang the end of an annotated transcript. However, it's worth noting that this is up to `3,875,686` more reads that might be mappable. This number is non-trivial, but quite far from the 90% rate of STAR. The rest of the reads, however, simply don't have support for alignment against the annotated transcriptome. **This suggests to me that STAR is probably aligning a lot of reads outside of annotated genes**. If you build the salmon index [using a full decoy of the genome](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/), then you might be able to evaluate intergenic mapping in the output in terms of `Number of fragments discarded because they are best-mapped to decoys`. However, in that case, these reads still won't contribute to transcript expression, as they do not align to annotated transcripts. Finally, if you suspect these reads might be coming from genes expressed in your sample but not present in the annotation, you might consider performing a transcript assembly on your data, using a tool like [scallop2](https://github.com/Shao-Group/scallop2) or [stringtie](https://github.com/gpertea/stringtie). Best,; Rob. P.S. I'm closing the thread, since I think the above answers your direct question, but please feel fr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126361719
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126583954:543,Integrability,protocol,protocol,543,"Ahh, that's the number of *mappings* discarded. No need to worry about that. Basically, that's the number of places where seeding was tried, but alignment failed. This is very common in alignment (a seed can't be extended to a high quality alignment). The number of fragments discarded is what matters (number of fragments where all alignment locations failed). The strand bias signifies that your library is likely strand specific, though you are mapping in unstranded mode. This means that even alignments that don't agree with the stranded protocol will be allowed. This looks like ISR (first read from the reverse strand) by the looks of it. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126583954
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126583954:213,Modifiability,extend,extended,213,"Ahh, that's the number of *mappings* discarded. No need to worry about that. Basically, that's the number of places where seeding was tried, but alignment failed. This is very common in alignment (a seed can't be extended to a high quality alignment). The number of fragments discarded is what matters (number of fragments where all alignment locations failed). The strand bias signifies that your library is likely strand specific, though you are mapping in unstranded mode. This means that even alignments that don't agree with the stranded protocol will be allowed. This looks like ISR (first read from the reverse strand) by the looks of it. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126583954
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943:2311,Performance,optimiz,optimizer,2311,"me sample with salmon index generated as discussed above and got this report ; [2022-05-14 01:26:06.437] [jointLog] [info] Computed 380,631 rich equivalence classes for further processing; [2022-05-14 01:26:06.437] [jointLog] [info] Counted 22,462,069 total reads in the equivalence classes ; [2022-05-14 01:26:06.454] [jointLog] [info] Number of mappings discarded because of alignment score : 236,393,072; [2022-05-14 01:26:06.454] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 3,028,418; [2022-05-14 01:26:06.454] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0; [2022-05-14 01:26:06.454] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 1,137,227; [2022-05-14 01:26:06.454] [jointLog] [info] Mapping rate = 57.216%. [2022-05-14 01:26:06.455] [jointLog] [info] finished quantifyLibrary(); [2022-05-14 01:26:06.485] [jointLog] [info] Starting optimizer; [2022-05-14 01:26:06.581] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2022-05-14 01:26:06.593] [jointLog] [info] iteration = 0 | max rel diff. = 10507; [2022-05-14 01:26:07.966] [jointLog] [info] iteration = 100 | max rel diff. = 17.2222; [2022-05-14 01:26:09.253] [jointLog] [info] iteration = 200 | max rel diff. = 12.5822; [2022-05-14 01:26:10.641] [jointLog] [info] iteration = 300 | max rel diff. = 12.6466; [2022-05-14 01:26:11.976] [jointLog] [info] iteration = 400 | max rel diff. = 4.95752; [2022-05-14 01:26:13.272] [jointLog] [info] iteration = 500 | max rel diff. = 0.754259; [2022-05-14 01:26:14.546] [jointLog] [info] iteration = 600 | max rel diff. = 0.148902; [2022-05-14 01:26:15.788] [jointLog] [info] iteration = 700 | max rel diff. = 0.117727; [2022-05-14 01:26:17.074] [jointLog] [info] iteration = 800 | max rel diff. = 0.166671; [2022-05-14 01:26:18.385] [jointLog] [info] iteration = 900 | max rel diff. = 0.068019; [2022-05-14 01:26:19.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943:3527,Performance,optimiz,optimizer,3527," [jointLog] [info] iteration = 100 | max rel diff. = 17.2222; [2022-05-14 01:26:09.253] [jointLog] [info] iteration = 200 | max rel diff. = 12.5822; [2022-05-14 01:26:10.641] [jointLog] [info] iteration = 300 | max rel diff. = 12.6466; [2022-05-14 01:26:11.976] [jointLog] [info] iteration = 400 | max rel diff. = 4.95752; [2022-05-14 01:26:13.272] [jointLog] [info] iteration = 500 | max rel diff. = 0.754259; [2022-05-14 01:26:14.546] [jointLog] [info] iteration = 600 | max rel diff. = 0.148902; [2022-05-14 01:26:15.788] [jointLog] [info] iteration = 700 | max rel diff. = 0.117727; [2022-05-14 01:26:17.074] [jointLog] [info] iteration = 800 | max rel diff. = 0.166671; [2022-05-14 01:26:18.385] [jointLog] [info] iteration = 900 | max rel diff. = 0.068019; [2022-05-14 01:26:19.646] [jointLog] [info] iteration = 1,000 | max rel diff. = 0.00671654; [2022-05-14 01:26:19.646] [jointLog] [info] iteration = 1,001 | max rel diff. = 0.00671654; [2022-05-14 01:26:19.655] [jointLog] [info] Finished optimizer; [2022-05-14 01:26:19.655] [jointLog] [info] writing output . Then I generated another index similar to [using a full decoy of the genome](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) as suggested above but I got this report. [2022-05-14 00:49:06.636] [jointLog] [info] Number of mappings discarded because of alignment score : 7,179,799; [2022-05-14 00:49:06.636] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 3,986,275; [2022-05-14 00:49:06.636] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 3,572,798; [2022-05-14 00:49:06.636] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 54,775; [2022-05-14 00:49:06.636] [jointLog] [info] Mapping rate = 62.2613%. [2022-05-14 00:49:06.636] [jointLog] [info] finished quantifyLibrary(); [2022-05-14 00:49:06.643] [jointLog] [info] Starting optimizer; [2022-05-14 00",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943:4502,Performance,optimiz,optimizer,4502,"ointLog] [info] Finished optimizer; [2022-05-14 01:26:19.655] [jointLog] [info] writing output . Then I generated another index similar to [using a full decoy of the genome](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) as suggested above but I got this report. [2022-05-14 00:49:06.636] [jointLog] [info] Number of mappings discarded because of alignment score : 7,179,799; [2022-05-14 00:49:06.636] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 3,986,275; [2022-05-14 00:49:06.636] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 3,572,798; [2022-05-14 00:49:06.636] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 54,775; [2022-05-14 00:49:06.636] [jointLog] [info] Mapping rate = 62.2613%. [2022-05-14 00:49:06.636] [jointLog] [info] finished quantifyLibrary(); [2022-05-14 00:49:06.643] [jointLog] [info] Starting optimizer; [2022-05-14 00:49:06.706] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2022-05-14 00:49:06.713] [jointLog] [info] iteration = 0 | max rel diff. = 8788.91; [2022-05-14 00:49:07.363] [jointLog] [info] iteration = 100 | max rel diff. = 12.9125; [2022-05-14 00:49:08.016] [jointLog] [info] iteration = 200 | max rel diff. = 10.1452; [2022-05-14 00:49:08.665] [jointLog] [info] iteration = 300 | max rel diff. = 10.5557; [2022-05-14 00:49:09.322] [jointLog] [info] iteration = 400 | max rel diff. = 5.35911; [2022-05-14 00:49:09.990] [jointLog] [info] iteration = 500 | max rel diff. = 0.278805; [2022-05-14 00:49:10.647] [jointLog] [info] iteration = 600 | max rel diff. = 4.69875; [2022-05-14 00:49:11.295] [jointLog] [info] iteration = 700 | max rel diff. = 0.696517; [2022-05-14 00:49:11.994] [jointLog] [info] iteration = 800 | max rel diff. = 3.63395; [2022-05-14 00:49:12.648] [jointLog] [info] iteration = 900 | max rel diff. = 0.0421211; [2022-05-14 00:49:13",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943:5717,Performance,optimiz,optimizer,5717,"fo] Number of fragments discarded because they are best-mapped to decoys : 3,572,798; [2022-05-14 00:49:06.636] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 54,775; [2022-05-14 00:49:06.636] [jointLog] [info] Mapping rate = 62.2613%. [2022-05-14 00:49:06.636] [jointLog] [info] finished quantifyLibrary(); [2022-05-14 00:49:06.643] [jointLog] [info] Starting optimizer; [2022-05-14 00:49:06.706] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2022-05-14 00:49:06.713] [jointLog] [info] iteration = 0 | max rel diff. = 8788.91; [2022-05-14 00:49:07.363] [jointLog] [info] iteration = 100 | max rel diff. = 12.9125; [2022-05-14 00:49:08.016] [jointLog] [info] iteration = 200 | max rel diff. = 10.1452; [2022-05-14 00:49:08.665] [jointLog] [info] iteration = 300 | max rel diff. = 10.5557; [2022-05-14 00:49:09.322] [jointLog] [info] iteration = 400 | max rel diff. = 5.35911; [2022-05-14 00:49:09.990] [jointLog] [info] iteration = 500 | max rel diff. = 0.278805; [2022-05-14 00:49:10.647] [jointLog] [info] iteration = 600 | max rel diff. = 4.69875; [2022-05-14 00:49:11.295] [jointLog] [info] iteration = 700 | max rel diff. = 0.696517; [2022-05-14 00:49:11.994] [jointLog] [info] iteration = 800 | max rel diff. = 3.63395; [2022-05-14 00:49:12.648] [jointLog] [info] iteration = 900 | max rel diff. = 0.0421211; [2022-05-14 00:49:13.295] [jointLog] [info] iteration = 1,000 | max rel diff. = 0.150166; [2022-05-14 00:49:13.608] [jointLog] [info] iteration = 1,047 | max rel diff. = 0.00869236; [2022-05-14 00:49:13.620] [jointLog] [info] Finished optimizer; [2022-05-14 00:49:13.620] [jointLog] [info] writing output . I thought that the difference between the 84% from STAR and 57% from Salmon will be due mapping to introns or intergenic region (non-coding part) which I will get if I run salmon with the full decoy index but I got only 62% mapping.; Am I missing something here, please?; Thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943:77,Testability,log,log,77,"Hi Rob ; Thanks for your reply, May I just show another exmaple. This is the log.final.out from STAR. Number of input reads | 39258388; Average input read length | 300; UNIQUE READS:; Uniquely mapped reads number | 33103781; Uniquely mapped reads % | 84.32%; Average mapped length | 297.80; Number of splices: Total | 23754767; Number of splices: Annotated (sjdb) | 23730217; Number of splices: GT/AG | 23569617; Number of splices: GC/AG | 108014; Number of splices: AT/AC | 18563; Number of splices: Non-canonical | 58573; Mismatch rate per base, % | 0.26%; Deletion rate per base | 0.03%; Deletion average length | 3.17; Insertion rate per base | 0.01%; Insertion average length | 1.45; MULTI-MAPPING READS:; Number of reads mapped to multiple loci | 2524124; % of reads mapped to multiple loci | 6.43%; Number of reads mapped to too many loci | 518050; % of reads mapped to too many loci | 1.32%; UNMAPPED READS:; Number of reads unmapped: too many mismatches | 0; % of reads unmapped: too many mismatches | 0.00%; Number of reads unmapped: too short | 1717592; % of reads unmapped: too short | 4.38%; Number of reads unmapped: other | 1394841; % of reads unmapped: other | 3.55%; CHIMERIC READS:; Number of chimeric reads | 0; % of chimeric reads | 0.00%; sample6/align_6BE_Log.final.out (END). I run the same sample with salmon index generated as discussed above and got this report ; [2022-05-14 01:26:06.437] [jointLog] [info] Computed 380,631 rich equivalence classes for further processing; [2022-05-14 01:26:06.437] [jointLog] [info] Counted 22,462,069 total reads in the equivalence classes ; [2022-05-14 01:26:06.454] [jointLog] [info] Number of mappings discarded because of alignment score : 236,393,072; [2022-05-14 01:26:06.454] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 3,028,418; [2022-05-14 01:26:06.454] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0; [2022-05-14 01:26:06.454] [jointLog] [inf",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126593943
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126604296:158,Safety,avoid,avoid,158,"Hi @Sa753 . Yes, there are some other considerations. In this second dataset salon maps > 3M reads to the decoy. Further, since salmon only uses the decoy to avoid spurious alignment, it will only report reads as decoy if they map contiguously to the genome, not if they are spliced (but not mapping to an annotated transcript). Anyway, those reads will not be counted toward the mapping rate. While star reports all the reads mapped, salmon's mapping rate only counts reads that will actually be used for quantification. The numbers these different tools report as the mapping rate are just not comparable in terms of their meaning. If you want a more direct comparison, try something like running the STAR reads through feature counts and see how many are assigned to annotated genes. Alternatively, you can run STAR with --quantMode transcriptomeSAM, and pass the resulting BAM file as input to salmon (since it can also do transcript quantification with a transcript-coordinate BAM file) and see how many reads make it into that quantification. When situations like this arise, they almost always result from reads arising outside of the annotation. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126604296
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:330,Availability,error,error,330,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:102,Performance,perform,performing,102,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:136,Performance,optimiz,optimization,136,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:458,Performance,optimiz,optimization,458,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:772,Performance,optimiz,optimization,772,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:404,Usability,simpl,simple,404,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:513,Usability,clear,clearing,513,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474291747:1117,Availability,down,down,1117,"In my experience it is normal to get a **much lower** transcriptome mapping rate for rRNA-depleted samples vs polyA-selected samples. . I'm getting ~21% mapping rate (using Gencode 41 transcripts) on human brain RNAseq samples sequenced several years ago using rRNA-depletion protocol (older Illumina Ribo Zero kits).; I was initially shocked (being used to seeing >90% mapping rates from HISAT2/STAR for these samples) but it turns out **this is normal** for this kind of samples, in this context.; HISAT2 reports 96% mapping rate on the same samples, but QC metrics (rnaseqc) for these HISAT2 alignments (using the same Gencode annotation) show a **65% intronic rate** and a **%23.5 exonic rate** (the rest being intergenic etc). So the _exonic rate_ is getting close to what Salmon is showing (and what it measures), thus I suppose it makes sense to see such a low mapping rate for Salmon on these samples.; (kallisto also reports ~21% pseudoaligned percentage on the same samples). I am only a bit disappointed that when I use `--validateMappings` with decoy sequences (whole genome) added, the mapping rate goes down to about **16.7%** -- as some reads map better to the decoys in that case (partially intronic reads etc.), but I also see a `higher number of fragments entirely discarded because of alignment score` (higher `num_fragments_filtered_vm` and much higher `num_alignments_below_threshold_for_mapped_fragments_vm`).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474291747
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474291747:276,Integrability,protocol,protocol,276,"In my experience it is normal to get a **much lower** transcriptome mapping rate for rRNA-depleted samples vs polyA-selected samples. . I'm getting ~21% mapping rate (using Gencode 41 transcripts) on human brain RNAseq samples sequenced several years ago using rRNA-depletion protocol (older Illumina Ribo Zero kits).; I was initially shocked (being used to seeing >90% mapping rates from HISAT2/STAR for these samples) but it turns out **this is normal** for this kind of samples, in this context.; HISAT2 reports 96% mapping rate on the same samples, but QC metrics (rnaseqc) for these HISAT2 alignments (using the same Gencode annotation) show a **65% intronic rate** and a **%23.5 exonic rate** (the rest being intergenic etc). So the _exonic rate_ is getting close to what Salmon is showing (and what it measures), thus I suppose it makes sense to see such a low mapping rate for Salmon on these samples.; (kallisto also reports ~21% pseudoaligned percentage on the same samples). I am only a bit disappointed that when I use `--validateMappings` with decoy sequences (whole genome) added, the mapping rate goes down to about **16.7%** -- as some reads map better to the decoys in that case (partially intronic reads etc.), but I also see a `higher number of fragments entirely discarded because of alignment score` (higher `num_fragments_filtered_vm` and much higher `num_alignments_below_threshold_for_mapped_fragments_vm`).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474291747
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474291747:1034,Security,validat,validateMappings,1034,"In my experience it is normal to get a **much lower** transcriptome mapping rate for rRNA-depleted samples vs polyA-selected samples. . I'm getting ~21% mapping rate (using Gencode 41 transcripts) on human brain RNAseq samples sequenced several years ago using rRNA-depletion protocol (older Illumina Ribo Zero kits).; I was initially shocked (being used to seeing >90% mapping rates from HISAT2/STAR for these samples) but it turns out **this is normal** for this kind of samples, in this context.; HISAT2 reports 96% mapping rate on the same samples, but QC metrics (rnaseqc) for these HISAT2 alignments (using the same Gencode annotation) show a **65% intronic rate** and a **%23.5 exonic rate** (the rest being intergenic etc). So the _exonic rate_ is getting close to what Salmon is showing (and what it measures), thus I suppose it makes sense to see such a low mapping rate for Salmon on these samples.; (kallisto also reports ~21% pseudoaligned percentage on the same samples). I am only a bit disappointed that when I use `--validateMappings` with decoy sequences (whole genome) added, the mapping rate goes down to about **16.7%** -- as some reads map better to the decoys in that case (partially intronic reads etc.), but I also see a `higher number of fragments entirely discarded because of alignment score` (higher `num_fragments_filtered_vm` and much higher `num_alignments_below_threshold_for_mapped_fragments_vm`).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474291747
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474336994:804,Security,access,accession,804,"Hi @gpertea,. One thought about losing reads to the decoys in this case is that the default strategy is to assign as decoy any read that maps strictly better to the decoy than the target (transcriptome). So, even if e.g. the intron describes a single extra base, then the read gets assigned as decoy rather than transcriptomic. This is actually something that is easy to customize the behavior of (i.e. to add some ""slack"" so that reads have to map better to the decoy by some threshold before being assigned to the decoy). . Out of curiosity, what is salmon's mapping rate on this sample without the decoy? When you add the decoy, how many reads are assigned to the decoy sequence? Not that I necessarily suspect anything awry, but we'd be interested in taking a look at such samples anyway; what's the accession for the one you mention above?. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474336994
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:116,Modifiability,extend,extending,116,"Thank you @rob-p for commenting, I presumed that could be the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:751,Security,validat,validateMappings,751,"Thank you @rob-p for commenting, I presumed that could be the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:902,Security,validat,validateMappings,902,"Thank you @rob-p for commenting, I presumed that could be the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:1130,Security,validat,validateMappings,1130,"the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of pre-mRNAs + mature RNAs in each locus.. What do you think?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:1938,Usability,guid,guided,1938,"the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of pre-mRNAs + mature RNAs in each locus.. What do you think?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/782#issuecomment-1142131363:857,Usability,simpl,simply,857,"Hi @jonahcullen,. No need to apologize! We should better document these numbers. Basically, the elements you point out : `num_decoy_fragments`, `num_dovetail_fragments` and `num_fragments_filtered_vm` are the fragments where alignment was *attempted* but subsequently failed. In these cases because (1) the read best mapped to a decoy, (2) the best alignment was dovetailed or (3) no alignment passed the alignment score threshold. In addition to this, fragments can fail to align when no sufficiently good seed is found such that alignment is not even attempted. This can happen e.g. if no 31-mer from the read matches the transcriptome/genome, or if the only matching 31-mers are degenerate in terms of their frequency (appear thousands of times and are therefore not useful for alignment). So, the most likely occurrence here is that these ~1M fragments simply had no alignment attempted. Let me know if this answers your question. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/782#issuecomment-1142131363
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440:108,Availability,avail,available,108,"Hi @silvanopiazza,. It would seem that this pre-compiled binary is making use of an instruction that is not available on the CPU on which you are executing `salmon`. In this case, there are a few alternatives:. * Try and run the command through a Docker image — the latest salmon is always available on [Dockerhub](https://hub.docker.com/layers/8712688/combinelab/salmon/latest/images/sha256-1c0b7e5b8a0996b6080cfc76fcd4e565f8c92689fe3cf1debc8b7493ae964c14?context=repo).; * Try and [install salmon via bioconda](https://anaconda.org/bioconda/salmon). The bioconda build may be making fewer assumptions about the target architecture than the pre-compiled github binary.; * Compile salmon from source locally. This will, of course, guarantee to only use instructions available on your hardware, though it's the most involved of these options.; ; Let me know if any of these work for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440:290,Availability,avail,available,290,"Hi @silvanopiazza,. It would seem that this pre-compiled binary is making use of an instruction that is not available on the CPU on which you are executing `salmon`. In this case, there are a few alternatives:. * Try and run the command through a Docker image — the latest salmon is always available on [Dockerhub](https://hub.docker.com/layers/8712688/combinelab/salmon/latest/images/sha256-1c0b7e5b8a0996b6080cfc76fcd4e565f8c92689fe3cf1debc8b7493ae964c14?context=repo).; * Try and [install salmon via bioconda](https://anaconda.org/bioconda/salmon). The bioconda build may be making fewer assumptions about the target architecture than the pre-compiled github binary.; * Compile salmon from source locally. This will, of course, guarantee to only use instructions available on your hardware, though it's the most involved of these options.; ; Let me know if any of these work for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440:766,Availability,avail,available,766,"Hi @silvanopiazza,. It would seem that this pre-compiled binary is making use of an instruction that is not available on the CPU on which you are executing `salmon`. In this case, there are a few alternatives:. * Try and run the command through a Docker image — the latest salmon is always available on [Dockerhub](https://hub.docker.com/layers/8712688/combinelab/salmon/latest/images/sha256-1c0b7e5b8a0996b6080cfc76fcd4e565f8c92689fe3cf1debc8b7493ae964c14?context=repo).; * Try and [install salmon via bioconda](https://anaconda.org/bioconda/salmon). The bioconda build may be making fewer assumptions about the target architecture than the pre-compiled github binary.; * Compile salmon from source locally. This will, of course, guarantee to only use instructions available on your hardware, though it's the most involved of these options.; ; Let me know if any of these work for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440:484,Deployability,install,install,484,"Hi @silvanopiazza,. It would seem that this pre-compiled binary is making use of an instruction that is not available on the CPU on which you are executing `salmon`. In this case, there are a few alternatives:. * Try and run the command through a Docker image — the latest salmon is always available on [Dockerhub](https://hub.docker.com/layers/8712688/combinelab/salmon/latest/images/sha256-1c0b7e5b8a0996b6080cfc76fcd4e565f8c92689fe3cf1debc8b7493ae964c14?context=repo).; * Try and [install salmon via bioconda](https://anaconda.org/bioconda/salmon). The bioconda build may be making fewer assumptions about the target architecture than the pre-compiled github binary.; * Compile salmon from source locally. This will, of course, guarantee to only use instructions available on your hardware, though it's the most involved of these options.; ; Let me know if any of these work for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440:338,Modifiability,layers,layers,338,"Hi @silvanopiazza,. It would seem that this pre-compiled binary is making use of an instruction that is not available on the CPU on which you are executing `salmon`. In this case, there are a few alternatives:. * Try and run the command through a Docker image — the latest salmon is always available on [Dockerhub](https://hub.docker.com/layers/8712688/combinelab/salmon/latest/images/sha256-1c0b7e5b8a0996b6080cfc76fcd4e565f8c92689fe3cf1debc8b7493ae964c14?context=repo).; * Try and [install salmon via bioconda](https://anaconda.org/bioconda/salmon). The bioconda build may be making fewer assumptions about the target architecture than the pre-compiled github binary.; * Compile salmon from source locally. This will, of course, guarantee to only use instructions available on your hardware, though it's the most involved of these options.; ; Let me know if any of these work for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1143871440
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:415,Availability,error,error,415,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:435,Availability,fault,fault,435,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:905,Availability,error,error,905,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:987,Availability,error,error,987,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:1024,Availability,error,error,1024,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:1126,Availability,Error,Error,1126,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:1206,Availability,Error,Error,1206,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:1245,Availability,Error,Error,1245,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:173,Deployability,install,install,173,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:890,Integrability,wrap,wrapper,890,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013:998,Integrability,wrap,wrapper,998,"Dear Rob, thank you for the fast replay. a) I tried to compile but it is not working for me either; ....; _[100%] Linking CXX executable salmon; /usr/bin/ld: ../../external/install/lib/libstaden-read.a(libstaden_read_la-open_trace_file.o): in function `find_file_url':; open_trace_file.c:(.text+0xf74): warning: the use of `tempnam' is dangerous, better use `mkstemp'; during IPA pass: icf; lto1: internal compiler error: Segmentation fault; 0xd12487 internal_error(char const*, ...); 	???:0; 0xe4f0b6 varpool_node::get_constructor(); 	???:0; 0xea01ff ipa_icf::sem_item_optimizer::subdivide_classes_by_equality(bool); 	???:0; 0x12ab4cb ipa_icf::sem_item_optimizer::execute(); 	???:0; Please submit a full bug report,; with preprocessed source if appropriate.; Please include the complete backtrace with any bug report.; See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.; lto-wrapper: fatal error: /usr/bin/c++ returned 1 exit status; compilation terminated.; /usr/bin/ld: error: lto-wrapper failed; collect2: error: ld returned 1 exit status; make[2]: *** [src/CMakeFiles/salmon.dir/build.make:486: src/salmon] Error 1; make[1]: *** [CMakeFiles/Makefile2:665: src/CMakeFiles/salmon.dir/all] Error 2; make: *** [Makefile:166: all] Error 2_. 2) bioconda on a server is a mess. it will be my last resort. 3) I am trying the Docker version and it seems to work!!!; ; PS It seems to me a be a little strange though that the binary version is not working. I tried on three different systems (all with ubuntu 22.04) with Xeon gold and intel i7 . thank you again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144000013
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144002615:234,Performance,optimiz,optimization,234,"Hi @silvanopiazza,. I'm glad Docker is working for you. If you compile from source, you should make sure to include `-DNO_IPO=TRUE` in your `cmake` command if you are compiling on GCC. This is because GCC support for inter-procedural optimization is rather broken currently.. I agree that it's strange to encounter such an illegal instruction. Especially since the machine doing the compiling is an older Xeon (circa 2017). I wonder if there's an easy way to figure out what the instruction is. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1144002615
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835:211,Availability,error,error,211,"Dear Rob,; a brief update:; 1) with the flag -DNO_IPO=TRUE the compilation worked perfectly. thank you . 2)following a guide found at stackoverlow ([Find which assembly instruction caused an Illegal Instruction error without debugging], I discover that the illegal instruction is **vfmsubsd**. ; I am not an expert at all in the field, but googling it seems to be a standard SSE instruction.; I am surprised indeed.; cpus tested: ; Intel Xeon Gold 5220 (72) ; Intel Xeon Gold 5317 (48); Intel i7-10750H (12). Best and thanks again; Silvano. Program terminated with signal SIGILL, Illegal instruction.; #0 0x00007fa222c47396 in __ieee754_pow_fma4 () from /dataraw/mouse/salmon-1.8.0_linux_x86_64/bin/../lib/libm.so.6. 0x7fa222c47396 <__ieee754_pow_fma4+182> vfmsubsd %xmm3,%xmm6,%xmm3,%xmm7",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835:19,Deployability,update,update,19,"Dear Rob,; a brief update:; 1) with the flag -DNO_IPO=TRUE the compilation worked perfectly. thank you . 2)following a guide found at stackoverlow ([Find which assembly instruction caused an Illegal Instruction error without debugging], I discover that the illegal instruction is **vfmsubsd**. ; I am not an expert at all in the field, but googling it seems to be a standard SSE instruction.; I am surprised indeed.; cpus tested: ; Intel Xeon Gold 5220 (72) ; Intel Xeon Gold 5317 (48); Intel i7-10750H (12). Best and thanks again; Silvano. Program terminated with signal SIGILL, Illegal instruction.; #0 0x00007fa222c47396 in __ieee754_pow_fma4 () from /dataraw/mouse/salmon-1.8.0_linux_x86_64/bin/../lib/libm.so.6. 0x7fa222c47396 <__ieee754_pow_fma4+182> vfmsubsd %xmm3,%xmm6,%xmm3,%xmm7",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835:422,Testability,test,tested,422,"Dear Rob,; a brief update:; 1) with the flag -DNO_IPO=TRUE the compilation worked perfectly. thank you . 2)following a guide found at stackoverlow ([Find which assembly instruction caused an Illegal Instruction error without debugging], I discover that the illegal instruction is **vfmsubsd**. ; I am not an expert at all in the field, but googling it seems to be a standard SSE instruction.; I am surprised indeed.; cpus tested: ; Intel Xeon Gold 5220 (72) ; Intel Xeon Gold 5317 (48); Intel i7-10750H (12). Best and thanks again; Silvano. Program terminated with signal SIGILL, Illegal instruction.; #0 0x00007fa222c47396 in __ieee754_pow_fma4 () from /dataraw/mouse/salmon-1.8.0_linux_x86_64/bin/../lib/libm.so.6. 0x7fa222c47396 <__ieee754_pow_fma4+182> vfmsubsd %xmm3,%xmm6,%xmm3,%xmm7",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835:119,Usability,guid,guide,119,"Dear Rob,; a brief update:; 1) with the flag -DNO_IPO=TRUE the compilation worked perfectly. thank you . 2)following a guide found at stackoverlow ([Find which assembly instruction caused an Illegal Instruction error without debugging], I discover that the illegal instruction is **vfmsubsd**. ; I am not an expert at all in the field, but googling it seems to be a standard SSE instruction.; I am surprised indeed.; cpus tested: ; Intel Xeon Gold 5220 (72) ; Intel Xeon Gold 5317 (48); Intel i7-10750H (12). Best and thanks again; Silvano. Program terminated with signal SIGILL, Illegal instruction.; #0 0x00007fa222c47396 in __ieee754_pow_fma4 () from /dataraw/mouse/salmon-1.8.0_linux_x86_64/bin/../lib/libm.so.6. 0x7fa222c47396 <__ieee754_pow_fma4+182> vfmsubsd %xmm3,%xmm6,%xmm3,%xmm7",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145876082:146,Deployability,install,install,146,"Thanks fornthe details, @silvanopiazza! I wonder if it would be fixed if you *removed* the copy of `libm` in the `lib` subdirectory of the salmon install folder?. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145876082
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1148320487:105,Deployability,install,installation,105,"Dear Rob, ; sorry for the late replay.; the culprit was indeed the libm.; when i REMOVED from the binary installation folder, salmon was able to produce the index. _[2022-06-07 09:43:33.637] [jLog] [info] done building index_. Bests; Silvano",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1148320487
https://github.com/COMBINE-lab/salmon/issues/784#issuecomment-1170721108:106,Deployability,pipeline,pipeline,106,"Could you please expand upon the question a bit? Are you asking if I would recommend the [nf-core RNA-seq pipeline](https://nf-co.re/rnaseq/usage) using `salmon` as the quantifier? In general, I think it is a nice pipeline that provides a good deal of information and a nice report. If you're looking for an off-the-shelf workflow for sample quantification for gene-level differential expression, I think that nf-core's RNA-seq workflow is a good option for quality-control and quantification. If you want to do transcript-level differential analysis, you may want to consider [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), in which case you need to make sure that salmon is run with parameters to generate inferential replicates (either `--numBootstraps X` or `--numGibbsSamples X`). You can ask over in the nf-core GitHub repo or Slack channel how best to pass those extra options. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/784#issuecomment-1170721108
https://github.com/COMBINE-lab/salmon/issues/784#issuecomment-1170721108:214,Deployability,pipeline,pipeline,214,"Could you please expand upon the question a bit? Are you asking if I would recommend the [nf-core RNA-seq pipeline](https://nf-co.re/rnaseq/usage) using `salmon` as the quantifier? In general, I think it is a nice pipeline that provides a good deal of information and a nice report. If you're looking for an off-the-shelf workflow for sample quantification for gene-level differential expression, I think that nf-core's RNA-seq workflow is a good option for quality-control and quantification. If you want to do transcript-level differential analysis, you may want to consider [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), in which case you need to make sure that salmon is run with parameters to generate inferential replicates (either `--numBootstraps X` or `--numGibbsSamples X`). You can ask over in the nf-core GitHub repo or Slack channel how best to pass those extra options. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/784#issuecomment-1170721108
https://github.com/COMBINE-lab/salmon/issues/785#issuecomment-1205698238:87,Deployability,pipeline,pipeline,87,"i didn't try to fix the star index, it was created automatically by the; bcbio_nextgen pipeline i was using. i did add my story to the issue in the; star github repo mentioned in the biostars thread:; https://github.com/alexdobin/STAR/issues/1140. On Tue, Aug 2, 2022 at 11:27 AM HeedukOh ***@***.***> wrote:. > i ran into the same problem and apparently it's a STAR issue:; > https://www.biostars.org/p/486346/; >; > ""...it seems STAR is doing something during the indexing step which is; > causing a slight mismatch for 23 of the transcripts.""; >; > Hi,; > Thanks for the reply!; > I see that you used Salmon for indexing to get around this issue. Did you; > figure out a way to make STAR work after that, or did you stick with Salmon?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/785#issuecomment-1202823526>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABGJSQSM6CJ2GDKQF3UOZKLVXE47NANCNFSM5ZOT3OOQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/785#issuecomment-1205698238
https://github.com/COMBINE-lab/salmon/issues/785#issuecomment-1205698238:1047,Integrability,Message,Message,1047,"i didn't try to fix the star index, it was created automatically by the; bcbio_nextgen pipeline i was using. i did add my story to the issue in the; star github repo mentioned in the biostars thread:; https://github.com/alexdobin/STAR/issues/1140. On Tue, Aug 2, 2022 at 11:27 AM HeedukOh ***@***.***> wrote:. > i ran into the same problem and apparently it's a STAR issue:; > https://www.biostars.org/p/486346/; >; > ""...it seems STAR is doing something during the indexing step which is; > causing a slight mismatch for 23 of the transcripts.""; >; > Hi,; > Thanks for the reply!; > I see that you used Salmon for indexing to get around this issue. Did you; > figure out a way to make STAR work after that, or did you stick with Salmon?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/785#issuecomment-1202823526>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABGJSQSM6CJ2GDKQF3UOZKLVXE47NANCNFSM5ZOT3OOQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/785#issuecomment-1205698238
https://github.com/COMBINE-lab/salmon/issues/786#issuecomment-1184344483:160,Testability,test,test,160,@lananh-ngn . if it is direct RNA seq as you mentioned then would it make more sense to run `minimap2` as folow:; `minimap2 -ax splice -uf -k14 transcriptom.fa test.fastq > result.sam `. `-ax map-ont ` is for Oxford Nanopore genomic reads as stated in minimap2 github.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/786#issuecomment-1184344483
https://github.com/COMBINE-lab/salmon/issues/786#issuecomment-1184348921:28,Integrability,message,message,28,"Hello, ; Thank you for your message. However, I am working with fission yeast transcriptome and since I am not expecting a lot of splicing junctions with this organism, I read that people recommended using -ax map-ont in this case?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/786#issuecomment-1184348921
https://github.com/COMBINE-lab/salmon/issues/786#issuecomment-1184610671:28,Integrability,message,message,28,"> Hello, Thank you for your message. However, I am working with fission yeast transcriptome and since I am not expecting a lot of splicing junctions with this organism, I read that people recommended using -ax map-ont in this case?. Ye, it is. If there are not many splicing junctions, then it is like runging with genomic reads. But still, I do not see any recommantion of doing it. . For your first questions, is `7ff17d41-0678-447f - acd0-57b53d35ba32` an ID of a read from your `fastq` file? For me, It does not look like a gene name. If it is an ID, how do you get this?. After I ran the quantificaton, what I get is like this：; gens, length and so on.; ![image](https://user-images.githubusercontent.com/14146871/179023113-d06ee0a3-4efd-406e-8736-895345cafae5.png). I import Genomes and bam file inoto IGV to check `ATMG01170.1`:; ![image](https://user-images.githubusercontent.com/14146871/179024217-63f8920f-0ab3-4c42-b4cf-24dbd9de1134.png). I am not familiar with IGV so I did not know how to import the .gtf file to see what you showed. ![image](https://user-images.githubusercontent.com/14146871/179024687-9aed6cfd-e203-415f-9f9f-c6501a915c27.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/786#issuecomment-1184610671
https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170723368:479,Deployability,install,install,479,"Hi @BenjaminDEMAILLE,. I think brew is a bit behind bioconda on this front, and, indeed, the M1 being a completely new architecture complicates things. I have an M1 Max and so there are plans to get a native compile going soon. For the time being, the recommended way to get `salmon` on an M1 (or M2) Mac is as suggested [here](https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671). Basically, you create an x86 conda environment (running under rosetta2) and install the latest version of salmon there. Rosetta2 is pretty amazing, and everything seems to run without a hitch, with nary a performance hit for the x86 -> ARM translation. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170723368
https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170723368:608,Performance,perform,performance,608,"Hi @BenjaminDEMAILLE,. I think brew is a bit behind bioconda on this front, and, indeed, the M1 being a completely new architecture complicates things. I have an M1 Max and so there are plans to get a native compile going soon. For the time being, the recommended way to get `salmon` on an M1 (or M2) Mac is as suggested [here](https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137183671). Basically, you create an x86 conda environment (running under rosetta2) and install the latest version of salmon there. Rosetta2 is pretty amazing, and everything seems to run without a hitch, with nary a performance hit for the x86 -> ARM translation. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170723368
https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170808558:20,Deployability,release,release,20,"Thanks ! ; When you release the apple silocon version. Is it possible to upload to brew ? Do you want to take in account the gpu with metal or the neural engine ? . > Le 30 juin 2022 à 05:58, Rob Patro ***@***.***> a écrit :; > ; > ﻿; > Hi @BenjaminDEMAILLE,; > ; > I think brew is a bit behind bioconda on this front, and, indeed, the M1 being a completely new architecture complicates things. I have an M1 Max and so there are plans to get a native compile going soon.; > ; > For the time being, the recommended way to get salmon on an M1 (or M2) Mac is as suggested here. Basically, you create an x86 conda environment (running under rosetta2) and install the latest version of salmon there. Rosetta2 is pretty amazing, and everything seems to run without a hitch, with nary a performance hit for the x86 -> ARM translation.; > ; > Best,; > Rob; > ; > —; > Reply to this email directly, view it on GitHub, or unsubscribe.; > You are receiving this because you were mentioned.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170808558
https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170808558:651,Deployability,install,install,651,"Thanks ! ; When you release the apple silocon version. Is it possible to upload to brew ? Do you want to take in account the gpu with metal or the neural engine ? . > Le 30 juin 2022 à 05:58, Rob Patro ***@***.***> a écrit :; > ; > ﻿; > Hi @BenjaminDEMAILLE,; > ; > I think brew is a bit behind bioconda on this front, and, indeed, the M1 being a completely new architecture complicates things. I have an M1 Max and so there are plans to get a native compile going soon.; > ; > For the time being, the recommended way to get salmon on an M1 (or M2) Mac is as suggested here. Basically, you create an x86 conda environment (running under rosetta2) and install the latest version of salmon there. Rosetta2 is pretty amazing, and everything seems to run without a hitch, with nary a performance hit for the x86 -> ARM translation.; > ; > Best,; > Rob; > ; > —; > Reply to this email directly, view it on GitHub, or unsubscribe.; > You are receiving this because you were mentioned.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170808558
https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170808558:780,Performance,perform,performance,780,"Thanks ! ; When you release the apple silocon version. Is it possible to upload to brew ? Do you want to take in account the gpu with metal or the neural engine ? . > Le 30 juin 2022 à 05:58, Rob Patro ***@***.***> a écrit :; > ; > ﻿; > Hi @BenjaminDEMAILLE,; > ; > I think brew is a bit behind bioconda on this front, and, indeed, the M1 being a completely new architecture complicates things. I have an M1 Max and so there are plans to get a native compile going soon.; > ; > For the time being, the recommended way to get salmon on an M1 (or M2) Mac is as suggested here. Basically, you create an x86 conda environment (running under rosetta2) and install the latest version of salmon there. Rosetta2 is pretty amazing, and everything seems to run without a hitch, with nary a performance hit for the x86 -> ARM translation.; > ; > Best,; > Rob; > ; > —; > Reply to this email directly, view it on GitHub, or unsubscribe.; > You are receiving this because you were mentioned.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/787#issuecomment-1170808558
https://github.com/COMBINE-lab/salmon/issues/788#issuecomment-1482927046:21,Availability,error,error,21,"I'm getting the same error as reported above. Copying the code i ran below:; ```; # download reference genome; curl -JLO https://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/latest_assembly_versions/GCF_009914755.1_T2T-CHM13v2.0/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna.gz; # extract chromosome names; grep ""^>"" <(gunzip -c GCF_009914755.1_T2T-CHM13v2.0_genomic.fna.gz) | cut -d "" "" -f 1 > GCF_009914755.1_T2T-CHM13v2.0_genomic.txt; # download transcriptome; curl -JLO https://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/latest_assembly_versions/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_rna.fna.gz; # combine transcriptome and genome, in that order; cat GCF_000001405.40_GRCh38.p14_rna.fna.gz GCF_009914755.1_T2T-CHM13v2.0_genomic.fna.gz > human_seq.fa.gz; # give to salmon to index; salmon index -t human_seq.fa.gz -i salmon_index -d GCF_009914755.1_T2T-CHM13v2.0_genomic.txt; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/788#issuecomment-1482927046
https://github.com/COMBINE-lab/salmon/issues/788#issuecomment-1482927046:84,Availability,down,download,84,"I'm getting the same error as reported above. Copying the code i ran below:; ```; # download reference genome; curl -JLO https://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/latest_assembly_versions/GCF_009914755.1_T2T-CHM13v2.0/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna.gz; # extract chromosome names; grep ""^>"" <(gunzip -c GCF_009914755.1_T2T-CHM13v2.0_genomic.fna.gz) | cut -d "" "" -f 1 > GCF_009914755.1_T2T-CHM13v2.0_genomic.txt; # download transcriptome; curl -JLO https://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/latest_assembly_versions/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_rna.fna.gz; # combine transcriptome and genome, in that order; cat GCF_000001405.40_GRCh38.p14_rna.fna.gz GCF_009914755.1_T2T-CHM13v2.0_genomic.fna.gz > human_seq.fa.gz; # give to salmon to index; salmon index -t human_seq.fa.gz -i salmon_index -d GCF_009914755.1_T2T-CHM13v2.0_genomic.txt; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/788#issuecomment-1482927046
https://github.com/COMBINE-lab/salmon/issues/788#issuecomment-1482927046:461,Availability,down,download,461,"I'm getting the same error as reported above. Copying the code i ran below:; ```; # download reference genome; curl -JLO https://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/latest_assembly_versions/GCF_009914755.1_T2T-CHM13v2.0/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna.gz; # extract chromosome names; grep ""^>"" <(gunzip -c GCF_009914755.1_T2T-CHM13v2.0_genomic.fna.gz) | cut -d "" "" -f 1 > GCF_009914755.1_T2T-CHM13v2.0_genomic.txt; # download transcriptome; curl -JLO https://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/latest_assembly_versions/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_rna.fna.gz; # combine transcriptome and genome, in that order; cat GCF_000001405.40_GRCh38.p14_rna.fna.gz GCF_009914755.1_T2T-CHM13v2.0_genomic.fna.gz > human_seq.fa.gz; # give to salmon to index; salmon index -t human_seq.fa.gz -i salmon_index -d GCF_009914755.1_T2T-CHM13v2.0_genomic.txt; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/788#issuecomment-1482927046
https://github.com/COMBINE-lab/salmon/issues/790#issuecomment-2028185516:124,Deployability,update,update,124,"@mousepixels Apologies for slow reply, I found myself circling back to this same issue with another project and thought I'd update the thread. Seems like there was some guidance all along regarding dealing with ONT data. See this [link ](https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/). . To summarize, looks like they advise _-N 100 -p 1.0_ for minimap2, which coincidently is what I have been doing as well. Hope that's helpful if you haven't already come up with a strategy.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/790#issuecomment-2028185516
https://github.com/COMBINE-lab/salmon/issues/790#issuecomment-2028185516:169,Usability,guid,guidance,169,"@mousepixels Apologies for slow reply, I found myself circling back to this same issue with another project and thought I'd update the thread. Seems like there was some guidance all along regarding dealing with ONT data. See this [link ](https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/). . To summarize, looks like they advise _-N 100 -p 1.0_ for minimap2, which coincidently is what I have been doing as well. Hope that's helpful if you haven't already come up with a strategy.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/790#issuecomment-2028185516
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752:380,Availability,repair,repair,380,"In this case, it is likely treating the left and right reads as orphans when mapping. Therefore, you're losing basically all of the benefit of having paired-end reads (which can be considerable) and also increasing the probability of spurious mapping (orphans generally map much more ambiguously than properly paired reads). Since you have the paired-end files, you should try to repair them (using something like BBMap's [re-pair tool](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/repair-guide/)) to get back properly-paired FASTQ files for analysis. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752:515,Availability,repair,repair-guide,515,"In this case, it is likely treating the left and right reads as orphans when mapping. Therefore, you're losing basically all of the benefit of having paired-end reads (which can be considerable) and also increasing the probability of spurious mapping (orphans generally map much more ambiguously than properly paired reads). Since you have the paired-end files, you should try to repair them (using something like BBMap's [re-pair tool](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/repair-guide/)) to get back properly-paired FASTQ files for analysis. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752:509,Usability,guid,guide,509,"In this case, it is likely treating the left and right reads as orphans when mapping. Therefore, you're losing basically all of the benefit of having paired-end reads (which can be considerable) and also increasing the probability of spurious mapping (orphans generally map much more ambiguously than properly paired reads). Since you have the paired-end files, you should try to repair them (using something like BBMap's [re-pair tool](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/repair-guide/)) to get back properly-paired FASTQ files for analysis. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752:522,Usability,guid,guide,522,"In this case, it is likely treating the left and right reads as orphans when mapping. Therefore, you're losing basically all of the benefit of having paired-end reads (which can be considerable) and also increasing the probability of spurious mapping (orphans generally map much more ambiguously than properly paired reads). Since you have the paired-end files, you should try to repair them (using something like BBMap's [re-pair tool](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/repair-guide/)) to get back properly-paired FASTQ files for analysis. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220248939:1083,Safety,detect,detected,1083,"It really doesn't seem like Salmon is treating them all as orphans. As an example, our forward file has 17385254 reads, reverse 17361911. Salmon says:. ```; At end of round 0; ==================; Observed 17361911 total fragments (17361911 in most recent round). [2022-04-11 23:11:53.713] [jointLog] [info] Computed 2,763,922 rich equivalence classes for further processing; [2022-04-11 23:11:53.731] [jointLog] [info] Counted 16,393,065 total reads in the equivalence classes ; ... [2022-04-11 23:11:53.743] [jointLog] [info] Number of mappings discarded because of alignment score : 680,083; [2022-04-11 23:11:53.743] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 263,662; [2022-04-11 23:11:53.743] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0; [2022-04-11 23:11:53.743] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 2,335; [2022-04-11 23:11:53.744] [jointLog] [info] Mapping rate = 94.4197%; ```. Also, library is auto-detected as IU and mean fragment length is computed as 417.8. This looks to me like most of the reads are recognized as paired, which in fact they are.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220248939
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220773109:139,Integrability,synchroniz,synchronization,139,"Hi @kdorman,. Ok, let me refine my response a little bit. When you said that the files were ""unpaired"" I was under the assumption that the synchronization between the left and right files was broken; that is that the read names in the left file are in a different order than the right file. In this case, `salmon` doesn't check explicitly, and it doesn't store reads internally and wait for the mate in the other file. Every time it reads a record from the left and right file together, it assumes they constitute a pair during sequencing. If the files become desynchronized, you will likely observe the behavior I mentioned in my initial response.; ; However, if the reads remain properly paired, then they will be mapped as such. Judging from the total number of reported fragments in the output above, it looks as though it's attempted to map the first `17361911` reads from both files, treating them as paired-end reads (ignoring the extra records in file 1 that have no mate in file 2). You could check if it's the case that these reads are properly paired, and that file 1 just contains some extra reads that were unpaired after filtering.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220773109
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941:142,Availability,down,downloaded,142,"Hi @esraagithub,. Thanks for the bug report. Can you tell me how the specific version of salmon you are using was installed (e.g. via source, downloaded from the ""releases page"", or installed via bioconda)? Would it be possible to share the contigs that cause this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941:265,Availability,error,error,265,"Hi @esraagithub,. Thanks for the bug report. Can you tell me how the specific version of salmon you are using was installed (e.g. via source, downloaded from the ""releases page"", or installed via bioconda)? Would it be possible to share the contigs that cause this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941:114,Deployability,install,installed,114,"Hi @esraagithub,. Thanks for the bug report. Can you tell me how the specific version of salmon you are using was installed (e.g. via source, downloaded from the ""releases page"", or installed via bioconda)? Would it be possible to share the contigs that cause this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941:163,Deployability,release,releases,163,"Hi @esraagithub,. Thanks for the bug report. Can you tell me how the specific version of salmon you are using was installed (e.g. via source, downloaded from the ""releases page"", or installed via bioconda)? Would it be possible to share the contigs that cause this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941:182,Deployability,install,installed,182,"Hi @esraagithub,. Thanks for the bug report. Can you tell me how the specific version of salmon you are using was installed (e.g. via source, downloaded from the ""releases page"", or installed via bioconda)? Would it be possible to share the contigs that cause this error?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767:388,Availability,down,downloaded,388,"Finalky i found that my transcript file was crashed during uploading to my; server; I repeated the process. It works now; Thank you so much. في الاثنين، ٢٢ أغسطس ٢٠٢٢ ٣:٢١ م Rob Patro ***@***.***> كتب:. > Hi @esraagithub <https://github.com/esraagithub>,; >; > Thanks for the bug report. Can you tell me how the specific version of; > salmon you are using was installed (e.g. via source, downloaded from the; > ""releases page"", or installed via bioconda)? Would it be possible to share; > the contigs that cause this error?; >; > Thanks,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMY4XZX6ZGAO5DOU5YOWTG3V2N5DZANCNFSM57HUQWJQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767:517,Availability,error,error,517,"Finalky i found that my transcript file was crashed during uploading to my; server; I repeated the process. It works now; Thank you so much. في الاثنين، ٢٢ أغسطس ٢٠٢٢ ٣:٢١ م Rob Patro ***@***.***> كتب:. > Hi @esraagithub <https://github.com/esraagithub>,; >; > Thanks for the bug report. Can you tell me how the specific version of; > salmon you are using was installed (e.g. via source, downloaded from the; > ""releases page"", or installed via bioconda)? Would it be possible to share; > the contigs that cause this error?; >; > Thanks,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMY4XZX6ZGAO5DOU5YOWTG3V2N5DZANCNFSM57HUQWJQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767:360,Deployability,install,installed,360,"Finalky i found that my transcript file was crashed during uploading to my; server; I repeated the process. It works now; Thank you so much. في الاثنين، ٢٢ أغسطس ٢٠٢٢ ٣:٢١ م Rob Patro ***@***.***> كتب:. > Hi @esraagithub <https://github.com/esraagithub>,; >; > Thanks for the bug report. Can you tell me how the specific version of; > salmon you are using was installed (e.g. via source, downloaded from the; > ""releases page"", or installed via bioconda)? Would it be possible to share; > the contigs that cause this error?; >; > Thanks,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMY4XZX6ZGAO5DOU5YOWTG3V2N5DZANCNFSM57HUQWJQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767:412,Deployability,release,releases,412,"Finalky i found that my transcript file was crashed during uploading to my; server; I repeated the process. It works now; Thank you so much. في الاثنين، ٢٢ أغسطس ٢٠٢٢ ٣:٢١ م Rob Patro ***@***.***> كتب:. > Hi @esraagithub <https://github.com/esraagithub>,; >; > Thanks for the bug report. Can you tell me how the specific version of; > salmon you are using was installed (e.g. via source, downloaded from the; > ""releases page"", or installed via bioconda)? Would it be possible to share; > the contigs that cause this error?; >; > Thanks,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMY4XZX6ZGAO5DOU5YOWTG3V2N5DZANCNFSM57HUQWJQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767:431,Deployability,install,installed,431,"Finalky i found that my transcript file was crashed during uploading to my; server; I repeated the process. It works now; Thank you so much. في الاثنين، ٢٢ أغسطس ٢٠٢٢ ٣:٢١ م Rob Patro ***@***.***> كتب:. > Hi @esraagithub <https://github.com/esraagithub>,; >; > Thanks for the bug report. Can you tell me how the specific version of; > salmon you are using was installed (e.g. via source, downloaded from the; > ""releases page"", or installed via bioconda)? Would it be possible to share; > the contigs that cause this error?; >; > Thanks,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMY4XZX6ZGAO5DOU5YOWTG3V2N5DZANCNFSM57HUQWJQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767
https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767:859,Integrability,Message,Message,859,"Finalky i found that my transcript file was crashed during uploading to my; server; I repeated the process. It works now; Thank you so much. في الاثنين، ٢٢ أغسطس ٢٠٢٢ ٣:٢١ م Rob Patro ***@***.***> كتب:. > Hi @esraagithub <https://github.com/esraagithub>,; >; > Thanks for the bug report. Can you tell me how the specific version of; > salmon you are using was installed (e.g. via source, downloaded from the; > ""releases page"", or installed via bioconda)? Would it be possible to share; > the contigs that cause this error?; >; > Thanks,; > Rob; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1222353941>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMY4XZX6ZGAO5DOU5YOWTG3V2N5DZANCNFSM57HUQWJQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/795#issuecomment-1223885767
https://github.com/COMBINE-lab/salmon/issues/796#issuecomment-1235769651:99,Availability,down,down,99,"Hi @matthew-valentine,. In general, extra non-primary alignments are OK. This can, of course, slow down quantification somewhat because many more alignments are being evaluated. However, salmon (with the `--ont` flag) is designed to consider the provided alignments and allocate the corresponding read proportionally according to all of the relevant probabilities (including alignment quality). If, under this aligner setting, there are many *highly* sub-optimal alignments being reported, you may consider filtering them out, but that shouldn't be strictly necessary. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/796#issuecomment-1235769651
https://github.com/COMBINE-lab/salmon/issues/796#issuecomment-1235769651:270,Energy Efficiency,allocate,allocate,270,"Hi @matthew-valentine,. In general, extra non-primary alignments are OK. This can, of course, slow down quantification somewhat because many more alignments are being evaluated. However, salmon (with the `--ont` flag) is designed to consider the provided alignments and allocate the corresponding read proportionally according to all of the relevant probabilities (including alignment quality). If, under this aligner setting, there are many *highly* sub-optimal alignments being reported, you may consider filtering them out, but that shouldn't be strictly necessary. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/796#issuecomment-1235769651
https://github.com/COMBINE-lab/salmon/issues/797#issuecomment-1238807484:249,Deployability,install,install,249,@Arturo3116 - looks like your file/directory structure is needed to help. You could go 1 level up from where you were running `salmon index` and share the output of the `tree` command (https://www.tecmint.com/linux-tree-command-examples/) . You can install tree via conda - https://anaconda.org/conda-forge/tree,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/797#issuecomment-1238807484
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883:1883,Availability,down,downstream,1883,"eports to be valid with respect to the annotated transcriptome. I am guessing that many alignments overhang the end of the annotated transcripts, and so STAR does not project them to the transcriptome and so salmon cannot count them. **In mapping mode**, the nf-core pipeline makes use of salmon's selective-alignment _with decoy sequences_. The main purpose of this is to avoid spurious mapping to transcriptomic sequences that may be similar to other unannotated sequences in the genome that are nonetheless a better match for the read (e.g. an unannotated possibly transcribed pseudogene). The way this works in practice is that both the transcript sequences themselves *and the full genome* are indexed. Any read that aligns _strictly better_ to the genome than the transcriptome is considered to map to a decoy, and is not used for the purposes of quantification. Consistent with the behavior I hypothesized above for STAR, if you have many softclipped bases at the end of the read that nonetheless match what is in the genome downstream of the end of the annotated transcript, you'll likely see these reads assigned as decoys. To check this, you can look at salmon's `meta_info.json` output file to see how many reads were mapped best to decoys. * Why do I see much higher counts for this gene with FeatureCounts?. * It depends on the specific behavior you invoke. However, my guess is that FeatureCounts is being run with flags such that reads that only somewhat overlap a feature are nonetheless assigned to it. This suggests that while no good alignment may actually exist to the annotated transcript, FeatureCounts is still assigning the read to that feature because it overlaps it to some degree and matches the corresponding location on the genome. Again, you can test this by changing the required overlap fraction of FeatureCounts. * Why does running salmon outside of nf-core produce much higher counts?. * Since you are indexing *just* the transcriptome, and not including the genome ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883:1118,Deployability,pipeline,pipeline,1118,"ainly be interested in hearing about this and understanding the likely source of this kind of discrepancy. From what you've explained, here is my current hypothesis of what's going on. * Why are the salmon counts much lower for this gene when using alignment-mode and mapping mode under nf-core?. * Though the behavior you observe is similar, I think the cause is somewhat different. **In alignment mode**, STAR is used for alignment. The alignments are made against the genome and then _projected_ onto the annotated transcriptome. STAR has many internal rules for when an alignment can be successfully projected or not. In this case, STAR limits the number of soft clips it will permit in an alignment that it reports to be valid with respect to the annotated transcriptome. I am guessing that many alignments overhang the end of the annotated transcripts, and so STAR does not project them to the transcriptome and so salmon cannot count them. **In mapping mode**, the nf-core pipeline makes use of salmon's selective-alignment _with decoy sequences_. The main purpose of this is to avoid spurious mapping to transcriptomic sequences that may be similar to other unannotated sequences in the genome that are nonetheless a better match for the read (e.g. an unannotated possibly transcribed pseudogene). The way this works in practice is that both the transcript sequences themselves *and the full genome* are indexed. Any read that aligns _strictly better_ to the genome than the transcriptome is considered to map to a decoy, and is not used for the purposes of quantification. Consistent with the behavior I hypothesized above for STAR, if you have many softclipped bases at the end of the read that nonetheless match what is in the genome downstream of the end of the annotated transcript, you'll likely see these reads assigned as decoys. To check this, you can look at salmon's `meta_info.json` output file to see how many reads were mapped best to decoys. * Why do I see much higher counts f",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883:3548,Energy Efficiency,reduce,reduce,3548," it to some degree and matches the corresponding location on the genome. Again, you can test this by changing the required overlap fraction of FeatureCounts. * Why does running salmon outside of nf-core produce much higher counts?. * Since you are indexing *just* the transcriptome, and not including the genome as decoy sequence (as is done in nf-core), then the only thing that will prevent reads from being assigned to the gene in question is if so much of the read overhangs off the end of the annotated transcript that no mapping matches the minimum required alignment score. This is likely to be a much more liberal threshold than what STAR allows, so it also explains why you see higher counts than when alignment mode is used. * Other thoughts / suggestions?. * So, there are several things that you might consider doing if you believe the correct behavior in your case is to assign these reads to such genes. First, when run in mapping mode, salmon has a `--softclipOverhangs` flag that will further reduce the penalty for reads overhanging the annotated end of a transcript. This will allow more reads to map to the transcript even if they can't obtain a good alignment score. Likewise, you can combine this with further reducing the required minimum score using the `--minScoreFraction` [parameter](https://salmon.readthedocs.io/en/latest/salmon.html#minscorefraction). Finally, looking forward, we have developed and been testing even more comprehensive solutions to cases when one wants to allow large amounts of soft-clipping (see e.g. [this tutorial](https://combine-lab.github.io/salmon-tutorials/2021/softclip/)). While those features have not yet been migrated into the main salmon branch, you may find the tutorial instructive and the corresponding feature branch useful. If you believe that the annotations themselves are incomplete/incorrect and that may be leading to some of this behavior, you might consider augmenting or updating those annotations. Finally, I'd be reticent to",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883:2177,Integrability,depend,depends,2177," main purpose of this is to avoid spurious mapping to transcriptomic sequences that may be similar to other unannotated sequences in the genome that are nonetheless a better match for the read (e.g. an unannotated possibly transcribed pseudogene). The way this works in practice is that both the transcript sequences themselves *and the full genome* are indexed. Any read that aligns _strictly better_ to the genome than the transcriptome is considered to map to a decoy, and is not used for the purposes of quantification. Consistent with the behavior I hypothesized above for STAR, if you have many softclipped bases at the end of the read that nonetheless match what is in the genome downstream of the end of the annotated transcript, you'll likely see these reads assigned as decoys. To check this, you can look at salmon's `meta_info.json` output file to see how many reads were mapped best to decoys. * Why do I see much higher counts for this gene with FeatureCounts?. * It depends on the specific behavior you invoke. However, my guess is that FeatureCounts is being run with flags such that reads that only somewhat overlap a feature are nonetheless assigned to it. This suggests that while no good alignment may actually exist to the annotated transcript, FeatureCounts is still assigning the read to that feature because it overlaps it to some degree and matches the corresponding location on the genome. Again, you can test this by changing the required overlap fraction of FeatureCounts. * Why does running salmon outside of nf-core produce much higher counts?. * Since you are indexing *just* the transcriptome, and not including the genome as decoy sequence (as is done in nf-core), then the only thing that will prevent reads from being assigned to the gene in question is if so much of the read overhangs off the end of the annotated transcript that no mapping matches the minimum required alignment score. This is likely to be a much more liberal threshold than what STAR allows, so",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883:1224,Safety,avoid,avoid,1224,"hypothesis of what's going on. * Why are the salmon counts much lower for this gene when using alignment-mode and mapping mode under nf-core?. * Though the behavior you observe is similar, I think the cause is somewhat different. **In alignment mode**, STAR is used for alignment. The alignments are made against the genome and then _projected_ onto the annotated transcriptome. STAR has many internal rules for when an alignment can be successfully projected or not. In this case, STAR limits the number of soft clips it will permit in an alignment that it reports to be valid with respect to the annotated transcriptome. I am guessing that many alignments overhang the end of the annotated transcripts, and so STAR does not project them to the transcriptome and so salmon cannot count them. **In mapping mode**, the nf-core pipeline makes use of salmon's selective-alignment _with decoy sequences_. The main purpose of this is to avoid spurious mapping to transcriptomic sequences that may be similar to other unannotated sequences in the genome that are nonetheless a better match for the read (e.g. an unannotated possibly transcribed pseudogene). The way this works in practice is that both the transcript sequences themselves *and the full genome* are indexed. Any read that aligns _strictly better_ to the genome than the transcriptome is considered to map to a decoy, and is not used for the purposes of quantification. Consistent with the behavior I hypothesized above for STAR, if you have many softclipped bases at the end of the read that nonetheless match what is in the genome downstream of the end of the annotated transcript, you'll likely see these reads assigned as decoys. To check this, you can look at salmon's `meta_info.json` output file to see how many reads were mapped best to decoys. * Why do I see much higher counts for this gene with FeatureCounts?. * It depends on the specific behavior you invoke. However, my guess is that FeatureCounts is being run with flags such t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883:2627,Testability,test,test,2627,"ap to a decoy, and is not used for the purposes of quantification. Consistent with the behavior I hypothesized above for STAR, if you have many softclipped bases at the end of the read that nonetheless match what is in the genome downstream of the end of the annotated transcript, you'll likely see these reads assigned as decoys. To check this, you can look at salmon's `meta_info.json` output file to see how many reads were mapped best to decoys. * Why do I see much higher counts for this gene with FeatureCounts?. * It depends on the specific behavior you invoke. However, my guess is that FeatureCounts is being run with flags such that reads that only somewhat overlap a feature are nonetheless assigned to it. This suggests that while no good alignment may actually exist to the annotated transcript, FeatureCounts is still assigning the read to that feature because it overlaps it to some degree and matches the corresponding location on the genome. Again, you can test this by changing the required overlap fraction of FeatureCounts. * Why does running salmon outside of nf-core produce much higher counts?. * Since you are indexing *just* the transcriptome, and not including the genome as decoy sequence (as is done in nf-core), then the only thing that will prevent reads from being assigned to the gene in question is if so much of the read overhangs off the end of the annotated transcript that no mapping matches the minimum required alignment score. This is likely to be a much more liberal threshold than what STAR allows, so it also explains why you see higher counts than when alignment mode is used. * Other thoughts / suggestions?. * So, there are several things that you might consider doing if you believe the correct behavior in your case is to assign these reads to such genes. First, when run in mapping mode, salmon has a `--softclipOverhangs` flag that will further reduce the penalty for reads overhanging the annotated end of a transcript. This will allow more reads to ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883:3973,Testability,test,testing,3973," read overhangs off the end of the annotated transcript that no mapping matches the minimum required alignment score. This is likely to be a much more liberal threshold than what STAR allows, so it also explains why you see higher counts than when alignment mode is used. * Other thoughts / suggestions?. * So, there are several things that you might consider doing if you believe the correct behavior in your case is to assign these reads to such genes. First, when run in mapping mode, salmon has a `--softclipOverhangs` flag that will further reduce the penalty for reads overhanging the annotated end of a transcript. This will allow more reads to map to the transcript even if they can't obtain a good alignment score. Likewise, you can combine this with further reducing the required minimum score using the `--minScoreFraction` [parameter](https://salmon.readthedocs.io/en/latest/salmon.html#minscorefraction). Finally, looking forward, we have developed and been testing even more comprehensive solutions to cases when one wants to allow large amounts of soft-clipping (see e.g. [this tutorial](https://combine-lab.github.io/salmon-tutorials/2021/softclip/)). While those features have not yet been migrated into the main salmon branch, you may find the tutorial instructive and the corresponding feature branch useful. If you believe that the annotations themselves are incomplete/incorrect and that may be leading to some of this behavior, you might consider augmenting or updating those annotations. Finally, I'd be reticent to just go with FeatureCounts instead here. While the heuristics employed by the overlap and counting rules may accord with what you expect for this gene or some subset of similar genes, the counting based approach implements several heuristics that can be problematic in a number of other scenarios. Hopefully this helps answer your question about this behavior. If you end up discussing this with the nf-core folks, I'd be happy to be involved in that discussion ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237133883
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190:384,Availability,avail,available,384,"Hi @FlorianRNA ! As stated in the [usage docs](https://nf-co.re/rnaseq/3.8.1/usage#quantification-options) for the nf-core/rnaseq pipeline:. ""Since v3.0 of the pipeline, featureCounts is no longer used to perform gene/transcript quantification, however it is still used to generate QC metrics based on [biotype](http://www.ensembl.org/info/genome/genebuild/biotypes.html) information available within GFF/GTF genome annotation files. This decision was made primarily because of the limitations of featureCounts to appropriately quantify gene expression data. Please see [Zhao et al., 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141910#pone-0141910-t001) and [Soneson et al., 2015](https://f1000research.com/articles/4-1521/v1)."". This is a common cause of confusion and I have tried to be as explicit about this in the docs. featureCounts is used to quantify features in the annotation by `gene_biotype` and not the actual gene / transcript features themselves. This may explain why you are seeing these discrepancies. However, I am still a little puzzled how you are able to directly compare the counts generated by featureCounts and Salmon (in either mode) because the core features that are being quantified should be different. Where did you get the plant reference genome from? If it's not from Ensembl then it probably isn't worth running the biotype quantification with featureCounts anyway because the GTF annotation files may not contain that information. There are some docs for this [here](https://nf-co.re/rnaseq/3.8.1/usage#prokaryotic-genome-annotations). Hope that helps and if you think we can improve the pipeline in any way please feel free to create an issue on the nf-core/rnaseq repo.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190:130,Deployability,pipeline,pipeline,130,"Hi @FlorianRNA ! As stated in the [usage docs](https://nf-co.re/rnaseq/3.8.1/usage#quantification-options) for the nf-core/rnaseq pipeline:. ""Since v3.0 of the pipeline, featureCounts is no longer used to perform gene/transcript quantification, however it is still used to generate QC metrics based on [biotype](http://www.ensembl.org/info/genome/genebuild/biotypes.html) information available within GFF/GTF genome annotation files. This decision was made primarily because of the limitations of featureCounts to appropriately quantify gene expression data. Please see [Zhao et al., 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141910#pone-0141910-t001) and [Soneson et al., 2015](https://f1000research.com/articles/4-1521/v1)."". This is a common cause of confusion and I have tried to be as explicit about this in the docs. featureCounts is used to quantify features in the annotation by `gene_biotype` and not the actual gene / transcript features themselves. This may explain why you are seeing these discrepancies. However, I am still a little puzzled how you are able to directly compare the counts generated by featureCounts and Salmon (in either mode) because the core features that are being quantified should be different. Where did you get the plant reference genome from? If it's not from Ensembl then it probably isn't worth running the biotype quantification with featureCounts anyway because the GTF annotation files may not contain that information. There are some docs for this [here](https://nf-co.re/rnaseq/3.8.1/usage#prokaryotic-genome-annotations). Hope that helps and if you think we can improve the pipeline in any way please feel free to create an issue on the nf-core/rnaseq repo.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190:160,Deployability,pipeline,pipeline,160,"Hi @FlorianRNA ! As stated in the [usage docs](https://nf-co.re/rnaseq/3.8.1/usage#quantification-options) for the nf-core/rnaseq pipeline:. ""Since v3.0 of the pipeline, featureCounts is no longer used to perform gene/transcript quantification, however it is still used to generate QC metrics based on [biotype](http://www.ensembl.org/info/genome/genebuild/biotypes.html) information available within GFF/GTF genome annotation files. This decision was made primarily because of the limitations of featureCounts to appropriately quantify gene expression data. Please see [Zhao et al., 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141910#pone-0141910-t001) and [Soneson et al., 2015](https://f1000research.com/articles/4-1521/v1)."". This is a common cause of confusion and I have tried to be as explicit about this in the docs. featureCounts is used to quantify features in the annotation by `gene_biotype` and not the actual gene / transcript features themselves. This may explain why you are seeing these discrepancies. However, I am still a little puzzled how you are able to directly compare the counts generated by featureCounts and Salmon (in either mode) because the core features that are being quantified should be different. Where did you get the plant reference genome from? If it's not from Ensembl then it probably isn't worth running the biotype quantification with featureCounts anyway because the GTF annotation files may not contain that information. There are some docs for this [here](https://nf-co.re/rnaseq/3.8.1/usage#prokaryotic-genome-annotations). Hope that helps and if you think we can improve the pipeline in any way please feel free to create an issue on the nf-core/rnaseq repo.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190:1651,Deployability,pipeline,pipeline,1651,"Hi @FlorianRNA ! As stated in the [usage docs](https://nf-co.re/rnaseq/3.8.1/usage#quantification-options) for the nf-core/rnaseq pipeline:. ""Since v3.0 of the pipeline, featureCounts is no longer used to perform gene/transcript quantification, however it is still used to generate QC metrics based on [biotype](http://www.ensembl.org/info/genome/genebuild/biotypes.html) information available within GFF/GTF genome annotation files. This decision was made primarily because of the limitations of featureCounts to appropriately quantify gene expression data. Please see [Zhao et al., 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141910#pone-0141910-t001) and [Soneson et al., 2015](https://f1000research.com/articles/4-1521/v1)."". This is a common cause of confusion and I have tried to be as explicit about this in the docs. featureCounts is used to quantify features in the annotation by `gene_biotype` and not the actual gene / transcript features themselves. This may explain why you are seeing these discrepancies. However, I am still a little puzzled how you are able to directly compare the counts generated by featureCounts and Salmon (in either mode) because the core features that are being quantified should be different. Where did you get the plant reference genome from? If it's not from Ensembl then it probably isn't worth running the biotype quantification with featureCounts anyway because the GTF annotation files may not contain that information. There are some docs for this [here](https://nf-co.re/rnaseq/3.8.1/usage#prokaryotic-genome-annotations). Hope that helps and if you think we can improve the pipeline in any way please feel free to create an issue on the nf-core/rnaseq repo.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190:205,Performance,perform,perform,205,"Hi @FlorianRNA ! As stated in the [usage docs](https://nf-co.re/rnaseq/3.8.1/usage#quantification-options) for the nf-core/rnaseq pipeline:. ""Since v3.0 of the pipeline, featureCounts is no longer used to perform gene/transcript quantification, however it is still used to generate QC metrics based on [biotype](http://www.ensembl.org/info/genome/genebuild/biotypes.html) information available within GFF/GTF genome annotation files. This decision was made primarily because of the limitations of featureCounts to appropriately quantify gene expression data. Please see [Zhao et al., 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141910#pone-0141910-t001) and [Soneson et al., 2015](https://f1000research.com/articles/4-1521/v1)."". This is a common cause of confusion and I have tried to be as explicit about this in the docs. featureCounts is used to quantify features in the annotation by `gene_biotype` and not the actual gene / transcript features themselves. This may explain why you are seeing these discrepancies. However, I am still a little puzzled how you are able to directly compare the counts generated by featureCounts and Salmon (in either mode) because the core features that are being quantified should be different. Where did you get the plant reference genome from? If it's not from Ensembl then it probably isn't worth running the biotype quantification with featureCounts anyway because the GTF annotation files may not contain that information. There are some docs for this [here](https://nf-co.re/rnaseq/3.8.1/usage#prokaryotic-genome-annotations). Hope that helps and if you think we can improve the pipeline in any way please feel free to create an issue on the nf-core/rnaseq repo.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1237865190
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213:932,Deployability,update,update,932,"Hello @rob-p,. thank you very much for your quick and detailed answer!. > Why are the salmon counts for this gene much lower when using alignment mode and mapping mode under nf-core?. This theory makes perfect sense and would explain the low counts for the short genes perfectly. When I check the bam files, I see quite a few reads that are only partially in the region of the particular feature. ; I looked at the meta_info.json: . ""num_bootstraps"": 0,; ""num_processed"": 36672829,; ""num_mapped"": 27737862,; ""num_decoy_fragments"": 2690181,; ""num_dovetail_fragments"": 59605,; ""num_fragments_filtered_vm"": 2897142,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 4117684,; ""percent_mapped"": 75.63600288376989,. It looks like the decoy percentage is substantial. ; I'll run the pseudo-aligment in decoy mode and take a look at the unmapped reads to see if there are many that could be mapped to the chloroplast genome (I'll update this later). > Why am I seeing much higher values for this gene with FeatureCounts?. I have now run FeatureCounts several times with different overlaps (minOverlap =25, minOverlap =50, minOverlap =75min Overlap =100) and indeed the counts have decreased (again the psbI example: 8685 , 6011, 4237, 1805 accordingly). Again, this is a good argument for the hypothesis put forward. >Why does running Salmon outside nf-core lead to much higher values?. Hopefully, after I run Decoy mode, this problem is solved. I also tried mapping mode with the --softclipOverhangs option. That increased the counts (psbI : 4696 counts); playing around with the --minScoreFraction flag in addition to the --softclipOverhangs flag also increased the numbers ( minScoreFraction= 0 ->psbI = 8496; minScoreFraction= 0.5 ->psbI = 5633; minScoreFraction= 0.7 ->psbI =3627 ). . So, in summary, your explanation seems to be completely correct. ; In the case that decoy mode resolves the difference between the pipeline and the run outside the pipeline, I would not give this to the nf-cor",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213:1923,Deployability,pipeline,pipeline,1923,"pdate this later). > Why am I seeing much higher values for this gene with FeatureCounts?. I have now run FeatureCounts several times with different overlaps (minOverlap =25, minOverlap =50, minOverlap =75min Overlap =100) and indeed the counts have decreased (again the psbI example: 8685 , 6011, 4237, 1805 accordingly). Again, this is a good argument for the hypothesis put forward. >Why does running Salmon outside nf-core lead to much higher values?. Hopefully, after I run Decoy mode, this problem is solved. I also tried mapping mode with the --softclipOverhangs option. That increased the counts (psbI : 4696 counts); playing around with the --minScoreFraction flag in addition to the --softclipOverhangs flag also increased the numbers ( minScoreFraction= 0 ->psbI = 8496; minScoreFraction= 0.5 ->psbI = 5633; minScoreFraction= 0.7 ->psbI =3627 ). . So, in summary, your explanation seems to be completely correct. ; In the case that decoy mode resolves the difference between the pipeline and the run outside the pipeline, I would not give this to the nf-core people. But I will if there are still large discrepancies after the run. I'm still not sure what the best parameters are for my analysis, but the --softclipOverhangs flag seems to be the best option for me now.; So thanks again!. @drpatelh. Thank you very much for your quick reply as well. ; I was a bit inaccurate when I said I used the FeatureCounts from the pipeline. I actually wasn't able to use the resulting .txt files. Instead, I used the resulting bam file from the pipeline to perform a FeatureCounts analysis on R. I hope this information answers the question of how I can compare the two results?; My genome and gtf file are from [EnsemblPlants](https://plants.ensembl.org/Arabidopsis_thaliana/Info/Index), so they should be fine. In the MultiQC file, the vast majority of reads align to protein coding regions according to FeatureCounts, so I hope my primary files are fine. . Thanks again for your help and time!. A",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213:1956,Deployability,pipeline,pipeline,1956,"pdate this later). > Why am I seeing much higher values for this gene with FeatureCounts?. I have now run FeatureCounts several times with different overlaps (minOverlap =25, minOverlap =50, minOverlap =75min Overlap =100) and indeed the counts have decreased (again the psbI example: 8685 , 6011, 4237, 1805 accordingly). Again, this is a good argument for the hypothesis put forward. >Why does running Salmon outside nf-core lead to much higher values?. Hopefully, after I run Decoy mode, this problem is solved. I also tried mapping mode with the --softclipOverhangs option. That increased the counts (psbI : 4696 counts); playing around with the --minScoreFraction flag in addition to the --softclipOverhangs flag also increased the numbers ( minScoreFraction= 0 ->psbI = 8496; minScoreFraction= 0.5 ->psbI = 5633; minScoreFraction= 0.7 ->psbI =3627 ). . So, in summary, your explanation seems to be completely correct. ; In the case that decoy mode resolves the difference between the pipeline and the run outside the pipeline, I would not give this to the nf-core people. But I will if there are still large discrepancies after the run. I'm still not sure what the best parameters are for my analysis, but the --softclipOverhangs flag seems to be the best option for me now.; So thanks again!. @drpatelh. Thank you very much for your quick reply as well. ; I was a bit inaccurate when I said I used the FeatureCounts from the pipeline. I actually wasn't able to use the resulting .txt files. Instead, I used the resulting bam file from the pipeline to perform a FeatureCounts analysis on R. I hope this information answers the question of how I can compare the two results?; My genome and gtf file are from [EnsemblPlants](https://plants.ensembl.org/Arabidopsis_thaliana/Info/Index), so they should be fine. In the MultiQC file, the vast majority of reads align to protein coding regions according to FeatureCounts, so I hope my primary files are fine. . Thanks again for your help and time!. A",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213:2365,Deployability,pipeline,pipeline,2365,"Why am I seeing much higher values for this gene with FeatureCounts?. I have now run FeatureCounts several times with different overlaps (minOverlap =25, minOverlap =50, minOverlap =75min Overlap =100) and indeed the counts have decreased (again the psbI example: 8685 , 6011, 4237, 1805 accordingly). Again, this is a good argument for the hypothesis put forward. >Why does running Salmon outside nf-core lead to much higher values?. Hopefully, after I run Decoy mode, this problem is solved. I also tried mapping mode with the --softclipOverhangs option. That increased the counts (psbI : 4696 counts); playing around with the --minScoreFraction flag in addition to the --softclipOverhangs flag also increased the numbers ( minScoreFraction= 0 ->psbI = 8496; minScoreFraction= 0.5 ->psbI = 5633; minScoreFraction= 0.7 ->psbI =3627 ). . So, in summary, your explanation seems to be completely correct. ; In the case that decoy mode resolves the difference between the pipeline and the run outside the pipeline, I would not give this to the nf-core people. But I will if there are still large discrepancies after the run. I'm still not sure what the best parameters are for my analysis, but the --softclipOverhangs flag seems to be the best option for me now.; So thanks again!. @drpatelh. Thank you very much for your quick reply as well. ; I was a bit inaccurate when I said I used the FeatureCounts from the pipeline. I actually wasn't able to use the resulting .txt files. Instead, I used the resulting bam file from the pipeline to perform a FeatureCounts analysis on R. I hope this information answers the question of how I can compare the two results?; My genome and gtf file are from [EnsemblPlants](https://plants.ensembl.org/Arabidopsis_thaliana/Info/Index), so they should be fine. In the MultiQC file, the vast majority of reads align to protein coding regions according to FeatureCounts, so I hope my primary files are fine. . Thanks again for your help and time!. All the best ; Florian",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213:2479,Deployability,pipeline,pipeline,2479,"Why am I seeing much higher values for this gene with FeatureCounts?. I have now run FeatureCounts several times with different overlaps (minOverlap =25, minOverlap =50, minOverlap =75min Overlap =100) and indeed the counts have decreased (again the psbI example: 8685 , 6011, 4237, 1805 accordingly). Again, this is a good argument for the hypothesis put forward. >Why does running Salmon outside nf-core lead to much higher values?. Hopefully, after I run Decoy mode, this problem is solved. I also tried mapping mode with the --softclipOverhangs option. That increased the counts (psbI : 4696 counts); playing around with the --minScoreFraction flag in addition to the --softclipOverhangs flag also increased the numbers ( minScoreFraction= 0 ->psbI = 8496; minScoreFraction= 0.5 ->psbI = 5633; minScoreFraction= 0.7 ->psbI =3627 ). . So, in summary, your explanation seems to be completely correct. ; In the case that decoy mode resolves the difference between the pipeline and the run outside the pipeline, I would not give this to the nf-core people. But I will if there are still large discrepancies after the run. I'm still not sure what the best parameters are for my analysis, but the --softclipOverhangs flag seems to be the best option for me now.; So thanks again!. @drpatelh. Thank you very much for your quick reply as well. ; I was a bit inaccurate when I said I used the FeatureCounts from the pipeline. I actually wasn't able to use the resulting .txt files. Instead, I used the resulting bam file from the pipeline to perform a FeatureCounts analysis on R. I hope this information answers the question of how I can compare the two results?; My genome and gtf file are from [EnsemblPlants](https://plants.ensembl.org/Arabidopsis_thaliana/Info/Index), so they should be fine. In the MultiQC file, the vast majority of reads align to protein coding regions according to FeatureCounts, so I hope my primary files are fine. . Thanks again for your help and time!. All the best ; Florian",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213:2491,Performance,perform,perform,2491,"Why am I seeing much higher values for this gene with FeatureCounts?. I have now run FeatureCounts several times with different overlaps (minOverlap =25, minOverlap =50, minOverlap =75min Overlap =100) and indeed the counts have decreased (again the psbI example: 8685 , 6011, 4237, 1805 accordingly). Again, this is a good argument for the hypothesis put forward. >Why does running Salmon outside nf-core lead to much higher values?. Hopefully, after I run Decoy mode, this problem is solved. I also tried mapping mode with the --softclipOverhangs option. That increased the counts (psbI : 4696 counts); playing around with the --minScoreFraction flag in addition to the --softclipOverhangs flag also increased the numbers ( minScoreFraction= 0 ->psbI = 8496; minScoreFraction= 0.5 ->psbI = 5633; minScoreFraction= 0.7 ->psbI =3627 ). . So, in summary, your explanation seems to be completely correct. ; In the case that decoy mode resolves the difference between the pipeline and the run outside the pipeline, I would not give this to the nf-core people. But I will if there are still large discrepancies after the run. I'm still not sure what the best parameters are for my analysis, but the --softclipOverhangs flag seems to be the best option for me now.; So thanks again!. @drpatelh. Thank you very much for your quick reply as well. ; I was a bit inaccurate when I said I used the FeatureCounts from the pipeline. I actually wasn't able to use the resulting .txt files. Instead, I used the resulting bam file from the pipeline to perform a FeatureCounts analysis on R. I hope this information answers the question of how I can compare the two results?; My genome and gtf file are from [EnsemblPlants](https://plants.ensembl.org/Arabidopsis_thaliana/Info/Index), so they should be fine. In the MultiQC file, the vast majority of reads align to protein coding regions according to FeatureCounts, so I hope my primary files are fine. . Thanks again for your help and time!. All the best ; Florian",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1238043213
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429:61,Deployability,pipeline,pipeline,61,"Ah, my bad. I assumed you were running featureCounts via the pipeline. Thanks for clarifying. Happy to incorporate changes into the pipeline in the future if they improve the default behaviour. For now, you can tweak the settings you provide the pipeline to incorporate the `--softclipOverhangs` parameter. You can put the snippet below in a file called `custom.config` and pass to the pipeline on the CLI with `-c custom.config`:. ```; process {; withName: '.*:QUANTIFY_SALMON:SALMON_QUANT' {; ext.args = '--softclipOverhangs'; }; }; ```. Let me know if you have any problems with this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429:132,Deployability,pipeline,pipeline,132,"Ah, my bad. I assumed you were running featureCounts via the pipeline. Thanks for clarifying. Happy to incorporate changes into the pipeline in the future if they improve the default behaviour. For now, you can tweak the settings you provide the pipeline to incorporate the `--softclipOverhangs` parameter. You can put the snippet below in a file called `custom.config` and pass to the pipeline on the CLI with `-c custom.config`:. ```; process {; withName: '.*:QUANTIFY_SALMON:SALMON_QUANT' {; ext.args = '--softclipOverhangs'; }; }; ```. Let me know if you have any problems with this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429:246,Deployability,pipeline,pipeline,246,"Ah, my bad. I assumed you were running featureCounts via the pipeline. Thanks for clarifying. Happy to incorporate changes into the pipeline in the future if they improve the default behaviour. For now, you can tweak the settings you provide the pipeline to incorporate the `--softclipOverhangs` parameter. You can put the snippet below in a file called `custom.config` and pass to the pipeline on the CLI with `-c custom.config`:. ```; process {; withName: '.*:QUANTIFY_SALMON:SALMON_QUANT' {; ext.args = '--softclipOverhangs'; }; }; ```. Let me know if you have any problems with this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429:386,Deployability,pipeline,pipeline,386,"Ah, my bad. I assumed you were running featureCounts via the pipeline. Thanks for clarifying. Happy to incorporate changes into the pipeline in the future if they improve the default behaviour. For now, you can tweak the settings you provide the pipeline to incorporate the `--softclipOverhangs` parameter. You can put the snippet below in a file called `custom.config` and pass to the pipeline on the CLI with `-c custom.config`:. ```; process {; withName: '.*:QUANTIFY_SALMON:SALMON_QUANT' {; ext.args = '--softclipOverhangs'; }; }; ```. Let me know if you have any problems with this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429:362,Modifiability,config,config,362,"Ah, my bad. I assumed you were running featureCounts via the pipeline. Thanks for clarifying. Happy to incorporate changes into the pipeline in the future if they improve the default behaviour. For now, you can tweak the settings you provide the pipeline to incorporate the `--softclipOverhangs` parameter. You can put the snippet below in a file called `custom.config` and pass to the pipeline on the CLI with `-c custom.config`:. ```; process {; withName: '.*:QUANTIFY_SALMON:SALMON_QUANT' {; ext.args = '--softclipOverhangs'; }; }; ```. Let me know if you have any problems with this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429
https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429:422,Modifiability,config,config,422,"Ah, my bad. I assumed you were running featureCounts via the pipeline. Thanks for clarifying. Happy to incorporate changes into the pipeline in the future if they improve the default behaviour. For now, you can tweak the settings you provide the pipeline to incorporate the `--softclipOverhangs` parameter. You can put the snippet below in a file called `custom.config` and pass to the pipeline on the CLI with `-c custom.config`:. ```; process {; withName: '.*:QUANTIFY_SALMON:SALMON_QUANT' {; ext.args = '--softclipOverhangs'; }; }; ```. Let me know if you have any problems with this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/798#issuecomment-1239100429
https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245687786:796,Deployability,pipeline,pipeline,796,"Hi @gamabunta313,. When you say `mapping-mode`, do you mean that you are passing a SAM file _into_ salmon rather than passing the FASTQ file and letting it perform the mapping itself? Because salmon makes use of a statistical inference procedure to determine the ultimate allocations of reads to transcripts, you cannot just count up the reads ""mapped"" to a transcript to obtain the count you see in the `quant.sf` file. Rather, when you see a read mapped to a transcript in the SAM/BAM file, you should interpret this as the read _could_ be assigned (most likely proportionally) to the transcript. The `--writeMappings` option is primarily intended for the mode where `salmon` performs the mapping itself, rather than the mode where aligned reads are provided as input. What does your alignment pipeline look like upstream of salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245687786
https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245687786:156,Performance,perform,perform,156,"Hi @gamabunta313,. When you say `mapping-mode`, do you mean that you are passing a SAM file _into_ salmon rather than passing the FASTQ file and letting it perform the mapping itself? Because salmon makes use of a statistical inference procedure to determine the ultimate allocations of reads to transcripts, you cannot just count up the reads ""mapped"" to a transcript to obtain the count you see in the `quant.sf` file. Rather, when you see a read mapped to a transcript in the SAM/BAM file, you should interpret this as the read _could_ be assigned (most likely proportionally) to the transcript. The `--writeMappings` option is primarily intended for the mode where `salmon` performs the mapping itself, rather than the mode where aligned reads are provided as input. What does your alignment pipeline look like upstream of salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245687786
https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245687786:678,Performance,perform,performs,678,"Hi @gamabunta313,. When you say `mapping-mode`, do you mean that you are passing a SAM file _into_ salmon rather than passing the FASTQ file and letting it perform the mapping itself? Because salmon makes use of a statistical inference procedure to determine the ultimate allocations of reads to transcripts, you cannot just count up the reads ""mapped"" to a transcript to obtain the count you see in the `quant.sf` file. Rather, when you see a read mapped to a transcript in the SAM/BAM file, you should interpret this as the read _could_ be assigned (most likely proportionally) to the transcript. The `--writeMappings` option is primarily intended for the mode where `salmon` performs the mapping itself, rather than the mode where aligned reads are provided as input. What does your alignment pipeline look like upstream of salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245687786
https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245700129:36,Performance,perform,perform,36,"I give .fastq directly to salmon to perform the alignment and I use the --writemapping option to have the bam on which performing other operations. I am focused on flags and mapq (field 3 and 5, respectively). I compared flags (field 3) between salmon and star (alignment performed on genome) and reads flagged as non-primary are comparable. I thought that salmon would assign counts to transcripts based on ""primary alignments"", but, filtering out secondary alignments I cannot reproduce counts in quant.sf. My question is: quant.sf is generated considering the output of --writemapping ? Thanks again for your quick response",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245700129
https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245700129:119,Performance,perform,performing,119,"I give .fastq directly to salmon to perform the alignment and I use the --writemapping option to have the bam on which performing other operations. I am focused on flags and mapq (field 3 and 5, respectively). I compared flags (field 3) between salmon and star (alignment performed on genome) and reads flagged as non-primary are comparable. I thought that salmon would assign counts to transcripts based on ""primary alignments"", but, filtering out secondary alignments I cannot reproduce counts in quant.sf. My question is: quant.sf is generated considering the output of --writemapping ? Thanks again for your quick response",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245700129
https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245700129:272,Performance,perform,performed,272,"I give .fastq directly to salmon to perform the alignment and I use the --writemapping option to have the bam on which performing other operations. I am focused on flags and mapq (field 3 and 5, respectively). I compared flags (field 3) between salmon and star (alignment performed on genome) and reads flagged as non-primary are comparable. I thought that salmon would assign counts to transcripts based on ""primary alignments"", but, filtering out secondary alignments I cannot reproduce counts in quant.sf. My question is: quant.sf is generated considering the output of --writemapping ? Thanks again for your quick response",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245700129
https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245730911:273,Performance,perform,performed,273,"Thanks for the quick response. I see the confusion. **In general, it is not possible to determine which reads are assigned to which transcripts given just the information in the SAM file written by `--writeMappings`**. This is because allocation of reads to transcripts is performed using a probabilistic model that takes into account many factors, so it's not as if salmon is just applying some filter to the mappings and then summing up some alignments. One way you could potentially get more insight is to use the [`postmaster`](https://github.com/COMBINE-lab/postmaster) tool. It takes as input the SAM file generated by the `--writeMappings` flag of `salmon`, and produces another SAM file annotating each alignment record with a `ZW` field, which records the posterior probability that this read is assigned to the corresponding transcript. This will give the right abundances in expectation (if you sum up the `ZW` field for each alignment, you should get something similar to the `quant.sf` results), but it is not guaranteed to be exactly the same assignment that salmon makes for each alignment itself.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1245730911
https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1246245810:148,Availability,reliab,reliable,148,"Fantastic, I'll give it a try! Moreover, I have another question regarding salmon quantification and ffpe degradation. I was wandering if salmon is reliable in quantifying highly degraded RNASeq samples. I was not able to find any information about it. Should I open another issue, or we can discuss in this one ? Thanks again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1246245810
https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1246245810:179,Availability,degraded,degraded,179,"Fantastic, I'll give it a try! Moreover, I have another question regarding salmon quantification and ffpe degradation. I was wandering if salmon is reliable in quantifying highly degraded RNASeq samples. I was not able to find any information about it. Should I open another issue, or we can discuss in this one ? Thanks again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/799#issuecomment-1246245810
https://github.com/COMBINE-lab/salmon/issues/803#issuecomment-1263615944:27,Modifiability,variab,variability,27,"Hi @SFonsecaCosta , . Some variability is expected since the read alignment methods are different. It's worth checking and generating summary stats for the unquantified genes, for example, their expression across samples, or if all the unquantified genes are mitochondrial/ribosomal of some sort.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/803#issuecomment-1263615944
https://github.com/COMBINE-lab/salmon/issues/803#issuecomment-1263626603:61,Modifiability,variab,variability,61,"Hi @k3yavi ,. Thanks a lot for you reply. ; Yes I agree that variability is expected, but my point is why these ~ 400 genes did not appear/exported in the quant.genes.sf file? some reason in special? Because they should be reported as well no?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/803#issuecomment-1263626603
https://github.com/COMBINE-lab/salmon/issues/803#issuecomment-1263826925:700,Performance,perform,performed,700,"@SFonsecaCosta,. Yes, the file is called `duplicate_clusters.tsv` and it is in the directory where the salmon *index* resides. The reason this is created is that when transcripts are sequence identical to each other, they cannot be independently quantified — that is, they are inferentially indistinguishable. So, the default strategy is to keep one representative from each indistinguishable cluster and to record the rest in the `duplicate_clusters` file. If you want to force salmon to quantify the duplicates (they should all just get equal abundance of 1 / D where D is the number of duplicates), you can pass `--keepDuplicates` when building the salmon index. When you quantify with alignments performed by STAR, no such duplicate removal is done by STAR upstream, and so all of the sequence identical transcripts are retained.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/803#issuecomment-1263826925
https://github.com/COMBINE-lab/salmon/issues/804#issuecomment-1271477486:82,Deployability,release,releases,82,"No worries; thanks for looping back around. As a side-note, v1.0.0 is quite a few releases old and it’s probably worth updating to the latest (v1.9.0) if that’s not too difficult on your end.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/804#issuecomment-1271477486
https://github.com/COMBINE-lab/salmon/issues/804#issuecomment-1271753132:10,Deployability,update,update,10,"Yes, will update it too. Thank you :)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/804#issuecomment-1271753132
https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282467290:582,Testability,test,test,582,"Hi @biobenkj,. Congratulations on publishing your new single-cell technology, and thanks for your interest in adding support to alevin(fry). . After adding the functionality to provide custom geometry for UMI and cellular barcode sequence through command line flags like `--umi-geometry` and `--barcode-geometry,` our general guidelines have been shifted against adding technology-specific command line flags to the alevin codebase. Rob might have more comments on that. Regarding the 0-length cell barcode, I recommend first trying to add the dummy CB before the UMI sequence as a test case. If it helps with your use case, we can discuss adding the ; 0-length cellular barcode functionality to the main codebase. Previously, paired-end read processing was not possible under the alevin framework, but with the publication of alevin-fry, the support for paired-end read (I think) has been added. @DongzeHE and @Gaura might have better thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282467290
https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282467290:326,Usability,guid,guidelines,326,"Hi @biobenkj,. Congratulations on publishing your new single-cell technology, and thanks for your interest in adding support to alevin(fry). . After adding the functionality to provide custom geometry for UMI and cellular barcode sequence through command line flags like `--umi-geometry` and `--barcode-geometry,` our general guidelines have been shifted against adding technology-specific command line flags to the alevin codebase. Rob might have more comments on that. Regarding the 0-length cell barcode, I recommend first trying to add the dummy CB before the UMI sequence as a test case. If it helps with your use case, we can discuss adding the ; 0-length cellular barcode functionality to the main codebase. Previously, paired-end read processing was not possible under the alevin framework, but with the publication of alevin-fry, the support for paired-end read (I think) has been added. @DongzeHE and @Gaura might have better thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282467290
https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282574837:376,Testability,test,test,376,Thanks so much @k3yavi! We will take a look and see about adding a small function to add a synthetic cell barcode in front of the UMI on read 2. I do think there is value in adding in 0-length cell barcode functionality for a variety of plate-based single cell approaches as well as allow for UMI containing bulk RNA-seq. We will follow up after @jamorrison and I discuss and test. @DongzeHE and @Gaura I'd love to hear your thoughts on this too. Thanks again!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282574837
https://github.com/COMBINE-lab/salmon/issues/806#issuecomment-1293938189:143,Testability,test,test,143,Thanks for this bug report @gringer! I have pushed a change to develop that should address this. Would you need me to produce an executable to test this out?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/806#issuecomment-1293938189
https://github.com/COMBINE-lab/salmon/issues/806#issuecomment-1294029352:32,Availability,error,error,32,"No, it's fine. I understand the error, it doesn't affect any of my workflow, and I can easily compensate for it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/806#issuecomment-1294029352
https://github.com/COMBINE-lab/salmon/issues/807#issuecomment-1293620131:525,Availability,robust,robust,525,"Hi Ryan,. Thanks for your kind words regarding salmon! First, I'll point out that *if it is desired*, you can also use STAR+Salmon in much the same way you've used STAR+RSEM (though STAR+salmon will be faster). To your question, if you want to use salmon's built-in selective-alignment, while choosing a k-mer length for this data, then the general rule of thumb is that the k-mer length should not be much longer than about half of the read length. In general, the alignment procedure and ultimate quantification are pretty robust to the specific choice of k, but you may want to try out a few values for one or two samples. Given these read lengths, I'd probably choose a value in {21, 23, 25}. If you have any follow-up questions after looking at those results, I'd be happy to discuss further!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/807#issuecomment-1293620131
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1323774035:0,Availability,Ping,Pinging,0,"Pinging @mikelove, who is certainly the person to give the best answer here. One clear difference to note though is that TPM is a length-normalized measure, while CPM is not. This alone means they will exhibit nontrivial differences.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1323774035
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1323774035:81,Usability,clear,clear,81,"Pinging @mikelove, who is certainly the person to give the best answer here. One clear difference to note though is that TPM is a length-normalized measure, while CPM is not. This alone means they will exhibit nontrivial differences.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1323774035
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833:223,Availability,down,downstream,223,"Dear all. Thank you for your prompt reply. ; @mikelove yes, the CPM is only cross-sample normalisation, but not cross genes. TPM is both cross-sample and cross-gene normalisation. Thus, in my mind, TPM is more suitable for downstream RNA-seq analysis, including clustering analysis, differential expression testing using Wilcoxon rank-sum test. Also, for accurately detecting differentially expressed genes, is it reasonable to overlap the results from different methods, such as edgeR+Wilcoxon rank-sum test?. Best regards,; Zheng zhuqing",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833:366,Safety,detect,detecting,366,"Dear all. Thank you for your prompt reply. ; @mikelove yes, the CPM is only cross-sample normalisation, but not cross genes. TPM is both cross-sample and cross-gene normalisation. Thus, in my mind, TPM is more suitable for downstream RNA-seq analysis, including clustering analysis, differential expression testing using Wilcoxon rank-sum test. Also, for accurately detecting differentially expressed genes, is it reasonable to overlap the results from different methods, such as edgeR+Wilcoxon rank-sum test?. Best regards,; Zheng zhuqing",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833:307,Testability,test,testing,307,"Dear all. Thank you for your prompt reply. ; @mikelove yes, the CPM is only cross-sample normalisation, but not cross genes. TPM is both cross-sample and cross-gene normalisation. Thus, in my mind, TPM is more suitable for downstream RNA-seq analysis, including clustering analysis, differential expression testing using Wilcoxon rank-sum test. Also, for accurately detecting differentially expressed genes, is it reasonable to overlap the results from different methods, such as edgeR+Wilcoxon rank-sum test?. Best regards,; Zheng zhuqing",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833:339,Testability,test,test,339,"Dear all. Thank you for your prompt reply. ; @mikelove yes, the CPM is only cross-sample normalisation, but not cross genes. TPM is both cross-sample and cross-gene normalisation. Thus, in my mind, TPM is more suitable for downstream RNA-seq analysis, including clustering analysis, differential expression testing using Wilcoxon rank-sum test. Also, for accurately detecting differentially expressed genes, is it reasonable to overlap the results from different methods, such as edgeR+Wilcoxon rank-sum test?. Best regards,; Zheng zhuqing",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833:504,Testability,test,test,504,"Dear all. Thank you for your prompt reply. ; @mikelove yes, the CPM is only cross-sample normalisation, but not cross genes. TPM is both cross-sample and cross-gene normalisation. Thus, in my mind, TPM is more suitable for downstream RNA-seq analysis, including clustering analysis, differential expression testing using Wilcoxon rank-sum test. Also, for accurately detecting differentially expressed genes, is it reasonable to overlap the results from different methods, such as edgeR+Wilcoxon rank-sum test?. Best regards,; Zheng zhuqing",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1324468833
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1325134215:248,Availability,recover,recovered,248,"There is a long literature about why we use counts or CPM (in either case, optionally with an effective transcript length offset) instead of raw TPM for statistical modeling. Using TPM throws out information about the sampling variation. It can be recovered in large sample datasets, but in small sample datasets, it's too much information loss. With respect to Wilcoxon, again, it's good to incorporate the inherent sampling variation of counts into the test statistic even with nonparametric schemes. This occurs in SAMseq (2013). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4605138/. ...and also in our method Swish (2019), which is based on SAMseq but designed specifically for output of methods like Salmon. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6765120/. Note that Swish is both 1) nonparametric 2) takes into account the multinomial-based sampling nature of sequencing data 3) also takes into account inferential uncertainty from multimapping reads (across isoforms, alleles, or genes).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1325134215
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1325134215:248,Safety,recover,recovered,248,"There is a long literature about why we use counts or CPM (in either case, optionally with an effective transcript length offset) instead of raw TPM for statistical modeling. Using TPM throws out information about the sampling variation. It can be recovered in large sample datasets, but in small sample datasets, it's too much information loss. With respect to Wilcoxon, again, it's good to incorporate the inherent sampling variation of counts into the test statistic even with nonparametric schemes. This occurs in SAMseq (2013). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4605138/. ...and also in our method Swish (2019), which is based on SAMseq but designed specifically for output of methods like Salmon. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6765120/. Note that Swish is both 1) nonparametric 2) takes into account the multinomial-based sampling nature of sequencing data 3) also takes into account inferential uncertainty from multimapping reads (across isoforms, alleles, or genes).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1325134215
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1325134215:455,Testability,test,test,455,"There is a long literature about why we use counts or CPM (in either case, optionally with an effective transcript length offset) instead of raw TPM for statistical modeling. Using TPM throws out information about the sampling variation. It can be recovered in large sample datasets, but in small sample datasets, it's too much information loss. With respect to Wilcoxon, again, it's good to incorporate the inherent sampling variation of counts into the test statistic even with nonparametric schemes. This occurs in SAMseq (2013). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4605138/. ...and also in our method Swish (2019), which is based on SAMseq but designed specifically for output of methods like Salmon. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6765120/. Note that Swish is both 1) nonparametric 2) takes into account the multinomial-based sampling nature of sequencing data 3) also takes into account inferential uncertainty from multimapping reads (across isoforms, alleles, or genes).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1325134215
https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1330832918:0,Availability,Ping,Pinging,0,Pinging @Gaura here — any idea what might be causing the inability to properly process the barcodes?. @cliftonlewis — would you be able to share a sampling of the reads for us to examine and help debug with?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1330832918
https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1332726376:205,Availability,error,error,205,"@cliftonlewis: could you tell us version of alevin-fry are you using? ; @rob-p: The cellbarcode length should be 21. It is variable b/w 19 or 20 so AC or A is added to make it 21. It could be the odd-even error we saw on previous version of alevin-fry. Wrt the run without `--justAlign`, I would need to take a closer look.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1332726376
https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1332726376:123,Modifiability,variab,variable,123,"@cliftonlewis: could you tell us version of alevin-fry are you using? ; @rob-p: The cellbarcode length should be 21. It is variable b/w 19 or 20 so AC or A is added to make it 21. It could be the odd-even error we saw on previous version of alevin-fry. Wrt the run without `--justAlign`, I would need to take a closer look.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1332726376
https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1344464188:75,Availability,error,error,75,"Hi, just wondering if you guys managed to understand what the source of my error was? I don't know if I just don't know how to work the sci-rna-seq3 data",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1344464188
https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1344650063:43,Testability,test,tested,43,"Hi @cliftonlewis, . Sorry for the delay. I tested it on another file and it worked fine. I would like to look at some info from your file. Could you:; 1. Post the `salmon` log of the first run, the one that you did with `--justAlign`; 2. There should be a `map.rad` file in your output directory (`SRR17122012`). Can you run the command: `alevin-fry view --rad map.rad > rad.txt` and share the rad.txt file? . Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1344650063
https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1344650063:172,Testability,log,log,172,"Hi @cliftonlewis, . Sorry for the delay. I tested it on another file and it worked fine. I would like to look at some info from your file. Could you:; 1. Post the `salmon` log of the first run, the one that you did with `--justAlign`; 2. There should be a `map.rad` file in your output directory (`SRR17122012`). Can you run the command: `alevin-fry view --rad map.rad > rad.txt` and share the rad.txt file? . Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1344650063
https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1344736974:46,Testability,test,tested,46,Thanks for reporting it @cliftonlewis. I have tested the fix and it works both with and without rad mode (`--justAlign`). The solution is in pr #817.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1344736974
https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1444418892:141,Deployability,release,release,141,Sorry I missed this! It's been on that branch (and committed) since Gaurav's PR. It's now been merged into master and included in the latest release (1.10.0).,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/813#issuecomment-1444418892
https://github.com/COMBINE-lab/salmon/issues/814#issuecomment-1336423789:0,Availability,Ping,Ping,0,"Ping @k3yavi here. One thing to try @umutcakir would be to see if you get a similar mapping rate outputting a RAD file for processing with `alevin-fry`. For example if you ran:. ```; salmon alevin -l ISR \; -1 ""Single_S1_L001_R1_001.fastq.gz"" \; -2 ""Single_S1_L001_R2_001.fastq.gz"" \; --chromiumV3 \; --sketch \; -i $my_index \; -p 48 \; -o outputfolder \; ```. what happens to the mapping rate then? Also, as long as you are careful to pass the files in the same relative order, you can directly pass multiple input read files to the `salmon alevin` command as a _space separated_ list; you don't have to concatenate them manually. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/814#issuecomment-1336423789
https://github.com/COMBINE-lab/salmon/issues/816#issuecomment-1344442434:106,Deployability,release,release,106,"Regarding the last point, cc @Gaura. Regarding a description of the read geometry, it can be found in the release notes for salmon 1.4.0 [here](https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0). Though, we should certainly add something to the full docs.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/816#issuecomment-1344442434
https://github.com/COMBINE-lab/salmon/issues/816#issuecomment-1344442434:182,Deployability,release,releases,182,"Regarding the last point, cc @Gaura. Regarding a description of the read geometry, it can be found in the release notes for salmon 1.4.0 [here](https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0). Though, we should certainly add something to the full docs.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/816#issuecomment-1344442434
https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141:421,Deployability,release,release,421,"Hi @ryanpe13002,. The bootstrap has no effect on the main `quant.sf` file (that is always the result of the main maximum likelihood estimate). All bootstrap samples are written to the `bootstraps.gz` file. If you load your data with the `fishpond` package in `R`, you can request to load the bootstraps to investigate them. Otherwise, if you use an uncertainty aware tool like [`swish`](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html) for differential testing, it will make use of the bootstraps automatically to account for inferential uncertainty when performing differential testing. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141
https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141:213,Performance,load,load,213,"Hi @ryanpe13002,. The bootstrap has no effect on the main `quant.sf` file (that is always the result of the main maximum likelihood estimate). All bootstrap samples are written to the `bootstraps.gz` file. If you load your data with the `fishpond` package in `R`, you can request to load the bootstraps to investigate them. Otherwise, if you use an uncertainty aware tool like [`swish`](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html) for differential testing, it will make use of the bootstraps automatically to account for inferential uncertainty when performing differential testing. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141
https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141:283,Performance,load,load,283,"Hi @ryanpe13002,. The bootstrap has no effect on the main `quant.sf` file (that is always the result of the main maximum likelihood estimate). All bootstrap samples are written to the `bootstraps.gz` file. If you load your data with the `fishpond` package in `R`, you can request to load the bootstraps to investigate them. Otherwise, if you use an uncertainty aware tool like [`swish`](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html) for differential testing, it will make use of the bootstraps automatically to account for inferential uncertainty when performing differential testing. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141
https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141:593,Performance,perform,performing,593,"Hi @ryanpe13002,. The bootstrap has no effect on the main `quant.sf` file (that is always the result of the main maximum likelihood estimate). All bootstrap samples are written to the `bootstraps.gz` file. If you load your data with the `fishpond` package in `R`, you can request to load the bootstraps to investigate them. Otherwise, if you use an uncertainty aware tool like [`swish`](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html) for differential testing, it will make use of the bootstraps automatically to account for inferential uncertainty when performing differential testing. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141
https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141:491,Testability,test,testing,491,"Hi @ryanpe13002,. The bootstrap has no effect on the main `quant.sf` file (that is always the result of the main maximum likelihood estimate). All bootstrap samples are written to the `bootstraps.gz` file. If you load your data with the `fishpond` package in `R`, you can request to load the bootstraps to investigate them. Otherwise, if you use an uncertainty aware tool like [`swish`](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html) for differential testing, it will make use of the bootstraps automatically to account for inferential uncertainty when performing differential testing. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141
https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141:617,Testability,test,testing,617,"Hi @ryanpe13002,. The bootstrap has no effect on the main `quant.sf` file (that is always the result of the main maximum likelihood estimate). All bootstrap samples are written to the `bootstraps.gz` file. If you load your data with the `fishpond` package in `R`, you can request to load the bootstraps to investigate them. Otherwise, if you use an uncertainty aware tool like [`swish`](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html) for differential testing, it will make use of the bootstraps automatically to account for inferential uncertainty when performing differential testing. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/818#issuecomment-1354070141
https://github.com/COMBINE-lab/salmon/issues/819#issuecomment-1358573944:164,Deployability,pipeline,pipeline,164,"Hi Rob,. Thank you so much for the explanation! Most of the samples were 80-85%, but some did dip as low as 75%; I ran FASTQC on all the samples before running the pipeline and they all looked fine (quite good quality, in fact). I also checked the `lib_format_counts.json` file for a few of the ""problem"" samples and it looks as you'd expect (~99% of reads map consistent with ISR orientation). Are there other diagnostics you might recommend running?. Thanks so much,; Ryan",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/819#issuecomment-1358573944
https://github.com/COMBINE-lab/salmon/issues/822#issuecomment-1376654942:119,Testability,log,log,119,"oh, Then can I rm duplicate identical reads in my bam file? Will this affect my results?; because i got the warning in log file: ; ![image](https://user-images.githubusercontent.com/45484925/211450308-2c13c0c6-7a63-4a08-9658-d991c4bf285a.png); and i check the bam file, actually, there are indeed three identical reads, its not paired,so got the warnings.(may be another read was filtered).; ![image](https://user-images.githubusercontent.com/45484925/211451982-e5c9dc34-b9f9-4120-82b0-4827aa4ffd92.png). So I wonder if duplicate reads can be deleted?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/822#issuecomment-1376654942
https://github.com/COMBINE-lab/salmon/issues/822#issuecomment-1376659692:75,Energy Efficiency,consumption,consumption,75,"This is a different issue. This means that your BAM-file is ill-formed for consumption by salmon. Basically, the BAM input requirements for salmon are the same as those for RSEM. Specifically,. * All alignments for a given read must appear contiguously in the BAM file.; * If you have paired-end data, then the alignments must be of the form:; ```; alignment_1 for left read; alignment_1 for right read; alignment_2 for left read; alignment_2 for right read; ...; alignment_k for left read; alignment_k for right read; ```. * You cannot mix alignments for paired-end and single-end reads in the same BAM file. These requirements must be satisfied or the BAM file cannot be properly parsed / interpreted. Typically, this is done by passing the proper arguments to the aligner used upstream of salmon (for which we recommend Bowtie2 if you are aligning to a _de novo_ transcriptome or STAR if you are doing reference-based quantification against a genome assembly and annotation).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/822#issuecomment-1376659692
https://github.com/COMBINE-lab/salmon/issues/824#issuecomment-1386426735:284,Integrability,depend,depending,284,"Hi @KS751515,. It appears that you have quantified the *genome* (i.e. you have computed an abundance for each chromosome) rather than the transcriptome (i.e. an abundance for each gene transcript). The reference should be the reference transcriptome rather than the reference genome (depending on the specific annotation, this should have on the order of ~150,000-200,000 reference sequences. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/824#issuecomment-1386426735
https://github.com/COMBINE-lab/salmon/issues/826#issuecomment-2228388816:156,Performance,perform,perform,156,"Dear Peter,. I am also trying salmon for miRNA quant and was searching for tips and experience from other users/developers on this scenario. How did salmon perform for you in this use case? . Gonçalo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/826#issuecomment-2228388816
https://github.com/COMBINE-lab/salmon/issues/829#issuecomment-1439983916:253,Deployability,release,releases,253,"Hi @zaherlab, . We recently noticed some strangeness with the bioconda build (probably recently auto-rebuilt with a new compiler) that we have to look into. Could you see if you observe the same behavior using the precompiles executable from the GitHub releases page?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/829#issuecomment-1439983916
https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441139338:737,Deployability,pipeline,pipeline,737,"Hi @charlesfoster,. Thanks for opening the issue. The second run looks like it ends before there is any information about mapped reads. Have mappings started to be reported at that point? I wonder if there is some issue related to the loading of the index. I have a few suggestions that may be worthwhile to try:. 1) Do you observe the same problem if you index only the transcriptome (i.e. if you don't also include the genome as a decoy)?. 2) If you are using nfcore/rnaseq you can also consider using the STAR => salmon path. Of course, I'm interested in addressing whatever the underlying issue here is anyway, but it's worth noting that this may be a viable alternative to allow you to process all of these samples using the nfcore pipeline in the meantime. This will align the reads to the genome using STAR (which gives the benefit of having a full decoy), project them to the transcriptome, and then quantify them. Also, if you can share a set of problematic reads (or even a subset of them that will reproduce the extreme slowness problem) privately, that would be very helpful in debugging. In addition to trying to debug what's going on here, I'd probably also try running them through [piscem](https://github.com/COMBINE-lab/piscem). While this isn't yet an actual substitute for salmon, it will help isolate if the problem is directly related to the index or something else. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441139338
https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441139338:235,Performance,load,loading,235,"Hi @charlesfoster,. Thanks for opening the issue. The second run looks like it ends before there is any information about mapped reads. Have mappings started to be reported at that point? I wonder if there is some issue related to the loading of the index. I have a few suggestions that may be worthwhile to try:. 1) Do you observe the same problem if you index only the transcriptome (i.e. if you don't also include the genome as a decoy)?. 2) If you are using nfcore/rnaseq you can also consider using the STAR => salmon path. Of course, I'm interested in addressing whatever the underlying issue here is anyway, but it's worth noting that this may be a viable alternative to allow you to process all of these samples using the nfcore pipeline in the meantime. This will align the reads to the genome using STAR (which gives the benefit of having a full decoy), project them to the transcriptome, and then quantify them. Also, if you can share a set of problematic reads (or even a subset of them that will reproduce the extreme slowness problem) privately, that would be very helpful in debugging. In addition to trying to debug what's going on here, I'd probably also try running them through [piscem](https://github.com/COMBINE-lab/piscem). While this isn't yet an actual substitute for salmon, it will help isolate if the problem is directly related to the index or something else. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441139338
https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948:494,Availability,down,downloaded,494,"Hi @rob-p,. Thanks for the speedy reply! There are definitely some strange things going on here. I can confirm that the second run (and the others that timed out) didn't produce any information about mapping. The outdir only contained empty subdirs + an empty log file:. ![image](https://user-images.githubusercontent.com/11418858/220816388-0a6272d4-a0c8-4e26-bf9f-15afda61cc44.png). 1. Thanks for the suggestion, I've now been investigating the potential of an index-related issue. Firstly, I downloaded the pre-built salmon index from refgenie using `refgenie pull hg38/salmon_sa_index`. I then ran `salmon quant` using this index and the singularity image of salmon v1.9.0. What, would you know: it worked in about 11 minutes. ```; <truncated>; [2023-02-23 14:46:31.892] [jointLog] [info] Aggregating expressions to gene level; [2023-02-23 14:46:32.452] [jointLog] [info] done; ```. This pre-built index does appear to be decoy-aware:. ```; [2023-02-23 14:38:21.709] [jointLog] [info] Number of decoys : 195; [2023-02-23 14:38:21.709] [jointLog] [info] First decoy index : 177412 ; ```. Secondly, I created a new transcriptome-only salmon index (`singularity run -B /data $SALMON_SIMG salmon index -t genome.transcripts.fa -i salmon_index -k 31`), then ran `salmon quant` again (as above) but using the new transcriptome-only index. Note: 'genome.transcripts.fa' is the transcripts file created during the `nf-core/rnaseq` pipeline. Again, this analysis completed properly in a reasonable time. Seems like there is something very wrong with the 'gentrome.fa' file that's being created by `nf-core/rnaseq`! It's just so odd that _some_ samples would work and others wouldn't. 2. It's definitely worth noting that I originally opted against using `star_salmon` with the following command:. ```; nextflow run nf-core/rnaseq --max_memory 55.GB --fasta /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz --gtf /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.106.gtf.gz -",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948
https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948:1426,Deployability,pipeline,pipeline,1426,"the suggestion, I've now been investigating the potential of an index-related issue. Firstly, I downloaded the pre-built salmon index from refgenie using `refgenie pull hg38/salmon_sa_index`. I then ran `salmon quant` using this index and the singularity image of salmon v1.9.0. What, would you know: it worked in about 11 minutes. ```; <truncated>; [2023-02-23 14:46:31.892] [jointLog] [info] Aggregating expressions to gene level; [2023-02-23 14:46:32.452] [jointLog] [info] done; ```. This pre-built index does appear to be decoy-aware:. ```; [2023-02-23 14:38:21.709] [jointLog] [info] Number of decoys : 195; [2023-02-23 14:38:21.709] [jointLog] [info] First decoy index : 177412 ; ```. Secondly, I created a new transcriptome-only salmon index (`singularity run -B /data $SALMON_SIMG salmon index -t genome.transcripts.fa -i salmon_index -k 31`), then ran `salmon quant` again (as above) but using the new transcriptome-only index. Note: 'genome.transcripts.fa' is the transcripts file created during the `nf-core/rnaseq` pipeline. Again, this analysis completed properly in a reasonable time. Seems like there is something very wrong with the 'gentrome.fa' file that's being created by `nf-core/rnaseq`! It's just so odd that _some_ samples would work and others wouldn't. 2. It's definitely worth noting that I originally opted against using `star_salmon` with the following command:. ```; nextflow run nf-core/rnaseq --max_memory 55.GB --fasta /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz --gtf /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.106.gtf.gz --skip_alignment --pseudo_aligner salmon --seq_center 'Ramaciotti Centre for Genomics' --input samplesheet.csv --outdir nf-core_results --save_merged_fastq true --skip_markduplicates true --extra_salmon_quant_args '--seqBias --gcBias --posBias' -profile singularity; ```. I'll re-run (a) using the refgenie salmon index specified; (b) with the `star_salmon` pathway to see if the decoy-aware index c",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948
https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948:2446,Deployability,install,installed,2446,"```. Secondly, I created a new transcriptome-only salmon index (`singularity run -B /data $SALMON_SIMG salmon index -t genome.transcripts.fa -i salmon_index -k 31`), then ran `salmon quant` again (as above) but using the new transcriptome-only index. Note: 'genome.transcripts.fa' is the transcripts file created during the `nf-core/rnaseq` pipeline. Again, this analysis completed properly in a reasonable time. Seems like there is something very wrong with the 'gentrome.fa' file that's being created by `nf-core/rnaseq`! It's just so odd that _some_ samples would work and others wouldn't. 2. It's definitely worth noting that I originally opted against using `star_salmon` with the following command:. ```; nextflow run nf-core/rnaseq --max_memory 55.GB --fasta /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz --gtf /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.106.gtf.gz --skip_alignment --pseudo_aligner salmon --seq_center 'Ramaciotti Centre for Genomics' --input samplesheet.csv --outdir nf-core_results --save_merged_fastq true --skip_markduplicates true --extra_salmon_quant_args '--seqBias --gcBias --posBias' -profile singularity; ```. I'll re-run (a) using the refgenie salmon index specified; (b) with the `star_salmon` pathway to see if the decoy-aware index created that way is appropriate. 3. Other; I've installed `piscem` and can give it a go, although it does seem more like a salmon index issue with `nf-core/rnaseq` from the debugging above. Do you agree? If so, I'll raise an issue there. Considering this, would it still be useful to have access to the reads? I've got the green light to share them if need be. If so, what's a good contact address to share a OneDrive link?. Thanks!; Charles. p.s. something else odd that I can dig into further later if need be is that the singularity version of salmon created an index in about 5 minutes, yet the conda version has been creating the index for nearly 20 minutes so far with no change...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948
https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948:2721,Energy Efficiency,green,green,2721,"```. Secondly, I created a new transcriptome-only salmon index (`singularity run -B /data $SALMON_SIMG salmon index -t genome.transcripts.fa -i salmon_index -k 31`), then ran `salmon quant` again (as above) but using the new transcriptome-only index. Note: 'genome.transcripts.fa' is the transcripts file created during the `nf-core/rnaseq` pipeline. Again, this analysis completed properly in a reasonable time. Seems like there is something very wrong with the 'gentrome.fa' file that's being created by `nf-core/rnaseq`! It's just so odd that _some_ samples would work and others wouldn't. 2. It's definitely worth noting that I originally opted against using `star_salmon` with the following command:. ```; nextflow run nf-core/rnaseq --max_memory 55.GB --fasta /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz --gtf /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.106.gtf.gz --skip_alignment --pseudo_aligner salmon --seq_center 'Ramaciotti Centre for Genomics' --input samplesheet.csv --outdir nf-core_results --save_merged_fastq true --skip_markduplicates true --extra_salmon_quant_args '--seqBias --gcBias --posBias' -profile singularity; ```. I'll re-run (a) using the refgenie salmon index specified; (b) with the `star_salmon` pathway to see if the decoy-aware index created that way is appropriate. 3. Other; I've installed `piscem` and can give it a go, although it does seem more like a salmon index issue with `nf-core/rnaseq` from the debugging above. Do you agree? If so, I'll raise an issue there. Considering this, would it still be useful to have access to the reads? I've got the green light to share them if need be. If so, what's a good contact address to share a OneDrive link?. Thanks!; Charles. p.s. something else odd that I can dig into further later if need be is that the singularity version of salmon created an index in about 5 minutes, yet the conda version has been creating the index for nearly 20 minutes so far with no change...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948
https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948:2687,Security,access,access,2687,"```. Secondly, I created a new transcriptome-only salmon index (`singularity run -B /data $SALMON_SIMG salmon index -t genome.transcripts.fa -i salmon_index -k 31`), then ran `salmon quant` again (as above) but using the new transcriptome-only index. Note: 'genome.transcripts.fa' is the transcripts file created during the `nf-core/rnaseq` pipeline. Again, this analysis completed properly in a reasonable time. Seems like there is something very wrong with the 'gentrome.fa' file that's being created by `nf-core/rnaseq`! It's just so odd that _some_ samples would work and others wouldn't. 2. It's definitely worth noting that I originally opted against using `star_salmon` with the following command:. ```; nextflow run nf-core/rnaseq --max_memory 55.GB --fasta /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz --gtf /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.106.gtf.gz --skip_alignment --pseudo_aligner salmon --seq_center 'Ramaciotti Centre for Genomics' --input samplesheet.csv --outdir nf-core_results --save_merged_fastq true --skip_markduplicates true --extra_salmon_quant_args '--seqBias --gcBias --posBias' -profile singularity; ```. I'll re-run (a) using the refgenie salmon index specified; (b) with the `star_salmon` pathway to see if the decoy-aware index created that way is appropriate. 3. Other; I've installed `piscem` and can give it a go, although it does seem more like a salmon index issue with `nf-core/rnaseq` from the debugging above. Do you agree? If so, I'll raise an issue there. Considering this, would it still be useful to have access to the reads? I've got the green light to share them if need be. If so, what's a good contact address to share a OneDrive link?. Thanks!; Charles. p.s. something else odd that I can dig into further later if need be is that the singularity version of salmon created an index in about 5 minutes, yet the conda version has been creating the index for nearly 20 minutes so far with no change...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948
https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948:260,Testability,log,log,260,"Hi @rob-p,. Thanks for the speedy reply! There are definitely some strange things going on here. I can confirm that the second run (and the others that timed out) didn't produce any information about mapping. The outdir only contained empty subdirs + an empty log file:. ![image](https://user-images.githubusercontent.com/11418858/220816388-0a6272d4-a0c8-4e26-bf9f-15afda61cc44.png). 1. Thanks for the suggestion, I've now been investigating the potential of an index-related issue. Firstly, I downloaded the pre-built salmon index from refgenie using `refgenie pull hg38/salmon_sa_index`. I then ran `salmon quant` using this index and the singularity image of salmon v1.9.0. What, would you know: it worked in about 11 minutes. ```; <truncated>; [2023-02-23 14:46:31.892] [jointLog] [info] Aggregating expressions to gene level; [2023-02-23 14:46:32.452] [jointLog] [info] done; ```. This pre-built index does appear to be decoy-aware:. ```; [2023-02-23 14:38:21.709] [jointLog] [info] Number of decoys : 195; [2023-02-23 14:38:21.709] [jointLog] [info] First decoy index : 177412 ; ```. Secondly, I created a new transcriptome-only salmon index (`singularity run -B /data $SALMON_SIMG salmon index -t genome.transcripts.fa -i salmon_index -k 31`), then ran `salmon quant` again (as above) but using the new transcriptome-only index. Note: 'genome.transcripts.fa' is the transcripts file created during the `nf-core/rnaseq` pipeline. Again, this analysis completed properly in a reasonable time. Seems like there is something very wrong with the 'gentrome.fa' file that's being created by `nf-core/rnaseq`! It's just so odd that _some_ samples would work and others wouldn't. 2. It's definitely worth noting that I originally opted against using `star_salmon` with the following command:. ```; nextflow run nf-core/rnaseq --max_memory 55.GB --fasta /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz --gtf /data/reference_genomes/GRCh38/Homo_sapiens.GRCh38.106.gtf.gz -",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/830#issuecomment-1441194948
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444189059:61,Availability,error,error,61,"I am not sure how the index here was created, but the actual error signifies that you are attempting to quantify the assembled transcripts using a recent version of salmon (`1.9.0` in this case) against an index that was created by a _very old_ version of salmon (`pre 1.0`). This is not supported, as the index format completely changed between pre v1.0 and post v1.0, and newer versions rely on a completely different data structure.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444189059
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444289845:562,Availability,error,error,562,"Hi all - this looks like an old version of Trinity. I'd suggest upgrading. For any version of Trinity, you can look at the Dockerfile to see what the; corresponding compatibilities are for versions that get co-installed for a; fully functional package. For example, in the current release, you'll find:. https://github.com/trinityrnaseq/trinityrnaseq/blob/5ce78d2b6d63aaae9fe491408311ebaf158deaa6/Docker/Dockerfile#L235. best,. ~brian. On Fri, Feb 24, 2023 at 1:19 PM Rob Patro ***@***.***> wrote:. > I am not sure how the index here was created, but the actual error; > signifies that you are attempting to quantify the assembled transcripts; > using a recent version of salmon (1.9.0 in this case) against an index; > that was created by a *very old* version of salmon (pre 1.0). This is not; > supported, as the index format completely changed between pre v1.0 and post; > v1.0, and newer versions rely on a completely different data structure.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444189059>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABZRKX5M6TN6DEQJQCP2AALWZD3TRANCNFSM6AAAAAAVHFUQRA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >. -- ; --; Brian J. Haas; The Broad Institute; http://broadinstitute.org/~bhaas <http://broad.mit.edu/~bhaas>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444289845
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444289845:210,Deployability,install,installed,210,"Hi all - this looks like an old version of Trinity. I'd suggest upgrading. For any version of Trinity, you can look at the Dockerfile to see what the; corresponding compatibilities are for versions that get co-installed for a; fully functional package. For example, in the current release, you'll find:. https://github.com/trinityrnaseq/trinityrnaseq/blob/5ce78d2b6d63aaae9fe491408311ebaf158deaa6/Docker/Dockerfile#L235. best,. ~brian. On Fri, Feb 24, 2023 at 1:19 PM Rob Patro ***@***.***> wrote:. > I am not sure how the index here was created, but the actual error; > signifies that you are attempting to quantify the assembled transcripts; > using a recent version of salmon (1.9.0 in this case) against an index; > that was created by a *very old* version of salmon (pre 1.0). This is not; > supported, as the index format completely changed between pre v1.0 and post; > v1.0, and newer versions rely on a completely different data structure.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444189059>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABZRKX5M6TN6DEQJQCP2AALWZD3TRANCNFSM6AAAAAAVHFUQRA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >. -- ; --; Brian J. Haas; The Broad Institute; http://broadinstitute.org/~bhaas <http://broad.mit.edu/~bhaas>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444289845
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444289845:281,Deployability,release,release,281,"Hi all - this looks like an old version of Trinity. I'd suggest upgrading. For any version of Trinity, you can look at the Dockerfile to see what the; corresponding compatibilities are for versions that get co-installed for a; fully functional package. For example, in the current release, you'll find:. https://github.com/trinityrnaseq/trinityrnaseq/blob/5ce78d2b6d63aaae9fe491408311ebaf158deaa6/Docker/Dockerfile#L235. best,. ~brian. On Fri, Feb 24, 2023 at 1:19 PM Rob Patro ***@***.***> wrote:. > I am not sure how the index here was created, but the actual error; > signifies that you are attempting to quantify the assembled transcripts; > using a recent version of salmon (1.9.0 in this case) against an index; > that was created by a *very old* version of salmon (pre 1.0). This is not; > supported, as the index format completely changed between pre v1.0 and post; > v1.0, and newer versions rely on a completely different data structure.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444189059>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABZRKX5M6TN6DEQJQCP2AALWZD3TRANCNFSM6AAAAAAVHFUQRA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >. -- ; --; Brian J. Haas; The Broad Institute; http://broadinstitute.org/~bhaas <http://broad.mit.edu/~bhaas>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444289845
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444289845:1268,Integrability,Message,Message,1268,"Hi all - this looks like an old version of Trinity. I'd suggest upgrading. For any version of Trinity, you can look at the Dockerfile to see what the; corresponding compatibilities are for versions that get co-installed for a; fully functional package. For example, in the current release, you'll find:. https://github.com/trinityrnaseq/trinityrnaseq/blob/5ce78d2b6d63aaae9fe491408311ebaf158deaa6/Docker/Dockerfile#L235. best,. ~brian. On Fri, Feb 24, 2023 at 1:19 PM Rob Patro ***@***.***> wrote:. > I am not sure how the index here was created, but the actual error; > signifies that you are attempting to quantify the assembled transcripts; > using a recent version of salmon (1.9.0 in this case) against an index; > that was created by a *very old* version of salmon (pre 1.0). This is not; > supported, as the index format completely changed between pre v1.0 and post; > v1.0, and newer versions rely on a completely different data structure.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444189059>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABZRKX5M6TN6DEQJQCP2AALWZD3TRANCNFSM6AAAAAAVHFUQRA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >. -- ; --; Brian J. Haas; The Broad Institute; http://broadinstitute.org/~bhaas <http://broad.mit.edu/~bhaas>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1444289845
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443:86,Availability,error,error,86,"Hello,; Thank you for your guidance on this question. However, I encountered the same error despite using the latest versions of Trinityrnaseq and salmon. I ran Trinity version 2.15.1 to generate Fasta files. I attempted to use Salmon version 1.10.1 for indexing, but I encountered this exception. Upon checking the Docker link provided in the comment, I found that the salmon version listed is 1.5.0. So, I tried using Salmon 1.5.0 and encountered the same error. Could you please advise me on how to resolve this issue? Thank you. ```; $./trinityrnaseq-v2.15.1/Trinity --seqType fq --max_memory 6 --samples_file sample.txt --min_kmer_cov 2 --no_parallel_norm_stats --output trinity_test_0210_1019 --CPU 6. ... $ ~/tools/salmon-latest_linux_x86_64/bin/salmon index --index quasi --type quasi --transcripts ~/first_try_Gall/trinity_test_0210_1019.Trinity.fasta ; Version Info: This is the most recent version of salmon.; Exception : [Error: RapMap-based indexing is not supported in this version of salmon.]; /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443:458,Availability,error,error,458,"Hello,; Thank you for your guidance on this question. However, I encountered the same error despite using the latest versions of Trinityrnaseq and salmon. I ran Trinity version 2.15.1 to generate Fasta files. I attempted to use Salmon version 1.10.1 for indexing, but I encountered this exception. Upon checking the Docker link provided in the comment, I found that the salmon version listed is 1.5.0. So, I tried using Salmon 1.5.0 and encountered the same error. Could you please advise me on how to resolve this issue? Thank you. ```; $./trinityrnaseq-v2.15.1/Trinity --seqType fq --max_memory 6 --samples_file sample.txt --min_kmer_cov 2 --no_parallel_norm_stats --output trinity_test_0210_1019 --CPU 6. ... $ ~/tools/salmon-latest_linux_x86_64/bin/salmon index --index quasi --type quasi --transcripts ~/first_try_Gall/trinity_test_0210_1019.Trinity.fasta ; Version Info: This is the most recent version of salmon.; Exception : [Error: RapMap-based indexing is not supported in this version of salmon.]; /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443:934,Availability,Error,Error,934,"Hello,; Thank you for your guidance on this question. However, I encountered the same error despite using the latest versions of Trinityrnaseq and salmon. I ran Trinity version 2.15.1 to generate Fasta files. I attempted to use Salmon version 1.10.1 for indexing, but I encountered this exception. Upon checking the Docker link provided in the comment, I found that the salmon version listed is 1.5.0. So, I tried using Salmon 1.5.0 and encountered the same error. Could you please advise me on how to resolve this issue? Thank you. ```; $./trinityrnaseq-v2.15.1/Trinity --seqType fq --max_memory 6 --samples_file sample.txt --min_kmer_cov 2 --no_parallel_norm_stats --output trinity_test_0210_1019 --CPU 6. ... $ ~/tools/salmon-latest_linux_x86_64/bin/salmon index --index quasi --type quasi --transcripts ~/first_try_Gall/trinity_test_0210_1019.Trinity.fasta ; Version Info: This is the most recent version of salmon.; Exception : [Error: RapMap-based indexing is not supported in this version of salmon.]; /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443:27,Usability,guid,guidance,27,"Hello,; Thank you for your guidance on this question. However, I encountered the same error despite using the latest versions of Trinityrnaseq and salmon. I ran Trinity version 2.15.1 to generate Fasta files. I attempted to use Salmon version 1.10.1 for indexing, but I encountered this exception. Upon checking the Docker link provided in the comment, I found that the salmon version listed is 1.5.0. So, I tried using Salmon 1.5.0 and encountered the same error. Could you please advise me on how to resolve this issue? Thank you. ```; $./trinityrnaseq-v2.15.1/Trinity --seqType fq --max_memory 6 --samples_file sample.txt --min_kmer_cov 2 --no_parallel_norm_stats --output trinity_test_0210_1019 --CPU 6. ... $ ~/tools/salmon-latest_linux_x86_64/bin/salmon index --index quasi --type quasi --transcripts ~/first_try_Gall/trinity_test_0210_1019.Trinity.fasta ; Version Info: This is the most recent version of salmon.; Exception : [Error: RapMap-based indexing is not supported in this version of salmon.]; /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443
https://github.com/COMBINE-lab/salmon/issues/833#issuecomment-1446342132:5,Availability,error,error,5,"This error looks like it is not coming from Salmon, but from some R program that is processing Salmon's results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/833#issuecomment-1446342132
https://github.com/COMBINE-lab/salmon/issues/833#issuecomment-1451389744:101,Availability,error,error,101,Thank you for the response. I suspect the same. mapping works fine normally but keeps giving me this error when I follow their protocol. They have this .R file that they say is bundled up with the index. Anyways I am not an expert in the field. Thank you for your quick response.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/833#issuecomment-1451389744
https://github.com/COMBINE-lab/salmon/issues/833#issuecomment-1451389744:127,Integrability,protocol,protocol,127,Thank you for the response. I suspect the same. mapping works fine normally but keeps giving me this error when I follow their protocol. They have this .R file that they say is bundled up with the index. Anyways I am not an expert in the field. Thank you for your quick response.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/833#issuecomment-1451389744
https://github.com/COMBINE-lab/salmon/issues/834#issuecomment-1450283495:626,Performance,load,load,626,"Hi @JSSaini,. The most common use case would be taxonomic abundance estimation in a DNA-seq library where the references are whole genomes of the potential taxa. The references indexed would be the gnomes (and / or contigs) of the potential organisms in the sample, and the output would be the estimated number of fragments arising from each contig / genome. If a given organism is split across multiple contigs, one would aggregate them post quantification in the same way one aggregates transcript abundance estimates to the gene level (i.e. the natural way to do this would be to create a contig to organism mapping and to load up the quantification results using tximport). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/834#issuecomment-1450283495
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853:56,Availability,ping,ping,56,"Hi @tillea and @nileshpatra, thanks for the report (and ping). Can you point me to a Docker / Singularity container of the relevant Debian build so I can try and reproduce locally? This will make debugging much easier. For example, I am unable to reproduce this issue building the latest release from the `master` branch using the latest [official Debian image](https://hub.docker.com/_/debian). In particular, release 1.10 addresses a rare (but stubborn) segfault that certainly was present in 1.9. However, the fix for this is in the corresponding tagged release of pufferfish, which is pulled in by the build script when salmon is built.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853:288,Deployability,release,release,288,"Hi @tillea and @nileshpatra, thanks for the report (and ping). Can you point me to a Docker / Singularity container of the relevant Debian build so I can try and reproduce locally? This will make debugging much easier. For example, I am unable to reproduce this issue building the latest release from the `master` branch using the latest [official Debian image](https://hub.docker.com/_/debian). In particular, release 1.10 addresses a rare (but stubborn) segfault that certainly was present in 1.9. However, the fix for this is in the corresponding tagged release of pufferfish, which is pulled in by the build script when salmon is built.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853:411,Deployability,release,release,411,"Hi @tillea and @nileshpatra, thanks for the report (and ping). Can you point me to a Docker / Singularity container of the relevant Debian build so I can try and reproduce locally? This will make debugging much easier. For example, I am unable to reproduce this issue building the latest release from the `master` branch using the latest [official Debian image](https://hub.docker.com/_/debian). In particular, release 1.10 addresses a rare (but stubborn) segfault that certainly was present in 1.9. However, the fix for this is in the corresponding tagged release of pufferfish, which is pulled in by the build script when salmon is built.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853:557,Deployability,release,release,557,"Hi @tillea and @nileshpatra, thanks for the report (and ping). Can you point me to a Docker / Singularity container of the relevant Debian build so I can try and reproduce locally? This will make debugging much easier. For example, I am unable to reproduce this issue building the latest release from the `master` branch using the latest [official Debian image](https://hub.docker.com/_/debian). In particular, release 1.10 addresses a rare (but stubborn) segfault that certainly was present in 1.9. However, the fix for this is in the corresponding tagged release of pufferfish, which is pulled in by the build script when salmon is built.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853:446,Testability,stub,stubborn,446,"Hi @tillea and @nileshpatra, thanks for the report (and ping). Can you point me to a Docker / Singularity container of the relevant Debian build so I can try and reproduce locally? This will make debugging much easier. For example, I am unable to reproduce this issue building the latest release from the `master` branch using the latest [official Debian image](https://hub.docker.com/_/debian). In particular, release 1.10 addresses a rare (but stubborn) segfault that certainly was present in 1.9. However, the fix for this is in the corresponding tagged release of pufferfish, which is pulled in by the build script when salmon is built.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1462538853
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:682,Availability,robust,robustly,682,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:113,Deployability,install,installed,113,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:390,Deployability,update,update,390,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:408,Deployability,install,install,408,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:711,Deployability,install,install,711,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:808,Deployability,install,install,808,"mage. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1279,Deployability,Install,Install,1279,"mage. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1306,Deployability,Install,Install,1306,"esting. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Ins",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1314,Deployability,configurat,configuration,1314,"esting. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Ins",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1330,Deployability,Release,Release,1330,"esting. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Ins",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1343,Deployability,Install,Installing,1343,"esting. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Ins",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1383,Deployability,Install,Installing,1383,"-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set pr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1439,Deployability,Install,Installing,1439,"dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; =================================================",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1485,Deployability,release,release,1485,"dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; =================================================",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1503,Deployability,Install,Installing,1503,"4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to you",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1546,Deployability,Install,Installing,1546,"-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PA",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1608,Deployability,Install,Installing,1608," -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ============================================================",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1660,Deployability,release,release,1660," -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ============================================================",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1678,Deployability,Install,Installing,1678,"; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1719,Deployability,Install,Installing,1719,"ich leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; S",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1777,Deployability,Install,Installing,1777,"d# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ................",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1825,Deployability,release,release,1825,"d# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ................",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1843,Deployability,Install,Installing,1843," Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_qua",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1881,Deployability,Install,Installing,1881,"t ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1922,Deployability,Install,Installing,1922,"[ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1965,Deployability,Install,Installing,1965,"et ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test tim",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2009,Deployability,Install,Installing,2009,"se4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` c",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2055,Deployability,Install,Installing,2055,"Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the inde",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2103,Deployability,Install,Installing,2103,"73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2153,Deployability,Install,Installing,2153," alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```;",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2205,Deployability,Install,Installing,2205,"1%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.f",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2259,Deployability,Install,Installing,2259,"oject...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```;",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2294,Deployability,Install,Installing,2294,"oject...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```;",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2336,Deployability,Install,Installation,2336,"talling: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:3361,Integrability,wrap,wrapping,3361,"ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector. 24; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] # segments = 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] total length = 19592; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Reading the reference files ...; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] positional integer width = 15; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] seqSize = 19592; [2023-03-10",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:1314,Modifiability,config,configuration,1314,"esting. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Ins",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:73,Performance,perform,performed,73,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:3488,Security,validat,validation,3488,"======; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector. 24; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] # segments = 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] total length = 19592; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Reading the reference files ...; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] positional integer width = 15; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] seqSize = 19592; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] rankSize = 19592; [2023-03-10 05:51:33.750] [puff::index::jointLog] ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:4807,Security,hash,hash,4807,"al # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector. 24; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] # segments = 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] total length = 19592; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Reading the reference files ...; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] positional integer width = 15; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] seqSize = 19592; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] rankSize = 19592; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] edgeVecSize = 0; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] num keys = 18902; for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000; [Building BooPHF] 100 % elapsed: 0 min 0 sec remaining: 0 min 0 sec; Bitarray 105024 bits (100.00 %) (array + ranks ); final hash 0 bits (0.00 %) (nb in final hash 0); [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] mphf size = 0.0125198 MB; [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] chunk size = 9796; [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] chunk 0 = [0, 9796); [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] chunk 1 = [9796, 19562); [2023-03-10 05:51:33.784] [puff::index::jointLog] [info] finished populating pos vector; [2023-03-10 05:51:33.784] [puff::index::jointLog] [info] writing index components; [2023-03-10 05:51:33.784] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2023-03-10 05:51:33.784] [jLog] [info] done building index; ```. So on `testing` at least, I can't yet reproduce this issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:4841,Security,hash,hash,4841,"al # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector. 24; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] # segments = 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] total length = 19592; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Reading the reference files ...; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] positional integer width = 15; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] seqSize = 19592; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] rankSize = 19592; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] edgeVecSize = 0; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] num keys = 18902; for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000; [Building BooPHF] 100 % elapsed: 0 min 0 sec remaining: 0 min 0 sec; Bitarray 105024 bits (100.00 %) (array + ranks ); final hash 0 bits (0.00 %) (nb in final hash 0); [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] mphf size = 0.0125198 MB; [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] chunk size = 9796; [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] chunk 0 = [0, 9796); [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] chunk 1 = [9796, 19562); [2023-03-10 05:51:33.784] [puff::index::jointLog] [info] finished populating pos vector; [2023-03-10 05:51:33.784] [puff::index::jointLog] [info] writing index components; [2023-03-10 05:51:33.784] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2023-03-10 05:51:33.784] [jLog] [info] done building index; ```. So on `testing` at least, I can't yet reproduce this issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:21,Testability,test,testing,21,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:188,Testability,test,testing,188,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:297,Testability,test,testing,297,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:338,Testability,test,testing,338,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:371,Testability,test,testing,371,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:727,Testability,test,test,727,"I also tried on the `testing` image. For more detail, here are the steps performed (perhaps taking a look at the installed packages will highlight a difference, as I did this from a clean testing Docker image, so my environment had nothing else in it). ### Attempt to reproduce segfault on Debian:testing. ```{bash}; $ docker pull debian:testing. $ docker run -it debian:testing. $ apt-get update. $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev. $ git clone https://github.com/COMBINE-lab/salmon.git; $ cd salmon; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE .. # GCC still doesn't handle LTO robustly; $ make -j8; $ make install; $ make test; ```. which leads to the output. ```; root@fd877e359439:/salmon/build# make install; [ 7%] Built target libcereal; [ 13%] Built target libtbb; [ 16%] Built target ntcard; [ 19%] Built target graphdump; [ 27%] Built target twopaco; [ 29%] Built target ksw2pp_sse2; [ 32%] Built target ksw2pp_sse4; [ 37%] Built target ksw2pp_basic; [ 38%] Built target ksw2pp; [ 59%] Built target puffer; [ 73%] Built target salmon_core; [ 76%] Built target alevin_core; [ 77%] Built target UnitTestsMain; [ 81%] Built target unitTests; [100%] Built target salmon; Install the project...; -- Install configuration: ""Release""; -- Installing: /salmon/lib/libntcard.a; -- Installing: /salmon/lib/ntcard/ntcard-targets.cmake; -- Installing: /salmon/lib/ntcard/ntcard-targets-release.cmake; -- Installing: /salmon/lib/libgraphdump.a; -- Installing: /salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2687,Testability,test,test,2687,"/salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2701,Testability,test,tests,2701,"/salmon/lib/graphdump/graphdump-targets.cmake; -- Installing: /salmon/lib/graphdump/graphdump-targets-release.cmake; -- Installing: /salmon/lib/libtwopaco.a; -- Installing: /salmon/lib/twopaco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2711,Testability,Test,Test,2711,"aco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of c",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2764,Testability,Test,Test,2764,"aco/twopaco-targets.cmake; -- Installing: /salmon/lib/twopaco/twopaco-targets-release.cmake; -- Installing: /salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of c",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2862,Testability,Test,Test,2862,"salmon/lib/libtbb.so; -- Installing: /salmon/lib/libtbb.so.12; -- Installing: /salmon/lib/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [202",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2928,Testability,test,tests,2928,"b/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2944,Testability,test,tests,2944,"b/libtbb.so.12.5; -- Installing: /salmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:2973,Testability,Test,Test,2973,"lmon/lib/libtbbmalloc.so; -- Installing: /salmon/lib/libtbbmalloc.so.2; -- Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector. 24; [2023-03-10 05:51:33.748] [puf",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:3017,Testability,test,test,3017," Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector. 24; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] # segments = 23; [2023-03-10 05:51:33.748] [puff",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:3047,Testability,test,test,3047," Installing: /salmon/lib/libtbbmalloc.so.2.5; -- Installing: /salmon/lib/libtbbmalloc_proxy.so; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2; -- Installing: /salmon/lib/libtbbmalloc_proxy.so.2.5; -- Installing: /salmon/bin/salmon; -- Installing: /salmon/lib/libsalmon_core.a. Installation complete. Please ensure the following paths are set properly.; ==========================================================================; Please add /salmon/bin to your PATH; Please add /salmon/lib to your LD_LIBRARY_PATH; ==========================================================================; root@fd877e359439:/salmon/build# make test; Running tests...; Test project /salmon/build; Start 1: unit_tests; 1/2 Test #1: unit_tests ....................... Passed 0.37 sec; Start 2: salmon_read_test_quasi; 2/2 Test #2: salmon_read_test_quasi ........... Passed 1.80 sec. 100% tests passed, 0 tests failed out of 2. Total Test time (real) = 2.17 sec; ```. The `make test` command itself runs the test the builds the index and maps the reads against it. Either way, I can do that explicitly too (from within build):. ```; $ ./src/salmon index -t ../sample_data/transcripts.fasta -i sample_idx; ```. returns succesfully with a built index. ```; ...; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] contig count for validation: 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector. 24; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] # segments = 23; [2023-03-10 05:51:33.748] [puff",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554:5512,Testability,test,testing,5512,"al # of Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of numerical Contigs : 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Total # of contig vec entries: 36; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] bits per offset entry 6; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Done constructing the contig vector. 24; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] # segments = 23; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] total length = 19592; [2023-03-10 05:51:33.748] [puff::index::jointLog] [info] Reading the reference files ...; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] positional integer width = 15; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] seqSize = 19592; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] rankSize = 19592; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] edgeVecSize = 0; [2023-03-10 05:51:33.750] [puff::index::jointLog] [info] num keys = 18902; for info, total work write each : 2.331 total work inram from level 3 : 4.322 total work raw : 25.000; [Building BooPHF] 100 % elapsed: 0 min 0 sec remaining: 0 min 0 sec; Bitarray 105024 bits (100.00 %) (array + ranks ); final hash 0 bits (0.00 %) (nb in final hash 0); [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] mphf size = 0.0125198 MB; [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] chunk size = 9796; [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] chunk 0 = [0, 9796); [2023-03-10 05:51:33.781] [puff::index::jointLog] [info] chunk 1 = [9796, 19562); [2023-03-10 05:51:33.784] [puff::index::jointLog] [info] finished populating pos vector; [2023-03-10 05:51:33.784] [puff::index::jointLog] [info] writing index components; [2023-03-10 05:51:33.784] [puff::index::jointLog] [info] finished writing dense pufferfish index; [2023-03-10 05:51:33.784] [jLog] [info] done building index; ```. So on `testing` at least, I can't yet reproduce this issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463312554
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463629042:46,Deployability,install,installed,46,I can try that later today. Does your list of installed dependencies look different?. The thing that's strange about this is that the main motivation for releasing 1.10 was a discovery of an intermittent segfault due to UB at exactly this spot. But that was resolved in the associated tagged pufferfish upstream and checked further with both valgrind and asan.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463629042
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463629042:56,Integrability,depend,dependencies,56,I can try that later today. Does your list of installed dependencies look different?. The thing that's strange about this is that the main motivation for releasing 1.10 was a discovery of an intermittent segfault due to UB at exactly this spot. But that was resolved in the associated tagged pufferfish upstream and checked further with both valgrind and asan.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463629042
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:162,Availability,echo,echo,162,"I've tried to reproduce the issue in docker by using the Build-Depends that are used in Debian:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE ..; $ make -j8; make -j8; [ 3%] Built target ksw2pp_sse4; [ 6%] Built target ntcard; [ 15%] Built target twopaco; [ 18%] Built target graphdump; [ 21%] Built target ksw2pp_sse2; [ 27%] Built target ksw2pp_basic; [ 43%] Built target salmon_core; [ 67%] Built target puffer; [ 68%] Built target ksw2pp; [ 69%] Built target UnitTestsMain; [ 73%] Built target alevin_core; [ 74%] Linking CXX executable unitTests; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_destroy':; (.text+0x21): undefined reference to `psl_free'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_use':; (.text+0xbc): undefined reference to `psl_latest'; /usr/bin/ld: (.text+0x157): undefined reference to `psl_builtin'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version':; (.text+0x129): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x16f): undefined reference to `ZSTD_versionNumber'; /usr/bin/ld: (.text+0x1e3): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x20e): undefined reference to `psl_get_version'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version_info':; (.text+0x386): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x3ad): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x3b8): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:3169,Availability,echo,echo,3169,"ext+0x475): undefined reference to `nghttp2_version'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-libssh2.o): in function `ssh_attach':; (.text+0x3e1): undefined reference to `libssh2_session_abstract'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-libssh2.o): in function `ssh_statemach_act':; (.text+0x4b1): undefined reference to `libssh2_session_set_blocking'; /usr/bin/ld: (.text+0x4fb): undefined reference to `libssh2_session_handshake'; /usr/bin/ld: (.text+0x58d): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x6a0): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x75d): undefined reference to `libssh2_knownhost_free'; ... So somehow this does not build - but I have the impression that the linker issues are caused by some missing CMAKE options (as well as using the build directory). Thus I used the cmake command line as its used in the Debian packaging:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliar",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:4789,Availability,fault,fault,4789," run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliary decoy sequence or the genome of the organism) be rovided during indexing. Further details can be found at https://salmon.readthedocs.io/en/latest/salmon.html#preparing-transcriptome-indices-mapping-based-mode.; [2023-03-10 11:56:01.435] [jLog] [info] building index; out : sample_salmon_quasi_index; [2023-03-10 11:56:01.435] [puff::index::jointLog] [info] Running fixFasta. [Step 1 of 4] : counting k-mers. [2023-03-10 11:56:01.441] [puff::index::jointLog] [info] Replaced 0 non-ATCG nucleotides; [2023-03-10 11:56:01.441] [puff::index::jointLog] [info] Clipped poly-A tails from 0 transcripts; wrote 15 cleaned references; Segmentation fault (core dumped). Please note that `-DNO_IPO=TRUE` is not part of the above cmake call. I've added this to the packaging, but there is no difference. Please also note that I did *not* cloned the Git repository but have downloaded the release tarball since this is what we are packaging. Hope this helps tracking down the issue. Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:5011,Availability,down,downloaded,5011," run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliary decoy sequence or the genome of the organism) be rovided during indexing. Further details can be found at https://salmon.readthedocs.io/en/latest/salmon.html#preparing-transcriptome-indices-mapping-based-mode.; [2023-03-10 11:56:01.435] [jLog] [info] building index; out : sample_salmon_quasi_index; [2023-03-10 11:56:01.435] [puff::index::jointLog] [info] Running fixFasta. [Step 1 of 4] : counting k-mers. [2023-03-10 11:56:01.441] [puff::index::jointLog] [info] Replaced 0 non-ATCG nucleotides; [2023-03-10 11:56:01.441] [puff::index::jointLog] [info] Clipped poly-A tails from 0 transcripts; wrote 15 cleaned references; Segmentation fault (core dumped). Please note that `-DNO_IPO=TRUE` is not part of the above cmake call. I've added this to the packaging, but there is no difference. Please also note that I did *not* cloned the Git repository but have downloaded the release tarball since this is what we are packaging. Hope this helps tracking down the issue. Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:5104,Availability,down,down,5104," run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliary decoy sequence or the genome of the organism) be rovided during indexing. Further details can be found at https://salmon.readthedocs.io/en/latest/salmon.html#preparing-transcriptome-indices-mapping-based-mode.; [2023-03-10 11:56:01.435] [jLog] [info] building index; out : sample_salmon_quasi_index; [2023-03-10 11:56:01.435] [puff::index::jointLog] [info] Running fixFasta. [Step 1 of 4] : counting k-mers. [2023-03-10 11:56:01.441] [puff::index::jointLog] [info] Replaced 0 non-ATCG nucleotides; [2023-03-10 11:56:01.441] [puff::index::jointLog] [info] Clipped poly-A tails from 0 transcripts; wrote 15 cleaned references; Segmentation fault (core dumped). Please note that `-DNO_IPO=TRUE` is not part of the above cmake call. I've added this to the packaging, but there is no difference. Please also note that I did *not* cloned the Git repository but have downloaded the release tarball since this is what we are packaging. Hope this helps tracking down the issue. Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:268,Deployability,update,update,268,"I've tried to reproduce the issue in docker by using the Build-Depends that are used in Debian:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE ..; $ make -j8; make -j8; [ 3%] Built target ksw2pp_sse4; [ 6%] Built target ntcard; [ 15%] Built target twopaco; [ 18%] Built target graphdump; [ 21%] Built target ksw2pp_sse2; [ 27%] Built target ksw2pp_basic; [ 43%] Built target salmon_core; [ 67%] Built target puffer; [ 68%] Built target ksw2pp; [ 69%] Built target UnitTestsMain; [ 73%] Built target alevin_core; [ 74%] Linking CXX executable unitTests; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_destroy':; (.text+0x21): undefined reference to `psl_free'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_use':; (.text+0xbc): undefined reference to `psl_latest'; /usr/bin/ld: (.text+0x157): undefined reference to `psl_builtin'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version':; (.text+0x129): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x16f): undefined reference to `ZSTD_versionNumber'; /usr/bin/ld: (.text+0x1e3): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x20e): undefined reference to `psl_get_version'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version_info':; (.text+0x386): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x3ad): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x3b8): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:282,Deployability,upgrade,upgrade,282,"I've tried to reproduce the issue in docker by using the Build-Depends that are used in Debian:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE ..; $ make -j8; make -j8; [ 3%] Built target ksw2pp_sse4; [ 6%] Built target ntcard; [ 15%] Built target twopaco; [ 18%] Built target graphdump; [ 21%] Built target ksw2pp_sse2; [ 27%] Built target ksw2pp_basic; [ 43%] Built target salmon_core; [ 67%] Built target puffer; [ 68%] Built target ksw2pp; [ 69%] Built target UnitTestsMain; [ 73%] Built target alevin_core; [ 74%] Linking CXX executable unitTests; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_destroy':; (.text+0x21): undefined reference to `psl_free'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_use':; (.text+0xbc): undefined reference to `psl_latest'; /usr/bin/ld: (.text+0x157): undefined reference to `psl_builtin'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version':; (.text+0x129): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x16f): undefined reference to `ZSTD_versionNumber'; /usr/bin/ld: (.text+0x1e3): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x20e): undefined reference to `psl_get_version'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version_info':; (.text+0x386): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x3ad): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x3b8): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:3275,Deployability,update,update,3275,"h':; (.text+0x3e1): undefined reference to `libssh2_session_abstract'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-libssh2.o): in function `ssh_statemach_act':; (.text+0x4b1): undefined reference to `libssh2_session_set_blocking'; /usr/bin/ld: (.text+0x4fb): undefined reference to `libssh2_session_handshake'; /usr/bin/ld: (.text+0x58d): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x6a0): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x75d): undefined reference to `libssh2_knownhost_free'; ... So somehow this does not build - but I have the impression that the linker issues are caused by some missing CMAKE options (as well as using the build directory). Thus I used the cmake command line as its used in the Debian packaging:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliary decoy sequence or the genome of the organism) be rovided during indexing. Further details can be found at https://salmon.readthedocs.io/en/latest/salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:3289,Deployability,upgrade,upgrade,3289,"h':; (.text+0x3e1): undefined reference to `libssh2_session_abstract'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-libssh2.o): in function `ssh_statemach_act':; (.text+0x4b1): undefined reference to `libssh2_session_set_blocking'; /usr/bin/ld: (.text+0x4fb): undefined reference to `libssh2_session_handshake'; /usr/bin/ld: (.text+0x58d): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x6a0): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x75d): undefined reference to `libssh2_knownhost_free'; ... So somehow this does not build - but I have the impression that the linker issues are caused by some missing CMAKE options (as well as using the build directory). Thus I used the cmake command line as its used in the Debian packaging:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliary decoy sequence or the genome of the organism) be rovided during indexing. Further details can be found at https://salmon.readthedocs.io/en/latest/salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:3474,Deployability,Release,Release,3474,"ence to `libssh2_session_handshake'; /usr/bin/ld: (.text+0x58d): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x6a0): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x75d): undefined reference to `libssh2_knownhost_free'; ... So somehow this does not build - but I have the impression that the linker issues are caused by some missing CMAKE options (as well as using the build directory). Thus I used the cmake command line as its used in the Debian packaging:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliary decoy sequence or the genome of the organism) be rovided during indexing. Further details can be found at https://salmon.readthedocs.io/en/latest/salmon.html#preparing-transcriptome-indices-mapping-based-mode.; [2023-03-10 11:56:01.435] [jLog] [info] building index; out : sample_salmon_quasi_index; [2023-03-10 11:56:01.435] [puff::index::jointLog] [info] Running fixFasta. [Step 1 of 4] : counting k-mers. [2023-03-10 11:56:01.441] [puff::index::jo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:5026,Deployability,release,release,5026," run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliary decoy sequence or the genome of the organism) be rovided during indexing. Further details can be found at https://salmon.readthedocs.io/en/latest/salmon.html#preparing-transcriptome-indices-mapping-based-mode.; [2023-03-10 11:56:01.435] [jLog] [info] building index; out : sample_salmon_quasi_index; [2023-03-10 11:56:01.435] [puff::index::jointLog] [info] Running fixFasta. [Step 1 of 4] : counting k-mers. [2023-03-10 11:56:01.441] [puff::index::jointLog] [info] Replaced 0 non-ATCG nucleotides; [2023-03-10 11:56:01.441] [puff::index::jointLog] [info] Clipped poly-A tails from 0 transcripts; wrote 15 cleaned references; Segmentation fault (core dumped). Please note that `-DNO_IPO=TRUE` is not part of the above cmake call. I've added this to the packaging, but there is no difference. Please also note that I did *not* cloned the Git repository but have downloaded the release tarball since this is what we are packaging. Hope this helps tracking down the issue. Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:63,Integrability,Depend,Depends,63,"I've tried to reproduce the issue in docker by using the Build-Depends that are used in Debian:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE ..; $ make -j8; make -j8; [ 3%] Built target ksw2pp_sse4; [ 6%] Built target ntcard; [ 15%] Built target twopaco; [ 18%] Built target graphdump; [ 21%] Built target ksw2pp_sse2; [ 27%] Built target ksw2pp_basic; [ 43%] Built target salmon_core; [ 67%] Built target puffer; [ 68%] Built target ksw2pp; [ 69%] Built target UnitTestsMain; [ 73%] Built target alevin_core; [ 74%] Linking CXX executable unitTests; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_destroy':; (.text+0x21): undefined reference to `psl_free'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_use':; (.text+0xbc): undefined reference to `psl_latest'; /usr/bin/ld: (.text+0x157): undefined reference to `psl_builtin'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version':; (.text+0x129): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x16f): undefined reference to `ZSTD_versionNumber'; /usr/bin/ld: (.text+0x1e3): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x20e): undefined reference to `psl_get_version'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version_info':; (.text+0x386): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x3ad): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x3b8): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:118,Testability,test,testing,118,"I've tried to reproduce the issue in docker by using the Build-Depends that are used in Debian:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE ..; $ make -j8; make -j8; [ 3%] Built target ksw2pp_sse4; [ 6%] Built target ntcard; [ 15%] Built target twopaco; [ 18%] Built target graphdump; [ 21%] Built target ksw2pp_sse2; [ 27%] Built target ksw2pp_basic; [ 43%] Built target salmon_core; [ 67%] Built target puffer; [ 68%] Built target ksw2pp; [ 69%] Built target UnitTestsMain; [ 73%] Built target alevin_core; [ 74%] Linking CXX executable unitTests; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_destroy':; (.text+0x21): undefined reference to `psl_free'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_use':; (.text+0xbc): undefined reference to `psl_latest'; /usr/bin/ld: (.text+0x157): undefined reference to `psl_builtin'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version':; (.text+0x129): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x16f): undefined reference to `ZSTD_versionNumber'; /usr/bin/ld: (.text+0x1e3): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x20e): undefined reference to `psl_get_version'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version_info':; (.text+0x386): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x3ad): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x3b8): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:151,Testability,test,testing,151,"I've tried to reproduce the issue in docker by using the Build-Depends that are used in Debian:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DNO_IPO=TRUE ..; $ make -j8; make -j8; [ 3%] Built target ksw2pp_sse4; [ 6%] Built target ntcard; [ 15%] Built target twopaco; [ 18%] Built target graphdump; [ 21%] Built target ksw2pp_sse2; [ 27%] Built target ksw2pp_basic; [ 43%] Built target salmon_core; [ 67%] Built target puffer; [ 68%] Built target ksw2pp; [ 69%] Built target UnitTestsMain; [ 73%] Built target alevin_core; [ 74%] Linking CXX executable unitTests; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_destroy':; (.text+0x21): undefined reference to `psl_free'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-psl.o): in function `Curl_psl_use':; (.text+0xbc): undefined reference to `psl_latest'; /usr/bin/ld: (.text+0x157): undefined reference to `psl_builtin'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version':; (.text+0x129): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x16f): undefined reference to `ZSTD_versionNumber'; /usr/bin/ld: (.text+0x1e3): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x20e): undefined reference to `psl_get_version'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-version.o): in function `curl_version_info':; (.text+0x386): undefined reference to `idn2_check_version'; /usr/bin/ld: (.text+0x3ad): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.text+0x3b8): undefined reference to `BrotliDecoderVersion'; /usr/bin/ld: (.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:3125,Testability,test,testing,3125,"ext+0x475): undefined reference to `nghttp2_version'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-libssh2.o): in function `ssh_attach':; (.text+0x3e1): undefined reference to `libssh2_session_abstract'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-libssh2.o): in function `ssh_statemach_act':; (.text+0x4b1): undefined reference to `libssh2_session_set_blocking'; /usr/bin/ld: (.text+0x4fb): undefined reference to `libssh2_session_handshake'; /usr/bin/ld: (.text+0x58d): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x6a0): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x75d): undefined reference to `libssh2_knownhost_free'; ... So somehow this does not build - but I have the impression that the linker issues are caused by some missing CMAKE options (as well as using the build directory). Thus I used the cmake command line as its used in the Debian packaging:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliar",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855:3158,Testability,test,testing,3158,"ext+0x475): undefined reference to `nghttp2_version'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-libssh2.o): in function `ssh_attach':; (.text+0x3e1): undefined reference to `libssh2_session_abstract'; /usr/bin/ld: /usr/lib/x86_64-linux-gnu/libcurl.a(libcurl_gnutls_la-libssh2.o): in function `ssh_statemach_act':; (.text+0x4b1): undefined reference to `libssh2_session_set_blocking'; /usr/bin/ld: (.text+0x4fb): undefined reference to `libssh2_session_handshake'; /usr/bin/ld: (.text+0x58d): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x6a0): undefined reference to `libssh2_hostkey_hash'; /usr/bin/ld: (.text+0x75d): undefined reference to `libssh2_knownhost_free'; ... So somehow this does not build - but I have the impression that the linker issues are caused by some missing CMAKE options (as well as using the build directory). Thus I used the cmake command line as its used in the Debian packaging:. $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt update; $ apt upgrade; $ apt build-dep salmon; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE; $ make; $ tar xaf sample_data.tgz; # src/salmon index -t sample_data/transcripts.fasta -i sample_salmon_quasi_index; Version Info: This is the most recent version of salmon.; index [""sample_salmon_quasi_index""] did not previously exist . . . creating it; [2023-03-10 11:56:01.434] [jLog] [warning] The salmon index is being built without any decoy sequences. It is recommended that decoy sequence (either computed auxiliar",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463708855
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463775612:153,Modifiability,config,configure,153,"Hi @tillea! Thanks for the added details, I'll try and repro with these. I noticed 2 other differences and wonder if they matter. The first is that your configure does `DUSE_SHARED_LIBS=TRUE`. Could there be shared libs found at runtime other than what is linked during compile?. The other is that you seem to be doing an in source build. I.e. building directly in the salmon dir rather than in a build directory. I'm actually a bit surprised the build works in that way, as it isn't designed to be an in source build. I can try and see if either of those matter on my end, but wonder if you have insight into either. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463775612
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:72,Availability,error,error,72,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:139,Availability,Error,Error,139,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:286,Availability,error,errors,286,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:957,Availability,fault,fault,957,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:433,Deployability,install,install,433,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:483,Deployability,install,install,483,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:168,Integrability,message,message,168,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:619,Integrability,depend,dependencies,619,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:894,Integrability,depend,dependent,894,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:262,Modifiability,Config,Configuring,262,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824:358,Testability,log,log,358,"Ok, when I attempt the build the way you say above, I get the following error during CMake:. ```; -- fetch PUFFERFISH exit code 127; CMake Error at CMakeLists.txt:317 (message):; Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; 127]. -- Configuring incomplete, errors occurred!; See also ""/salmon-1.10.0/build/CMakeFiles/CMakeOutput.log"".; ```. It seems `wget`, `curl` and `unzip` were missing, and I had to install them. After that, I was able to build and install. At that point, I was able to reproduce the issue! So, it seems to me the underlying problem is coming from one of the upstream dependencies (i.e. libraries being linked to). I will try see if I can find the offender. In general, we like to statically link salmon for exactly this reason. Outside of package systems with which I am familiar (e.g. conda), we don't have a lot of experience in specifying dependent package version constrains, which I believe to be at fault here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1463973824
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:398,Deployability,configurat,configurations,398,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:27,Modifiability,config,configure,27,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:312,Modifiability,config,configure,312,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:398,Modifiability,config,configurations,398,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:228,Usability,simpl,simple,228,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885:31,Integrability,depend,dependency,31,"So, it turns out that with the dependency setup that is pulled in, I can't even get `salmon` to build without `USE_SHARED=TRUE`. I think at this point, it's not clear the segfault is due to something that is broken / can be fixed in the salmon code itself. Rather, it's likely due to a misversioning of a dependency that is pulled in and then linked against. For the time being, I think the options are to do a more ""standard"" build (i.e. like the first one I suggested that pull in only that minimal set of dependencies and let salmon itself statically link the rest), or to try and look at the upstream shared libraries being linked and figure out which of them is misversioned. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885:305,Integrability,depend,dependency,305,"So, it turns out that with the dependency setup that is pulled in, I can't even get `salmon` to build without `USE_SHARED=TRUE`. I think at this point, it's not clear the segfault is due to something that is broken / can be fixed in the salmon code itself. Rather, it's likely due to a misversioning of a dependency that is pulled in and then linked against. For the time being, I think the options are to do a more ""standard"" build (i.e. like the first one I suggested that pull in only that minimal set of dependencies and let salmon itself statically link the rest), or to try and look at the upstream shared libraries being linked and figure out which of them is misversioned. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885:508,Integrability,depend,dependencies,508,"So, it turns out that with the dependency setup that is pulled in, I can't even get `salmon` to build without `USE_SHARED=TRUE`. I think at this point, it's not clear the segfault is due to something that is broken / can be fixed in the salmon code itself. Rather, it's likely due to a misversioning of a dependency that is pulled in and then linked against. For the time being, I think the options are to do a more ""standard"" build (i.e. like the first one I suggested that pull in only that minimal set of dependencies and let salmon itself statically link the rest), or to try and look at the upstream shared libraries being linked and figure out which of them is misversioned. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885:161,Usability,clear,clear,161,"So, it turns out that with the dependency setup that is pulled in, I can't even get `salmon` to build without `USE_SHARED=TRUE`. I think at this point, it's not clear the segfault is due to something that is broken / can be fixed in the salmon code itself. Rather, it's likely due to a misversioning of a dependency that is pulled in and then linked against. For the time being, I think the options are to do a more ""standard"" build (i.e. like the first one I suggested that pull in only that minimal set of dependencies and let salmon itself statically link the rest), or to try and look at the upstream shared libraries being linked and figure out which of them is misversioned. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548:74,Availability,error,error,74,"> Ok, when I attempt the build the way you say above, I get the following error during CMake:; > ; > ```; > -- fetch PUFFERFISH exit code 127; > CMake Error at CMakeLists.txt:317 (message):; > Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; > 127]. Did you do the. ```; apt build-dep salmon; ```. step? I can't imagine that you get this problem if you follow my log step by step. Debian is usually using dynamic linking. By having all Build-Dependencies (which is ensured in the step above) the existence of the libraries is granted and the options for cmake I specified are ensuring that the libs are found. Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548:151,Availability,Error,Error,151,"> Ok, when I attempt the build the way you say above, I get the following error during CMake:; > ; > ```; > -- fetch PUFFERFISH exit code 127; > CMake Error at CMakeLists.txt:317 (message):; > Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; > 127]. Did you do the. ```; apt build-dep salmon; ```. step? I can't imagine that you get this problem if you follow my log step by step. Debian is usually using dynamic linking. By having all Build-Dependencies (which is ensured in the step above) the existence of the libraries is granted and the options for cmake I specified are ensuring that the libs are found. Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548:180,Integrability,message,message,180,"> Ok, when I attempt the build the way you say above, I get the following error during CMake:; > ; > ```; > -- fetch PUFFERFISH exit code 127; > CMake Error at CMakeLists.txt:317 (message):; > Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; > 127]. Did you do the. ```; apt build-dep salmon; ```. step? I can't imagine that you get this problem if you follow my log step by step. Debian is usually using dynamic linking. By having all Build-Dependencies (which is ensured in the step above) the existence of the libraries is granted and the options for cmake I specified are ensuring that the libs are found. Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548:467,Integrability,Depend,Dependencies,467,"> Ok, when I attempt the build the way you say above, I get the following error during CMake:; > ; > ```; > -- fetch PUFFERFISH exit code 127; > CMake Error at CMakeLists.txt:317 (message):; > Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; > 127]. Did you do the. ```; apt build-dep salmon; ```. step? I can't imagine that you get this problem if you follow my log step by step. Debian is usually using dynamic linking. By having all Build-Dependencies (which is ensured in the step above) the existence of the libraries is granted and the options for cmake I specified are ensuring that the libs are found. Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548:388,Testability,log,log,388,"> Ok, when I attempt the build the way you say above, I get the following error during CMake:; > ; > ```; > -- fetch PUFFERFISH exit code 127; > CMake Error at CMakeLists.txt:317 (message):; > Could not fetch pufferfish source [fetchPufferfish.sh returned exit code; > 127]. Did you do the. ```; apt build-dep salmon; ```. step? I can't imagine that you get this problem if you follow my log step by step. Debian is usually using dynamic linking. By having all Build-Dependencies (which is ensured in the step above) the existence of the libraries is granted and the options for cmake I specified are ensuring that the libs are found. Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464176548
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:477,Availability,echo,echo,477,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:587,Deployability,update,update,587,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:605,Deployability,upgrade,upgrade,605,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:624,Deployability,install,install,624,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:993,Deployability,Release,Release,993,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:1268,Deployability,install,install,1268,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:1349,Deployability,install,installed,1349,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:1508,Deployability,install,install,1508,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:104,Integrability,depend,dependencies,104,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:160,Integrability,depend,dependencies,160,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:795,Modifiability,config,config,795,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:433,Testability,test,testing,433,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:466,Testability,test,testing,466,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279:1284,Testability,test,test,1284,"Hi @tillea,. It seems this is exactly the problem. The build deps here are not quite correct. There are dependencies that salmon no longer has, and some of the dependencies it does have are out of date and can't be used from upstream (e.g. libstaden in the latest version, among others). On the bright side, it's not the dynamic linking alone that is problematic. The following works fine on my end:. ```{bash}; $ docker pull debian:testing; $ docker run -it debian:testing; $ echo ""deb-src http://deb.debian.org/debian unstable main"" > /etc/apt/sources.list.d/10-debsrc.list; $ apt-get update; $ apt-get upgrade; $ apt-get install build-essential git libboost-all-dev liblzma-dev libbz2-dev cmake zlib1g-dev curl unzip wget libcurl4-openssl-dev libtbb-dev libtbb12 liblzma-dev libjemalloc2 pkg-config libgff-dev; $ wget https://github.com/COMBINE-lab/salmon/archive/refs/tags/v1.10.0.tar.gz; $ tar xaf v1.10.0.tar.gz; $ cd salmon-1.10.0; $ mkdir build && cd build; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; $ make -j8; $ make install; $ make test; ```. This is preferring dynamic linking, and the resulting installed executable runs fine without a segfault. Can you try this on your end? Then the thing to do may be to find what is discordant between the packages I install above and what gets pulled in by `apt build-dep salmon`. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464182279
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988:942,Availability,down,download,942,"Hi @rob-p,; thanks a lot for your investigation. Could you please be more verbose on those incorrect Build-Depends? What dependencies can be removed (if not used they should not really harm, thought but you are correct that it makes sense to remove these) and more importantly which can not be used. For instance if we can't use libstaden as packaged we have a problem. All preconditions for a Debian package have to be packaged first. Fetching something from network is not permitted at package build time.; Thus I simply tried changing the cmake options to. ```; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; ```. which does not change the SEGFAULT problem. If the issue belongs to something we need to download from somewhere please let me know what looks suspicious to you. This would be helpful since we could either add it to the Debian package source in debian/missing-sources ... or rather fix the predependency that would break salmon.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988:592,Deployability,Release,Release,592,"Hi @rob-p,; thanks a lot for your investigation. Could you please be more verbose on those incorrect Build-Depends? What dependencies can be removed (if not used they should not really harm, thought but you are correct that it makes sense to remove these) and more importantly which can not be used. For instance if we can't use libstaden as packaged we have a problem. All preconditions for a Debian package have to be packaged first. Fetching something from network is not permitted at package build time.; Thus I simply tried changing the cmake options to. ```; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; ```. which does not change the SEGFAULT problem. If the issue belongs to something we need to download from somewhere please let me know what looks suspicious to you. This would be helpful since we could either add it to the Debian package source in debian/missing-sources ... or rather fix the predependency that would break salmon.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988:107,Integrability,Depend,Depends,107,"Hi @rob-p,; thanks a lot for your investigation. Could you please be more verbose on those incorrect Build-Depends? What dependencies can be removed (if not used they should not really harm, thought but you are correct that it makes sense to remove these) and more importantly which can not be used. For instance if we can't use libstaden as packaged we have a problem. All preconditions for a Debian package have to be packaged first. Fetching something from network is not permitted at package build time.; Thus I simply tried changing the cmake options to. ```; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; ```. which does not change the SEGFAULT problem. If the issue belongs to something we need to download from somewhere please let me know what looks suspicious to you. This would be helpful since we could either add it to the Debian package source in debian/missing-sources ... or rather fix the predependency that would break salmon.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988:121,Integrability,depend,dependencies,121,"Hi @rob-p,; thanks a lot for your investigation. Could you please be more verbose on those incorrect Build-Depends? What dependencies can be removed (if not used they should not really harm, thought but you are correct that it makes sense to remove these) and more importantly which can not be used. For instance if we can't use libstaden as packaged we have a problem. All preconditions for a Debian package have to be packaged first. Fetching something from network is not permitted at package build time.; Thus I simply tried changing the cmake options to. ```; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; ```. which does not change the SEGFAULT problem. If the issue belongs to something we need to download from somewhere please let me know what looks suspicious to you. This would be helpful since we could either add it to the Debian package source in debian/missing-sources ... or rather fix the predependency that would break salmon.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988:516,Usability,simpl,simply,516,"Hi @rob-p,; thanks a lot for your investigation. Could you please be more verbose on those incorrect Build-Depends? What dependencies can be removed (if not used they should not really harm, thought but you are correct that it makes sense to remove these) and more importantly which can not be used. For instance if we can't use libstaden as packaged we have a problem. All preconditions for a Debian package have to be packaged first. Fetching something from network is not permitted at package build time.; Thus I simply tried changing the cmake options to. ```; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; ```. which does not change the SEGFAULT problem. If the issue belongs to something we need to download from somewhere please let me know what looks suspicious to you. This would be helpful since we could either add it to the Debian package source in debian/missing-sources ... or rather fix the predependency that would break salmon.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:499,Availability,down,downloaded,499,"Hi Andreas,. So I don't know if there is a easy way to get the specific list of reverse dependencies, but then we can cross-check it with my explicit list above:. ```; build-essential; git ; libboost-all-dev ; liblzma-dev ; libbz2-dev ; cmake ; zlib1g-dev ; curl ; unzip ; wget ; libcurl4-openssl-dev ; libtbb-dev ; libtbb12 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:1070,Availability,down,download,1070,"ut then we can cross-check it with my explicit list above:. ```; build-essential; git ; libboost-all-dev ; liblzma-dev ; libbz2-dev ; cmake ; zlib1g-dev ; curl ; unzip ; wget ; libcurl4-openssl-dev ; libtbb-dev ; libtbb12 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:692,Deployability,release,release,692,"Hi Andreas,. So I don't know if there is a easy way to get the specific list of reverse dependencies, but then we can cross-check it with my explicit list above:. ```; build-essential; git ; libboost-all-dev ; liblzma-dev ; libbz2-dev ; cmake ; zlib1g-dev ; curl ; unzip ; wget ; libcurl4-openssl-dev ; libtbb-dev ; libtbb12 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:714,Deployability,release,release,714,"Hi Andreas,. So I don't know if there is a easy way to get the specific list of reverse dependencies, but then we can cross-check it with my explicit list above:. ```; build-essential; git ; libboost-all-dev ; liblzma-dev ; libbz2-dev ; cmake ; zlib1g-dev ; curl ; unzip ; wget ; libcurl4-openssl-dev ; libtbb-dev ; libtbb12 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:1208,Deployability,release,releases,1208," ; libboost-all-dev ; liblzma-dev ; libbz2-dev ; cmake ; zlib1g-dev ; curl ; unzip ; wget ; libcurl4-openssl-dev ; libtbb-dev ; libtbb12 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depe",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:1253,Deployability,release,release,1253,"2 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depended upon, we currently don't have the capacity to tackle that ourselves and don't have a suitable mechanism to replace the current app",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:1374,Deployability,release,release,1374,"2 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depended upon, we currently don't have the capacity to tackle that ourselves and don't have a suitable mechanism to replace the current app",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:1603,Deployability,install,installed,1603," dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depended upon, we currently don't have the capacity to tackle that ourselves and don't have a suitable mechanism to replace the current approach to obtaining the dependent pufferfish files. However, this question is very important as e.g. a segfault exactly like the one you are encountering was actually a bug in the pufferfish source used in the 1.9.0 release of `salmon` that was _fixed_ for the 1.10.0 release. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:1982,Deployability,release,releases,1982," dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depended upon, we currently don't have the capacity to tackle that ourselves and don't have a suitable mechanism to replace the current approach to obtaining the dependent pufferfish files. However, this question is very important as e.g. a segfault exactly like the one you are encountering was actually a bug in the pufferfish source used in the 1.9.0 release of `salmon` that was _fixed_ for the 1.10.0 release. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:2539,Deployability,release,release,2539," dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depended upon, we currently don't have the capacity to tackle that ourselves and don't have a suitable mechanism to replace the current approach to obtaining the dependent pufferfish files. However, this question is very important as e.g. a segfault exactly like the one you are encountering was actually a bug in the pufferfish source used in the 1.9.0 release of `salmon` that was _fixed_ for the 1.10.0 release. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:2591,Deployability,release,release,2591," dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depended upon, we currently don't have the capacity to tackle that ourselves and don't have a suitable mechanism to replace the current approach to obtaining the dependent pufferfish files. However, this question is very important as e.g. a segfault exactly like the one you are encountering was actually a bug in the pufferfish source used in the 1.9.0 release of `salmon` that was _fixed_ for the 1.10.0 release. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:88,Integrability,depend,dependencies,88,"Hi Andreas,. So I don't know if there is a easy way to get the specific list of reverse dependencies, but then we can cross-check it with my explicit list above:. ```; build-essential; git ; libboost-all-dev ; liblzma-dev ; libbz2-dev ; cmake ; zlib1g-dev ; curl ; unzip ; wget ; libcurl4-openssl-dev ; libtbb-dev ; libtbb12 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:611,Integrability,depend,dependencies,611,"Hi Andreas,. So I don't know if there is a easy way to get the specific list of reverse dependencies, but then we can cross-check it with my explicit list above:. ```; build-essential; git ; libboost-all-dev ; liblzma-dev ; libbz2-dev ; cmake ; zlib1g-dev ; curl ; unzip ; wget ; libcurl4-openssl-dev ; libtbb-dev ; libtbb12 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:1182,Integrability,depend,dependencies,1182," ; libboost-all-dev ; liblzma-dev ; libbz2-dev ; cmake ; zlib1g-dev ; curl ; unzip ; wget ; libcurl4-openssl-dev ; libtbb-dev ; libtbb12 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depe",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:1771,Integrability,depend,dependency,1771," dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depended upon, we currently don't have the capacity to tackle that ourselves and don't have a suitable mechanism to replace the current approach to obtaining the dependent pufferfish files. However, this question is very important as e.g. a segfault exactly like the one you are encountering was actually a bug in the pufferfish source used in the 1.9.0 release of `salmon` that was _fixed_ for the 1.10.0 release. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:2185,Integrability,depend,depended,2185," dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depended upon, we currently don't have the capacity to tackle that ourselves and don't have a suitable mechanism to replace the current approach to obtaining the dependent pufferfish files. However, this question is very important as e.g. a segfault exactly like the one you are encountering was actually a bug in the pufferfish source used in the 1.9.0 release of `salmon` that was _fixed_ for the 1.10.0 release. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:2347,Integrability,depend,dependent,2347," dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as when `salmon` is built for other distribution channels like `conda`. While I am happy to have someone figure out how to package that up as a package that can be put into the repo and depended upon, we currently don't have the capacity to tackle that ourselves and don't have a suitable mechanism to replace the current approach to obtaining the dependent pufferfish files. However, this question is very important as e.g. a segfault exactly like the one you are encountering was actually a bug in the pufferfish source used in the 1.9.0 release of `salmon` that was _fixed_ for the 1.10.0 release. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233:360,Modifiability,config,config,360,"Hi Andreas,. So I don't know if there is a easy way to get the specific list of reverse dependencies, but then we can cross-check it with my explicit list above:. ```; build-essential; git ; libboost-all-dev ; liblzma-dev ; libbz2-dev ; cmake ; zlib1g-dev ; curl ; unzip ; wget ; libcurl4-openssl-dev ; libtbb-dev ; libtbb12 ; liblzma-dev ; libjemalloc2 ; pkg-config ; libgff-dev; ```. One thing I noticed during build is that, while I included `libjemalloc2` here, the salmon build procedure still downloaded and built `jemalloc`. However, I don't _think_ jemalloc is the thing causing the segfault. Regarding dependencies that can't be used — the current `libstaden` is behind the upstream release. The upstream release contains an important bug fix for a bug (and suggested fix that we proposed to the developer) upon which we rely. More importantly, afaik there is no relevant `libpufferfish-dev` package (we certainly have not made one), and so there is not even e.g. a check in the `CMakeLists.txt` file. Salmon's build always tries to run `fetchPufferfish.sh` to download the relevant `pufferfish` source files needed to build `salmon`. Critically, the relevant `pufferfish` dependencies and `salmon` releases move in lockstep. Each new `salmon` release it accompanied by a new tag in the `pufferfish` repo (so that the specific source used to build a given `salmon` release is fixed and easily trackable). So, I think the easiest way to move forward is to:. * do a diff of my list of packages above with what is pulled in by `apt build-dep salmon`. * figure out why, even when `libjemalloc2` is installed, the build system tries to build `jemalloc` itself (maybe we need the dev package?). * determine what folks want to do upstream about the lockstep pufferfish dependency. Right now, the `fetchPufferfish.sh` script pulls a tagged tarball from github and checks that the sha matches, and moves the relevant source files into place. This is true both when we build our own releases as well as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464538233
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:700,Availability,down,downloaded,700,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:1045,Availability,down,downloading,1045,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:1105,Availability,down,downloaded,1105,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:109,Deployability,upgrade,upgrade,109,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:197,Deployability,update,update,197,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:244,Deployability,upgrade,upgraded,244,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:1160,Deployability,install,installed,1160,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:983,Testability,test,test,983,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:204,Usability,simpl,simply,204,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:626,Availability,down,downloaded,626,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:486,Deployability,install,install,486,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:755,Deployability,release,release,755,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:771,Deployability,install,installed,771,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:811,Deployability,install,install,811,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:1180,Deployability,update,updated,1180,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:1252,Deployability,install,installed,1252,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:1302,Deployability,install,install,1302,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:390,Integrability,depend,dependencies,390,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:418,Integrability,depend,dependency,418,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:512,Integrability,depend,dependencies,512,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:570,Testability,test,testing,570,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:244,Usability,clear,clearly,244,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:1280,Usability,simpl,simple,1280,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711:466,Availability,failure,failure,466,"Hi @tillea and @nileshpatra,. Ok, I dug deeper and found out what's going on. The culprit is, in fact, `libcereal`. The problem is that `libcereal` bumped patch versions only since the version corresponding to the headers included in `pufferfish`, but their changes are not, in fact, backwards compatible! This lead to a version mismatch between the headers used in `pufferfish` and the headers found from the installed package, ultimately resulting in an assertion failure in `rapidjson` (which cereal is using) and a segfault. On the plus side, this was relatively easy to fix by bumping the included cereal headers in pufferfish. I also updated the `Findcereal.cmake` module and added a version constraint so that we now require the new version (1.3.2). This is now tagged and released as `salmon 1.10.1`. Please give that a go when you have a chance. I'll note that, before this is added upstream in debian, I'd still advocate for fixing the `libstaden` package to update to the new version. I'd also recommend moving to dependencies like the ones I've included above to remove some really antiquated dependencies that salmon no longer requires but are still being pulled in. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711:155,Deployability,patch,patch,155,"Hi @tillea and @nileshpatra,. Ok, I dug deeper and found out what's going on. The culprit is, in fact, `libcereal`. The problem is that `libcereal` bumped patch versions only since the version corresponding to the headers included in `pufferfish`, but their changes are not, in fact, backwards compatible! This lead to a version mismatch between the headers used in `pufferfish` and the headers found from the installed package, ultimately resulting in an assertion failure in `rapidjson` (which cereal is using) and a segfault. On the plus side, this was relatively easy to fix by bumping the included cereal headers in pufferfish. I also updated the `Findcereal.cmake` module and added a version constraint so that we now require the new version (1.3.2). This is now tagged and released as `salmon 1.10.1`. Please give that a go when you have a chance. I'll note that, before this is added upstream in debian, I'd still advocate for fixing the `libstaden` package to update to the new version. I'd also recommend moving to dependencies like the ones I've included above to remove some really antiquated dependencies that salmon no longer requires but are still being pulled in. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711:410,Deployability,install,installed,410,"Hi @tillea and @nileshpatra,. Ok, I dug deeper and found out what's going on. The culprit is, in fact, `libcereal`. The problem is that `libcereal` bumped patch versions only since the version corresponding to the headers included in `pufferfish`, but their changes are not, in fact, backwards compatible! This lead to a version mismatch between the headers used in `pufferfish` and the headers found from the installed package, ultimately resulting in an assertion failure in `rapidjson` (which cereal is using) and a segfault. On the plus side, this was relatively easy to fix by bumping the included cereal headers in pufferfish. I also updated the `Findcereal.cmake` module and added a version constraint so that we now require the new version (1.3.2). This is now tagged and released as `salmon 1.10.1`. Please give that a go when you have a chance. I'll note that, before this is added upstream in debian, I'd still advocate for fixing the `libstaden` package to update to the new version. I'd also recommend moving to dependencies like the ones I've included above to remove some really antiquated dependencies that salmon no longer requires but are still being pulled in. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711:640,Deployability,update,updated,640,"Hi @tillea and @nileshpatra,. Ok, I dug deeper and found out what's going on. The culprit is, in fact, `libcereal`. The problem is that `libcereal` bumped patch versions only since the version corresponding to the headers included in `pufferfish`, but their changes are not, in fact, backwards compatible! This lead to a version mismatch between the headers used in `pufferfish` and the headers found from the installed package, ultimately resulting in an assertion failure in `rapidjson` (which cereal is using) and a segfault. On the plus side, this was relatively easy to fix by bumping the included cereal headers in pufferfish. I also updated the `Findcereal.cmake` module and added a version constraint so that we now require the new version (1.3.2). This is now tagged and released as `salmon 1.10.1`. Please give that a go when you have a chance. I'll note that, before this is added upstream in debian, I'd still advocate for fixing the `libstaden` package to update to the new version. I'd also recommend moving to dependencies like the ones I've included above to remove some really antiquated dependencies that salmon no longer requires but are still being pulled in. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711:780,Deployability,release,released,780,"Hi @tillea and @nileshpatra,. Ok, I dug deeper and found out what's going on. The culprit is, in fact, `libcereal`. The problem is that `libcereal` bumped patch versions only since the version corresponding to the headers included in `pufferfish`, but their changes are not, in fact, backwards compatible! This lead to a version mismatch between the headers used in `pufferfish` and the headers found from the installed package, ultimately resulting in an assertion failure in `rapidjson` (which cereal is using) and a segfault. On the plus side, this was relatively easy to fix by bumping the included cereal headers in pufferfish. I also updated the `Findcereal.cmake` module and added a version constraint so that we now require the new version (1.3.2). This is now tagged and released as `salmon 1.10.1`. Please give that a go when you have a chance. I'll note that, before this is added upstream in debian, I'd still advocate for fixing the `libstaden` package to update to the new version. I'd also recommend moving to dependencies like the ones I've included above to remove some really antiquated dependencies that salmon no longer requires but are still being pulled in. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711:969,Deployability,update,update,969,"Hi @tillea and @nileshpatra,. Ok, I dug deeper and found out what's going on. The culprit is, in fact, `libcereal`. The problem is that `libcereal` bumped patch versions only since the version corresponding to the headers included in `pufferfish`, but their changes are not, in fact, backwards compatible! This lead to a version mismatch between the headers used in `pufferfish` and the headers found from the installed package, ultimately resulting in an assertion failure in `rapidjson` (which cereal is using) and a segfault. On the plus side, this was relatively easy to fix by bumping the included cereal headers in pufferfish. I also updated the `Findcereal.cmake` module and added a version constraint so that we now require the new version (1.3.2). This is now tagged and released as `salmon 1.10.1`. Please give that a go when you have a chance. I'll note that, before this is added upstream in debian, I'd still advocate for fixing the `libstaden` package to update to the new version. I'd also recommend moving to dependencies like the ones I've included above to remove some really antiquated dependencies that salmon no longer requires but are still being pulled in. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711:1025,Integrability,depend,dependencies,1025,"Hi @tillea and @nileshpatra,. Ok, I dug deeper and found out what's going on. The culprit is, in fact, `libcereal`. The problem is that `libcereal` bumped patch versions only since the version corresponding to the headers included in `pufferfish`, but their changes are not, in fact, backwards compatible! This lead to a version mismatch between the headers used in `pufferfish` and the headers found from the installed package, ultimately resulting in an assertion failure in `rapidjson` (which cereal is using) and a segfault. On the plus side, this was relatively easy to fix by bumping the included cereal headers in pufferfish. I also updated the `Findcereal.cmake` module and added a version constraint so that we now require the new version (1.3.2). This is now tagged and released as `salmon 1.10.1`. Please give that a go when you have a chance. I'll note that, before this is added upstream in debian, I'd still advocate for fixing the `libstaden` package to update to the new version. I'd also recommend moving to dependencies like the ones I've included above to remove some really antiquated dependencies that salmon no longer requires but are still being pulled in. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711:1105,Integrability,depend,dependencies,1105,"Hi @tillea and @nileshpatra,. Ok, I dug deeper and found out what's going on. The culprit is, in fact, `libcereal`. The problem is that `libcereal` bumped patch versions only since the version corresponding to the headers included in `pufferfish`, but their changes are not, in fact, backwards compatible! This lead to a version mismatch between the headers used in `pufferfish` and the headers found from the installed package, ultimately resulting in an assertion failure in `rapidjson` (which cereal is using) and a segfault. On the plus side, this was relatively easy to fix by bumping the included cereal headers in pufferfish. I also updated the `Findcereal.cmake` module and added a version constraint so that we now require the new version (1.3.2). This is now tagged and released as `salmon 1.10.1`. Please give that a go when you have a chance. I'll note that, before this is added upstream in debian, I'd still advocate for fixing the `libstaden` package to update to the new version. I'd also recommend moving to dependencies like the ones I've included above to remove some really antiquated dependencies that salmon no longer requires but are still being pulled in. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711:456,Testability,assert,assertion,456,"Hi @tillea and @nileshpatra,. Ok, I dug deeper and found out what's going on. The culprit is, in fact, `libcereal`. The problem is that `libcereal` bumped patch versions only since the version corresponding to the headers included in `pufferfish`, but their changes are not, in fact, backwards compatible! This lead to a version mismatch between the headers used in `pufferfish` and the headers found from the installed package, ultimately resulting in an assertion failure in `rapidjson` (which cereal is using) and a segfault. On the plus side, this was relatively easy to fix by bumping the included cereal headers in pufferfish. I also updated the `Findcereal.cmake` module and added a version constraint so that we now require the new version (1.3.2). This is now tagged and released as `salmon 1.10.1`. Please give that a go when you have a chance. I'll note that, before this is added upstream in debian, I'd still advocate for fixing the `libstaden` package to update to the new version. I'd also recommend moving to dependencies like the ones I've included above to remove some really antiquated dependencies that salmon no longer requires but are still being pulled in. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1465096711
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1467446759:58,Deployability,update,update,58,"Hi,; I confirm that `salmon 1.10.1` fixes the issue. I'll update `libstaden` in Debian. We probably should talk about some separate `pufferfish` package for the next Debian release. We actually [started]( https://salsa.debian.org/med-team/pufferfish/) with the packaging but this has somehow stalled (I don't remember the reasons but I'll keep an eye on this soon (but its too late for bookworm).; Kind regards and thanks a lot for the fix, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1467446759
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1467446759:173,Deployability,release,release,173,"Hi,; I confirm that `salmon 1.10.1` fixes the issue. I'll update `libstaden` in Debian. We probably should talk about some separate `pufferfish` package for the next Debian release. We actually [started]( https://salsa.debian.org/med-team/pufferfish/) with the packaging but this has somehow stalled (I don't remember the reasons but I'll keep an eye on this soon (but its too late for bookworm).; Kind regards and thanks a lot for the fix, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1467446759
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365:792,Integrability,protocol,protocol,792,"Hello Rob,. Thank you for your quick reply to my question. My MEGAHIT and Trinity assemblies were not built with strand-aware flags. I made decoy-aware transcriptomes using a MEGAHIT assembly, a Trinity Assembly, and a published transcriptome from the same species, and when I ran my read files through salmon using ""A"" as the library type and each of the three indexes, all three were detected as ""most likely library type IU"". It's strange that once all three have been compiled into a single assembly using Evigene, salmon detects the ISR library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. ; Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol _is_ worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. Thanks for answering my noobie questions. Holly",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365:949,Integrability,protocol,protocol,949,"Hello Rob,. Thank you for your quick reply to my question. My MEGAHIT and Trinity assemblies were not built with strand-aware flags. I made decoy-aware transcriptomes using a MEGAHIT assembly, a Trinity Assembly, and a published transcriptome from the same species, and when I ran my read files through salmon using ""A"" as the library type and each of the three indexes, all three were detected as ""most likely library type IU"". It's strange that once all three have been compiled into a single assembly using Evigene, salmon detects the ISR library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. ; Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol _is_ worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. Thanks for answering my noobie questions. Holly",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365:1088,Integrability,protocol,protocol,1088,"Hello Rob,. Thank you for your quick reply to my question. My MEGAHIT and Trinity assemblies were not built with strand-aware flags. I made decoy-aware transcriptomes using a MEGAHIT assembly, a Trinity Assembly, and a published transcriptome from the same species, and when I ran my read files through salmon using ""A"" as the library type and each of the three indexes, all three were detected as ""most likely library type IU"". It's strange that once all three have been compiled into a single assembly using Evigene, salmon detects the ISR library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. ; Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol _is_ worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. Thanks for answering my noobie questions. Holly",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365:386,Safety,detect,detected,386,"Hello Rob,. Thank you for your quick reply to my question. My MEGAHIT and Trinity assemblies were not built with strand-aware flags. I made decoy-aware transcriptomes using a MEGAHIT assembly, a Trinity Assembly, and a published transcriptome from the same species, and when I ran my read files through salmon using ""A"" as the library type and each of the three indexes, all three were detected as ""most likely library type IU"". It's strange that once all three have been compiled into a single assembly using Evigene, salmon detects the ISR library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. ; Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol _is_ worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. Thanks for answering my noobie questions. Holly",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365:526,Safety,detect,detects,526,"Hello Rob,. Thank you for your quick reply to my question. My MEGAHIT and Trinity assemblies were not built with strand-aware flags. I made decoy-aware transcriptomes using a MEGAHIT assembly, a Trinity Assembly, and a published transcriptome from the same species, and when I ran my read files through salmon using ""A"" as the library type and each of the three indexes, all three were detected as ""most likely library type IU"". It's strange that once all three have been compiled into a single assembly using Evigene, salmon detects the ISR library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. ; Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol _is_ worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. Thanks for answering my noobie questions. Holly",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365:1067,Safety,detect,detect,1067,"Hello Rob,. Thank you for your quick reply to my question. My MEGAHIT and Trinity assemblies were not built with strand-aware flags. I made decoy-aware transcriptomes using a MEGAHIT assembly, a Trinity Assembly, and a published transcriptome from the same species, and when I ran my read files through salmon using ""A"" as the library type and each of the three indexes, all three were detected as ""most likely library type IU"". It's strange that once all three have been compiled into a single assembly using Evigene, salmon detects the ISR library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. ; Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol _is_ worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. Thanks for answering my noobie questions. Holly",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1492849365
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427:990,Integrability,protocol,protocol,990,"Hi Holley,. Thanks for the response: Here are some followup thoughts . >It's strange that once all three have been compiled into a single assembly using Evigene, salmon detects the ISR library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. That does seem strange, but I honestly don't know much about `Evigene` or what it's doing in combining these assemblies. When you specify ""IU"", the mappings will generally be _more_ lenient (i.e. you'd expect to get more mappings) than when you specify ""ISR"". The ""A"" flag just looks at how the first 10,000 reads map and guesses the library type based on that. On thing to make sure of is that your reads aren't ""ordered"" in any way, such that you'd expect the first 10,000 to deviate in any meaningful way from the statistics of the reads of the reads. > Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol is worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? . So there are really 2 questions here. *If* the data are stranded, then yes, it's worthwhile to use stranded flags in both assembly and quantification. This is because stranded protocols will allow you to better disambiguate (a) overlapping genes and (b) reads that are ambiguous between sequence-similar genes that happen to reside on different strands. The *second* question is why Trinity or MEGAHIT wouldn't detect this. The main reason for this is that these are assembly tools. Without access to a reference genome, there is no principled way for these tools to know what the orientation of a read is _a priori_, so they generally rely on the user to specify if the reads are stranded o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427:1147,Integrability,protocol,protocol,1147," library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. That does seem strange, but I honestly don't know much about `Evigene` or what it's doing in combining these assemblies. When you specify ""IU"", the mappings will generally be _more_ lenient (i.e. you'd expect to get more mappings) than when you specify ""ISR"". The ""A"" flag just looks at how the first 10,000 reads map and guesses the library type based on that. On thing to make sure of is that your reads aren't ""ordered"" in any way, such that you'd expect the first 10,000 to deviate in any meaningful way from the statistics of the reads of the reads. > Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol is worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? . So there are really 2 questions here. *If* the data are stranded, then yes, it's worthwhile to use stranded flags in both assembly and quantification. This is because stranded protocols will allow you to better disambiguate (a) overlapping genes and (b) reads that are ambiguous between sequence-similar genes that happen to reside on different strands. The *second* question is why Trinity or MEGAHIT wouldn't detect this. The main reason for this is that these are assembly tools. Without access to a reference genome, there is no principled way for these tools to know what the orientation of a read is _a priori_, so they generally rely on the user to specify if the reads are stranded or unstranded. > Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification i",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427:1284,Integrability,protocol,protocol,1284," library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. That does seem strange, but I honestly don't know much about `Evigene` or what it's doing in combining these assemblies. When you specify ""IU"", the mappings will generally be _more_ lenient (i.e. you'd expect to get more mappings) than when you specify ""ISR"". The ""A"" flag just looks at how the first 10,000 reads map and guesses the library type based on that. On thing to make sure of is that your reads aren't ""ordered"" in any way, such that you'd expect the first 10,000 to deviate in any meaningful way from the statistics of the reads of the reads. > Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol is worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? . So there are really 2 questions here. *If* the data are stranded, then yes, it's worthwhile to use stranded flags in both assembly and quantification. This is because stranded protocols will allow you to better disambiguate (a) overlapping genes and (b) reads that are ambiguous between sequence-similar genes that happen to reside on different strands. The *second* question is why Trinity or MEGAHIT wouldn't detect this. The main reason for this is that these are assembly tools. Without access to a reference genome, there is no principled way for these tools to know what the orientation of a read is _a priori_, so they generally rely on the user to specify if the reads are stranded or unstranded. > Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification i",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427:1486,Integrability,protocol,protocols,1486,"ou'd expect to get more mappings) than when you specify ""ISR"". The ""A"" flag just looks at how the first 10,000 reads map and guesses the library type based on that. On thing to make sure of is that your reads aren't ""ordered"" in any way, such that you'd expect the first 10,000 to deviate in any meaningful way from the statistics of the reads of the reads. > Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol is worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? . So there are really 2 questions here. *If* the data are stranded, then yes, it's worthwhile to use stranded flags in both assembly and quantification. This is because stranded protocols will allow you to better disambiguate (a) overlapping genes and (b) reads that are ambiguous between sequence-similar genes that happen to reside on different strands. The *second* question is why Trinity or MEGAHIT wouldn't detect this. The main reason for this is that these are assembly tools. Without access to a reference genome, there is no principled way for these tools to know what the orientation of a read is _a priori_, so they generally rely on the user to specify if the reads are stranded or unstranded. > Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. The Trinity command to specify the strandedness is `--SS_lib_type` (see e.g. [here](https://scilifelab.github.io/courses/ngsintro/1604/labs/rnaseqDenovo)). By default, Trinity will assume unstranded reads (as that's the safest default assumption). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427:169,Safety,detect,detects,169,"Hi Holley,. Thanks for the response: Here are some followup thoughts . >It's strange that once all three have been compiled into a single assembly using Evigene, salmon detects the ISR library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. That does seem strange, but I honestly don't know much about `Evigene` or what it's doing in combining these assemblies. When you specify ""IU"", the mappings will generally be _more_ lenient (i.e. you'd expect to get more mappings) than when you specify ""ISR"". The ""A"" flag just looks at how the first 10,000 reads map and guesses the library type based on that. On thing to make sure of is that your reads aren't ""ordered"" in any way, such that you'd expect the first 10,000 to deviate in any meaningful way from the statistics of the reads of the reads. > Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol is worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? . So there are really 2 questions here. *If* the data are stranded, then yes, it's worthwhile to use stranded flags in both assembly and quantification. This is because stranded protocols will allow you to better disambiguate (a) overlapping genes and (b) reads that are ambiguous between sequence-similar genes that happen to reside on different strands. The *second* question is why Trinity or MEGAHIT wouldn't detect this. The main reason for this is that these are assembly tools. Without access to a reference genome, there is no principled way for these tools to know what the orientation of a read is _a priori_, so they generally rely on the user to specify if the reads are stranded o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427:1263,Safety,detect,detect,1263," library type. When I change the ""A"" to ""ISR"" or ""IU"", the % mapped changes a lot. That does seem strange, but I honestly don't know much about `Evigene` or what it's doing in combining these assemblies. When you specify ""IU"", the mappings will generally be _more_ lenient (i.e. you'd expect to get more mappings) than when you specify ""ISR"". The ""A"" flag just looks at how the first 10,000 reads map and guesses the library type based on that. On thing to make sure of is that your reads aren't ""ordered"" in any way, such that you'd expect the first 10,000 to deviate in any meaningful way from the statistics of the reads of the reads. > Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol is worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? . So there are really 2 questions here. *If* the data are stranded, then yes, it's worthwhile to use stranded flags in both assembly and quantification. This is because stranded protocols will allow you to better disambiguate (a) overlapping genes and (b) reads that are ambiguous between sequence-similar genes that happen to reside on different strands. The *second* question is why Trinity or MEGAHIT wouldn't detect this. The main reason for this is that these are assembly tools. Without access to a reference genome, there is no principled way for these tools to know what the orientation of a read is _a priori_, so they generally rely on the user to specify if the reads are stranded or unstranded. > Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification i",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427:1721,Safety,detect,detect,1721,"ou'd expect to get more mappings) than when you specify ""ISR"". The ""A"" flag just looks at how the first 10,000 reads map and guesses the library type based on that. On thing to make sure of is that your reads aren't ""ordered"" in any way, such that you'd expect the first 10,000 to deviate in any meaningful way from the statistics of the reads of the reads. > Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol is worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? . So there are really 2 questions here. *If* the data are stranded, then yes, it's worthwhile to use stranded flags in both assembly and quantification. This is because stranded protocols will allow you to better disambiguate (a) overlapping genes and (b) reads that are ambiguous between sequence-similar genes that happen to reside on different strands. The *second* question is why Trinity or MEGAHIT wouldn't detect this. The main reason for this is that these are assembly tools. Without access to a reference genome, there is no principled way for these tools to know what the orientation of a read is _a priori_, so they generally rely on the user to specify if the reads are stranded or unstranded. > Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. The Trinity command to specify the strandedness is `--SS_lib_type` (see e.g. [here](https://scilifelab.github.io/courses/ngsintro/1604/labs/rnaseqDenovo)). By default, Trinity will assume unstranded reads (as that's the safest default assumption). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427:2426,Safety,safe,safest,2426,"ou'd expect to get more mappings) than when you specify ""ISR"". The ""A"" flag just looks at how the first 10,000 reads map and guesses the library type based on that. On thing to make sure of is that your reads aren't ""ordered"" in any way, such that you'd expect the first 10,000 to deviate in any meaningful way from the statistics of the reads of the reads. > Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol is worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? . So there are really 2 questions here. *If* the data are stranded, then yes, it's worthwhile to use stranded flags in both assembly and quantification. This is because stranded protocols will allow you to better disambiguate (a) overlapping genes and (b) reads that are ambiguous between sequence-similar genes that happen to reside on different strands. The *second* question is why Trinity or MEGAHIT wouldn't detect this. The main reason for this is that these are assembly tools. Without access to a reference genome, there is no principled way for these tools to know what the orientation of a read is _a priori_, so they generally rely on the user to specify if the reads are stranded or unstranded. > Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. The Trinity command to specify the strandedness is `--SS_lib_type` (see e.g. [here](https://scilifelab.github.io/courses/ngsintro/1604/labs/rnaseqDenovo)). By default, Trinity will assume unstranded reads (as that's the safest default assumption). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427
https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427:1801,Security,access,access,1801,"ou'd expect to get more mappings) than when you specify ""ISR"". The ""A"" flag just looks at how the first 10,000 reads map and guesses the library type based on that. On thing to make sure of is that your reads aren't ""ordered"" in any way, such that you'd expect the first 10,000 to deviate in any meaningful way from the statistics of the reads of the reads. > Is it better to build assemblies with strand-aware flags? If so, does it usually make a large difference to quantification results, or a minor one? I don't know what protocol the sequencing facility used, but I am sure I could ask them. I gather from my recent reading that the extra information gained by using a stranded protocol is worthwhile, so I would expect that the sequencing facility used one, but why doesn't Trinity or MEGAHIT detect the sequecing protocol that was used? . So there are really 2 questions here. *If* the data are stranded, then yes, it's worthwhile to use stranded flags in both assembly and quantification. This is because stranded protocols will allow you to better disambiguate (a) overlapping genes and (b) reads that are ambiguous between sequence-similar genes that happen to reside on different strands. The *second* question is why Trinity or MEGAHIT wouldn't detect this. The main reason for this is that these are assembly tools. Without access to a reference genome, there is no principled way for these tools to know what the orientation of a read is _a priori_, so they generally rely on the user to specify if the reads are stranded or unstranded. > Or, if you have to specify it, why do none of the example Trinity commands I've come across include this option? It doesn't strike me as a commonly used specification in making assemblies. The Trinity command to specify the strandedness is `--SS_lib_type` (see e.g. [here](https://scilifelab.github.io/courses/ngsintro/1604/labs/rnaseqDenovo)). By default, Trinity will assume unstranded reads (as that's the safest default assumption). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/840#issuecomment-1493034427
https://github.com/COMBINE-lab/salmon/issues/841#issuecomment-1542672313:377,Availability,down,downloads,377,"If you are not building from source, then `0.10.1` is identical to `0.10.0`, there are no new features, behavior changes, or bug fixes. The entire purpose of `0.10.1` is to fix the source build on specific versions of Debian linux using system versions of dependencies. Thus, the pre-compiled versions of `0.10.0` and `0.10.1` (which are not build on a Debian system and which downloads and builds the dependencies directly) will not have any differences.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/841#issuecomment-1542672313
https://github.com/COMBINE-lab/salmon/issues/841#issuecomment-1542672313:256,Integrability,depend,dependencies,256,"If you are not building from source, then `0.10.1` is identical to `0.10.0`, there are no new features, behavior changes, or bug fixes. The entire purpose of `0.10.1` is to fix the source build on specific versions of Debian linux using system versions of dependencies. Thus, the pre-compiled versions of `0.10.0` and `0.10.1` (which are not build on a Debian system and which downloads and builds the dependencies directly) will not have any differences.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/841#issuecomment-1542672313
https://github.com/COMBINE-lab/salmon/issues/841#issuecomment-1542672313:402,Integrability,depend,dependencies,402,"If you are not building from source, then `0.10.1` is identical to `0.10.0`, there are no new features, behavior changes, or bug fixes. The entire purpose of `0.10.1` is to fix the source build on specific versions of Debian linux using system versions of dependencies. Thus, the pre-compiled versions of `0.10.0` and `0.10.1` (which are not build on a Debian system and which downloads and builds the dependencies directly) will not have any differences.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/841#issuecomment-1542672313
https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720:1190,Performance,optimiz,optimization,1190,"Hi @apredeus,. In short, what you explain in the first paragraph is right and is the expected behavior. However, to simplify the parsing algorithm (i.e. to ensure that BAM input can be parsed in bounded memory), both `salmon` and `RSEM` require that all of the alignments for a given read are adjacent within the input BAM file. If this is violated, they will be treated as different reads. In other words, if you have something like:. ```; read1:aln1; read1:aln2; read1:aln3; read2:aln1; read2:aln2; read2:aln3; ```. then in total, 2 ""reads"" worth of mass will be assigned (probabilistically across the targets). However, if you have. ```; read1:aln1; read1:aln2; read2:aln1; read1:aln3; read2:aln2; read2:aln3; ```. Then there will be *4* total reads assigned. Each time the query name (read name modulo 1/2 of a paired-end read) changes in the BAM stream, it is assumed to be a new read, and its alignments are dealt with separately. Both Bowtie2 and STAR (when projecting genomic alignments to the transcriptome) will follow this convention by default, but I'm not certain the same is true for other aligners. Again, this restriction is present in both `RSEM` and `salmon`, and it's an optimization that is made because otherwise there can be unbounded distance in the worst case between the different alignments for a read and so the parser would either have to hold all alignments in memory (which is very bad), or make many passes over the input BAM (which is also very bad) to perform quantification. Let me know if you think this may be the issue in your case. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720
https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720:1485,Performance,perform,perform,1485,"Hi @apredeus,. In short, what you explain in the first paragraph is right and is the expected behavior. However, to simplify the parsing algorithm (i.e. to ensure that BAM input can be parsed in bounded memory), both `salmon` and `RSEM` require that all of the alignments for a given read are adjacent within the input BAM file. If this is violated, they will be treated as different reads. In other words, if you have something like:. ```; read1:aln1; read1:aln2; read1:aln3; read2:aln1; read2:aln2; read2:aln3; ```. then in total, 2 ""reads"" worth of mass will be assigned (probabilistically across the targets). However, if you have. ```; read1:aln1; read1:aln2; read2:aln1; read1:aln3; read2:aln2; read2:aln3; ```. Then there will be *4* total reads assigned. Each time the query name (read name modulo 1/2 of a paired-end read) changes in the BAM stream, it is assumed to be a new read, and its alignments are dealt with separately. Both Bowtie2 and STAR (when projecting genomic alignments to the transcriptome) will follow this convention by default, but I'm not certain the same is true for other aligners. Again, this restriction is present in both `RSEM` and `salmon`, and it's an optimization that is made because otherwise there can be unbounded distance in the worst case between the different alignments for a read and so the parser would either have to hold all alignments in memory (which is very bad), or make many passes over the input BAM (which is also very bad) to perform quantification. Let me know if you think this may be the issue in your case. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720
https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720:116,Usability,simpl,simplify,116,"Hi @apredeus,. In short, what you explain in the first paragraph is right and is the expected behavior. However, to simplify the parsing algorithm (i.e. to ensure that BAM input can be parsed in bounded memory), both `salmon` and `RSEM` require that all of the alignments for a given read are adjacent within the input BAM file. If this is violated, they will be treated as different reads. In other words, if you have something like:. ```; read1:aln1; read1:aln2; read1:aln3; read2:aln1; read2:aln2; read2:aln3; ```. then in total, 2 ""reads"" worth of mass will be assigned (probabilistically across the targets). However, if you have. ```; read1:aln1; read1:aln2; read2:aln1; read1:aln3; read2:aln2; read2:aln3; ```. Then there will be *4* total reads assigned. Each time the query name (read name modulo 1/2 of a paired-end read) changes in the BAM stream, it is assumed to be a new read, and its alignments are dealt with separately. Both Bowtie2 and STAR (when projecting genomic alignments to the transcriptome) will follow this convention by default, but I'm not certain the same is true for other aligners. Again, this restriction is present in both `RSEM` and `salmon`, and it's an optimization that is made because otherwise there can be unbounded distance in the worst case between the different alignments for a read and so the parser would either have to hold all alignments in memory (which is very bad), or make many passes over the input BAM (which is also very bad) to perform quantification. Let me know if you think this may be the issue in your case. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720
https://github.com/COMBINE-lab/salmon/issues/845#issuecomment-1570842745:16,Availability,error,error,16,"I had a similar error, but was able to fix it by adding the line:; export LD_LIBRARY_PATH=/path/to/lib:$LD_LIBRARY_PATH ; to my .bashrc file, so maybe that's something to try if you haven't already. (There's a similar issue described here: #219).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/845#issuecomment-1570842745
https://github.com/COMBINE-lab/salmon/issues/847#issuecomment-1533935566:119,Availability,fault,fault,119,"There is nothing obvious about the command you provided that looks incorrect. Can you please check if the segmentation fault still occurs with the most recent release of salmon (either 1.10 or 1.10.1)?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/847#issuecomment-1533935566
https://github.com/COMBINE-lab/salmon/issues/847#issuecomment-1533935566:159,Deployability,release,release,159,"There is nothing obvious about the command you provided that looks incorrect. Can you please check if the segmentation fault still occurs with the most recent release of salmon (either 1.10 or 1.10.1)?. Thanks,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/847#issuecomment-1533935566
https://github.com/COMBINE-lab/salmon/issues/850#issuecomment-2052164298:567,Usability,guid,guidance,567,"Hello @Starahoush, greetings from Brazil! I'm an undergraduate student in Biomedical Informatics at the Federal University of Paraná, currently involved in a scientific initiation project in an immunology lab. I'm working extensively with FASTQ files from samples sequenced on the BD Rhapsody V1 platform and I've been facing a challenge: a significant portion of the reads are being discarded due to ""noisy cellular barcodes"", with around 50% of the reads affected. Could you please share if you've encountered a similar situation in your experiment or provide some guidance on how to address this issue? I appreciate your attention and assistance!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/850#issuecomment-2052164298
https://github.com/COMBINE-lab/salmon/issues/851#issuecomment-1563024409:157,Security,validat,validateMappings,157,"Hi @KAffoh,. Please let me know if this is resolved. If not, you can try running:. ```; salmon --no-version-check quant -i salmon_index -l A -1 $FW -2 $RV --validateMappings -o /Volumes/Ultra_Touch/malaria/Salmon/$FILEBASE/; ```. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/851#issuecomment-1563024409
https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394:163,Availability,error,errors,163,"Hi @SSaleem94,. As the message suggests, your command is missing the required `--output` argument. Of course, it seems your command includes `-o`. The rest of the errors suggest that the command line is not being properly parsed. It looks like the part after the first line break is not being interpreted as a continuation of the same line. I think this is because the line extension character in the shell is not `/`, but is `\`. Maybe try the following:; ; ```; F=$(cat file_names.txt); for i in ${F}; do; F1=../processed_fastq/${i}_R1_001_val_1.fastq.gz; F2=../processed_fastq/${i}_R2_001_val_2.fastq.gz; echo ""performing salmon quant on ${i}""; salmon quant -i gencode_v43_index -l A -1 ${F1} -2 ${F2} -p 64 \; --validateMappings --writeUnmappedNames -o ${i}; echo ""finish quantifying ${i}""; done; ```. **Also**, as is suggested by the `salmon` message itself, you may want to consider upgrading to the latest version of `salmon`. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394
https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394:608,Availability,echo,echo,608,"Hi @SSaleem94,. As the message suggests, your command is missing the required `--output` argument. Of course, it seems your command includes `-o`. The rest of the errors suggest that the command line is not being properly parsed. It looks like the part after the first line break is not being interpreted as a continuation of the same line. I think this is because the line extension character in the shell is not `/`, but is `\`. Maybe try the following:; ; ```; F=$(cat file_names.txt); for i in ${F}; do; F1=../processed_fastq/${i}_R1_001_val_1.fastq.gz; F2=../processed_fastq/${i}_R2_001_val_2.fastq.gz; echo ""performing salmon quant on ${i}""; salmon quant -i gencode_v43_index -l A -1 ${F1} -2 ${F2} -p 64 \; --validateMappings --writeUnmappedNames -o ${i}; echo ""finish quantifying ${i}""; done; ```. **Also**, as is suggested by the `salmon` message itself, you may want to consider upgrading to the latest version of `salmon`. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394
https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394:763,Availability,echo,echo,763,"Hi @SSaleem94,. As the message suggests, your command is missing the required `--output` argument. Of course, it seems your command includes `-o`. The rest of the errors suggest that the command line is not being properly parsed. It looks like the part after the first line break is not being interpreted as a continuation of the same line. I think this is because the line extension character in the shell is not `/`, but is `\`. Maybe try the following:; ; ```; F=$(cat file_names.txt); for i in ${F}; do; F1=../processed_fastq/${i}_R1_001_val_1.fastq.gz; F2=../processed_fastq/${i}_R2_001_val_2.fastq.gz; echo ""performing salmon quant on ${i}""; salmon quant -i gencode_v43_index -l A -1 ${F1} -2 ${F2} -p 64 \; --validateMappings --writeUnmappedNames -o ${i}; echo ""finish quantifying ${i}""; done; ```. **Also**, as is suggested by the `salmon` message itself, you may want to consider upgrading to the latest version of `salmon`. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394
https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394:23,Integrability,message,message,23,"Hi @SSaleem94,. As the message suggests, your command is missing the required `--output` argument. Of course, it seems your command includes `-o`. The rest of the errors suggest that the command line is not being properly parsed. It looks like the part after the first line break is not being interpreted as a continuation of the same line. I think this is because the line extension character in the shell is not `/`, but is `\`. Maybe try the following:; ; ```; F=$(cat file_names.txt); for i in ${F}; do; F1=../processed_fastq/${i}_R1_001_val_1.fastq.gz; F2=../processed_fastq/${i}_R2_001_val_2.fastq.gz; echo ""performing salmon quant on ${i}""; salmon quant -i gencode_v43_index -l A -1 ${F1} -2 ${F2} -p 64 \; --validateMappings --writeUnmappedNames -o ${i}; echo ""finish quantifying ${i}""; done; ```. **Also**, as is suggested by the `salmon` message itself, you may want to consider upgrading to the latest version of `salmon`. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394
https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394:848,Integrability,message,message,848,"Hi @SSaleem94,. As the message suggests, your command is missing the required `--output` argument. Of course, it seems your command includes `-o`. The rest of the errors suggest that the command line is not being properly parsed. It looks like the part after the first line break is not being interpreted as a continuation of the same line. I think this is because the line extension character in the shell is not `/`, but is `\`. Maybe try the following:; ; ```; F=$(cat file_names.txt); for i in ${F}; do; F1=../processed_fastq/${i}_R1_001_val_1.fastq.gz; F2=../processed_fastq/${i}_R2_001_val_2.fastq.gz; echo ""performing salmon quant on ${i}""; salmon quant -i gencode_v43_index -l A -1 ${F1} -2 ${F2} -p 64 \; --validateMappings --writeUnmappedNames -o ${i}; echo ""finish quantifying ${i}""; done; ```. **Also**, as is suggested by the `salmon` message itself, you may want to consider upgrading to the latest version of `salmon`. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394
https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394:614,Performance,perform,performing,614,"Hi @SSaleem94,. As the message suggests, your command is missing the required `--output` argument. Of course, it seems your command includes `-o`. The rest of the errors suggest that the command line is not being properly parsed. It looks like the part after the first line break is not being interpreted as a continuation of the same line. I think this is because the line extension character in the shell is not `/`, but is `\`. Maybe try the following:; ; ```; F=$(cat file_names.txt); for i in ${F}; do; F1=../processed_fastq/${i}_R1_001_val_1.fastq.gz; F2=../processed_fastq/${i}_R2_001_val_2.fastq.gz; echo ""performing salmon quant on ${i}""; salmon quant -i gencode_v43_index -l A -1 ${F1} -2 ${F2} -p 64 \; --validateMappings --writeUnmappedNames -o ${i}; echo ""finish quantifying ${i}""; done; ```. **Also**, as is suggested by the `salmon` message itself, you may want to consider upgrading to the latest version of `salmon`. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394
https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394:716,Security,validat,validateMappings,716,"Hi @SSaleem94,. As the message suggests, your command is missing the required `--output` argument. Of course, it seems your command includes `-o`. The rest of the errors suggest that the command line is not being properly parsed. It looks like the part after the first line break is not being interpreted as a continuation of the same line. I think this is because the line extension character in the shell is not `/`, but is `\`. Maybe try the following:; ; ```; F=$(cat file_names.txt); for i in ${F}; do; F1=../processed_fastq/${i}_R1_001_val_1.fastq.gz; F2=../processed_fastq/${i}_R2_001_val_2.fastq.gz; echo ""performing salmon quant on ${i}""; salmon quant -i gencode_v43_index -l A -1 ${F1} -2 ${F2} -p 64 \; --validateMappings --writeUnmappedNames -o ${i}; echo ""finish quantifying ${i}""; done; ```. **Also**, as is suggested by the `salmon` message itself, you may want to consider upgrading to the latest version of `salmon`. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/854#issuecomment-1599169394
https://github.com/COMBINE-lab/salmon/issues/855#issuecomment-1604792956:237,Deployability,install,installed,237,"Hi @ulin27,. This isn't a `salmon`-related issue. The script is trying to run an instance of R, and within R to load the `edgeR` package to perform some normalization. It looks like the location you are running this doesn't have `edgeR` installed. I would check in with the people who installed `trinity` on your computer or, if that was you, ask upstream in the `trinity` user group. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/855#issuecomment-1604792956
https://github.com/COMBINE-lab/salmon/issues/855#issuecomment-1604792956:285,Deployability,install,installed,285,"Hi @ulin27,. This isn't a `salmon`-related issue. The script is trying to run an instance of R, and within R to load the `edgeR` package to perform some normalization. It looks like the location you are running this doesn't have `edgeR` installed. I would check in with the people who installed `trinity` on your computer or, if that was you, ask upstream in the `trinity` user group. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/855#issuecomment-1604792956
https://github.com/COMBINE-lab/salmon/issues/855#issuecomment-1604792956:112,Performance,load,load,112,"Hi @ulin27,. This isn't a `salmon`-related issue. The script is trying to run an instance of R, and within R to load the `edgeR` package to perform some normalization. It looks like the location you are running this doesn't have `edgeR` installed. I would check in with the people who installed `trinity` on your computer or, if that was you, ask upstream in the `trinity` user group. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/855#issuecomment-1604792956
https://github.com/COMBINE-lab/salmon/issues/855#issuecomment-1604792956:140,Performance,perform,perform,140,"Hi @ulin27,. This isn't a `salmon`-related issue. The script is trying to run an instance of R, and within R to load the `edgeR` package to perform some normalization. It looks like the location you are running this doesn't have `edgeR` installed. I would check in with the people who installed `trinity` on your computer or, if that was you, ask upstream in the `trinity` user group. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/855#issuecomment-1604792956
https://github.com/COMBINE-lab/salmon/pull/856#issuecomment-1607761575:219,Deployability,patch,patch,219,"Thanks @A-N-Other! I changed the base branch to develop, as we generally pull everything through that branch before it makes it to master. Otherwise, these changes look good. Hopefully we'll get around to pushing out a patch release with this change soon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/856#issuecomment-1607761575
https://github.com/COMBINE-lab/salmon/pull/856#issuecomment-1607761575:225,Deployability,release,release,225,"Thanks @A-N-Other! I changed the base branch to develop, as we generally pull everything through that branch before it makes it to master. Otherwise, these changes look good. Hopefully we'll get around to pushing out a patch release with this change soon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/856#issuecomment-1607761575
https://github.com/COMBINE-lab/salmon/pull/856#issuecomment-1607764883:30,Deployability,patch,patching,30,"Happy to help! Have just been patching for the `salmon` package in`spack`, so figured I'd do a PR here as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/856#issuecomment-1607764883
https://github.com/COMBINE-lab/salmon/issues/859#issuecomment-1609970132:85,Deployability,install,installed,85,"Hi @SSaleem94,. This is an issue upstream in the `SUPPA` tool (or the way that it is installed). This is not an issue with `salmon`. I would recommend posting this issue to the `SUPPA` GitHub repository. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/859#issuecomment-1609970132
https://github.com/COMBINE-lab/salmon/issues/860#issuecomment-1620414936:119,Modifiability,config,configure,119,The GCC compiler has a long standing bug. You should clean the build directory and pass `-DNO_IPO=TRUE` to the `cmake` configure step. This will disable LTO and allow the compilation to complete.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/860#issuecomment-1620414936
https://github.com/COMBINE-lab/salmon/issues/861#issuecomment-1642782444:99,Integrability,protocol,protocol,99,"Hi @jeremymsimon,. I think this is something that we do want to support. Are you familiar with the protocol and what types of changes / additions might need to be made in order to support processing this type of data?. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/861#issuecomment-1642782444
https://github.com/COMBINE-lab/salmon/issues/861#issuecomment-1642867731:401,Testability,test,testing,401,"Hey @rob-p ,; I know virtually nothing about this, but I've been having somewhat related discussions lately with @mourisl about their method TRUST4 (https://github.com/liulab-dfci/TRUST4). Li seemingly has interest in helping out with this, and coincidentally is the developer of chromap too in case support for ATACseq is also coming soon!. I'll let you both take it from here and looking forward to testing!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/861#issuecomment-1642867731
https://github.com/COMBINE-lab/salmon/issues/862#issuecomment-1653566342:14,Deployability,update,update,14,I just had to update Salmon to the last. Solved,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/862#issuecomment-1653566342
https://github.com/COMBINE-lab/salmon/issues/863#issuecomment-2241619736:770,Integrability,Message,Message,770,"Yes, but it was a while ago. I think I had realised that I aligned to the; genome (Step 2 in my original post), when Salmon documentation specifically; says you should align to the transcriptome. So I redid the alignment and it; worked, if I'm remembering correctly. On Thu, 18 Jul 2024, 16:41 YIGUIz, ***@***.***> wrote:. > Hi, I encountered the same issue. Have you managed to solve it?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/863#issuecomment-2236553993>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AOFX65IGY6W3PKGRNNK7WOLZM7AY5AVCNFSM6AAAAABLCWZAB2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDEMZWGU2TGOJZGM>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/863#issuecomment-2241619736
https://github.com/COMBINE-lab/salmon/issues/864#issuecomment-1660467862:180,Availability,avail,available,180,"Hi @najibveto,. I do not have access to a windows machine, unfortunately, so I can not test this directly. It would seem that somehow the appropriate version of `libstdc++` is not available or is not being found? I would recommend to raise this issue over on the [`bioconda` repository](https://github.com/bioconda/bioconda-recipes/issues) or in their [gitter channel](https://app.gitter.im/#/room/#bioconda_Lobby:gitter.im). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/864#issuecomment-1660467862
https://github.com/COMBINE-lab/salmon/issues/864#issuecomment-1660467862:30,Security,access,access,30,"Hi @najibveto,. I do not have access to a windows machine, unfortunately, so I can not test this directly. It would seem that somehow the appropriate version of `libstdc++` is not available or is not being found? I would recommend to raise this issue over on the [`bioconda` repository](https://github.com/bioconda/bioconda-recipes/issues) or in their [gitter channel](https://app.gitter.im/#/room/#bioconda_Lobby:gitter.im). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/864#issuecomment-1660467862
https://github.com/COMBINE-lab/salmon/issues/864#issuecomment-1660467862:87,Testability,test,test,87,"Hi @najibveto,. I do not have access to a windows machine, unfortunately, so I can not test this directly. It would seem that somehow the appropriate version of `libstdc++` is not available or is not being found? I would recommend to raise this issue over on the [`bioconda` repository](https://github.com/bioconda/bioconda-recipes/issues) or in their [gitter channel](https://app.gitter.im/#/room/#bioconda_Lobby:gitter.im). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/864#issuecomment-1660467862
https://github.com/COMBINE-lab/salmon/issues/865#issuecomment-1664415328:145,Integrability,protocol,protocols,145,"Hi @uveksamoosmeh,. Can you explain a bit your usecase? The `--features` option is designed for indexing short features for specific single-cell protocols (e.g. HTO tags etc.). If you are trying to index a standard transcriptome, this is not the purpose of this option. Rather, for that you should pass the transcriptome `FASTA` file as the input to the regular `salmon index` command. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/865#issuecomment-1664415328
https://github.com/COMBINE-lab/salmon/issues/866#issuecomment-1664067688:164,Integrability,depend,depend,164,"Hi @pabloaledo,. Thanks for the report! I am not familiar with spack. However, the bigget issue here is that I don't believe that salmon, by itself, should require/depend upon libhts. It uses libstaden for its sam/bam parsing, but that should be a self-contained dependency. We should figure out why libhts is being pulled in here. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/866#issuecomment-1664067688
https://github.com/COMBINE-lab/salmon/issues/866#issuecomment-1664067688:263,Integrability,depend,dependency,263,"Hi @pabloaledo,. Thanks for the report! I am not familiar with spack. However, the bigget issue here is that I don't believe that salmon, by itself, should require/depend upon libhts. It uses libstaden for its sam/bam parsing, but that should be a self-contained dependency. We should figure out why libhts is being pulled in here. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/866#issuecomment-1664067688
https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1694754832:254,Availability,error,error,254,"Hi @Alecrim24,. It looks like the list of r1 files are being interpreted as a single, long, filename. Same with the list of r2 files. Any idea why that's the case? They should be a space-separated list (of course, there *are* a ton of them here, but the error clearly suggests they are being interpreted as a single, long, filename). --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1694754832
https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1694754832:260,Usability,clear,clearly,260,"Hi @Alecrim24,. It looks like the list of r1 files are being interpreted as a single, long, filename. Same with the list of r2 files. Any idea why that's the case? They should be a space-separated list (of course, there *are* a ton of them here, but the error clearly suggests they are being interpreted as a single, long, filename). --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1694754832
https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1695989396:199,Performance,load,load,199,"Hi rob. Ignore my last email. I did it again using this script. #!/bin/bash; #SBATCH -p shared; #SBATCH -c 100; #SBATCH --mem=200G; #SBATCH --gres=tmp:300G; #SBATCH -t 36:00:00. module purge; module load bioinformatics; module load salmon/1.10.1. cd /nobackup/qkdf72/Trinity/Reads/All-H.m-reads/Trimmed/. # Set the path to the Salmon index; salmon_index=""/nobackup/qkdf72/Trinity/Reads/All-H.m-reads/Trimmed/salmon_index"". # Set the path to the directory containing all the FASTQ files; fastq_dir=""/nobackup/qkdf72/Trinity/Reads/All-H.m-reads/Trimmed"". # Create an array of left and right read files; left_files=(; P1_H.m_1_221020_L002_R1.fastq.gz P1_H.m_21-29_221020_L002_R1.fastq.gz P2-4-10_221020_L002_R1.fastq.gz P2-6-12_221020_L002_R1.fastq.gz P3_40-48_221020_L002_R1.fastq.gz; P1_H.m_15-23_221020_L002_R1.fastq.gz P1_H.m_24-32_221020_L002_R1.fastq.gz P2-44-51_221020_L002_R1.fastq.gz P3_36-44_221020_L002_R1.fastq.gz P3_41-49_221020_L002_R1.fastq.gz; P1_H.m_16-24_221020_L002_R1.fastq.gz P1_H.m_26-34_221020_L002_R1.fastq.gz P2-45-54_221020_L002_R1.fastq.gz P3_37-46_221020_L002_R1.fastq.gz P3_42-50_221020_L002_R1.fastq.gz; P1_H.m_18-26_221020_L002_R1.fastq.gz P2-10-17_221020_L002_R1.fastq.gz P2-46-53_221020_L002_R1.fastq.gz P3_38-45_221020_L002_R1.fastq.gz P3_43-52_221020_L002_R1.fastq.gz; P1_H.m_19-27_221020_L002_R1.fastq.gz P2-11-18_221020_L002_R1.fastq.gz P2-5-11_221020_L002_R1.fastq.gz P3_39-47_221020_L002_R1.fastq.gz; ). right_files=(; P1_H.m_1_221020_L002_R2.fastq.gz P1_H.m_21-29_221020_L002_R2.fastq.gz P2-4-10_221020_L002_R2.fastq.gz P2-6-12_221020_L002_R2.fastq.gz P3_40-48_221020_L002_R2.fastq.gz; P1_H.m_15-23_221020_L002_R2.fastq.gz P1_H.m_24-32_221020_L002_R2.fastq.gz P2-44-51_221020_L002_R2.fastq.gz P3_36-44_221020_L002_R2.fastq.gz P3_41-49_221020_L002_R2.fastq.gz; P1_H.m_16-24_221020_L002_R2.fastq.gz P1_H.m_26-34_221020_L002_R2.fastq.gz P2-45-54_221020_L002_R2.fastq.gz P3_37-46_221020_L002_R2.fastq.gz P3_42-50_221020_L002_R2.fastq.gz; P1_H.m_18-26_221020_L002_R2.fa",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1695989396
https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1695989396:227,Performance,load,load,227,"Hi rob. Ignore my last email. I did it again using this script. #!/bin/bash; #SBATCH -p shared; #SBATCH -c 100; #SBATCH --mem=200G; #SBATCH --gres=tmp:300G; #SBATCH -t 36:00:00. module purge; module load bioinformatics; module load salmon/1.10.1. cd /nobackup/qkdf72/Trinity/Reads/All-H.m-reads/Trimmed/. # Set the path to the Salmon index; salmon_index=""/nobackup/qkdf72/Trinity/Reads/All-H.m-reads/Trimmed/salmon_index"". # Set the path to the directory containing all the FASTQ files; fastq_dir=""/nobackup/qkdf72/Trinity/Reads/All-H.m-reads/Trimmed"". # Create an array of left and right read files; left_files=(; P1_H.m_1_221020_L002_R1.fastq.gz P1_H.m_21-29_221020_L002_R1.fastq.gz P2-4-10_221020_L002_R1.fastq.gz P2-6-12_221020_L002_R1.fastq.gz P3_40-48_221020_L002_R1.fastq.gz; P1_H.m_15-23_221020_L002_R1.fastq.gz P1_H.m_24-32_221020_L002_R1.fastq.gz P2-44-51_221020_L002_R1.fastq.gz P3_36-44_221020_L002_R1.fastq.gz P3_41-49_221020_L002_R1.fastq.gz; P1_H.m_16-24_221020_L002_R1.fastq.gz P1_H.m_26-34_221020_L002_R1.fastq.gz P2-45-54_221020_L002_R1.fastq.gz P3_37-46_221020_L002_R1.fastq.gz P3_42-50_221020_L002_R1.fastq.gz; P1_H.m_18-26_221020_L002_R1.fastq.gz P2-10-17_221020_L002_R1.fastq.gz P2-46-53_221020_L002_R1.fastq.gz P3_38-45_221020_L002_R1.fastq.gz P3_43-52_221020_L002_R1.fastq.gz; P1_H.m_19-27_221020_L002_R1.fastq.gz P2-11-18_221020_L002_R1.fastq.gz P2-5-11_221020_L002_R1.fastq.gz P3_39-47_221020_L002_R1.fastq.gz; ). right_files=(; P1_H.m_1_221020_L002_R2.fastq.gz P1_H.m_21-29_221020_L002_R2.fastq.gz P2-4-10_221020_L002_R2.fastq.gz P2-6-12_221020_L002_R2.fastq.gz P3_40-48_221020_L002_R2.fastq.gz; P1_H.m_15-23_221020_L002_R2.fastq.gz P1_H.m_24-32_221020_L002_R2.fastq.gz P2-44-51_221020_L002_R2.fastq.gz P3_36-44_221020_L002_R2.fastq.gz P3_41-49_221020_L002_R2.fastq.gz; P1_H.m_16-24_221020_L002_R2.fastq.gz P1_H.m_26-34_221020_L002_R2.fastq.gz P2-45-54_221020_L002_R2.fastq.gz P3_37-46_221020_L002_R2.fastq.gz P3_42-50_221020_L002_R2.fastq.gz; P1_H.m_18-26_221020_L002_R2.fa",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1695989396
https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1695989396:2651,Security,validat,validateMappings,2651,"002_R2.fastq.gz; P1_H.m_15-23_221020_L002_R2.fastq.gz P1_H.m_24-32_221020_L002_R2.fastq.gz P2-44-51_221020_L002_R2.fastq.gz P3_36-44_221020_L002_R2.fastq.gz P3_41-49_221020_L002_R2.fastq.gz; P1_H.m_16-24_221020_L002_R2.fastq.gz P1_H.m_26-34_221020_L002_R2.fastq.gz P2-45-54_221020_L002_R2.fastq.gz P3_37-46_221020_L002_R2.fastq.gz P3_42-50_221020_L002_R2.fastq.gz; P1_H.m_18-26_221020_L002_R2.fastq.gz P2-10-17_221020_L002_R2.fastq.gz P2-46-53_221020_L002_R2.fastq.gz P3_38-45_221020_L002_R2.fastq.gz P3_43-52_221020_L002_R2.fastq.gz; P1_H.m_19-27_221020_L002_R2.fastq.gz P2-11-18_221020_L002_R2.fastq.gz P2-5-11_221020_L002_R2.fastq.gz P3_39-47_221020_L002_R2.fastq.gz. ). # Loop through the read files and run Salmon quant; for i in ""${!left_files[@]}""; do; left_file=""${left_files[i]}""; right_file=""${right_files[i]}"". # Extract the sample name; sample=$(basename ""$left_file"" ""_L002_R1.fastq.gz""). # Run Salmon quant with the current read files; salmon quant -i ""$salmon_index"" -l IU -1 ""$fastq_dir/$left_file"" -2 ""$fastq_dir/$right_file"" --validateMappings -o ""salmon_out/${sample}_quant""; done. and the results of the first 2 look like this. Name Length EffectiveLength TPM NumReads; TRINITY_DN1448606_c0_g1_i1 472 275.399 0.000000 0.000; TRINITY_DN1448584_c0_g1_i1 394 201.561 0.000000 0.000; TRINITY_DN1448585_c0_g1_i2 237 72.382 0.000000 0.000; TRINITY_DN1448598_c0_g1_i1 227 65.738 0.000000 0.000; TRINITY_DN1448598_c1_g1_i1 254 84.301 0.000000 0.000; TRINITY_DN1448554_c0_g1_i1 349 160.724 0.000000 0.000; TRINITY_DN1448554_c1_g1_i1 247 79.278 0.000000 0.000; TRINITY_DN1448554_c2_g1_i1 242 75.824 0.000000 0.000; TRINITY_DN1448616_c0_g1_i1 313 129.689 0.000000 0.000; [qkdf72@login2.ham8 P1_H.m_1_221020_quant]$ cd ..; [qkdf72@login2.ham8 salmon_out]$ cd P1_H.m_21-29_221020_quant/; [qkdf72@login2.ham8 P1_H.m_21-29_221020_quant]$ head quant.sf; Name Length EffectiveLength TPM NumReads; TRINITY_DN1448606_c0_g1_i1 472 298.999 0.000000 0.000; TRINITY_DN1448584_c0_g1_i1 394 222.370 0.00000",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1695989396
https://github.com/COMBINE-lab/salmon/issues/871#issuecomment-1706854563:36,Deployability,install,installing,36,I seem to have a general issue with installing packages through bioconda. I will close this issue and move it to the conda GitHub.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/871#issuecomment-1706854563
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:340,Availability,ping,ping,340,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:526,Integrability,wrap,wrapper,526,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:597,Integrability,interface,interfaced,597,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:131,Usability,simpl,simply,131,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:516,Usability,simpl,simpleaf,516,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:612,Usability,simpl,simpleaf,612,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:713,Usability,user experience,user experience,713,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732630683:241,Availability,down,downstream,241,"Thanks Rob. The sublib BC seems to be 6nt for a total of 30nt so good on that front. If @DongzeHE or @k3yavi know the chemistry already that would be great, if not I can try to figure it out by inspecting the reads. ; I'm using `alevin-fry` downstream but I thought this step (barcode extraction) had to happen via `salmon alevin`?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732630683
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331:207,Integrability,wrap,wrapper,207,"Hi @davidaknowles,. Indeed — the barcode extraction will happen either in `salmon alevin` or, if you are using the new `piscem` module for mapping prior to quantification (both are exposed in the `simpleaf` wrapper tool to simplify single-cell processing with `alevin-fry`), then it will happen there. We have a new very general and much more capable module in the works that will be able to handle all manners of single-cell geometry, but nothing about the Parse library seems beyond the capabilities of the current geometry processing code. If you share some reads, we can also try and take a look and figure out where the last BC resides. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331:181,Security,expose,exposed,181,"Hi @davidaknowles,. Indeed — the barcode extraction will happen either in `salmon alevin` or, if you are using the new `piscem` module for mapping prior to quantification (both are exposed in the `simpleaf` wrapper tool to simplify single-cell processing with `alevin-fry`), then it will happen there. We have a new very general and much more capable module in the works that will be able to handle all manners of single-cell geometry, but nothing about the Parse library seems beyond the capabilities of the current geometry processing code. If you share some reads, we can also try and take a look and figure out where the last BC resides. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331:197,Usability,simpl,simpleaf,197,"Hi @davidaknowles,. Indeed — the barcode extraction will happen either in `salmon alevin` or, if you are using the new `piscem` module for mapping prior to quantification (both are exposed in the `simpleaf` wrapper tool to simplify single-cell processing with `alevin-fry`), then it will happen there. We have a new very general and much more capable module in the works that will be able to handle all manners of single-cell geometry, but nothing about the Parse library seems beyond the capabilities of the current geometry processing code. If you share some reads, we can also try and take a look and figure out where the last BC resides. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331:223,Usability,simpl,simplify,223,"Hi @davidaknowles,. Indeed — the barcode extraction will happen either in `salmon alevin` or, if you are using the new `piscem` module for mapping prior to quantification (both are exposed in the `simpleaf` wrapper tool to simplify single-cell processing with `alevin-fry`), then it will happen there. We have a new very general and much more capable module in the works that will be able to handle all manners of single-cell geometry, but nothing about the Parse library seems beyond the capabilities of the current geometry processing code. If you share some reads, we can also try and take a look and figure out where the last BC resides. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331
https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739843386:102,Deployability,release,release,102,"Thanks for reporting this @alexdhill. There was a bug addressed in version 1.10 (the first bug in the release notes [here](https://github.com/COMBINE-lab/salmon/releases/tag/v1.10.0)) that could be related to this. If you *do* encounter this in any samples under 1.10, please let us know. In which case, keeping track of the offending sample might be the most useful way to try and dig into it further. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739843386
https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739843386:161,Deployability,release,releases,161,"Thanks for reporting this @alexdhill. There was a bug addressed in version 1.10 (the first bug in the release notes [here](https://github.com/COMBINE-lab/salmon/releases/tag/v1.10.0)) that could be related to this. If you *do* encounter this in any samples under 1.10, please let us know. In which case, keeping track of the offending sample might be the most useful way to try and dig into it further. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739843386
https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739857661:28,Deployability,release,release,28,"Good to hear, I checked the release log but I wasn't able to confirm whether I was using the bugged conda build since we are using docker biocontainers (build v1.9.0--h7e5ed60_1). I'll upgrade our pipeline and close the issue after a new run of the same data if the problem seems to be resolved. Best,; Alex",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739857661
https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739857661:185,Deployability,upgrade,upgrade,185,"Good to hear, I checked the release log but I wasn't able to confirm whether I was using the bugged conda build since we are using docker biocontainers (build v1.9.0--h7e5ed60_1). I'll upgrade our pipeline and close the issue after a new run of the same data if the problem seems to be resolved. Best,; Alex",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739857661
https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739857661:197,Deployability,pipeline,pipeline,197,"Good to hear, I checked the release log but I wasn't able to confirm whether I was using the bugged conda build since we are using docker biocontainers (build v1.9.0--h7e5ed60_1). I'll upgrade our pipeline and close the issue after a new run of the same data if the problem seems to be resolved. Best,; Alex",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739857661
https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739857661:36,Testability,log,log,36,"Good to hear, I checked the release log but I wasn't able to confirm whether I was using the bugged conda build since we are using docker biocontainers (build v1.9.0--h7e5ed60_1). I'll upgrade our pipeline and close the issue after a new run of the same data if the problem seems to be resolved. Best,; Alex",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/876#issuecomment-1739857661
https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1743291304:772,Deployability,update,update,772,"Hi @Vida0,. Thanks for pointing this out. The short answer is that the `--eqclasses` option (which is different from the `--dumpEq` option) is designed, as you suggest, to allow using a pre-computed set of equivalence classes for quantification (thus bypassing the need to read in and parse alignments from a BAM file, or to read in and selectively-align reads from a FASTQ file). This, of course, requires the equivalence classes to have already been constructed and dumped in the proper format (see the `--dumpEq` option mentioned above). Finally, I'll mention that with a quick search through the [documentation at ReadTheDocs](https://salmon.readthedocs.io/en/latest/salmon.html), I couldn't find our current description of the `--eqclasses` option (which I'd like to update). Could you point me at where you came across it?. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1743291304
https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1744787632:937,Availability,error,error,937,"Hi @rob-p, ; Thank you for getting back to me so quickly and thank you for the explanation. ; I found this info about the `--eqclasses ` parameter in the Salmon quant help manual for alignment-based mode:; <img width=""626"" alt=""Screenshot 2023-10-03 at 11 40 56"" src=""https://github.com/COMBINE-lab/salmon/assets/76558077/dc98c406-759f-4543-8cdd-5299d24775eb"">; Everything else I tried was a guess.; I wasn’t sure how to get the right file, so I thought it might be the **eq_classes.txt.gz** file, made using the `--dumpEq` option. I read about [—dumpEq option ](https://salmon.readthedocs.io/en/latest/salmon.html#dumpeq) and I found some explanation of [equivalence class file](https://salmon.readthedocs.io/en/latest/file_formats.html#equivalence-class-file). So, I made this file using the `--dumpEq` option and then used it with `--eqclasses` option, but I made mistake by also providing a BAM file with `-a` option. When I got the error, i thought maybe this file was supposed to be used instead of the alignment BAM file, not together with, and now i understand.; I'm still not clear: is this file only used when I want to analyze the same sample multiple times, with different options?; Also, I noticed the `--eqclasses` parameter isn’t in the help manual for salmon quant in mapping-based mode because it’s not there when I run `salmon quant --help-reads`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1744787632
https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1744787632:1085,Usability,clear,clear,1085,"Hi @rob-p, ; Thank you for getting back to me so quickly and thank you for the explanation. ; I found this info about the `--eqclasses ` parameter in the Salmon quant help manual for alignment-based mode:; <img width=""626"" alt=""Screenshot 2023-10-03 at 11 40 56"" src=""https://github.com/COMBINE-lab/salmon/assets/76558077/dc98c406-759f-4543-8cdd-5299d24775eb"">; Everything else I tried was a guess.; I wasn’t sure how to get the right file, so I thought it might be the **eq_classes.txt.gz** file, made using the `--dumpEq` option. I read about [—dumpEq option ](https://salmon.readthedocs.io/en/latest/salmon.html#dumpeq) and I found some explanation of [equivalence class file](https://salmon.readthedocs.io/en/latest/file_formats.html#equivalence-class-file). So, I made this file using the `--dumpEq` option and then used it with `--eqclasses` option, but I made mistake by also providing a BAM file with `-a` option. When I got the error, i thought maybe this file was supposed to be used instead of the alignment BAM file, not together with, and now i understand.; I'm still not clear: is this file only used when I want to analyze the same sample multiple times, with different options?; Also, I noticed the `--eqclasses` parameter isn’t in the help manual for salmon quant in mapping-based mode because it’s not there when I run `salmon quant --help-reads`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1744787632
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1753070571:0,Availability,Ping,Pinging,0,"Pinging @k3yavi / @DongzeHE here in case they have an idea of what would be used for this protocol. If we know the fragment geometry, I imagine we could just use the custom geometry flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1753070571
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1753070571:90,Integrability,protocol,protocol,90,"Pinging @k3yavi / @DongzeHE here in case they have an idea of what would be used for this protocol. If we know the fragment geometry, I imagine we could just use the custom geometry flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1753070571
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1753075151:90,Integrability,protocol,protocol-specific-notes,90,"We have that here, right? https://salmon.readthedocs.io/en/latest/alevin.html#single-cell-protocol-specific-notes",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1753075151
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139:891,Integrability,wrap,wrapper,891,"Hi @mugpeng,. This is because it is covered by the custom geometry specification (as laid out in the docs). I agree it's nice to have a specific flag for each geometry, rather than to have to e.g. specify the custom geometry each time. We are working on good solutions to that at a higher level (e.g. in our `simpleaf` tool where users can register their own custom geometry specifications and refer to them by name). However, in `salmon`/`alevin` right now, the named geometries are hard-coded, and so to have a specific `--indropV2` flag, that would have to be added to the argument parser and then mapped to the specific underlying geometry in the code. This isn't hard, but as the number of different chemistries proliferates, it's not ultimately a scalable solution. So, the current recommendation would be to use the custom geometry flags as specified in the documentation, or adopt a wrapper like `simpleaf` and add `indropV2` to your custom geometry specification library. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139:753,Performance,scalab,scalable,753,"Hi @mugpeng,. This is because it is covered by the custom geometry specification (as laid out in the docs). I agree it's nice to have a specific flag for each geometry, rather than to have to e.g. specify the custom geometry each time. We are working on good solutions to that at a higher level (e.g. in our `simpleaf` tool where users can register their own custom geometry specifications and refer to them by name). However, in `salmon`/`alevin` right now, the named geometries are hard-coded, and so to have a specific `--indropV2` flag, that would have to be added to the argument parser and then mapped to the specific underlying geometry in the code. This isn't hard, but as the number of different chemistries proliferates, it's not ultimately a scalable solution. So, the current recommendation would be to use the custom geometry flags as specified in the documentation, or adopt a wrapper like `simpleaf` and add `indropV2` to your custom geometry specification library. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139:309,Usability,simpl,simpleaf,309,"Hi @mugpeng,. This is because it is covered by the custom geometry specification (as laid out in the docs). I agree it's nice to have a specific flag for each geometry, rather than to have to e.g. specify the custom geometry each time. We are working on good solutions to that at a higher level (e.g. in our `simpleaf` tool where users can register their own custom geometry specifications and refer to them by name). However, in `salmon`/`alevin` right now, the named geometries are hard-coded, and so to have a specific `--indropV2` flag, that would have to be added to the argument parser and then mapped to the specific underlying geometry in the code. This isn't hard, but as the number of different chemistries proliferates, it's not ultimately a scalable solution. So, the current recommendation would be to use the custom geometry flags as specified in the documentation, or adopt a wrapper like `simpleaf` and add `indropV2` to your custom geometry specification library. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139:905,Usability,simpl,simpleaf,905,"Hi @mugpeng,. This is because it is covered by the custom geometry specification (as laid out in the docs). I agree it's nice to have a specific flag for each geometry, rather than to have to e.g. specify the custom geometry each time. We are working on good solutions to that at a higher level (e.g. in our `simpleaf` tool where users can register their own custom geometry specifications and refer to them by name). However, in `salmon`/`alevin` right now, the named geometries are hard-coded, and so to have a specific `--indropV2` flag, that would have to be added to the argument parser and then mapped to the specific underlying geometry in the code. This isn't hard, but as the number of different chemistries proliferates, it's not ultimately a scalable solution. So, the current recommendation would be to use the custom geometry flags as specified in the documentation, or adopt a wrapper like `simpleaf` and add `indropV2` to your custom geometry specification library. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139
https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757689389:218,Availability,down,downstream,218,"The sequence bias model is generic. That is, it does not start out with any assumption about the prevalence or use of specific hexamers. Rather, it trains a VLMM to assess bias in the context surrounding (upstream and downstream) read start positions. It makes sense to use this flag to model types of sequence-specific non-uniformity in read start positions even beyond those caused by non-random hexamer priming.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757689389
https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757735883:234,Usability,clear,clear,234,"Hey Rob, not sure if I understood your answer. ; During cDNA synthesis, we use random 9-mer (not 6-mer). My impression from the documentation is that -seqbias can only be used if cDNA synthesis was done using 6-mer. This point is not clear in your answer for me. ; Could you please clarify?; Many thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757735883
https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757810371:714,Deployability,release,release,714,"Certainly. Let me be more specific. The model itself is general, though we train it on a context that is of length 8. Specifically, the model that is used (the context) is defined here. https://github.com/COMBINE-lab/salmon/blob/1c3f6c014ce77ec593d5b37ee2bb0cf9feddf123/src/SBModel.cpp#L20. If you look further up in that file, you can see that it's reasonably easy to consider different contexts — it's just a matter of how the model is initialized. However, this isn't something that we currently expose as a runtime parameter. However, if you have a reason to believe that a different model topology would work better in your context, we'd be happy to help you try it out (and enable such a feature in a future release). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757810371
https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757810371:499,Security,expose,expose,499,"Certainly. Let me be more specific. The model itself is general, though we train it on a context that is of length 8. Specifically, the model that is used (the context) is defined here. https://github.com/COMBINE-lab/salmon/blob/1c3f6c014ce77ec593d5b37ee2bb0cf9feddf123/src/SBModel.cpp#L20. If you look further up in that file, you can see that it's reasonably easy to consider different contexts — it's just a matter of how the model is initialized. However, this isn't something that we currently expose as a runtime parameter. However, if you have a reason to believe that a different model topology would work better in your context, we'd be happy to help you try it out (and enable such a feature in a future release). Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757810371
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1758110567:536,Availability,avail,available,536,"Hi @astrdhr,. Thank you for the bug report. I was wondering if you can run . ```; salmon --version; ```. under the invocation that is failing. I bring this up because your output starts with:. ```; Version Info: ### PLEASE UPGRADE SALMON ###; ```. and this should not happen if you are using the most recent version (there was a segfault related bugfix directly related to what you are seeing in v1.10). It's possible that if you are running salmon using some sort of script or job submission system, that the version of salmon that is available in your `PATH` isn't the same as the most recent one you have installed. P.S. I'll also note that v0.14 and v1.10 don't have compatible indices, which can also cause a segfault. You should make sure that the index was generated with the version of salmon with which you are attempting to quantify. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1758110567
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1758110567:223,Deployability,UPGRADE,UPGRADE,223,"Hi @astrdhr,. Thank you for the bug report. I was wondering if you can run . ```; salmon --version; ```. under the invocation that is failing. I bring this up because your output starts with:. ```; Version Info: ### PLEASE UPGRADE SALMON ###; ```. and this should not happen if you are using the most recent version (there was a segfault related bugfix directly related to what you are seeing in v1.10). It's possible that if you are running salmon using some sort of script or job submission system, that the version of salmon that is available in your `PATH` isn't the same as the most recent one you have installed. P.S. I'll also note that v0.14 and v1.10 don't have compatible indices, which can also cause a segfault. You should make sure that the index was generated with the version of salmon with which you are attempting to quantify. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1758110567
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1758110567:608,Deployability,install,installed,608,"Hi @astrdhr,. Thank you for the bug report. I was wondering if you can run . ```; salmon --version; ```. under the invocation that is failing. I bring this up because your output starts with:. ```; Version Info: ### PLEASE UPGRADE SALMON ###; ```. and this should not happen if you are using the most recent version (there was a segfault related bugfix directly related to what you are seeing in v1.10). It's possible that if you are running salmon using some sort of script or job submission system, that the version of salmon that is available in your `PATH` isn't the same as the most recent one you have installed. P.S. I'll also note that v0.14 and v1.10 don't have compatible indices, which can also cause a segfault. You should make sure that the index was generated with the version of salmon with which you are attempting to quantify. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1758110567
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766370214:506,Availability,error,error,506,"Hi Rob,. Thanks for your reply. When I run salmon --version I get this:. ```; <jemalloc>: MADV_DONTNEED does not work (memset will be used instead); <jemalloc>: (This is the expected behaviour if you are running under QEMU); salmon 1.10.2; ```. I'm running the script using Nextflow in a Docker container. However whether I run the script locally, within Nextflow or on a HPC cluster, it weirdly runs using salmon v0.14.1 (despite me specifying in my environment.yml file to use 1.10.2) and gives the same error. I also installed Salmon through a bioconda channel, not sure if that has any impact. On your last point - I haven't noticed different versions being used but I'll look out for this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766370214
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766370214:520,Deployability,install,installed,520,"Hi Rob,. Thanks for your reply. When I run salmon --version I get this:. ```; <jemalloc>: MADV_DONTNEED does not work (memset will be used instead); <jemalloc>: (This is the expected behaviour if you are running under QEMU); salmon 1.10.2; ```. I'm running the script using Nextflow in a Docker container. However whether I run the script locally, within Nextflow or on a HPC cluster, it weirdly runs using salmon v0.14.1 (despite me specifying in my environment.yml file to use 1.10.2) and gives the same error. I also installed Salmon through a bioconda channel, not sure if that has any impact. On your last point - I haven't noticed different versions being used but I'll look out for this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766370214
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766508995:497,Availability,fault,fault,497,"Hi @astrdhr,. Ok, so the difference between the version you get on the command line, versus the version you get when you actually attempt to run your script to process your data, is certainly a point of concern. In general, the behavior you are seeing during runtime seems like it may be an artifact of not having a compatible index. Is it possible for you to do a ""test run"" outside of the Nextflow script? Since you are getting v1.10.2 locally, and this version should work without segmentation fault, that would at least let us narrow the issue down to different versions of salmon being invoked at different stages of the pipeline. At that point, it may be a Nextflow / nf-core issue, but those folks are *great* and will be able to help in a jiffy!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766508995
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766508995:548,Availability,down,down,548,"Hi @astrdhr,. Ok, so the difference between the version you get on the command line, versus the version you get when you actually attempt to run your script to process your data, is certainly a point of concern. In general, the behavior you are seeing during runtime seems like it may be an artifact of not having a compatible index. Is it possible for you to do a ""test run"" outside of the Nextflow script? Since you are getting v1.10.2 locally, and this version should work without segmentation fault, that would at least let us narrow the issue down to different versions of salmon being invoked at different stages of the pipeline. At that point, it may be a Nextflow / nf-core issue, but those folks are *great* and will be able to help in a jiffy!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766508995
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766508995:626,Deployability,pipeline,pipeline,626,"Hi @astrdhr,. Ok, so the difference between the version you get on the command line, versus the version you get when you actually attempt to run your script to process your data, is certainly a point of concern. In general, the behavior you are seeing during runtime seems like it may be an artifact of not having a compatible index. Is it possible for you to do a ""test run"" outside of the Nextflow script? Since you are getting v1.10.2 locally, and this version should work without segmentation fault, that would at least let us narrow the issue down to different versions of salmon being invoked at different stages of the pipeline. At that point, it may be a Nextflow / nf-core issue, but those folks are *great* and will be able to help in a jiffy!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766508995
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766508995:366,Testability,test,test,366,"Hi @astrdhr,. Ok, so the difference between the version you get on the command line, versus the version you get when you actually attempt to run your script to process your data, is certainly a point of concern. In general, the behavior you are seeing during runtime seems like it may be an artifact of not having a compatible index. Is it possible for you to do a ""test run"" outside of the Nextflow script? Since you are getting v1.10.2 locally, and this version should work without segmentation fault, that would at least let us narrow the issue down to different versions of salmon being invoked at different stages of the pipeline. At that point, it may be a Nextflow / nf-core issue, but those folks are *great* and will be able to help in a jiffy!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1766508995
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1944356621:370,Deployability,install,installing,370,"Hi @rob-p,. I think I have figured out the issue. It seems like there's a dependency conflict with the ICU (international components for unicode) package between Salmon and R. It's been mentioned in this issue as well: https://github.com/COMBINE-lab/salmon/issues/594. I cannot have both the newest version of R and Salmon in the same environment. For context I've been installing Salmon>=1.10.1 through the bioconda channel, and base-r>=4.3.2 through conda-forge. Whenever I have R in the same environment, Salmon defaults to v0.14.1 during use (but the newest version when on the command line). If I remove R, Salmon defaults to the newest version during use and on the command line and works as normal.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1944356621
https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1944356621:74,Integrability,depend,dependency,74,"Hi @rob-p,. I think I have figured out the issue. It seems like there's a dependency conflict with the ICU (international components for unicode) package between Salmon and R. It's been mentioned in this issue as well: https://github.com/COMBINE-lab/salmon/issues/594. I cannot have both the newest version of R and Salmon in the same environment. For context I've been installing Salmon>=1.10.1 through the bioconda channel, and base-r>=4.3.2 through conda-forge. Whenever I have R in the same environment, Salmon defaults to v0.14.1 during use (but the newest version when on the command line). If I remove R, Salmon defaults to the newest version during use and on the command line and works as normal.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/881#issuecomment-1944356621
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120:34,Deployability,install,install,34,"Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. note: these resolver issues are a conda problem, and there’s nothin we as the salmon devs can do. So if you’d like to be able to avoid specifying the version, even when you put it in an env with arbitrary other software, I suggest making aMWE and opening an issue upstream in conda/bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120:168,Deployability,install,install,168,"Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. note: these resolver issues are a conda problem, and there’s nothin we as the salmon devs can do. So if you’d like to be able to avoid specifying the version, even when you put it in an env with arbitrary other software, I suggest making aMWE and opening an issue upstream in conda/bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120:209,Deployability,install,installing,209,"Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. note: these resolver issues are a conda problem, and there’s nothin we as the salmon devs can do. So if you’d like to be able to avoid specifying the version, even when you put it in an env with arbitrary other software, I suggest making aMWE and opening an issue upstream in conda/bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120:259,Deployability,install,install,259,"Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. note: these resolver issues are a conda problem, and there’s nothin we as the salmon devs can do. So if you’d like to be able to avoid specifying the version, even when you put it in an env with arbitrary other software, I suggest making aMWE and opening an issue upstream in conda/bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120:0,Integrability,Depend,Depending,0,"Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. note: these resolver issues are a conda problem, and there’s nothin we as the salmon devs can do. So if you’d like to be able to avoid specifying the version, even when you put it in an env with arbitrary other software, I suggest making aMWE and opening an issue upstream in conda/bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120:442,Safety,avoid,avoid,442,"Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. note: these resolver issues are a conda problem, and there’s nothin we as the salmon devs can do. So if you’d like to be able to avoid specifying the version, even when you put it in an env with arbitrary other software, I suggest making aMWE and opening an issue upstream in conda/bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784085120
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784111111:247,Testability,test,testing,247,"I didn't realize the resolver was so screwed up. And yeah, I get it that it's not your responsibility to fix it! In retrospect, as we're always specify versions in Snakemake profiles, the problem should be a non-issue. I was just doing some local testing when I encountered the ""issue"". Thx,; Adam",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784111111
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351:840,Deployability,install,install,840,"Fwiw, the conda resolver is not the only issue:. >conda create -n salmon -c bioconda -c conda-forge salmon=1.10.2. Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 8:04 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784085120&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=Ea4-F5juBTywwyjamWmiXQu3PVrQ4kCnIg-68wR1Pa4&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUHDLECWA4NGVBBX7R3YBZA5HAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGA4DKMJSGA&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=UIOMil_E-TPQw6P8DHvvV7-jaFu1apAxBIJgLzjUtvs&e=>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351:974,Deployability,install,install,974,"Fwiw, the conda resolver is not the only issue:. >conda create -n salmon -c bioconda -c conda-forge salmon=1.10.2. Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 8:04 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784085120&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=Ea4-F5juBTywwyjamWmiXQu3PVrQ4kCnIg-68wR1Pa4&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUHDLECWA4NGVBBX7R3YBZA5HAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGA4DKMJSGA&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=UIOMil_E-TPQw6P8DHvvV7-jaFu1apAxBIJgLzjUtvs&e=>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351:1015,Deployability,install,installing,1015,"Fwiw, the conda resolver is not the only issue:. >conda create -n salmon -c bioconda -c conda-forge salmon=1.10.2. Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 8:04 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784085120&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=Ea4-F5juBTywwyjamWmiXQu3PVrQ4kCnIg-68wR1Pa4&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUHDLECWA4NGVBBX7R3YBZA5HAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGA4DKMJSGA&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=UIOMil_E-TPQw6P8DHvvV7-jaFu1apAxBIJgLzjUtvs&e=>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351:1065,Deployability,install,install,1065,"Fwiw, the conda resolver is not the only issue:. >conda create -n salmon -c bioconda -c conda-forge salmon=1.10.2. Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 8:04 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784085120&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=Ea4-F5juBTywwyjamWmiXQu3PVrQ4kCnIg-68wR1Pa4&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUHDLECWA4NGVBBX7R3YBZA5HAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGA4DKMJSGA&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=UIOMil_E-TPQw6P8DHvvV7-jaFu1apAxBIJgLzjUtvs&e=>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351:806,Integrability,Depend,Depending,806,"Fwiw, the conda resolver is not the only issue:. >conda create -n salmon -c bioconda -c conda-forge salmon=1.10.2. Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 8:04 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784085120&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=Ea4-F5juBTywwyjamWmiXQu3PVrQ4kCnIg-68wR1Pa4&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUHDLECWA4NGVBBX7R3YBZA5HAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGA4DKMJSGA&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=UIOMil_E-TPQw6P8DHvvV7-jaFu1apAxBIJgLzjUtvs&e=>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351:1965,Integrability,Message,Message,1965,"Fwiw, the conda resolver is not the only issue:. >conda create -n salmon -c bioconda -c conda-forge salmon=1.10.2. Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 8:04 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Depending on what environment you install in (other conda software), the default resolver in conda is messed up. You should just explicitly ask for 1.10.2 with “ conda install salmon=1.10.2”. You can also try installing it in a clean conda env which is how I install most of my conda software anyway. Best,; Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784085120&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=Ea4-F5juBTywwyjamWmiXQu3PVrQ4kCnIg-68wR1Pa4&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUHDLECWA4NGVBBX7R3YBZA5HAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGA4DKMJSGA&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=BuO8x-09ODKHZCV2IlEsuaycWlFfWjCrfXJ-22PbmV0x8PssZEMVgCYeWBbR1GlW&s=UIOMil_E-TPQw6P8DHvvV7-jaFu1apAxBIJgLzjUtvs&e=>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784119351
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784122719:145,Testability,test,test,145,"I'm going to cc @dpryan79 on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test `simpleaf`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784122719
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784122719:151,Usability,simpl,simpleaf,151,"I'm going to cc @dpryan79 on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test `simpleaf`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784122719
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021:2291,Integrability,Message,Message,2291,"ta.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. this is the first time I've encountered an issue where something that is ""supposed"" to be there can't be found. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 10:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; State change ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). I'm going to cc @dpryan79<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_dpryan79&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=hPsmfTmSAfiwDhS2JFQyD0EUHl5m5sbj_n46DYj6tdM&e=> on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test simpleaf. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784122719&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=6Y-rQOzzAA-t9QV8NyfcMVeySD2an4xeN1HsDqa6VpQ&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUDFRQIHN4AMNBHWCN3YBZOUTAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGEZDENZRHE&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=ckSFRx1FekMV-wL0KtdZFPdtgCB1DiAziHIsdrF0cKQ&e=>.; You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021:73,Testability,test,testing,73,"conda create never seems to even get out of the gate ... a little bit of testing strongly suggests that version of salmon can't be found :. conda create -n owlVsunicorn -c bioconda owlVsunicorn; Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. this is the first time I've encountered an issue where something that is ""supposed"" to be there can't be found. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 10:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; State change ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). I'm going to cc @dpryan79<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_dpryan79&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=hPsmfTmSAfiwDhS2JFQyD0EUHl5m5sbj_n46DYj6tdM&e=> on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test simpleaf. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784122719&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=6Y-rQOzzAA-t9QV8NyfcMVeySD2an4xeN1HsDqa6VpQ&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUDFRQIHN4AMNBHWCN3YBZOUTAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PN",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021:1420,Testability,test,test,1420,"environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. this is the first time I've encountered an issue where something that is ""supposed"" to be there can't be found. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 10:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; State change ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). I'm going to cc @dpryan79<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_dpryan79&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=hPsmfTmSAfiwDhS2JFQyD0EUHl5m5sbj_n46DYj6tdM&e=> on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test simpleaf. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784122719&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=6Y-rQOzzAA-t9QV8NyfcMVeySD2an4xeN1HsDqa6VpQ&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUDFRQIHN4AMNBHWCN3YBZOUTAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGEZDENZRHE&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=ckSFRx1FekMV-wL0KtdZFPdtgCB1DiAziHIsdrF0cKQ&e=>.; You are receiving this because you mo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021:1425,Usability,simpl,simpleaf,1425,"environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. this is the first time I've encountered an issue where something that is ""supposed"" to be there can't be found. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 10:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; State change ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). I'm going to cc @dpryan79<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_dpryan79&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=hPsmfTmSAfiwDhS2JFQyD0EUHl5m5sbj_n46DYj6tdM&e=> on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test simpleaf. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784122719&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=6Y-rQOzzAA-t9QV8NyfcMVeySD2an4xeN1HsDqa6VpQ&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUDFRQIHN4AMNBHWCN3YBZOUTAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGEZDENZRHE&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=ckSFRx1FekMV-wL0KtdZFPdtgCB1DiAziHIsdrF0cKQ&e=>.; You are receiving this because you mo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784137337:473,Usability,usab,usable,473,"Hi @adamfreedman,. I think this is just conda being very very very slow. For me, the below command, replacing `mamba` with `conda` (but keeping the switched channel order) eventually did work, but took several minutes to ""collect package metadata"". However, the following works fine for me (and finishes in ~1 minute):. ```; mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2; ```. Can you use the `mamba` resolver in your environment? Conda has become hardly usable over the years, but `mamba` works quite well as a fast replacement. I'll also note that I swapped the order of `conda-forge` and `bioconda` as the docs specify that `bioconda` should preferably come last in the list of channels. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784137337
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:46,Deployability,install,install,46,"I think you're right wrt conda. I was able to install 1.10.2 with mamba fairly easily. We've been moving away from conda (towards mamba) but this didn't cross my mind when I was playing in my sandbox. Might be some cluster latency issues combined with conda's snail's pace causing the problem on our end. Thx for the quick replies!. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:2454,Integrability,Message,Message,2454,": +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUCOMVRRPOAZQL2EIITYBZVT5AVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGEZTOMZTG4&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=54-iPwwQkGRgqbmGQptKb39rCEfDF7oE_8NSR2kN4Xs&e=>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:192,Modifiability,sandbox,sandbox,192,"I think you're right wrt conda. I was able to install 1.10.2 with mamba fairly easily. We've been moving away from conda (towards mamba) but this didn't cross my mind when I was playing in my sandbox. Might be some cluster latency issues combined with conda's snail's pace causing the problem on our end. Thx for the quick replies!. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:223,Performance,latency,latency,223,"I think you're right wrt conda. I was able to install 1.10.2 with mamba fairly easily. We've been moving away from conda (towards mamba) but this didn't cross my mind when I was playing in my sandbox. Might be some cluster latency issues combined with conda's snail's pace causing the problem on our end. Thx for the quick replies!. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:192,Testability,sandbox,sandbox,192,"I think you're right wrt conda. I was able to install 1.10.2 with mamba fairly easily. We've been moving away from conda (towards mamba) but this didn't cross my mind when I was playing in my sandbox. Might be some cluster latency issues combined with conda's snail's pace causing the problem on our end. Thx for the quick replies!. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:1378,Usability,usab,usable,1378,"ulty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUCOMVRRPOAZQL2EIITYBZVT5AVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGEZTOMZTG4&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=54-iPwwQkGRgqbmGQptKb3",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329:210,Deployability,pipeline,pipeline,210,"I think your first suggestion would be the best option - although not sure you want worm transcripts in your decoy file. An alternative is to just quantify the entire collection of sequences. This is what [our pipeline](https://github.com/Novartis/pisces) does. This mostly just consists of running salmon plus some functionality for building custom index files and splitting output for multi-species analysis. You can specify your GTF/FASTA pairs using a [config file](https://github.com/Novartis/pisces/blob/9936079ac75d4b75be95bad5bc962465e8c5f458/pisces/config.json#L46-L49) and the pipeline builds index files, helps run salmon and some basic QC, and outputs separate expression matrices for each individual organism - normalized back to TPM space. The approach seems to work well and is flexible enough for users to specify custom transcriptomes for chimeric/mixed samples or extensive genetic engineering.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329
https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329:587,Deployability,pipeline,pipeline,587,"I think your first suggestion would be the best option - although not sure you want worm transcripts in your decoy file. An alternative is to just quantify the entire collection of sequences. This is what [our pipeline](https://github.com/Novartis/pisces) does. This mostly just consists of running salmon plus some functionality for building custom index files and splitting output for multi-species analysis. You can specify your GTF/FASTA pairs using a [config file](https://github.com/Novartis/pisces/blob/9936079ac75d4b75be95bad5bc962465e8c5f458/pisces/config.json#L46-L49) and the pipeline builds index files, helps run salmon and some basic QC, and outputs separate expression matrices for each individual organism - normalized back to TPM space. The approach seems to work well and is flexible enough for users to specify custom transcriptomes for chimeric/mixed samples or extensive genetic engineering.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329
https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329:457,Modifiability,config,config,457,"I think your first suggestion would be the best option - although not sure you want worm transcripts in your decoy file. An alternative is to just quantify the entire collection of sequences. This is what [our pipeline](https://github.com/Novartis/pisces) does. This mostly just consists of running salmon plus some functionality for building custom index files and splitting output for multi-species analysis. You can specify your GTF/FASTA pairs using a [config file](https://github.com/Novartis/pisces/blob/9936079ac75d4b75be95bad5bc962465e8c5f458/pisces/config.json#L46-L49) and the pipeline builds index files, helps run salmon and some basic QC, and outputs separate expression matrices for each individual organism - normalized back to TPM space. The approach seems to work well and is flexible enough for users to specify custom transcriptomes for chimeric/mixed samples or extensive genetic engineering.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329
https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329:558,Modifiability,config,config,558,"I think your first suggestion would be the best option - although not sure you want worm transcripts in your decoy file. An alternative is to just quantify the entire collection of sequences. This is what [our pipeline](https://github.com/Novartis/pisces) does. This mostly just consists of running salmon plus some functionality for building custom index files and splitting output for multi-species analysis. You can specify your GTF/FASTA pairs using a [config file](https://github.com/Novartis/pisces/blob/9936079ac75d4b75be95bad5bc962465e8c5f458/pisces/config.json#L46-L49) and the pipeline builds index files, helps run salmon and some basic QC, and outputs separate expression matrices for each individual organism - normalized back to TPM space. The approach seems to work well and is flexible enough for users to specify custom transcriptomes for chimeric/mixed samples or extensive genetic engineering.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329
https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329:793,Modifiability,flexible,flexible,793,"I think your first suggestion would be the best option - although not sure you want worm transcripts in your decoy file. An alternative is to just quantify the entire collection of sequences. This is what [our pipeline](https://github.com/Novartis/pisces) does. This mostly just consists of running salmon plus some functionality for building custom index files and splitting output for multi-species analysis. You can specify your GTF/FASTA pairs using a [config file](https://github.com/Novartis/pisces/blob/9936079ac75d4b75be95bad5bc962465e8c5f458/pisces/config.json#L46-L49) and the pipeline builds index files, helps run salmon and some basic QC, and outputs separate expression matrices for each individual organism - normalized back to TPM space. The approach seems to work well and is flexible enough for users to specify custom transcriptomes for chimeric/mixed samples or extensive genetic engineering.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/901#issuecomment-1828116329
https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339:25,Energy Efficiency,adapt,adaptors,25,"Hi,. If I don't trim the adaptors and still use --ont will I still get correct quantification? Is adaptor trimming very essential? Is there a way I can use salmon without adaptor trimming?. Also, can you please clarify about the secondary alignmenmts if these are included in Salmon or not?. Thanks,; Harsha; ________________________________; From: Feng Yan ***@***.***>; Sent: 08 January 2024 23:30; To: COMBINE-lab/salmon ***@***.***>; Cc: Harshangda Karan Puri ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] Quantification in Alignment mode for Nanopore Data (Issue #903). also interested to know how Salmon uses secondary alignment. Because I found this tutorial https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ [combine-lab.github.io]<https://urldefense.com/v3/__https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTnGob8fw$> actually includes secondary alignments.; And based on my experience, secondary alignments are used by Salmon, because when I give a BAM before and after removing secondary (-F 256 flag in samtools), the results are different. —; Reply to this email directly, view it on GitHub [github.com]<https://urldefense.com/v3/__https://github.com/COMBINE-lab/salmon/issues/903*issuecomment-1881982972__;Iw!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTEiG0xQE$>, or unsubscribe [github.com]<https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/A3SZAPCLOZYB72ZEIEEXH43YNR6S7AVCNFSM6AAAAABANBCPNSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQOBRHE4DEOJXGI__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTntkMlxE$>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339
https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339:98,Energy Efficiency,adapt,adaptor,98,"Hi,. If I don't trim the adaptors and still use --ont will I still get correct quantification? Is adaptor trimming very essential? Is there a way I can use salmon without adaptor trimming?. Also, can you please clarify about the secondary alignmenmts if these are included in Salmon or not?. Thanks,; Harsha; ________________________________; From: Feng Yan ***@***.***>; Sent: 08 January 2024 23:30; To: COMBINE-lab/salmon ***@***.***>; Cc: Harshangda Karan Puri ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] Quantification in Alignment mode for Nanopore Data (Issue #903). also interested to know how Salmon uses secondary alignment. Because I found this tutorial https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ [combine-lab.github.io]<https://urldefense.com/v3/__https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTnGob8fw$> actually includes secondary alignments.; And based on my experience, secondary alignments are used by Salmon, because when I give a BAM before and after removing secondary (-F 256 flag in samtools), the results are different. —; Reply to this email directly, view it on GitHub [github.com]<https://urldefense.com/v3/__https://github.com/COMBINE-lab/salmon/issues/903*issuecomment-1881982972__;Iw!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTEiG0xQE$>, or unsubscribe [github.com]<https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/A3SZAPCLOZYB72ZEIEEXH43YNR6S7AVCNFSM6AAAAABANBCPNSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQOBRHE4DEOJXGI__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTntkMlxE$>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339
https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339:171,Energy Efficiency,adapt,adaptor,171,"Hi,. If I don't trim the adaptors and still use --ont will I still get correct quantification? Is adaptor trimming very essential? Is there a way I can use salmon without adaptor trimming?. Also, can you please clarify about the secondary alignmenmts if these are included in Salmon or not?. Thanks,; Harsha; ________________________________; From: Feng Yan ***@***.***>; Sent: 08 January 2024 23:30; To: COMBINE-lab/salmon ***@***.***>; Cc: Harshangda Karan Puri ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] Quantification in Alignment mode for Nanopore Data (Issue #903). also interested to know how Salmon uses secondary alignment. Because I found this tutorial https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ [combine-lab.github.io]<https://urldefense.com/v3/__https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTnGob8fw$> actually includes secondary alignments.; And based on my experience, secondary alignments are used by Salmon, because when I give a BAM before and after removing secondary (-F 256 flag in samtools), the results are different. —; Reply to this email directly, view it on GitHub [github.com]<https://urldefense.com/v3/__https://github.com/COMBINE-lab/salmon/issues/903*issuecomment-1881982972__;Iw!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTEiG0xQE$>, or unsubscribe [github.com]<https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/A3SZAPCLOZYB72ZEIEEXH43YNR6S7AVCNFSM6AAAAABANBCPNSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQOBRHE4DEOJXGI__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTntkMlxE$>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339
https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339:1968,Integrability,Message,Message,1968,"Hi,. If I don't trim the adaptors and still use --ont will I still get correct quantification? Is adaptor trimming very essential? Is there a way I can use salmon without adaptor trimming?. Also, can you please clarify about the secondary alignmenmts if these are included in Salmon or not?. Thanks,; Harsha; ________________________________; From: Feng Yan ***@***.***>; Sent: 08 January 2024 23:30; To: COMBINE-lab/salmon ***@***.***>; Cc: Harshangda Karan Puri ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] Quantification in Alignment mode for Nanopore Data (Issue #903). also interested to know how Salmon uses secondary alignment. Because I found this tutorial https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ [combine-lab.github.io]<https://urldefense.com/v3/__https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTnGob8fw$> actually includes secondary alignments.; And based on my experience, secondary alignments are used by Salmon, because when I give a BAM before and after removing secondary (-F 256 flag in samtools), the results are different. —; Reply to this email directly, view it on GitHub [github.com]<https://urldefense.com/v3/__https://github.com/COMBINE-lab/salmon/issues/903*issuecomment-1881982972__;Iw!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTEiG0xQE$>, or unsubscribe [github.com]<https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/A3SZAPCLOZYB72ZEIEEXH43YNR6S7AVCNFSM6AAAAABANBCPNSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQOBRHE4DEOJXGI__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTntkMlxE$>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339
https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339:25,Modifiability,adapt,adaptors,25,"Hi,. If I don't trim the adaptors and still use --ont will I still get correct quantification? Is adaptor trimming very essential? Is there a way I can use salmon without adaptor trimming?. Also, can you please clarify about the secondary alignmenmts if these are included in Salmon or not?. Thanks,; Harsha; ________________________________; From: Feng Yan ***@***.***>; Sent: 08 January 2024 23:30; To: COMBINE-lab/salmon ***@***.***>; Cc: Harshangda Karan Puri ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] Quantification in Alignment mode for Nanopore Data (Issue #903). also interested to know how Salmon uses secondary alignment. Because I found this tutorial https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ [combine-lab.github.io]<https://urldefense.com/v3/__https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTnGob8fw$> actually includes secondary alignments.; And based on my experience, secondary alignments are used by Salmon, because when I give a BAM before and after removing secondary (-F 256 flag in samtools), the results are different. —; Reply to this email directly, view it on GitHub [github.com]<https://urldefense.com/v3/__https://github.com/COMBINE-lab/salmon/issues/903*issuecomment-1881982972__;Iw!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTEiG0xQE$>, or unsubscribe [github.com]<https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/A3SZAPCLOZYB72ZEIEEXH43YNR6S7AVCNFSM6AAAAABANBCPNSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQOBRHE4DEOJXGI__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTntkMlxE$>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339
https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339:98,Modifiability,adapt,adaptor,98,"Hi,. If I don't trim the adaptors and still use --ont will I still get correct quantification? Is adaptor trimming very essential? Is there a way I can use salmon without adaptor trimming?. Also, can you please clarify about the secondary alignmenmts if these are included in Salmon or not?. Thanks,; Harsha; ________________________________; From: Feng Yan ***@***.***>; Sent: 08 January 2024 23:30; To: COMBINE-lab/salmon ***@***.***>; Cc: Harshangda Karan Puri ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] Quantification in Alignment mode for Nanopore Data (Issue #903). also interested to know how Salmon uses secondary alignment. Because I found this tutorial https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ [combine-lab.github.io]<https://urldefense.com/v3/__https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTnGob8fw$> actually includes secondary alignments.; And based on my experience, secondary alignments are used by Salmon, because when I give a BAM before and after removing secondary (-F 256 flag in samtools), the results are different. —; Reply to this email directly, view it on GitHub [github.com]<https://urldefense.com/v3/__https://github.com/COMBINE-lab/salmon/issues/903*issuecomment-1881982972__;Iw!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTEiG0xQE$>, or unsubscribe [github.com]<https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/A3SZAPCLOZYB72ZEIEEXH43YNR6S7AVCNFSM6AAAAABANBCPNSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQOBRHE4DEOJXGI__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTntkMlxE$>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339
https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339:171,Modifiability,adapt,adaptor,171,"Hi,. If I don't trim the adaptors and still use --ont will I still get correct quantification? Is adaptor trimming very essential? Is there a way I can use salmon without adaptor trimming?. Also, can you please clarify about the secondary alignmenmts if these are included in Salmon or not?. Thanks,; Harsha; ________________________________; From: Feng Yan ***@***.***>; Sent: 08 January 2024 23:30; To: COMBINE-lab/salmon ***@***.***>; Cc: Harshangda Karan Puri ***@***.***>; Author ***@***.***>; Subject: Re: [COMBINE-lab/salmon] Quantification in Alignment mode for Nanopore Data (Issue #903). also interested to know how Salmon uses secondary alignment. Because I found this tutorial https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ [combine-lab.github.io]<https://urldefense.com/v3/__https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTnGob8fw$> actually includes secondary alignments.; And based on my experience, secondary alignments are used by Salmon, because when I give a BAM before and after removing secondary (-F 256 flag in samtools), the results are different. —; Reply to this email directly, view it on GitHub [github.com]<https://urldefense.com/v3/__https://github.com/COMBINE-lab/salmon/issues/903*issuecomment-1881982972__;Iw!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTEiG0xQE$>, or unsubscribe [github.com]<https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/A3SZAPCLOZYB72ZEIEEXH43YNR6S7AVCNFSM6AAAAABANBCPNSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQOBRHE4DEOJXGI__;!!PDiH4ENfjr2_Jw!GTZeAEdMSJcSBTPXhWuSsmLuX2WDzuNuNgqT04lADpRqOWyHssr_JALdqVa1JBOS9RHGRa9M6SeJKoxo6T7o5_O0bvsV-KkgNb45i4uTntkMlxE$>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/903#issuecomment-1884769339
https://github.com/COMBINE-lab/salmon/issues/910#issuecomment-1918166379:0,Deployability,Update,Updated,0,"Updated Expected behavior: ; A clear and concise description of what you expected to happen.; I aim to retain all gene IDs, and for those represented by multiple lines, I intend to calculate the sum of values for each unique gene ID. I came across a few posts regarding this issue, but have not found a good solution for salmon quantmerge yet",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/910#issuecomment-1918166379
https://github.com/COMBINE-lab/salmon/issues/910#issuecomment-1918166379:31,Usability,clear,clear,31,"Updated Expected behavior: ; A clear and concise description of what you expected to happen.; I aim to retain all gene IDs, and for those represented by multiple lines, I intend to calculate the sum of values for each unique gene ID. I came across a few posts regarding this issue, but have not found a good solution for salmon quantmerge yet",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/910#issuecomment-1918166379
https://github.com/COMBINE-lab/salmon/issues/912#issuecomment-1954908448:8,Deployability,update,update,8,Just an update in case anyone else is encountering this issue. I was able to install salmon using these instructions: CONDA_SUBDIR=osx-64 conda create -n rosetta; conda activate rosetta; conda env config vars set CONDA_SUBDIR=osx-64; conda install salmon,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/912#issuecomment-1954908448
https://github.com/COMBINE-lab/salmon/issues/912#issuecomment-1954908448:77,Deployability,install,install,77,Just an update in case anyone else is encountering this issue. I was able to install salmon using these instructions: CONDA_SUBDIR=osx-64 conda create -n rosetta; conda activate rosetta; conda env config vars set CONDA_SUBDIR=osx-64; conda install salmon,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/912#issuecomment-1954908448
https://github.com/COMBINE-lab/salmon/issues/912#issuecomment-1954908448:240,Deployability,install,install,240,Just an update in case anyone else is encountering this issue. I was able to install salmon using these instructions: CONDA_SUBDIR=osx-64 conda create -n rosetta; conda activate rosetta; conda env config vars set CONDA_SUBDIR=osx-64; conda install salmon,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/912#issuecomment-1954908448
https://github.com/COMBINE-lab/salmon/issues/912#issuecomment-1954908448:197,Modifiability,config,config,197,Just an update in case anyone else is encountering this issue. I was able to install salmon using these instructions: CONDA_SUBDIR=osx-64 conda create -n rosetta; conda activate rosetta; conda env config vars set CONDA_SUBDIR=osx-64; conda install salmon,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/912#issuecomment-1954908448
https://github.com/COMBINE-lab/salmon/issues/915#issuecomment-2000571627:87,Deployability,install,installed,87,"Seems like a similar issue to #480, where the boost-cpp depenendency is supposed to be installed from the conda-forge channel. When you installed salmon, did you have the conda-forge channel installed?. Solution _*may*_ be: `conda uninstall -n salmon salmon && conda install -n salmon -c conda-forge -c bioconda salmon`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/915#issuecomment-2000571627
https://github.com/COMBINE-lab/salmon/issues/915#issuecomment-2000571627:136,Deployability,install,installed,136,"Seems like a similar issue to #480, where the boost-cpp depenendency is supposed to be installed from the conda-forge channel. When you installed salmon, did you have the conda-forge channel installed?. Solution _*may*_ be: `conda uninstall -n salmon salmon && conda install -n salmon -c conda-forge -c bioconda salmon`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/915#issuecomment-2000571627
https://github.com/COMBINE-lab/salmon/issues/915#issuecomment-2000571627:191,Deployability,install,installed,191,"Seems like a similar issue to #480, where the boost-cpp depenendency is supposed to be installed from the conda-forge channel. When you installed salmon, did you have the conda-forge channel installed?. Solution _*may*_ be: `conda uninstall -n salmon salmon && conda install -n salmon -c conda-forge -c bioconda salmon`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/915#issuecomment-2000571627
https://github.com/COMBINE-lab/salmon/issues/915#issuecomment-2000571627:267,Deployability,install,install,267,"Seems like a similar issue to #480, where the boost-cpp depenendency is supposed to be installed from the conda-forge channel. When you installed salmon, did you have the conda-forge channel installed?. Solution _*may*_ be: `conda uninstall -n salmon salmon && conda install -n salmon -c conda-forge -c bioconda salmon`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/915#issuecomment-2000571627
https://github.com/COMBINE-lab/salmon/issues/919#issuecomment-2009036657:138,Availability,error,error,138,"The ~/data/Sus_scrofa.Sscrofa11.1.dna.toplevel.fa is genome, the ./SQANTI3_1_output/cdna.new.fa is trans, therefore I don't know where is error.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/919#issuecomment-2009036657
https://github.com/COMBINE-lab/salmon/issues/919#issuecomment-2013695547:370,Availability,error,error,370,"You specify in the `salmon index` command that the cda.new.fa sequences are the decoys though.; `-d /home/Dingzifeng/SQANTI3_1_output/cdna.decoys.txt`; This line means ""use the sequences with the titles in the text file as my decoy sequences.""; If the `cda.new.fa` sequences are not supposed to be decoys, remove that flag. Otherwise, my above answer should resolve the error.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/919#issuecomment-2013695547
https://github.com/COMBINE-lab/salmon/issues/920#issuecomment-2013709916:936,Integrability,Message,Message,936,"It doesn’t have a problem with a single bam file. Which is the reason make; me wonder maybe my command is wrong.; Sure, I can share some of the bam files later today. On Thu, Mar 21, 2024 at 4:48 PM Alex D Hill ***@***.***>; wrote:. > Can you head any of the bam files? The fact that they are all *.txt.bam; > seems suspiciously like they are renamed text files.; >; > Does this happen if you run a single bam file at once?; >; > Would also be helpful to fill out the rest of the missing info requested; > (To Reproduce, Expected Behavior, Desktop, etc.); >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/920#issuecomment-2013705067>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AHJBWLH4NQP66UEUFSFESXDYZNBRHAVCNFSM6AAAAABFBYUNKOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMJTG4YDKMBWG4>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/920#issuecomment-2013709916
https://github.com/COMBINE-lab/salmon/issues/920#issuecomment-2014070596:648,Energy Efficiency,efficient,efficient,648,"Actually, ; I was wondering if I could use this method to help me quantify my Nanopore library with barcode sequence?; I have already demultiplexed the ONT library to each individual barcode by using some other tools and generated a meta table with matched barcode and readID.; It would be great if you guys have any ideas on how can I generate a barcode-gene count matrix from it.; My current workflow is aligning via minimap2 and subset the bam file to each barcode by matching the readID, and use Salmon or other tools to quantify the counts, and compile the matrix together. But it took a very long time and memory.; Maybe there's another more efficient way.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/920#issuecomment-2014070596
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:48,Availability,error,error,48,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:357,Availability,error,error,357,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:613,Availability,error,errors,613,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:851,Availability,error,errors,851,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:958,Availability,error,errors,958,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:1059,Availability,error,errors,1059,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:554,Usability,learn,learns,554,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:914,Usability,learn,learned,914,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/927#issuecomment-2067759510:38,Deployability,install,install,38,"I somehow deleted it and forced it to install 1.10.3 and problem solved. ; ""conda install bioconda::salmon=1.10.3"" does not work; I have to do ""conda install salmon=1.10.3""",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/927#issuecomment-2067759510
https://github.com/COMBINE-lab/salmon/issues/927#issuecomment-2067759510:82,Deployability,install,install,82,"I somehow deleted it and forced it to install 1.10.3 and problem solved. ; ""conda install bioconda::salmon=1.10.3"" does not work; I have to do ""conda install salmon=1.10.3""",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/927#issuecomment-2067759510
https://github.com/COMBINE-lab/salmon/issues/927#issuecomment-2067759510:150,Deployability,install,install,150,"I somehow deleted it and forced it to install 1.10.3 and problem solved. ; ""conda install bioconda::salmon=1.10.3"" does not work; I have to do ""conda install salmon=1.10.3""",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/927#issuecomment-2067759510
https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2166074428:252,Modifiability,config,configurable,252,"To partially answer my own question, I think [you need 50k reads](https://github.com/COMBINE-lab/salmon/blob/a2f6912b3f9f9af91e3a4b0d74adcb3bdc4c9a32/include/LibraryTypeDetector.hpp#L157) to get a proper strandedness assessment, and I don't think it's configurable.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2166074428
https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2173852278:117,Deployability,pipeline,pipeline,117,"Hey @pinin4fjords, did you fully figure it out? I'm having the exact same question while implementing my own RNA-seq pipeline that closely resembles nf-core's one.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2173852278
https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2175817517:36,Testability,log,logic,36,"@tdsone I _think_ this is the right logic: https://github.com/COMBINE-lab/salmon/blob/master/include/LibraryTypeDetector.hpp. . The main source of my confusion on this post was that I think Salmon just chucks back 'IU' for read numbers below 50k. It confused me less on realistic read numbers. I've simplified [quite a bit](https://github.com/nf-core/rnaseq/blob/bc6189f09954c0d00a71ac43b2ccf69ef22bbd82/subworkflows/local/utils_nfcore_rnaseq_pipeline/main.nf#L587) to just work with the strandedness component, using the numbers from lib_format_counts.json. It seems to produce results broadly as expected, but might be a bit naive, for example the numbers are mappings rather than fragments which could throw things off a bit.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2175817517
https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2175817517:299,Usability,simpl,simplified,299,"@tdsone I _think_ this is the right logic: https://github.com/COMBINE-lab/salmon/blob/master/include/LibraryTypeDetector.hpp. . The main source of my confusion on this post was that I think Salmon just chucks back 'IU' for read numbers below 50k. It confused me less on realistic read numbers. I've simplified [quite a bit](https://github.com/nf-core/rnaseq/blob/bc6189f09954c0d00a71ac43b2ccf69ef22bbd82/subworkflows/local/utils_nfcore_rnaseq_pipeline/main.nf#L587) to just work with the strandedness component, using the numbers from lib_format_counts.json. It seems to produce results broadly as expected, but might be a bit naive, for example the numbers are mappings rather than fragments which could throw things off a bit.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2175817517
https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696:289,Deployability,patch,patchwork,289,"I managed to figure out what I assume is the up-to-date way of doing things. I will post the code that works for me here in case anyone else comes across this issue:; ```; suppressPackageStartupMessages({; library(fishpond); library(tximport); library(devtools); library(ggplot2); library(patchwork); # Just install Seurat like normal. Tutorial's allusion to a ""spatial"" branch appears to be outdated.; library(Seurat); }). # navigate to data directory; wkdir <- ""path/to/alevin_data/""; setwd(wkdir). # load in alevin output; files <- file.path(""alevin_out/alevin/quants_mat.gz""); file.exists(files). # set prefix for output files; prefix = ""alevin"". # tximport loads the alevin data into R; txi <- tximport(files = files, type = ""alevin""). # Creating a Seurat object with spatial assay; assay <- ""Spatial""; brain <- CreateSeuratObject(counts = txi$counts, project = ""SPATIAL"", assay = assay); brain. # loading the 10x image data; seqdir <- ""path/to/10x_imaging_data/""; image.data <- Read10X_Image(paste0(seqdir,""spatial/"")). # Since the names of alevin cb is different from 10x; # we rename the cells and filter the image data; # to have the metadata for only quantified cells; image.data@boundaries$centroids@cells <- gsub(""-1"", """", image.data@boundaries$centroids@cells); common.cells <- intersect(Cells(x = brain), image.data@boundaries$centroids@cells). # Subset the centroids object; centroids <- image.data@boundaries$centroids. # Find indices of common cells; common_indices <- which(centroids@cells %in% common.cells). # Subset the cells and coords slots; centroids@cells <- centroids@cells[common_indices]; centroids@coords <- centroids@coords[common_indices, ]. # Update the image.data object with the subsetted centroids; image.data@boundaries$centroids <- centroids. # Update the brain object accordingly; brain <- subset(brain, cells = common.cells). # adding image data to Seurat object; DefaultAssay(object = image.data) <- ""Spatial""; brain@images[['slice']] <- image.data; ```; The re",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696
https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696:308,Deployability,install,install,308,"I managed to figure out what I assume is the up-to-date way of doing things. I will post the code that works for me here in case anyone else comes across this issue:; ```; suppressPackageStartupMessages({; library(fishpond); library(tximport); library(devtools); library(ggplot2); library(patchwork); # Just install Seurat like normal. Tutorial's allusion to a ""spatial"" branch appears to be outdated.; library(Seurat); }). # navigate to data directory; wkdir <- ""path/to/alevin_data/""; setwd(wkdir). # load in alevin output; files <- file.path(""alevin_out/alevin/quants_mat.gz""); file.exists(files). # set prefix for output files; prefix = ""alevin"". # tximport loads the alevin data into R; txi <- tximport(files = files, type = ""alevin""). # Creating a Seurat object with spatial assay; assay <- ""Spatial""; brain <- CreateSeuratObject(counts = txi$counts, project = ""SPATIAL"", assay = assay); brain. # loading the 10x image data; seqdir <- ""path/to/10x_imaging_data/""; image.data <- Read10X_Image(paste0(seqdir,""spatial/"")). # Since the names of alevin cb is different from 10x; # we rename the cells and filter the image data; # to have the metadata for only quantified cells; image.data@boundaries$centroids@cells <- gsub(""-1"", """", image.data@boundaries$centroids@cells); common.cells <- intersect(Cells(x = brain), image.data@boundaries$centroids@cells). # Subset the centroids object; centroids <- image.data@boundaries$centroids. # Find indices of common cells; common_indices <- which(centroids@cells %in% common.cells). # Subset the cells and coords slots; centroids@cells <- centroids@cells[common_indices]; centroids@coords <- centroids@coords[common_indices, ]. # Update the image.data object with the subsetted centroids; image.data@boundaries$centroids <- centroids. # Update the brain object accordingly; brain <- subset(brain, cells = common.cells). # adding image data to Seurat object; DefaultAssay(object = image.data) <- ""Spatial""; brain@images[['slice']] <- image.data; ```; The re",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696
https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696:1675,Deployability,Update,Update,1675,"ry(tximport); library(devtools); library(ggplot2); library(patchwork); # Just install Seurat like normal. Tutorial's allusion to a ""spatial"" branch appears to be outdated.; library(Seurat); }). # navigate to data directory; wkdir <- ""path/to/alevin_data/""; setwd(wkdir). # load in alevin output; files <- file.path(""alevin_out/alevin/quants_mat.gz""); file.exists(files). # set prefix for output files; prefix = ""alevin"". # tximport loads the alevin data into R; txi <- tximport(files = files, type = ""alevin""). # Creating a Seurat object with spatial assay; assay <- ""Spatial""; brain <- CreateSeuratObject(counts = txi$counts, project = ""SPATIAL"", assay = assay); brain. # loading the 10x image data; seqdir <- ""path/to/10x_imaging_data/""; image.data <- Read10X_Image(paste0(seqdir,""spatial/"")). # Since the names of alevin cb is different from 10x; # we rename the cells and filter the image data; # to have the metadata for only quantified cells; image.data@boundaries$centroids@cells <- gsub(""-1"", """", image.data@boundaries$centroids@cells); common.cells <- intersect(Cells(x = brain), image.data@boundaries$centroids@cells). # Subset the centroids object; centroids <- image.data@boundaries$centroids. # Find indices of common cells; common_indices <- which(centroids@cells %in% common.cells). # Subset the cells and coords slots; centroids@cells <- centroids@cells[common_indices]; centroids@coords <- centroids@coords[common_indices, ]. # Update the image.data object with the subsetted centroids; image.data@boundaries$centroids <- centroids. # Update the brain object accordingly; brain <- subset(brain, cells = common.cells). # adding image data to Seurat object; DefaultAssay(object = image.data) <- ""Spatial""; brain@images[['slice']] <- image.data; ```; The rest of the tutorial (plotting and clustering) is fairly standard and should work just fine. Of course, if your setup differs substantially from mine, it is certainly possible you will encounter different behavior so stay vigilent!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696
https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696:1782,Deployability,Update,Update,1782,"ry(tximport); library(devtools); library(ggplot2); library(patchwork); # Just install Seurat like normal. Tutorial's allusion to a ""spatial"" branch appears to be outdated.; library(Seurat); }). # navigate to data directory; wkdir <- ""path/to/alevin_data/""; setwd(wkdir). # load in alevin output; files <- file.path(""alevin_out/alevin/quants_mat.gz""); file.exists(files). # set prefix for output files; prefix = ""alevin"". # tximport loads the alevin data into R; txi <- tximport(files = files, type = ""alevin""). # Creating a Seurat object with spatial assay; assay <- ""Spatial""; brain <- CreateSeuratObject(counts = txi$counts, project = ""SPATIAL"", assay = assay); brain. # loading the 10x image data; seqdir <- ""path/to/10x_imaging_data/""; image.data <- Read10X_Image(paste0(seqdir,""spatial/"")). # Since the names of alevin cb is different from 10x; # we rename the cells and filter the image data; # to have the metadata for only quantified cells; image.data@boundaries$centroids@cells <- gsub(""-1"", """", image.data@boundaries$centroids@cells); common.cells <- intersect(Cells(x = brain), image.data@boundaries$centroids@cells). # Subset the centroids object; centroids <- image.data@boundaries$centroids. # Find indices of common cells; common_indices <- which(centroids@cells %in% common.cells). # Subset the cells and coords slots; centroids@cells <- centroids@cells[common_indices]; centroids@coords <- centroids@coords[common_indices, ]. # Update the image.data object with the subsetted centroids; image.data@boundaries$centroids <- centroids. # Update the brain object accordingly; brain <- subset(brain, cells = common.cells). # adding image data to Seurat object; DefaultAssay(object = image.data) <- ""Spatial""; brain@images[['slice']] <- image.data; ```; The rest of the tutorial (plotting and clustering) is fairly standard and should work just fine. Of course, if your setup differs substantially from mine, it is certainly possible you will encounter different behavior so stay vigilent!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696
https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696:503,Performance,load,load,503,"I managed to figure out what I assume is the up-to-date way of doing things. I will post the code that works for me here in case anyone else comes across this issue:; ```; suppressPackageStartupMessages({; library(fishpond); library(tximport); library(devtools); library(ggplot2); library(patchwork); # Just install Seurat like normal. Tutorial's allusion to a ""spatial"" branch appears to be outdated.; library(Seurat); }). # navigate to data directory; wkdir <- ""path/to/alevin_data/""; setwd(wkdir). # load in alevin output; files <- file.path(""alevin_out/alevin/quants_mat.gz""); file.exists(files). # set prefix for output files; prefix = ""alevin"". # tximport loads the alevin data into R; txi <- tximport(files = files, type = ""alevin""). # Creating a Seurat object with spatial assay; assay <- ""Spatial""; brain <- CreateSeuratObject(counts = txi$counts, project = ""SPATIAL"", assay = assay); brain. # loading the 10x image data; seqdir <- ""path/to/10x_imaging_data/""; image.data <- Read10X_Image(paste0(seqdir,""spatial/"")). # Since the names of alevin cb is different from 10x; # we rename the cells and filter the image data; # to have the metadata for only quantified cells; image.data@boundaries$centroids@cells <- gsub(""-1"", """", image.data@boundaries$centroids@cells); common.cells <- intersect(Cells(x = brain), image.data@boundaries$centroids@cells). # Subset the centroids object; centroids <- image.data@boundaries$centroids. # Find indices of common cells; common_indices <- which(centroids@cells %in% common.cells). # Subset the cells and coords slots; centroids@cells <- centroids@cells[common_indices]; centroids@coords <- centroids@coords[common_indices, ]. # Update the image.data object with the subsetted centroids; image.data@boundaries$centroids <- centroids. # Update the brain object accordingly; brain <- subset(brain, cells = common.cells). # adding image data to Seurat object; DefaultAssay(object = image.data) <- ""Spatial""; brain@images[['slice']] <- image.data; ```; The re",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696
https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696:662,Performance,load,loads,662,"I managed to figure out what I assume is the up-to-date way of doing things. I will post the code that works for me here in case anyone else comes across this issue:; ```; suppressPackageStartupMessages({; library(fishpond); library(tximport); library(devtools); library(ggplot2); library(patchwork); # Just install Seurat like normal. Tutorial's allusion to a ""spatial"" branch appears to be outdated.; library(Seurat); }). # navigate to data directory; wkdir <- ""path/to/alevin_data/""; setwd(wkdir). # load in alevin output; files <- file.path(""alevin_out/alevin/quants_mat.gz""); file.exists(files). # set prefix for output files; prefix = ""alevin"". # tximport loads the alevin data into R; txi <- tximport(files = files, type = ""alevin""). # Creating a Seurat object with spatial assay; assay <- ""Spatial""; brain <- CreateSeuratObject(counts = txi$counts, project = ""SPATIAL"", assay = assay); brain. # loading the 10x image data; seqdir <- ""path/to/10x_imaging_data/""; image.data <- Read10X_Image(paste0(seqdir,""spatial/"")). # Since the names of alevin cb is different from 10x; # we rename the cells and filter the image data; # to have the metadata for only quantified cells; image.data@boundaries$centroids@cells <- gsub(""-1"", """", image.data@boundaries$centroids@cells); common.cells <- intersect(Cells(x = brain), image.data@boundaries$centroids@cells). # Subset the centroids object; centroids <- image.data@boundaries$centroids. # Find indices of common cells; common_indices <- which(centroids@cells %in% common.cells). # Subset the cells and coords slots; centroids@cells <- centroids@cells[common_indices]; centroids@coords <- centroids@coords[common_indices, ]. # Update the image.data object with the subsetted centroids; image.data@boundaries$centroids <- centroids. # Update the brain object accordingly; brain <- subset(brain, cells = common.cells). # adding image data to Seurat object; DefaultAssay(object = image.data) <- ""Spatial""; brain@images[['slice']] <- image.data; ```; The re",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696
https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696:903,Performance,load,loading,903,"I managed to figure out what I assume is the up-to-date way of doing things. I will post the code that works for me here in case anyone else comes across this issue:; ```; suppressPackageStartupMessages({; library(fishpond); library(tximport); library(devtools); library(ggplot2); library(patchwork); # Just install Seurat like normal. Tutorial's allusion to a ""spatial"" branch appears to be outdated.; library(Seurat); }). # navigate to data directory; wkdir <- ""path/to/alevin_data/""; setwd(wkdir). # load in alevin output; files <- file.path(""alevin_out/alevin/quants_mat.gz""); file.exists(files). # set prefix for output files; prefix = ""alevin"". # tximport loads the alevin data into R; txi <- tximport(files = files, type = ""alevin""). # Creating a Seurat object with spatial assay; assay <- ""Spatial""; brain <- CreateSeuratObject(counts = txi$counts, project = ""SPATIAL"", assay = assay); brain. # loading the 10x image data; seqdir <- ""path/to/10x_imaging_data/""; image.data <- Read10X_Image(paste0(seqdir,""spatial/"")). # Since the names of alevin cb is different from 10x; # we rename the cells and filter the image data; # to have the metadata for only quantified cells; image.data@boundaries$centroids@cells <- gsub(""-1"", """", image.data@boundaries$centroids@cells); common.cells <- intersect(Cells(x = brain), image.data@boundaries$centroids@cells). # Subset the centroids object; centroids <- image.data@boundaries$centroids. # Find indices of common cells; common_indices <- which(centroids@cells %in% common.cells). # Subset the cells and coords slots; centroids@cells <- centroids@cells[common_indices]; centroids@coords <- centroids@coords[common_indices, ]. # Update the image.data object with the subsetted centroids; image.data@boundaries$centroids <- centroids. # Update the brain object accordingly; brain <- subset(brain, cells = common.cells). # adding image data to Seurat object; DefaultAssay(object = image.data) <- ""Spatial""; brain@images[['slice']] <- image.data; ```; The re",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/942#issuecomment-2204802696
https://github.com/COMBINE-lab/salmon/issues/943#issuecomment-2284917014:95,Deployability,update,update,95,"Following up on this, if I was to fork `salmon` and fix this, would the correct solution be to update the following line? https://github.com/COMBINE-lab/salmon/blob/a2f6912b3f9f9af91e3a4b0d74adcb3bdc4c9a32/src/AlevinUtils.cpp#L1204",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/943#issuecomment-2284917014
https://github.com/COMBINE-lab/salmon/issues/952#issuecomment-2292317033:560,Testability,log,log,560,"I realized that most of these unassigned reads are probably paired-end reads that didn't match the specified the libType, which was ""IU"", or inward, not stranded. So I ran `samtools stats` on my BAM file to verify that.; ```; SN inward oriented pairs: 6191674; SN outward oriented pairs: 13515; ```; The inward pairs 6191674 is close to the pairs Salmon assigned, which was 6192944, but not the same. That's OK, considering Salmon and samtools probably have different ways of defining inward, outward read pairs.; I think it's helpful if Salmon can say in the log how many reads were excluded, for what reason. Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/952#issuecomment-2292317033
https://github.com/COMBINE-lab/salmon/issues/966#issuecomment-2416748677:101,Performance,perform,performance,101,"I agree – I wasn’t aware of that one. I’ve tested that and it has the same effect as the other flag, performance looks good.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/966#issuecomment-2416748677
https://github.com/COMBINE-lab/salmon/issues/966#issuecomment-2416748677:43,Testability,test,tested,43,"I agree – I wasn’t aware of that one. I’ve tested that and it has the same effect as the other flag, performance looks good.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/966#issuecomment-2416748677
