quality_attribute,keyword,matched_word,sentence,source,filename,author,repo,version,wiki,url
Energy Efficiency,efficient,efficient,"﻿. Hail | Get Help . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. ",MatchSource.WIKI,gethelp.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/gethelp.html
Performance,load,loaded,"﻿. Hail | Get Help . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. ",MatchSource.WIKI,gethelp.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/gethelp.html
Deployability,install,install,ariant_indices_EAS.rst.txt; panukb_ld_variant_indices_EUR.rst.txt; panukb_ld_variant_indices_MID.rst.txt; panukb_meta_analysis_all_ancestries.rst.txt; panukb_meta_analysis_high_quality.rst.txt; panukb_summary_stats.rst.txt; UK_Biobank_Rapid_GWAS_both_sexes.rst.txt; UK_Biobank_Rapid_GWAS_female.rst.txt; UK_Biobank_Rapid_GWAS_male.rst.txt. schemas.rst.txt. /experimental; ; hail.experimental.DB.rst.txt; index.rst.txt; ldscsim.rst.txt. /functions; ; collections.rst.txt; constructors.rst.txt; core.rst.txt; genetics.rst.txt; hail.expr.builders.CaseBuilder.rst.txt; hail.expr.builders.SwitchBuilder.rst.txt; index.rst.txt; numeric.rst.txt; random.rst.txt; stats.rst.txt; string.rst.txt. /genetics; ; hail.genetics.AlleleType.rst.txt; hail.genetics.Call.rst.txt; hail.genetics.Locus.rst.txt; hail.genetics.Pedigree.rst.txt; hail.genetics.ReferenceGenome.rst.txt; hail.genetics.Trio.rst.txt; index.rst.txt. /ggplot; ; index.rst.txt. /guides; ; agg.rst.txt; annotation.rst.txt; genetics.rst.txt. /install; ; azure.rst.txt; dataproc.rst.txt; linux.rst.txt; macosx.rst.txt; other-cluster.rst.txt; try.rst.txt. /linalg; . /utils; ; index.rst.txt. hail.linalg.BlockMatrix.rst.txt; index.rst.txt. /methods; ; genetics.rst.txt; impex.rst.txt; index.rst.txt; misc.rst.txt; relatedness.rst.txt; stats.rst.txt. /nd; ; index.rst.txt. /overview; ; expressions.rst.txt; index.rst.txt; matrix_table.rst.txt; table.rst.txt. /stats; ; hail.stats.LinearMixedModel.rst.txt; index.rst.txt. /tutorials; ; 01-genome-wide-association-study.ipynb.txt; 03-tables.ipynb.txt; 04-aggregation.ipynb.txt; 05-filter-annotate.ipynb.txt; 06-joins.ipynb.txt; 07-matrixtable.ipynb.txt; 08-plotting.ipynb.txt; 09-ggplot.ipynb.txt. /utils; ; index.rst.txt. /vds; ; hail.vds.combiner.load_combiner.rst.txt; hail.vds.combiner.new_combiner.rst.txt; hail.vds.combiner.VariantDatasetCombiner.rst.txt; hail.vds.combiner.VDSMetadata.rst.txt; hail.vds.filter_chromosomes.rst.txt; hail.vds.filter_intervals.rst.txt; hail.vds.filter_samples.rst.txt; ,MatchSource.WIKI,index-wcopy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index-wcopy.html
Usability,guid,guides,AMR.rst.txt; panukb_ld_variant_indices_CSA.rst.txt; panukb_ld_variant_indices_EAS.rst.txt; panukb_ld_variant_indices_EUR.rst.txt; panukb_ld_variant_indices_MID.rst.txt; panukb_meta_analysis_all_ancestries.rst.txt; panukb_meta_analysis_high_quality.rst.txt; panukb_summary_stats.rst.txt; UK_Biobank_Rapid_GWAS_both_sexes.rst.txt; UK_Biobank_Rapid_GWAS_female.rst.txt; UK_Biobank_Rapid_GWAS_male.rst.txt. schemas.rst.txt. /experimental; ; hail.experimental.DB.rst.txt; index.rst.txt; ldscsim.rst.txt. /functions; ; collections.rst.txt; constructors.rst.txt; core.rst.txt; genetics.rst.txt; hail.expr.builders.CaseBuilder.rst.txt; hail.expr.builders.SwitchBuilder.rst.txt; index.rst.txt; numeric.rst.txt; random.rst.txt; stats.rst.txt; string.rst.txt. /genetics; ; hail.genetics.AlleleType.rst.txt; hail.genetics.Call.rst.txt; hail.genetics.Locus.rst.txt; hail.genetics.Pedigree.rst.txt; hail.genetics.ReferenceGenome.rst.txt; hail.genetics.Trio.rst.txt; index.rst.txt. /ggplot; ; index.rst.txt. /guides; ; agg.rst.txt; annotation.rst.txt; genetics.rst.txt. /install; ; azure.rst.txt; dataproc.rst.txt; linux.rst.txt; macosx.rst.txt; other-cluster.rst.txt; try.rst.txt. /linalg; . /utils; ; index.rst.txt. hail.linalg.BlockMatrix.rst.txt; index.rst.txt. /methods; ; genetics.rst.txt; impex.rst.txt; index.rst.txt; misc.rst.txt; relatedness.rst.txt; stats.rst.txt. /nd; ; index.rst.txt. /overview; ; expressions.rst.txt; index.rst.txt; matrix_table.rst.txt; table.rst.txt. /stats; ; hail.stats.LinearMixedModel.rst.txt; index.rst.txt. /tutorials; ; 01-genome-wide-association-study.ipynb.txt; 03-tables.ipynb.txt; 04-aggregation.ipynb.txt; 05-filter-annotate.ipynb.txt; 06-joins.ipynb.txt; 07-matrixtable.ipynb.txt; 08-plotting.ipynb.txt; 09-ggplot.ipynb.txt. /utils; ; index.rst.txt. /vds; ; hail.vds.combiner.load_combiner.rst.txt; hail.vds.combiner.new_combiner.rst.txt; hail.vds.combiner.VariantDatasetCombiner.rst.txt; hail.vds.combiner.VDSMetadata.rst.txt; hail.vds.filter_chromosomes.rst.txt; hail,MatchSource.WIKI,index-wcopy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index-wcopy.html
Deployability,install,install,"﻿. Hail | Index . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scala",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
Energy Efficiency,power,powerful,"Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Powering genomic analysis, at every scale; Cloud-native genomic dataframes and batch computing. Install; Hail Query; Hail Batch; Get Help. ; import hail as hl. mt = hl.read_matrix_table('resources/post_qc.mt'); mt = mt.filter_rows(hl.agg.call_stats(mt.GT, mt.alleles).AF[1] > 0.01); pca_scores = hl.hwe_normalized_pca(mt.GT, k = 5, True)[1]; mt = mt.annotate_cols(pca = pca_scores[mt.s]). gwas = hl.linear_regression_rows(; y=mt.pheno.caffeine_consumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.is_female,; mt.pca.scores[0], mt.pca.scores[1],; mt.pca.scores[2]]). p = hl.plot.manhattan(gwas.p_value); show(p); ; ; GWAS with Hail (click to show code). Install. pip install hail. Hail requires Python 3 and the; Java 11 JRE.; ; GNU/Linux will also need the C and C++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis plat",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
Integrability,integrat,integrated,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
Performance,scalab,scalable,"++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowled",MatchSource.WIKI,index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/index.html
Deployability,install,installed,"﻿. Hail | References . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226–237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336–347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333–342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leońska-Duniec, A., Borczyk, M., Korostyński, M. et al. Genetic variants in myostatin; 	 and its receptors promote elite athlete status. BMC Genomics 2",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
Energy Efficiency,power,power,"6-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis converge to implicate negative selection and neuronal processes in the; 	 aetiology of schizophrenia. Mol Psychiatry 27, 3699–3707; 	 (2022). https://doi.org/10.1038/s41380-022-01621-8 https://www.nature.com/articles/s41380-022-01621-8. 	 Mitja, K.I., et al. FinnGen: Unique genetic insights from combining isolated population; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders wit",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
Integrability,integrat,integrate,"pulation; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al. “Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services”, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/ar",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
Safety,risk,risk," Science. Blog. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226–237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336–347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333–342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leońska-Duniec, A., Borczyk, M., Korostyński, M. et al. Genetic variants in myostatin; 	 and its receptors promote elite athlete status. BMC Genomics 24, 761; 	 (2023). https://doi.org/10.1186/s12864-023-09869-2 https://link.springer.com/article/1",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
Testability,log,login,"imer’s disease. Transl; 	 Psychiatry 12, 523; 	 (2022). https://doi.org/10.1038/s41398-022-02281-6 https://www.nature.com/articles/s41398-022-02281-6. Wadon, M.E., Fenner, E., Kendall, K.M. et al. Clinical and genotypic analysis in; 	 determining dystonia non-motor phenotypic heterogeneity: a UK Biobank study. J Neurol; 	 269, 6436–6451 (2022). https://doi.org/10.1007/s00415-022-11307-4 https://link.springer.com/article/10.1007/s00415-022-11307-4. 	 Andi Madihah Manggabarani, Takuyu Hashiguchi, Masatsugu Hashiguchi, Atsushi Hayashi,; 	 Masataka Kikuchi, Yusdar Mustamin, Masaru Bamba, Kunihiro Kodama, Takanari Tanabata,; 	 Sachiko Isobe, Hidenori Tanaka, Ryo Akashi, Akihiro Nakaya, Shusei Sato, Construction of; 	 prediction models for growth traits of soybean cultivars based on phenotyping in diverse; 	 genotype and environment combinations, DNA Research, Volume 29, Issue 4, August 2022,; 	 dsac024, https://doi.org/10.1093/dnares/dsac024 https://academic.oup.com/dnaresearch/article/29/4/dsac024/6653298?login=false. 	 Chaffin, M., Papangeli, I., Simonson, B. et al. Single-nucleus profiling of human; 	 dilated and hypertrophic cardiomyopathy. Nature 608, 174–180; 	 (2022). https://doi.org/10.1038/s41586-022-04817-8 https://www.nature.com/articles/s41586-022-04817-8. 	 Lee, J., Lee, J., Jeon, S. et al. A database of 5305 healthy Korean individuals reveals; 	 genetic and clinical implications for an East Asian population. Exp Mol Med 54,; 	 1862–1871; 	 (2022). https://doi.org/10.1038/s12276-022-00871-4 https://www.nature.com/articles/s12276-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis converge to implicate negative selection and neuronal processes in the; 	 aetiology of schizophrenia. Mol Psychiatry 27, 3699–3707; 	 (2022). https://doi.org/10.1038/s41380-022-01621-8 https://www.nature.com/articles/s41380-022-01621-8. 	 Mitja, K.I., et al. FinnGen: Unique genetic insights from combining isolated p",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
Usability,learn,learning-enabled,"tem nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 1371–1387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Josefine U Melchiorsen, Kimmie V Sørensen, Jette Bork-Jensen, Hüsün S Kizilkaya, Lærke S; 	 Gasbjerg, Alexander S Hauser, Jørgen Rungby, Henrik T Sørensen, Allan Vaag, Jens S; 	 Nielsen, Oluf Pedersen, Allan Linneberg, Bolette Hartmann, Anette P Gjesing, Jens J; 	 Holst, Torben Hansen, Mette M Rosenkilde, Niels Grarup, Rare Heterozygous; 	 Loss-of-Function Variants in the Human GLP-1 Receptor Are Not Associated With; 	 Cardiometabolic Phenotypes, The Journal of Clinical Endocrinology & Metabolism, Volume; 	 108, Issue 11, November 2023, Pages; 	 2821–2833, https://doi.org/10.1210/clinem/dgad290. https://academic.oup.com/jcem/article/108/11/2821/7180819. 	 Vukadinovic, Milos et al. Deep learning-enabled analysis of medical images identifies; 	 cardiac sphericity as an early marker of cardiomyopathy and related outcomes. Med,; 	 Volume 4, Issue 4, 252 - 262.e3. https://www.cell.com/med/fulltext/S2666-6340(23)00069-7. 	 Epi25 Collaborative; Chen S, Neale BM, Berkovic SF. Shared and distinct ultra-rare; 	 genetic risk for diverse epilepsies: A whole-exome sequencing study of 54,423; 	 individuals across multiple genetic ancestries. medRxiv [Preprint]. 2023 Feb; 	 24:2023.02.22.23286310. doi: 10.1101/2023.02.22.23286310. PMID: 36865150; PMCID:; 	 PMC9980234. https://pubmed.ncbi.nlm.nih.gov/36865150/. 	 Kurki, M.I., Karjalainen, J., Palta, P. et al. FinnGen provides genetic insights from a; 	 well-phenotyped isolated population. Nature 613, 508–518; 	 (2023). https://doi.org/10.1038/s41586-022-05473-8 https://www.nature.com/articles/s41586-022-05473-8. 	 Mortensen, Ó., Thomsen, E., Lydersen, L.N. et al. FarGen: Elucidating the distribution; 	 of coding variants in the isolated population of the Faroe Islands. Eur J Hum Genet 31,; 	 329–",MatchSource.WIKI,references.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/references.html
Integrability,interface,interface,"﻿. Hail | Tutorial . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Import, prototype, scale; ; Perform analyses with distributed; dataframe-like; collections. import hail as hl; mt = hl.import_vcf('gs://bucket/path/myVCF.vcf.bgz'); mt.write('gs://bucket/path/dataset.mt', overwrite=True); # read matrix into env; mt = hl.read_matrix_table('gs://bucket/path/dataset.mt'); mt1 = hl.import_vcf('/path/to/my.vcf.bgz'); mt2 = hl.import_bgen('/path/to/my.bgen'); mt3 = hl.import_plink(bed='/path/to/my.bed',; bim='/path/to/my.bim',; fam='/path/to/my.fam'). Input Unification; ; Import formats such as bed, bgen, plink, or vcf, and manipulate them using a common dataframe-like interface. Genomic Dataframes; For large and dense structured matrices, like sequencing data, coordinate representations are; both; hard to work with and computationally inefficient. A core piece of Hail functionality is the; MatrixTable, a 2-dimensional generalization of Table. The MatrixTable makes it possible to; filter,; annotate, and aggregate symmetrically over rows and columns. # What is a MatrixTable?; mt.describe(widget=True). # filter to rare, loss-of-function variants; mt = mt.filter_rows(mt.variant_qc.AF[1] < 0.005); mt = mt.filter_rows(mt.csq == 'LOF'); . # run sample QC and save into matrix table; mt = hl.sample_qc(mt). # filter for samples that are > 95% call rate; mt = mt.filter_cols(mt.sample_qc.call_rate >= 0.95) . # run variant QC and save into matrix table; mt = hl.variant_qc(mt). # filter for variants that are >95% call rate and >1% frequency; mt = mt.filter_rows(mt.variant_qc.call_rate > 0.95); mt = mt.filter_rows(mt.variant_qc_.AF[1] > 0.01). Simplified Analysis; Hail makes it easy to analyze your data. Let's start by filtering a dataset by variant and sample; quality metrics, like call rate and allele frequency. Quality Control Procedures; Quality control procedures, like sex check, are made easy using Hail's declarative syntax. imputed_sex =",MatchSource.WIKI,tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/tutorial.html
Safety,predict,predictor,"= mt.filter_rows(mt.csq == 'LOF'); . # run sample QC and save into matrix table; mt = hl.sample_qc(mt). # filter for samples that are > 95% call rate; mt = mt.filter_cols(mt.sample_qc.call_rate >= 0.95) . # run variant QC and save into matrix table; mt = hl.variant_qc(mt). # filter for variants that are >95% call rate and >1% frequency; mt = mt.filter_rows(mt.variant_qc.call_rate > 0.95); mt = mt.filter_rows(mt.variant_qc_.AF[1] > 0.01). Simplified Analysis; Hail makes it easy to analyze your data. Let's start by filtering a dataset by variant and sample; quality metrics, like call rate and allele frequency. Quality Control Procedures; Quality control procedures, like sex check, are made easy using Hail's declarative syntax. imputed_sex = hl.impute_sex(mt.GT); mt = mt.annotate_cols(; sex_check = imputed_sex[mt.s].is_female == mt.reported_female; ). # must use Google cloud platform for this to work ; # annotation with vep; mt = hl.vep(mt). Variant Effect Predictor; Annotating variants with Variant effect predictor has never been easier. Rare-Variant Association Testing; Perform Gene Burden Tests on sequencing data with just a few lines of Python. gene_intervals = hl.read_table(""gs://my_bucket/gene_intervals.t""); mt = mt.annotate_rows(; gene = gene_intervals.index(mt.locus, all_matches=True).gene_name; ). mt = mt.explode_rows(mt.gene); mt = (mt.group_rows_by(mt.gene); .aggregate(burden = hl.agg.count_where(mt.GT.is_non_ref()))). result = hl.linear_regression_rows(y=mt.phenotype, x=mt.burden). # generate and save PC scores; eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(mt.GT, k=4). # run linear regression for the first 4 PCs; mt = mt.annotate_cols(scores = pca_scores[mt.sample_id].scores); results = hl.linear_regression_rows(; y=mt.phenotype,; x=mt.GT.n_alt_alleles(),; covariates=[; 1, mt.scores[0], mt.scores[1], mt.scores[2], mt.scores[3]]; ). Principal Component Analysis (PCA); Adjusting GWAS models with principal components as covariates has never been easier. ",MatchSource.WIKI,tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/tutorial.html
Availability,down,downloads," something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text files). Links to; the relevant publications and raw data downloads are included where applicable. Important Notes¶. Multiallelic variants¶; Annotations in the database are keyed by biallelic variants. For some annotations, this means Hail’s split_multi() method; has been used to split multiallelic variants into biallelics. Warning; It is recommended to run split_multi() on your VDS before using annotate_variants_db(). You can use; annotate_variants_db() without first splitting multiallelic variants, but any multiallelics in your VDS will not be annotated.; If you first split these variants, the resulting biallelic variants may then be annotated by the database. VEP annotations¶; VEP annotations are included in this database under the root va.vep. To add VEP annotations, the annotate_variants_db(); method runs Hail’s vep() method on your VDS. This means that your cluster must be properly initialized as described in the; Running VEP section in this discussion post. Warning; If you want to add VEP annotations to your VDS, make sure to add ",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
Deployability,pipeline,pipelines,"﻿. . Annotation Database — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected ",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
Performance,load,load,"0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text f",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
Testability,test,test,"entation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text files). Links to; the relevant publications and raw data downloads are included where applicable. Important Notes¶. Multiallelic variants¶; Annotations in the database are key",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
Usability,simpl,simple,"0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text f",MatchSource.WIKI,docs/0.1/annotationdb.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/annotationdb.html
Integrability,interface,interface,"﻿. . Python API — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API. View page source. Python API¶; This is the API documentation for Hail, and provides detailed information; on the Python programming interface.; Classes. hail.HailContext; The main entry point for Hail functionality. hail.VariantDataset; Hail’s primary representation of genomic data, a matrix keyed by sample and variant. hail.KeyTable; Hail’s version of a SQL table where columns can be designated as keys. hail.KinshipMatrix; Represents a symmetric matrix encoding the relatedness of each pair of samples in the accompanying sample list. hail.LDMatrix; Represents a symmetric matrix encoding the Pearson correlation between each pair of variants in the accompanying variant list. Modules. representation; expr; utils. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/api.html
Energy Efficiency,power,power,"ean): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; logP (Boolean) – If true, probabilities are returned as log(p). dpois(x: Double, lambda: Double): Double. Returns Prob(\(X\) = x) from a Poisson distribution with rate parameter lambda.; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative. drop(s: Struct, identifiers: String*): Struct. Return a new Struct with the a subset of fields not matching identifiers.; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in drop(s, gene, function); result: {nHet: 12}. Arguments. s (Struct) – Struct to drop fields from.; identifiers (String*) – Field names to drop from s. Multiple arguments allowed. exp(x: Double): Double. Returns Euler’s number e raised to the power of the given value x.; Arguments. x (Double) – the exponent to raise e to. fet(a: Int, b: Int, c: Int, d: Int): Struct{pValue:Double,oddsRatio:Double,ci95Lower:Double,ci95Upper:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio; ci95Lower (Double) – lower bound for 95% confidence interval; ci95Upper (Double) – upper bound for 95% confidence interval. Calculates the p-value, odds ratio, and 95% confidence interval with Fisher’s exact test (FET) for 2x2 tables.; Examples; Annotate each variant with Fisher’s exact test association results (assumes minor/major allele count variant annotations have been computed):; >>> (vds.annotate_variants_expr(; ... 'va.fet = let macCase = gs.filter(g => sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'macControl = gs.filter(g => !sa.pheno.isCase).map(g => g.nNonRefAlleles()).sum() and '; ... 'majCase = gs.filter(g => sa.pheno.isCase).map(g => 2 - g.nNonRefAlleles()).sum() and '; ... 'majControl = gs.filter(g =",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
Modifiability,variab,variable," (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at ",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
Testability,test,test,"﻿. . Functions — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Functions. View page source. Functions¶. binomTest(x: Int, n: Int, p: Double, alternative: String): Double. Returns the p-value from the exact binomial test of the null hypothesis that success has probability p, given x successes in n trials.; Examples; Test each variant for allele balance across all heterozygous genotypes, under the null hypothesis that the two alleles are sampled with equal probability.; >>> (vds.split_multi(); ... .annotate_variants_expr(; ... 'va.ab_binom_test = let all_samples_ad = gs.filter(g => g.isHet).map(g => g.ad).sum() in '; ... 'binomTest(all_samples_ad[1], all_samples_ad.sum(), 0.5, ""two.sided"")')). Arguments. x (Int) – Number of successes; n (Int) – Number of trials; p (Double) – Probability of success under the null hypothesis; alternative (String) – Alternative hypothesis, must be “two.sided”, “greater” or “less”. chisq(c1: Int, c2: Int, c3: Int, c4: Int): Struct{pValue:Double,oddsRatio:Double}. pValue (Double) – p-value; oddsRatio (Double) – odds ratio. Calculates p-value (Chi-square approximation) and odds ratio for 2x2 table; Arguments. c1 (Int) – value for cell 1; c2 (Int) – value for cell 2; c3 (Int) – value for cell 3; c4 (Int) – value for cell 4. combineVariants(left: Variant, right: Variant): Struct{variant:Variant,laIndices:Dict[Int,Int],raIndices:Dict[Int,Int]}. variant (Variant) – Resulting combined variant.; laIndices (Dict[Int, Int]) – Mapping from new to old allele index for the left variant.; raIndices (Dict[Int, Int]) – Mapping from new to old allele index for the right variant. Combines the alleles of two variants at the same locus, making sure that ref and alt alleles are represented uniformely.;",MatchSource.WIKI,docs/0.1/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/functions.html
Performance,cache,cache,bute). aggregate_by_key() (hail.KeyTable method). (hail.VariantDataset method). allele() (hail.representation.Variant method). alt (hail.representation.AltAllele attribute). alt() (hail.representation.Variant method). alt_allele() (hail.representation.Variant method). alt_alleles (hail.representation.Variant attribute). AltAllele (class in hail.representation). annotate() (hail.KeyTable method). annotate_alleles_expr() (hail.VariantDataset method). annotate_genotypes_expr() (hail.VariantDataset method). annotate_global() (hail.VariantDataset method). annotate_global_expr() (hail.VariantDataset method). annotate_samples_expr() (hail.VariantDataset method). annotate_samples_table() (hail.VariantDataset method). annotate_variants_db() (hail.VariantDataset method). annotate_variants_expr() (hail.VariantDataset method). annotate_variants_table() (hail.VariantDataset method). annotate_variants_vds() (hail.VariantDataset method). B. balding_nichols_model() (hail.HailContext method). C. cache() (hail.KeyTable method). (hail.VariantDataset method). Call (class in hail.representation). category() (hail.representation.AltAllele method). colkey_schema (hail.VariantDataset attribute). collect() (hail.KeyTable method). columns (hail.KeyTable attribute). complete_trios() (hail.representation.Pedigree method). concordance() (hail.VariantDataset method). contains() (hail.representation.Interval method). contig (hail.representation.Locus attribute). (hail.representation.Variant attribute). count() (hail.KeyTable method). (hail.VariantDataset method). count_variants() (hail.VariantDataset method). D. deduplicate() (hail.VariantDataset method). delete_va_attribute() (hail.VariantDataset method). dosage() (hail.representation.Genotype method). dp (hail.representation.Genotype attribute). drop() (hail.KeyTable method). drop_samples() (hail.VariantDataset method). drop_variants() (hail.VariantDataset method). E. end (hail.representation.Interval attribute). eval_expr() (hail.HailContext me,MatchSource.WIKI,docs/0.1/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/genindex.html
Testability,log,logreg,hondrial() (hail.representation.Variant method). is_MNP() (hail.representation.AltAllele method). is_not_called() (hail.representation.Call method). (hail.representation.Genotype method). is_SNP() (hail.representation.AltAllele method). is_transition() (hail.representation.AltAllele method). is_transversion() (hail.representation.AltAllele method). J. join() (hail.KeyTable method). (hail.VariantDataset method). K. key (hail.KeyTable attribute). key_by() (hail.KeyTable method). key_schema (hail.KinshipMatrix attribute). KeyTable (class in hail). KinshipMatrix (class in hail). L. ld_matrix() (hail.VariantDataset method). ld_prune() (hail.VariantDataset method). LDMatrix (class in hail). linreg() (hail.VariantDataset method). linreg3() (hail.VariantDataset method). linreg_burden() (hail.VariantDataset method). linreg_multi_pheno() (hail.VariantDataset method). lmmreg() (hail.VariantDataset method). Locus (class in hail.representation). locus() (hail.representation.Variant method). logreg() (hail.VariantDataset method). logreg_burden() (hail.VariantDataset method). M. make_table() (hail.VariantDataset method). matrix() (hail.KinshipMatrix method). (hail.LDMatrix method). maximal_independent_set() (hail.KeyTable method). mendel_errors() (hail.VariantDataset method). min_rep() (hail.VariantDataset method). mother (hail.representation.Trio attribute). N. naive_coalesce() (hail.VariantDataset method). num_alleles() (hail.representation.Variant method). num_alt_alleles() (hail.representation.Call method). (hail.representation.Genotype method). (hail.representation.Variant method). num_columns (hail.KeyTable attribute). num_genotypes() (hail.representation.Variant method). num_mismatch() (hail.representation.AltAllele method). num_partitions() (hail.KeyTable method). (hail.VariantDataset method). num_samples (hail.VariantDataset attribute). O. od() (hail.representation.Genotype method). one_hot_alleles() (hail.representation.Call method). (hail.representation.Genotype method),MatchSource.WIKI,docs/0.1/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/genindex.html
Availability,down,download,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Deployability,continuous,continuous,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Energy Efficiency,power,powerful,"patible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in t",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Integrability,integrat,integration,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Modifiability,variab,variables,"ODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Performance,optimiz,optimized,"a version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Safety,avoid,avoid,"ilarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log wi",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Security,access,accessed,"n run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/distributions/hail-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by running the following commands. Note that you have to pass in; the existing SparkContext instance sc to the HailContext; constructor. >>> from hail import *; >>> hc = HailContext(). Files can be accessed from both Hadoop and Google Storage. If you’re running on Google’s Dataproc, you’ll want to store your files in Google Storage. In most on premises clusters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a ",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Testability,test,tests,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Usability,learn,learn,"patible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in t",MatchSource.WIKI,docs/0.1/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/getting_started.html
Availability,error,error,"rix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_running; Return the running Hail context in this Python session. grep; Grep big files, like, really fast. import_bgen; Import .bgen file(s) as variant dataset. import_gen; Import .gen file(s) as variant dataset. import_plink; Import PLINK binary file (BED, BIM, FAM",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
Deployability,configurat,configuration,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
Integrability,depend,depends,"es the representation of missing data in the table. Note; The comment and missing parameters are NOT regexes. The no_header option indicates that the file has no header line. If this option is passed, ; then the column names will be f0, f1, … fN (0-indexed).; The types option allows the user to pass the types of columns in the table. This is a ; dict keyed by str, with Type values. See the examples above for; a standard usage. Additionally, this option can be used to override type imputation. For example,; if a column in a file refers to chromosome and does not contain any sex chromosomes, it will be; imputed as an integer, while most Hail methods expect chromosome to be passed as a string. Using; the impute=True mode and passing types={'Chromosome': TString()} will solve this problem.; The min_partitions option can be used to increase the number of partitions (level of sharding); of an imported table. The default partition size depends on file system and a number of other ; factors (including the min_block_size of the hail context), but usually is between 32M and 128M. Parameters:; paths (str or list of str) – Files to import.; key (str or list of str) – Key column(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – File has no header and the N columns are named f0, f1, … fN (0-indexed); impute (bool) – Impute column types from the file; comment (str or None) – Skip lines beginning with the given pattern; delimiter (str) – Field delimiter regex; missing (str) – Specify identifier to be treated as missing; types (dict with str keys and Type values) – Define types of fields in annotations files; quote (str or None) – Quote character. Returns:Key table constructed from text table. Return type:KeyTable. import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, store_gq=False, pp_as_pl=False, skip_bad_ad=False, generic=False, call_fields=[])[source]¶; Import VCF file(s) as variant dataset.;",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
Modifiability,config,configuration,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
Performance,load,load,"s at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing a BGEN file as a VDS (assuming it has already been indexed).; >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
Safety,recover,recovery,"Parameters:; populations (int) – Number of populations.; samples (int) – Number of samples.; variants (int) – Number of variants.; num_partitions (int) – Number of partitions.; pop_dist (array of float or None) – Unnormalized population distribution; fst (array of float or None) – \(F_{ST}\) values; af_dist (UniformDist or BetaDist or TruncatedBetaDist) – Ancestral allele frequency distribution; seed (int) – Random seed. Returns:Variant dataset simulated using the Balding-Nichols model. Return type:VariantDataset. eval_expr(expr)[source]¶; Evaluate an expression. Parameters:expr (str) – Expression to evaluate. Return type:annotation. eval_expr_typed(expr)[source]¶; Evaluate an expression and return the result as well as its type. Parameters:expr (str) – Expression to evaluate. Return type:(annotation, Type). static get_running()[source]¶; Return the running Hail context in this Python session.; Example; >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen f",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
Testability,log,log,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,docs/0.1/hail.HailContext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html
Availability,error,error,"val. If the .bed file has four or more columns, then Hail will store the fourth column in the table:. interval (Interval) - Genomic interval.; target (String) - Fourth column of .bed file. UCSC bed files can have up to 12 fields, ; but Hail will only ever look at the first four. Hail ignores header lines in BED files. Caution; UCSC BED files are 0-indexed and end-exclusive. The line “5 100 105” will contain; locus 5:105 but not 5:100. Details here. Parameters:path (str) – Path to .bed file. Return type:KeyTable. static import_fam(path, quantitative=False, delimiter='\\\\s+', missing='NA')[source]¶; Import PLINK .fam file into a key table.; Examples; Import case-control phenotype data from a tab-separated PLINK .fam file into sample; annotations:; >>> fam_kt = KeyTable.import_fam('data/myStudy.fam'). In Hail, unlike PLINK, the user must explicitly distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype without quantitative=True will return an error; (unless all values happen to be 0, 1, 2, and -9):; >>> fam_kt = KeyTable.import_fam('data/myStudy.fam', quantitative=True). Columns; The column, types, and missing values are shown below. ID (String) – Sample ID (key column); famID (String) – Family ID (missing = “0”); patID (String) – Paternal ID (missing = “0”); matID (String) – Maternal ID (missing = “0”); isFemale (Boolean) – Sex (missing = “NA”, “-9”, “0”). One of:. isCase (Boolean) – Case-control phenotype (missing = “0”, “-9”, non-numeric or the missing argument, if given.; qPheno (Double) – Quantitative phenotype (missing = “NA” or the missing argument, if given. Parameters:; path (str) – Path to .fam file.; quantitative (bool) – If True, .fam phenotype is interpreted as quantitative.; delimiter (str) – .fam file field delimiter regex.; missing (str) – The string used to denote missing values.; For case-control, 0, -9, and non-numeric are also treated; as missing. Returns:Key table with information from .fam file. Return ",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
Deployability,pipeline,pipelines,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
Modifiability,config,config,"; [[3,4], []]. a; 2; [[3,4], []]. Explode c2 once and c3 twice:; >>> kt3.explode(['c2', 'c3', 'c3']). c1; c2; c3. a; 1; 3. a; 2; 3. a; 1; 4. a; 2; 4. Parameters:column_names (str or list of str) – Column name(s) to be exploded. Returns:Key table with columns exploded. Return type:KeyTable. export(output, types_file=None, header=True, parallel=False)[source]¶; Export to a TSV file.; Examples; Rename column names of key table and export to file:; >>> (kt1.rename({'HT' : 'Height'}); ... .export(""output/kt1_renamed.tsv"")). Parameters:; output (str) – Output file path.; types_file (str) – Output path of types file.; header (bool) – Write a header using the column names.; parallel (bool) – If true, writes a set of files (one per partition) rather than serially concatenating these files. export_cassandra(address, keyspace, table, block_size=100, rate=1000)[source]¶; Export to Cassandra. Warning; export_cassandra() is EXPERIMENTAL. export_elasticsearch(host, port, index, index_type, block_size, config=None, verbose=True)[source]¶; Export to Elasticsearch. Warning; export_elasticsearch() is EXPERIMENTAL. export_mongodb(mode='append')[source]¶; Export to MongoDB. Warning; export_mongodb() is EXPERIMENTAL. export_solr(zk_host, collection, block_size=100)[source]¶; Export to Solr. Warning; export_solr() is EXPERIMENTAL. filter(expr, keep=True)[source]¶; Filter rows.; Examples; Keep rows where C1 equals 5:; >>> kt_result = kt1.filter(""C1 == 5""). Remove rows where C1 equals 10:; >>> kt_result = kt1.filter(""C1 == 10"", keep=False). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Caution; When expr evaluates to missing, the row will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep rows where expr is true. Returns:Filtered key table. Return type:KeyTable. flatten()[source]¶; ",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
Performance,cache,cache,"mported from a text file or Spark DataFrame with import_table(); or from_dataframe(), generated from a variant dataset; with aggregate_by_key(), make_table(),; samples_table(), or variants_table().; In the examples below, we have imported two key tables from text files (kt1 and kt2).; >>> kt1 = hc.import_table('data/kt_example1.tsv', impute=True). ID; HT; SEX; X; Z; C1; C2; C3. 1; 65; M; 5; 4; 2; 50; 5. 2; 72; M; 6; 3; 2; 61; 1. 3; 70; F; 7; 3; 10; 81; -5. 4; 60; F; 8; 2; 11; 90; -10. >>> kt2 = hc.import_table('data/kt_example2.tsv', impute=True). ID; A; B. 1; 65; cat. 2; 72; dog. 3; 70; mouse. 4; 60; rabbit. Variables:hc (HailContext) – Hail Context. Attributes. columns; Names of all columns. key; List of key columns. num_columns; Number of columns. schema; Table schema. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate columns programmatically. annotate; Add new columns computed from existing columns. cache; Mark this key table to be cached in memory. collect; Collect table to a local list. count; Count the number of rows. drop; Drop columns. exists; Evaluate whether a boolean expression is true for at least one row. expand_types; Expand types Locus, Interval, AltAllele, Variant, Genotype, Char, Set and Dict. explode; Explode columns of this key table. export; Export to a TSV file. export_cassandra; Export to Cassandra. export_elasticsearch; Export to Elasticsearch. export_mongodb; Export to MongoDB. export_solr; Export to Solr. filter; Filter rows. flatten; Flatten nested Structs. forall; Evaluate whether a boolean expression is true for all rows. from_dataframe; Convert Spark SQL DataFrame to key table. from_pandas; Convert Pandas DataFrame to key table. from_py. import_bed; Import a UCSC .bed file as a key table. import_fam; Import PLINK .fam file into a key table. import_interval_list; Import an interval list file in the GATK standard format. indexed; Add the numerical index of each row as a new column.",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
Safety,avoid,avoid,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
Usability,simpl,simple,"ive of the end position. Note; Hail uses the following ordering for contigs: 1-22 sorted numerically, then X, Y, MT,; then alphabetically for any contig not matching the standard human chromosomes. Caution; The interval parser for these files does not support the full range of formats supported; by the python parser parse(). ‘k’, ‘m’, ‘start’, and ‘end’ are all; invalid motifs in the contig:start-end format here. Parameters:filename (str) – Path to file. Returns:Interval-keyed table. Return type:KeyTable. indexed(name='index')[source]¶; Add the numerical index of each row as a new column.; Examples; >>> ind_kt = kt1.indexed(). Notes; This method returns a table with a new column whose name is; given by the name parameter, with type Long. The value; of this column is the numerical index of each row, starting; from 0. Methods that respect ordering (like KeyTable.take(); or KeyTable.export() will return rows in order.; This method is helpful for creating a unique integer index for rows; of a table, so that more complex types can be encoded as a simple; number. Parameters:name (str) – Name of index column. Returns:Table with a new index column. Return type:KeyTable. join(right, how='inner')[source]¶; Join two key tables together.; Examples; Join kt1 to kt2 to produce kt_joined:; >>> kt_result = kt1.key_by('ID').join(kt2.key_by('ID')). Notes:; Hail supports four types of joins specified by how:. inner – Key must be present in both kt1 and kt2.; outer – Key present in kt1 or kt2. For keys only in kt1, the value of non-key columns from kt2 is set to missing.; Likewise, for keys only in kt2, the value of non-key columns from kt1 is set to missing.; left – Key present in kt1. For keys only in kt1, the value of non-key columns from kt2 is set to missing.; right – Key present in kt2. For keys only in kt2, the value of non-key columns from kt1 is set to missing. The non-key fields in kt2 must have non-overlapping column names with kt1.; Both key tables must have the same number ",MatchSource.WIKI,docs/0.1/hail.KeyTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html
Integrability,depend,depending,"﻿. . KinshipMatrix — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; KinshipMatrix. View page source. KinshipMatrix¶. class hail.KinshipMatrix(jkm)[source]¶; Represents a symmetric matrix encoding the relatedness of each pair of samples in the accompanying sample list.; The output formats are consistent with PLINK formats as created by the make-rel and make-grm commands and used by GCTA.; Attributes. key_schema; Returns the signature of the key indexing this matrix. Methods. __init__. export_gcta_grm; Export kinship matrix as .grm file. export_gcta_grm_bin; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. export_id_file; Export samples as .id file. export_rel; Export kinship matrix as .rel file. export_tsv; Export kinship matrix to tab-delimited text file with sample list as header. matrix; Gets the matrix backing this kinship matrix. sample_list; Gets the list of samples. export_gcta_grm(output)[source]¶; Export kinship matrix as .grm file. See PLINK formats. Parameters:output (str) – File path for output. export_gcta_grm_bin(output, opt_n_file=None)[source]¶; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. See PLINK formats. Parameters:; output (str) – File path for output.; opt_n_file (str or None) – The file path to the N file. export_id_file(output)[source]¶; Export samples as .id file. See PLINK formats. Parameters:output (str) – File path for output. export_rel(output)[source]¶; Export kinship matrix as .rel file. See PLINK formats. Parameters:output (str) – File path for output. export_tsv(output)[source]¶; Export kinship matrix to tab-delimited text file",MatchSource.WIKI,docs/0.1/hail.KinshipMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.KinshipMatrix.html
Availability,error,errors,"is VDS. ld_prune; Prune variants in linkage disequilibrium (LD). linreg; Test each variant for association using linear regression. linreg3; Test each variant for association with multiple phenotypes using linear regression. linreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the linear regression model. linreg_multi_pheno; Test each variant for association with multiple phenotypes using linear regression. lmmreg; Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. logreg; Test each variant for association using logistic regression. logreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the logistic regression model. make_table; Produce a key with one row per variant and one or more columns per sample. mendel_errors; Find Mendel errors; count per variant, individual and nuclear family. min_rep; Gives minimal, left-aligned representation of alleles. naive_coalesce; Naively descrease the number of partitions. num_partitions; Number of partitions. pc_relate; Compute relatedness estimates between individuals using a variant of the PC-Relate method. pca; Run Principal Component Analysis (PCA) on the matrix of genotypes. persist; Persist this variant dataset to memory and/or disk. query_genotypes; Performs aggregation queries over genotypes, and returns Python object(s). query_genotypes_typed; Performs aggregation queries over genotypes, and returns Python object(s) and type(s). query_samples; Performs aggregation queries over samples and sample annotations, and returns Python object(s). query_samples_typed; Performs aggregation queries over samples and sample annotations, and returns Python object(s) and type(s). query_variants; Performs aggregation queries over variants and variant annotations, and returns Python object(s). quer",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Deployability,pipeline,pipeline,"ul concordance statistics. This value is the number of genotypes ; which were called (homozygous reference, heterozygous, or homozygous variant) in both datasets, ; but where the call did not match between the two.; The column concordance matches the structure of the global summmary, which is detailed above. Once again,; the first index into this array is the state on the left, and the second index is the state on the right.; For example, concordance[1][4] is the number of “no call” genotypes on the left that were called ; homozygous variant on the right. Parameters:right (VariantDataset) – right hand variant dataset for concordance. Returns:The global concordance statistics, a key table with sample concordance; statistics, and a key table with variant concordance statistics. Return type:(list of list of int, KeyTable, KeyTable). count()[source]¶; Returns number of samples and variants in the dataset.; Examples; >>> samples, variants = vds.count(). Notes; This is also the fastest way to force evaluation of a Hail pipeline. Returns:The sample and variant counts. Return type:(int, int). count_variants()[source]¶; Count number of variants in variant dataset. Return type:long. deduplicate()[source]¶; Remove duplicate variants. Returns:Deduplicated variant dataset. Return type:VariantDataset. delete_va_attribute(ann_path, attribute)[source]¶; Removes an attribute from a variant annotation field.; Attributes are key/value pairs that can be attached to a variant annotation field.; The following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. INFO fields attributes (attached to (va.info.*)):; ‘Number’: The arity of the field. Can take values; 0 (Boolean flag),; 1 (single value),; R (one value per allele, including the reference),; A (one value per non-reference allele),; G (one value per genotype), and; . (any number of values); When importing: The value in read from the VCF INFO field definition; When expor",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Energy Efficiency,efficient,efficiently,"iants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove variants overlapping; an interval if False. Returns:Filtered variant dataset. Return type:VariantDataset. filter_multi()[source]¶; Filter out multi-allelic sites. Important; The genotype_schema() must be of type TGenotype in order to use this method. This method is much less computationally expensive than; split_multi(), and can also be used to produce; a variant dataset that can be used with methods that do not; support multiallelic variants. Returns:Dataset with no multiallelic sites, which can; be used for biallelic-only methods. Return type:VariantDataset. filter_samples_expr(expr, keep=True)[source]¶; Filter samples with the expression language.; Examples; ",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Integrability,depend,depends,"pulation or cohort, then the vds_key argument must be passed to describe the key in the dataset ; to use for the join. This argument expects a list of Hail expressions whose types match, in order, the ; table’s key types.; Each expression in the list vds_key has the following symbols in; scope:. s (String): sample ID; sa: sample annotations. The root and expr arguments. Note; One of root or expr is required, but not both. The expr parameter expects an annotation expression involving sa (the existing ; sample annotations in the dataset) and table (a struct containing the columns in ; the table), like sa.col1 = table.col1, sa.col2 = table.col2 or sa = merge(sa, table).; The root parameter expects an annotation path beginning in sa, like sa.annotations.; Passing root='sa.annotations' is exactly the same as passing expr='sa.annotations = table'.; expr has the following symbols in scope:. sa: sample annotations; table: See note. Note; The value of table inside root/expr depends on the number of values in the key table, ; as well as the product argument. There are three behaviors based on the number of values; and one branch for product being true and false, for a total of six modes:. Number of value columns; product; Type of table; Value of table. More than 2; False; Struct; Struct with an element for each column. 1; False; T; The value column. 0; False; Boolean; Existence of any matching key. More than 2; True; Array[Struct]; An array with a struct for each matching key. 1; True; Array[T]; An array with a value for each matching key. 0; True; Int; The number of matching keys. Common uses for the expr argument; Put annotations on the top level under sa; expr='sa = merge(sa, table)'. Annotate only specific annotations from the table; expr='sa.annotations = select(table, toKeep1, toKeep2, toKeep3)'. The above is equivalent to; expr='''sa.annotations.toKeep1 = table.toKeep1,; sa.annotations.toKeep2 = table.toKeep2,; sa.annotations.toKeep3 = table.toKeep3'''. Finally, for mor",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Modifiability,variab,variable,"],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotation to add to global; annotation_type (Type) – Hail type of annotation. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global_expr(expr)[source]¶; Annotate global with expression.; Example; Annotate global with an array of populations:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. global: global annotations. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_samples_expr(expr)[source]¶; Annotate samples with expression.; Examples; Compute per-sample GQ statistics for hets:; >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:; >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).co",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Performance,load,load,"﻿. . VariantDataset — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; VariantDataset. View page source. VariantDataset¶. class hail.VariantDataset(hc, jvds)[source]¶; Hail’s primary representation of genomic data, a matrix keyed by sample and variant.; Variant datasets may be generated from other formats using the HailContext import methods,; constructed from a variant-keyed KeyTable using VariantDataset.from_table(),; and simulated using balding_nichols_model().; Once a variant dataset has been written to disk with write(),; use read() to load the variant dataset into the environment.; >>> vds = hc.read(""data/example.vds""). Variables:hc (HailContext) – Hail Context. Attributes. colkey_schema; Returns the signature of the column key (sample) contained in this VDS. genotype_schema; Returns the signature of the genotypes contained in this VDS. global_schema; Returns the signature of the global annotations contained in this VDS. globals; Return global annotations as a Python object. num_samples; Number of samples. rowkey_schema; Returns the signature of the row key (variant) contained in this VDS. sample_annotations; Return a dict of sample annotations. sample_ids; Return sampleIDs. sample_schema; Returns the signature of the sample annotations contained in this VDS. variant_schema; Returns the signature of the variant annotations contained in this VDS. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate by user-defined key and aggregation expressions to produce a KeyTable. annotate_alleles_expr; Annotate alleles with expression. annotate_genotypes_expr; Annotate genotypes with expression. annotate_global; Add global annotat",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Safety,predict,predicting," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Security,access,accessed,"e resulting genotype schema is not TGenotype,; subsequent function calls on the annotated variant dataset may not work such as; pca() and linreg().; Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to TGenotype.; Genotypes are immutable. For example, if g is initially of type Genotype, the expression; g.gt = g.gt + 1 will return a Struct with one field gt of type Int and NOT a Genotype; with the gt incremented by 1. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global(path, annotation, annotation_type)[source]¶; Add global annotations from Python objects.; Examples; Add populations as a global annotation:; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotation to add to global; annotation_type (Type) – Hail type of annotation. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global_expr(expr)[source]¶; Annotate global with expression.; Example; Annotate global with an array of populations:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. global: global annotations. Parameters:expr (str or list of str) – Annotation expression. Returns:Annota",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Testability,test,test,"rix (GRM). hardcalls; Drop all genotype fields except the GT field. ibd; Compute matrix of identity-by-descent estimations. ibd_prune; Prune samples from the VariantDataset based on ibd() PI_HAT measures of relatedness. impute_sex; Impute sex of samples by calculating inbreeding coefficient on the X chromosome. join; Join two variant datasets. ld_matrix; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. ld_prune; Prune variants in linkage disequilibrium (LD). linreg; Test each variant for association using linear regression. linreg3; Test each variant for association with multiple phenotypes using linear regression. linreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the linear regression model. linreg_multi_pheno; Test each variant for association with multiple phenotypes using linear regression. lmmreg; Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. logreg; Test each variant for association using logistic regression. logreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the logistic regression model. make_table; Produce a key with one row per variant and one or more columns per sample. mendel_errors; Find Mendel errors; count per variant, individual and nuclear family. min_rep; Gives minimal, left-aligned representation of alleles. naive_coalesce; Naively descrease the number of partitions. num_partitions; Number of partitions. pc_relate; Compute relatedness estimates between individuals using a variant of the PC-Relate method. pca; Run Principal Component Analysis (PCA) on the matrix of genotypes. persist; Persist this variant dataset to memory and/or disk. query_genotypes; Performs aggregation queries over genotypes, and returns Python object(s). query_genotypes_typed; Performs",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Usability,simpl,simplest,"aGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.;",MatchSource.WIKI,docs/0.1/hail.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html
Testability,test,tests,"﻿. . Contents — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Contents. View page source. Contents¶. Getting Started; Running Hail locally; Building Hail from source; BLAS and LAPACK; Running the tests. Overview; Variant Dataset (VDS); Expressions. Tutorials; Hail Overview; Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Language Constructs; Operators; Types; Functions. Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Database Query; Documentation; Important Notes; Suggest additions or edits. Other Resources; Hadoop Glob Patterns; SQL. Indices and tables¶. Index; Search Page. Next . © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/index.html
Integrability,depend,depending,"﻿. . Language Constructs — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Language Constructs. View page source. Language Constructs¶. va.foo = 5 + va.bar. Annotation expression. Bind variable va.foo to the result of evaluating 5 + va.bar. if (p) a else b. The value of the conditional is the value of a or b depending on p. If p is missing, the value of the conditional is missing.; if (5 % 2 == 0) 5 else 7; 7. if (5 > NA: Int) 5 else 7; NA: Int. let v1 = e1 and v2 = e2 and … and vn = en in b. Bind variables v1 through vn to result of evaluating the ei. The value of the let is the value of b. v1 is visible in e2 through en, etc.; let v1 = 5 and v2 = 7 and v3 = 2 in v1 * v2 * v3; 70. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/language_constructs.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/language_constructs.html
Modifiability,variab,variable,"﻿. . Language Constructs — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Language Constructs. View page source. Language Constructs¶. va.foo = 5 + va.bar. Annotation expression. Bind variable va.foo to the result of evaluating 5 + va.bar. if (p) a else b. The value of the conditional is the value of a or b depending on p. If p is missing, the value of the conditional is missing.; if (5 % 2 == 0) 5 else 7; 7. if (5 > NA: Int) 5 else 7; NA: Int. let v1 = e1 and v2 = e2 and … and vn = en in b. Bind variables v1 through vn to result of evaluating the ei. The value of the let is the value of b. v1 is visible in e2 through en, etc.; let v1 = 5 and v2 = 7 and v3 = 2 in v1 * v2 * v3; 70. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/language_constructs.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/language_constructs.html
Modifiability,variab,variable," operand by the right (modulus). 7 % 2; 1. 10 % 4; 2. // – Floor division - division that results into whole number adjusted to the left in the number line. 7 // 2; 3. -7 // 2; -4. Array[Numeric]¶; If one of the two operands is a scalar, the operation will be applied to each element of the Array. If both operands are Arrays, the operation will be applied positionally. This will fail if the array dimensions do not match. + – Add two operands. [1, 2, 3] + [1, 1, 1]; [2, 3, 4]. [2, 0, 1] + 5; [7, 5, 6]. - – Subtract right operand from the left. [1, 2, 3] - [1, 1, 1]; [0, 1, 2]. [2, 0, 1] - 5; [-3, -5, -4]. 3 - [2, 4, 5]; [1, -1, -2]. * – Multiply two operands. [1, 2, 3] * [1, 1, 1]; [1, 2, 3]. [2, 0, 1] * 5; [10, 0, 5]. / – Divide left operand by the right one. Always results in a Double. [1, 2, 3] / [1, 4, 9]; [1.0, 0.5, 0.333]. [2, 0, 1] / 5; [0.4, 0.0, 0.2]. 5 / [2, 4, 1]; [2.5, 1.25, 5.0]. Comparison¶. == – True if the left operand is equal to the right operand. [1, 2, 3] == [1, 2, 3]; true. != – True if the left operand is not equal to the right operand. [1, 2, 3] != [4, 5, 6]; true. < – True if the left operand is less than the right operand. 5 < 3; False. <= – True if the left operand is less than or equal to the right operand. 3 <= 5; True. > – True if the left operand is greater than the right operand. 7 > 2; True. >= – True if the left operand is greater than or equal to the right operand. 3 >= 9; False. ~ – True if a regular expression pattern matches the target string. ""1KG"" ~ ""Cohort_1KG_NA12878""; True. Logical¶. && – True if both the left and right operands are true. (5 >= 3) && (2 < 10); True. || – True if at least one operand is true. (5 <= 3) || (2 < 10); True. ! – Negates a boolean variable. Returns false if the variable is true and true if the variable is false. !(5 >= 3); False. String¶. + – Concatenate two strings together. ""a"" + ""b""; ""ab"". Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/operators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/operators.html
Energy Efficiency,efficient,efficient," importing genotype data from a standard file format such as VCF, PLINK Binary files, GEN, or BGEN files into Hail’s Variant Dataset format.; Next, samples and variants are annotated with additional meta-information such as phenotype for samples and functional consequence for variants.; Samples, variants, and genotypes are filtered from the dataset based on expressions constructed using Hail’s Domain-Specific Language.; Once the dataset has been cleaned, various analytic methods such as PCA and logistic regression are used to find genetic associations.; Lastly, data is exported to a variety of file formats. Variant Dataset (VDS)¶. Hail represents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of co",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
Integrability,depend,dependent,",; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by va",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
Modifiability,variab,variable,"sents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to exp",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
Security,access,access,"hon API; Annotation Database; Other Resources. Hail. Docs »; Overview. View page source. Overview¶; A typical workflow in Hail begins with importing genotype data from a standard file format such as VCF, PLINK Binary files, GEN, or BGEN files into Hail’s Variant Dataset format.; Next, samples and variants are annotated with additional meta-information such as phenotype for samples and functional consequence for variants.; Samples, variants, and genotypes are filtered from the dataset based on expressions constructed using Hail’s Domain-Specific Language.; Once the dataset has been cleaned, various analytic methods such as PCA and logistic regression are used to find genetic associations.; Lastly, data is exported to a variety of file formats. Variant Dataset (VDS)¶. Hail represents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside ",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
Testability,log,logistic,"riables to Methods; Filtering; Add New Annotations; Computed From Existing Annotations; Variant Annotation Computed from a Genotype Aggregable (gs); Sample Annotation Computed from a Genotype Aggregable (gs); Global Annotation Computed from a Sample Aggregable (samples); Global Annotation Computed from a Variant Aggregable (variants). Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Overview. View page source. Overview¶; A typical workflow in Hail begins with importing genotype data from a standard file format such as VCF, PLINK Binary files, GEN, or BGEN files into Hail’s Variant Dataset format.; Next, samples and variants are annotated with additional meta-information such as phenotype for samples and functional consequence for variants.; Samples, variants, and genotypes are filtered from the dataset based on expressions constructed using Hail’s Domain-Specific Language.; Once the dataset has been cleaned, various analytic methods such as PCA and logistic regression are used to find genetic associations.; Lastly, data is exported to a variety of file formats. Variant Dataset (VDS)¶. Hail represents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this",MatchSource.WIKI,docs/0.1/overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/overview.html
Energy Efficiency,efficient,efficiently,"nal” part means that the data is managed by an entity outside Hive (and; Impala). The table schema is read from one of the Parquet files in the VDS file; hierarchy.; To generate a Hive file:. Copy a VCF file into HDFS. $ hadoop fs -put src/test/resources/sample.vcf.bgz sample.vcf.bgz. Convert the VCF file into a VDS using Hail:; >>> hc.import_vcf(""sample.vcf.bgz"").write(""sample.vds"", parquet_genotypes=True). Note the use of parquet_genotypes=True, which writes the genotype; information using Parquet structures, rather than an opaque binary; representation that cannot be queried using SQL. Register the VDS as a Hive table. $ PARQUET_DATA_FILE=$(hadoop fs -stat '%n' hdfs:///user/$USER/sample.vds/rdd.parquet/*.parquet | head -1); $ impala-shell -q ""CREATE EXTERNAL TABLE variants LIKE PARQUET 'hdfs:///user/$USER/sample.vds/rdd.parquet/$PARQUET_DATA_FILE' STORED AS PARQUET LOCATION 'hdfs:///user/$USER/sample.vds/rdd.parquet'"". It is good practice to run Impala’s COMPUTE STATS command on the newly-created table, so that subsequent queries run efficiently.; $ impala-shell -q ""COMPUTE STATS variants"". Before running any queries it’s worth understanding the table schema, which is easily; done by calling DESCRIBE on the table:; $ impala-shell -q ""DESCRIBE variants"". +-------------+----------------------------------+-----------------------------+; | name | type | comment |; +-------------+----------------------------------+-----------------------------+; | variant | struct< | Inferred from Parquet file. |; | | contig:string, | |; | | start:int, | |; | | ref:string, | |; | | altalleles:array<struct< | |; | | ref:string, | |; | | alt:string | |; | | >> | |; | | > | |; | annotations | struct< | Inferred from Parquet file. |; | | rsid:string, | |; | | qual:double, | |; | | filters:array<string>, | |; | | pass:boolean, | |; | | info:struct< | |; | | negative_train_site:boolean, | |; | | hwp:double, | |; | | ac:array<int>, | |; ...; | | > | |; | | > | |; | gs | array<struct< | Infer",MatchSource.WIKI,docs/0.1/sql.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/sql.html
Security,access,access,"﻿. . Querying using SQL — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources; Hadoop Glob Patterns; SQL; Impala. Hail. Docs »; Other Resources »; Querying using SQL. View page source. Querying using SQL¶; Since Hail uses the Parquet file format for data storage, a Hail VDS can be queried using; Hadoop SQL tools, like Hive or Impala. This mode of access may be convenient for users; who have ad hoc queries that they are able to express in SQL.; Note that SQL access is read-only: it is not possible to write Hail datasets using; SQL at the current time. Impala¶; Each VDS should be registered in the Hive metastore to allow Impala to query it (Impala uses Hive’s metastore to store table metadata). This is done by creating an external table in Hive, the “external” part means that the data is managed by an entity outside Hive (and; Impala). The table schema is read from one of the Parquet files in the VDS file; hierarchy.; To generate a Hive file:. Copy a VCF file into HDFS. $ hadoop fs -put src/test/resources/sample.vcf.bgz sample.vcf.bgz. Convert the VCF file into a VDS using Hail:; >>> hc.import_vcf(""sample.vcf.bgz"").write(""sample.vds"", parquet_genotypes=True). Note the use of parquet_genotypes=True, which writes the genotype; information using Parquet structures, rather than an opaque binary; representation that cannot be queried using SQL. Register the VDS as a Hive table. $ PARQUET_DATA_FILE=$(hadoop fs -stat '%n' hdfs:///user/$USER/sample.vds/rdd.parquet/*.parquet | head -1); $ impala-shell -q ""CREATE EXTERNAL TABLE variants LIKE PARQUET 'hdfs:///user/$USER/sample.vds/rdd.parquet/$PARQUET_DATA_FILE' STORED AS PARQUET LOCATION 'hdfs:///user/$USER/sample.vds/rdd.parquet'"". It is good practice to run Impala’s COMPUTE STATS command on the newly-created table, so that subsequent queries run efficiently.; $",MatchSource.WIKI,docs/0.1/sql.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/sql.html
Testability,test,test,"ls; Expression Language; Python API; Annotation Database; Other Resources; Hadoop Glob Patterns; SQL; Impala. Hail. Docs »; Other Resources »; Querying using SQL. View page source. Querying using SQL¶; Since Hail uses the Parquet file format for data storage, a Hail VDS can be queried using; Hadoop SQL tools, like Hive or Impala. This mode of access may be convenient for users; who have ad hoc queries that they are able to express in SQL.; Note that SQL access is read-only: it is not possible to write Hail datasets using; SQL at the current time. Impala¶; Each VDS should be registered in the Hive metastore to allow Impala to query it (Impala uses Hive’s metastore to store table metadata). This is done by creating an external table in Hive, the “external” part means that the data is managed by an entity outside Hive (and; Impala). The table schema is read from one of the Parquet files in the VDS file; hierarchy.; To generate a Hive file:. Copy a VCF file into HDFS. $ hadoop fs -put src/test/resources/sample.vcf.bgz sample.vcf.bgz. Convert the VCF file into a VDS using Hail:; >>> hc.import_vcf(""sample.vcf.bgz"").write(""sample.vds"", parquet_genotypes=True). Note the use of parquet_genotypes=True, which writes the genotype; information using Parquet structures, rather than an opaque binary; representation that cannot be queried using SQL. Register the VDS as a Hive table. $ PARQUET_DATA_FILE=$(hadoop fs -stat '%n' hdfs:///user/$USER/sample.vds/rdd.parquet/*.parquet | head -1); $ impala-shell -q ""CREATE EXTERNAL TABLE variants LIKE PARQUET 'hdfs:///user/$USER/sample.vds/rdd.parquet/$PARQUET_DATA_FILE' STORED AS PARQUET LOCATION 'hdfs:///user/$USER/sample.vds/rdd.parquet'"". It is good practice to run Impala’s COMPUTE STATS command on the newly-created table, so that subsequent queries run efficiently.; $ impala-shell -q ""COMPUTE STATS variants"". Before running any queries it’s worth understanding the table schema, which is easily; done by calling DESCRIBE on the table:; $ ",MatchSource.WIKI,docs/0.1/sql.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/sql.html
Availability,down,download,"﻿. . Tutorials — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to c",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
Modifiability,variab,variables,"﻿. . Tutorials — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to c",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
Testability,test,test,"te sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by population stratification. Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Ra",MatchSource.WIKI,docs/0.1/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html
Availability,down,downcoded,"oat – Convert value to a Float.; toInt(): Int – Convert value to an Integer.; toLong(): Long – Convert value to a Long. Float¶. abs(): Float – Returns the absolute value of a number.; max(a: Float): Float – Returns the maximum value.; min(a: Float): Float – Returns the minimum value.; signum(): Int – Returns the sign of a number (1, 0, or -1).; toDouble(): Double – Convert value to a Double.; toFloat(): Float – Convert value to a Float.; toInt(): Int – Convert value to an Integer.; toLong(): Long – Convert value to a Long. Genotype¶; A Genotype is a Hail data type representing a genotype in the Variant Dataset. It is referred to as g in the expression language. ad: Array[Int] – allelic depth for each allele. call(): Call – the integer gt = k*(k+1)/2 + j for call j/k (0 = 0/0, 1 = 0/1, 2 = 1/1, 3 = 0/2, etc.). dosage: Double – the expected number of non-reference alleles based on genotype probabilities. dp: Int – the total number of informative reads. fakeRef: Boolean – True if this genotype was downcoded in split_multi(). This can happen if a 1/2 call is split to 0/1, 0/1. fractionReadsRef(): Double – the ratio of ref reads to the sum of all informative reads. gp: Array[Double] – the linear-scaled probabilities. gq: Int – the difference between the two smallest PL entries. gt: Int – the integer gt = k*(k+1)/2 + j for call j/k (0 = 0/0, 1 = 0/1, 2 = 1/1, 3 = 0/2, etc.). gtj(): Int – the index of allele j for call j/k (0 = ref, 1 = first alt allele, etc.). gtk(): Int – the index of allele k for call j/k (0 = ref, 1 = first alt allele, etc.). isCalled(): Boolean – True if the genotype is not ./.. isCalledNonRef(): Boolean – True if either g.isHet or g.isHomVar is true. isHet(): Boolean – True if this call is heterozygous. isHetNonRef(): Boolean – True if this call is j/k with j>0. isHetRef(): Boolean – True if this call is 0/k with k>0. isHomRef(): Boolean – True if this call is 0/0. isHomVar(): Boolean – True if this call is j/j with j>0. isLinearScale: Boolean – True ",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
Integrability,depend,depending," HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Aggregable; Aggregable[Array[Double]]; Aggregable[Array[Float]]; Aggregable[Array[Int]]; Aggregable[Array[Long]]; Aggregable[Double]; Aggregable[Float]; Aggregable[Genotype]; Aggregable[Int]; Aggregable[Long]; Aggregable[T]. AltAllele; Array; Array[Array[T]]; Array[Boolean]; Array[Double]; Array[Float]; Array[Int]; Array[Long]; Array[String]; Array[T]. Boolean; Call; Dict; Double; Float; Genotype; Int; Interval; Locus; Long; Set; Set[Double]; Set[Float]; Set[Int]; Set[Long]; Set[Set[T]]; Set[String]; Set[T]. String; Struct; Variant. Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Types. View page source. Types¶. Aggregable¶; An Aggregable is a Hail data type representing a distributed row or column of a matrix. Hail exposes a number of methods to compute on aggregables depending on the data type. Aggregable[Array[Double]]¶. sum(): Array[Double] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Float]]¶. sum(): Array[Float] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Int]]¶. sum(): Array[Int]. Compute the sum by index. All elements in the aggregable must have the same length.; Examples; Count the total number of occurrences of each allele across samples, per variant:; >>> vds_result = vds.annotate_variants_expr('va.AC = gs.map(g => g.oneHotAlleles(v)).sum()'). Aggregable[Array[Long]]¶. sum(): Array[Long] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Double]¶. hist(start: Double, end: Double, bins: Int): Struct{binEdges:Array[Double],binFrequencies:Array[Long],nLess:Long,nGreater:Long}. binEdges (Array[Double]) – Array of bin cutoffs; binFrequencies (Array[Long]) – Number of e",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
Modifiability,variab,variable," vds.annotate_variants_expr('va.AC = gs.map(g => g.oneHotAlleles(v)).sum()'). Aggregable[Array[Long]]¶. sum(): Array[Long] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Double]¶. hist(start: Double, end: Double, bins: Int): Struct{binEdges:Array[Double],binFrequencies:Array[Long],nLess:Long,nGreater:Long}. binEdges (Array[Double]) – Array of bin cutoffs; binFrequencies (Array[Long]) – Number of elements that fall in each bin.; nLess (Long) – Number of elements less than the minimum bin; nGreater (Long) – Number of elements greater than the maximum bin. Compute frequency distributions of numeric parameters.; Examples; Compute GQ-distributions per variant:; >>> vds_result = vds.annotate_variants_expr('va.gqHist = gs.map(g => g.gq).hist(0, 100, 20)'). Compute global GQ-distribution:; >>> gq_hist = vds.query_genotypes('gs.map(g => g.gq).hist(0, 100, 100)'). Notes. The start, end, and bins params are no-scope parameters, which means that while computations like 100 / 4 are acceptable, variable references like global.nBins are not.; Bin size is calculated from (end - start) / bins; (bins + 1) breakpoints are generated from the range (start to end by binsize); Each bin is left-inclusive, right-exclusive except the last bin, which includes the maximum value. This means that if there are N total bins, there will be N + 1 elements in binEdges. For the invocation hist(0, 3, 3), binEdges would be [0, 1, 2, 3] where the bins are [0, 1), [1, 2), [2, 3]. Arguments. start (Double) – Starting point of first bin; end (Double) – End point of last bin; bins (Int) – Number of bins to create. max(): Double – Compute the maximum of all non-missing elements. The empty max is missing. min(): Double – Compute the minimum of all non-missing elements. The empty min is missing. product(): Double – Compute the product of all non-missing elements. The empty product is one. stats(): Struct{mean:Double,stdev:Double,min:Double,max:Double,nNotMissing:",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
Security,expose,exposes," HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Aggregable; Aggregable[Array[Double]]; Aggregable[Array[Float]]; Aggregable[Array[Int]]; Aggregable[Array[Long]]; Aggregable[Double]; Aggregable[Float]; Aggregable[Genotype]; Aggregable[Int]; Aggregable[Long]; Aggregable[T]. AltAllele; Array; Array[Array[T]]; Array[Boolean]; Array[Double]; Array[Float]; Array[Int]; Array[Long]; Array[String]; Array[T]. Boolean; Call; Dict; Double; Float; Genotype; Int; Interval; Locus; Long; Set; Set[Double]; Set[Float]; Set[Int]; Set[Long]; Set[Set[T]]; Set[String]; Set[T]. String; Struct; Variant. Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Types. View page source. Types¶. Aggregable¶; An Aggregable is a Hail data type representing a distributed row or column of a matrix. Hail exposes a number of methods to compute on aggregables depending on the data type. Aggregable[Array[Double]]¶. sum(): Array[Double] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Float]]¶. sum(): Array[Float] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Int]]¶. sum(): Array[Int]. Compute the sum by index. All elements in the aggregable must have the same length.; Examples; Count the total number of occurrences of each allele across samples, per variant:; >>> vds_result = vds.annotate_variants_expr('va.AC = gs.map(g => g.oneHotAlleles(v)).sum()'). Aggregable[Array[Long]]¶. sum(): Array[Long] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Double]¶. hist(start: Double, end: Double, bins: Int): Struct{binEdges:Array[Double],binFrequencies:Array[Long],nLess:Long,nGreater:Long}. binEdges (Array[Double]) – Array of bin cutoffs; binFrequencies (Array[Long]) – Number of e",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
Testability,test,test,"ection.; median(): Long – Median value of the collection.; min(): Long – Smallest element in the collection.; product(): Long – Product of all elements in the collection (returns 1 if empty).; sum(): Long – Sum of all elements in the collection. Set[Set[T]]¶. flatten(): Set[T]. Flattens a nested set by concatenating all its elements into a single set.; let s = [[1, 2].toSet(), [3, 4].toSet()].toSet() in s.flatten(); result: Set(1, 2, 3, 4). Set[String]¶. mkString(delimiter: String): String. Concatenates all elements of this set into a single string where each element is separated by the delimiter.; let s = [1, 2, 3].toSet() in s.mkString("",""); result: ""1,2,3"". Arguments. delimiter (String) – String that separates each element. Set[T]¶. add(a: T): Set[T] – Returns the result of adding the element a to this Set. contains(x: T): Boolean. Returns true if the element x is contained in the set, otherwise false.; let s = [1, 2, 3].toSet() in s.contains(5); result: false. Arguments. x (T) – Value to test. difference(a: Set[T]): Set[T] – Returns the elements of this Set that are not in Set a. exists(expr: T => Boolean): Boolean. Returns a boolean which is true if any element in the set satisfies the condition given by expr and false otherwise.; let s = [0, 2, 4, 6, 8, 10].toSet() in s.exists(e => e % 2 == 1); result: false. Arguments. expr (T => Boolean) – Lambda expression. filter(expr: T => Boolean): Set[T]. Returns a new set subsetted to the elements where expr evaluates to true.; let s = [1, 4, 5, 6, 10].toSet() in s.filter(e => e >= 5); result: Set(5, 6, 10). Arguments. expr (T => Boolean) – Lambda expression. find(expr: T => Boolean): T. Returns the first non-missing element of the array for which expr is true. If no element satisfies the predicate, find returns NA.; let s = [1, 2, 3].toSet() in s.find(e => e % 3 == 0); result: 3. Arguments. expr (T => Boolean) – Lambda expression. flatMap(expr: T => Set[U]): Set[U]. Returns a new set by applying a function to each subs",MatchSource.WIKI,docs/0.1/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/types.html
Availability,down,downsample,"ords of expr. fraction(predicate); Compute the fraction of records where predicate is True. hardy_weinberg_test(expr[, one_sided]); Performs test of Hardy-Weinberg equilibrium. explode(f, array_agg_expr); Explode an array or set expression to aggregate the elements of all records. filter(condition, aggregation); Filter records according to a predicate. inbreeding(expr, prior); Compute inbreeding statistics on calls. call_stats(call, alleles); Compute useful call statistics. info_score(gp); Compute the IMPUTE information score. hist(expr, start, end, bins); Compute binned counts of a numeric expression. linreg(y, x[, nested_dim, weight]); Compute multivariate linear regression statistics. corr(x, y); Computes the Pearson correlation coefficient between x and y. group_by(group, agg_expr); Compute aggregation statistics stratified by one or more groups. array_agg(f, array); Aggregate an array element-wise using a user-specified aggregation function. downsample(x, y[, label, n_divisions]); Downsample (x, y) coordinate datapoints. approx_cdf(expr[, k, _raw]); Produce a summary of the distribution of values. hail.expr.aggregators.collect(expr)[source]; Collect records into an array.; Examples; Collect the ID field where HT is greater than 68:; >>> table1.aggregate(hl.agg.filter(table1.HT > 68, hl.agg.collect(table1.ID))); [2, 3]. Notes; The element order of the resulting array is not guaranteed, and in some; cases is non-deterministic.; Use collect_as_set() to collect unique items. Warning; Collecting a large number of items can cause out-of-memory exceptions. Parameters:; expr (Expression) – Expression to collect. Returns:; ArrayExpression – Array of all expr records. hail.expr.aggregators.collect_as_set(expr)[source]; Collect records into a set.; Examples; Collect the unique ID field where HT is greater than 68:; >>> table1.aggregate(hl.agg.filter(table1.HT > 68, hl.agg.collect_as_set(table1.ID))); {2, 3}. Note that when collecting a set-typed field with collect_as_set",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
Deployability,update,updated,"alues to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value values(i) occupying indices ranks(i) (inclusive) to; ranks(i+1) (exclusive).; The returned struct also contains an array _compaction_counts, which is; used internally to support downstream error estimation. Warning; This is an approximate and nondeterministic method. Parameters:. expr (Expression) – Expression to collect.; k (int) – Parameter controlling the accuracy vs. memory usage tradeoff. Returns:; StructExpression – Struct containing values and ranks arrays. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
Integrability,depend,dependent,"he effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; beta (tarray of tfloat64):; Estimated regression coefficient for each covariate.; standard_error (tarray of tfloat64):; Estimated standard error for each covariate.; t_stat (tarray of tfloat64):; t-statistic for each covariate.; p_value (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be include",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
Modifiability,variab,variable," (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be included in the; calculation.; In the case that there are no non-missing pairs, the result will be missing. See also; linreg(). Parameters:. x (Expression of type tfloat64); y (Expression of type tfloat64). Returns:; Float64Expression. hail.expr.aggregators.group_by(group, agg_expr)[source]; Compute aggregation statistics stratified ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
Performance,perform,performs,":; Expression of type tint64 or tfloat64 – Product of records of expr. hail.expr.aggregators.fraction(predicate)[source]; Compute the fraction of records where predicate is True.; Examples; Compute the fraction of rows where SEX is “F” and HT > 65:; >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; Missing values for predicate are treated as False. Parameters:; predicate (BooleanExpression) – Boolean predicate. Returns:; Expression of type tfloat64 – Fraction of records where predicate is True. hail.expr.aggregators.hardy_weinberg_test(expr, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; Test each row of a dataset:; >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
Security,expose,exposed,"﻿. Hail | ; Aggregators. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Aggregators. View page source. Aggregators; The aggregators module is exposed as hl.agg, e.g. hl.agg.sum. collect(expr); Collect records into an array. collect_as_set(expr); Collect records into a set. count(); Count the number of records. count_where(condition); Count the number of records where a predicate is True. counter(expr, *[, weight]); Count the occurrences of each unique record and return a dictionary. any(condition); Returns True if condition is True for any record. all(condition); Returns True if condition is True for every record. take(expr, n[, ordering]); Take n records of expr, optionally ordered by ordering. min(expr); Compute the minimum expr. max(expr); Compute the maximum expr. sum(expr); Compute the sum of all records of expr. array_sum(expr); Compute the coordinate-wise sum of all records of expr. mean(expr); Compute the mean value of records of expr. approx_quantiles(expr, qs[, k]); Compute an array of approximate quantiles. approx_median(expr[, k]); Compute the approximate median. stats(expr); Compute a number of useful statistics about expr. product(expr); Compute the product of all records of expr. fraction(predicate); Compute the fraction of records where predicate is True. hardy_weinberg_test(expr[, one_sided]); Performs test of Hardy-Weinberg equilibrium. explode(f, array_agg_expr); Explode an array or set ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
Testability,test,test,"); Count the number of records where a predicate is True. counter(expr, *[, weight]); Count the occurrences of each unique record and return a dictionary. any(condition); Returns True if condition is True for any record. all(condition); Returns True if condition is True for every record. take(expr, n[, ordering]); Take n records of expr, optionally ordered by ordering. min(expr); Compute the minimum expr. max(expr); Compute the maximum expr. sum(expr); Compute the sum of all records of expr. array_sum(expr); Compute the coordinate-wise sum of all records of expr. mean(expr); Compute the mean value of records of expr. approx_quantiles(expr, qs[, k]); Compute an array of approximate quantiles. approx_median(expr[, k]); Compute the approximate median. stats(expr); Compute a number of useful statistics about expr. product(expr); Compute the product of all records of expr. fraction(predicate); Compute the fraction of records where predicate is True. hardy_weinberg_test(expr[, one_sided]); Performs test of Hardy-Weinberg equilibrium. explode(f, array_agg_expr); Explode an array or set expression to aggregate the elements of all records. filter(condition, aggregation); Filter records according to a predicate. inbreeding(expr, prior); Compute inbreeding statistics on calls. call_stats(call, alleles); Compute useful call statistics. info_score(gp); Compute the IMPUTE information score. hist(expr, start, end, bins); Compute binned counts of a numeric expression. linreg(y, x[, nested_dim, weight]); Compute multivariate linear regression statistics. corr(x, y); Computes the Pearson correlation coefficient between x and y. group_by(group, agg_expr); Compute aggregation statistics stratified by one or more groups. array_agg(f, array); Aggregate an array element-wise using a user-specified aggregation function. downsample(x, y[, label, n_divisions]); Downsample (x, y) coordinate datapoints. approx_cdf(expr[, k, _raw]); Produce a summary of the distribution of values. hail.expr.aggr",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
Usability,intuit,intuition,"alues to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value values(i) occupying indices ranks(i) (inclusive) to; ranks(i+1) (exclusive).; The returned struct also contains an array _compaction_counts, which is; used internally to support downstream error estimation. Warning; This is an approximate and nondeterministic method. Parameters:. expr (Expression) – Expression to collect.; k (int) – Parameter controlling the accuracy vs. memory usage tradeoff. Returns:; StructExpression – Struct containing values and ranks arrays. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/aggregators.html
Availability,avail,available,"ion Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:.",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
Deployability,pipeline,pipelines,"﻿. Hail | ; Annotation Database. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the pan",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
Energy Efficiency,charge,charges,"ion Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:.",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
Security,access,accessible,"﻿. Hail | ; Annotation Database. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the pan",MatchSource.WIKI,docs/0.2/annotation_database_ui.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html
Availability,avail,available,"o my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global rando",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
Deployability,configurat,configuration,"mentation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
Integrability,interface,interface,"﻿. Hail | ; Hail Query Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Table; GroupedTable; MatrixTable; GroupedMatrixTable. Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions; init(); asc(); desc(); stop(); spark_context(); tmp_dir(); default_reference(); get_reference(); set_global_seed(); reset_global_randomness(); citation(); version(). hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API. View page source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
Modifiability,config,configure," source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl;",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
Performance,load,loaded,"bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Name of a previously loaded reference genome or one of Hail’s built-in; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'. Returns:; ReferenceGenome. hail.set_global_seed(seed)[source]; Deprecated.; Has no effect. To ensure reproducible randomness, use the global_seed; argument to init() and reset_global_randomness().; See the random functions reference docs for more. Parameters:; seed (int) – Integer used to seed Hail’s random number generator. hail.reset_global_randomness()[source]; Restore global randomness to initial state for test reproducibility. hail.citation(*, bibtex=False)[source]; Generate a Hail citation. Parameters:; bibtex (bool) – Generate a citation in BibTeX form. Returns:; str. hail.version()[source]; Get the installed Hail version. Returns:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
Security,access,access," Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Table; GroupedTable; MatrixTable; GroupedMatrixTable. Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions; init(); asc(); desc(); stop(); spark_context(); tmp_dir(); default_reference(); get_reference(); set_global_seed(); reset_global_randomness(); citation(); version(). hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API. View page source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
Testability,log,log," source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl;",MatchSource.WIKI,docs/0.2/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/api.html
Availability,avail,available,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
Deployability,deploy,deploy,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
Usability,learn,learn,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/batch_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/batch_api.html
Availability,error,error,"ary version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some da",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Deployability,release,released," Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Energy Efficiency,reduce,reduce," already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_fro",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Integrability,depend,depend,"ped to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that constru",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Modifiability,config,configuration,"submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Performance,perform,performance,ckMatrix from_numpy correctness bug; Bug fixes; Versioning. Version 0.2.89; Version 0.2.88; Version 0.2.87; Bug fixes. Version 0.2.86; Bug fixes; Performance improvements. Version 0.2.85; Bug fixes; New features. Version 0.2.84; Bug fixes; New features. Version 0.2.83; Bug fixes; New features; hailctl dataproc. Version 0.2.82; Bug fixes; New features; Performance Improvements; Python and Java Support; File Format. Version 0.2.81; hailctl dataproc. Version 0.2.80; New features; hailctl dataproc. Version 0.2.79; Bug fixes; New features. Version 0.2.78; Bug fixes; New features; Performance Improvements. Version 0.2.77; Bug fixes. Version 0.2.76; Bug fixes. Version 0.2.75; Bug fixes; New features; Performance improvements. Version 0.2.74; Bug fixes. Version 0.2.73; Bug fixes. Version 0.2.72; New Features; Bug fixes. Version 0.2.71; New Features; Bug fixes; hailctl dataproc. Version 0.2.70; Version 0.2.69; New Features; Bug fixes; hailctl dataproc. Version 0.2.68; Version 0.2.67; Critical performance fix. Version 0.2.66; New features. Version 0.2.65; Default Spark Version Change; New features; Performance improvements; Bug fixes. Version 0.2.64; New features; Bug fixes. Version 0.2.63; Bug fixes; Performance Improvements. Version 0.2.62; New features; Bug fixes; Performance improvements. Version 0.2.61; New features; Bug fixes. Version 0.2.60; New features; Bug fixes; hailctl dataproc. Version 0.2.59; Datasets / Annotation DB; hailctl dataproc. Version 0.2.58; New features; Bug fixes; Performance improvements; hailctl dataproc; Deprecations. Version 0.2.57; New features. Version 0.2.56; New features; Performance; Bug fixes; hailctl dataproc. Version 0.2.55; Performance; Bug fixes; File Format. Version 0.2.54; VCF Combiner; New features; Bug fixes. Version 0.2.53; Bug fixes. Version 0.2.52; Bug fixes. Version 0.2.51; Bug fixes. Version 0.2.50; Bug fixes; New features. Version 0.2.49; Bug fixes. Version 0.2.48; Bug fixes. Version 0.2.47; Bug fixes. Version 0.2.46; Site; Bug,MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Safety,timeout,timeout,"ies; instead. Version 0.2.128; Released 2024-02-16; In GCP, the Hail Annotation DB and Datasets API have moved from; multi-regional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output pa",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Security,access,access,"ary version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some da",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Testability,log,log,"project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot r",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Usability,clear,clearly,".; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. ",MatchSource.WIKI,docs/0.2/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/change_log.html
Deployability,update,updated,"﻿. Hail | ; Cheat Sheets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Cheat Sheets. View page source. Cheat Sheets. Note; Hail’s cheat sheets are relatively new. We welcome suggestions; for additional cheatsheets, as well as feedback about our documentation. If; you’d like to add a cheatsheet to the documentation, make a pull request!. Hail Tables Cheat Sheet; Hail MatrixTables Cheat Sheet. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cheatsheets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cheatsheets.html
Usability,feedback,feedback,"﻿. Hail | ; Cheat Sheets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Cheat Sheets. View page source. Cheat Sheets. Note; Hail’s cheat sheets are relatively new. We welcome suggestions; for additional cheatsheets, as well as feedback about our documentation. If; you’d like to add a cheatsheet to the documentation, make a pull request!. Hail Tables Cheat Sheet; Hail MatrixTables Cheat Sheet. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cheatsheets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cheatsheets.html
Availability,error,erroring,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
Deployability,configurat,configuration,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
Modifiability,variab,variables,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
Security,access,access,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/configuration_reference.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html
Availability,avail,available,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets.html
Deployability,pipeline,pipeline,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets.html
Energy Efficiency,charge,charges,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets.html
Performance,load,load,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets.html
Deployability,update,updated," experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions. View page source. Expressions. eval(expression); Evaluate a Hail expression, returning the result. Expression; Base class for Hail expressions. ArrayExpression; Expression of type tarray. ArrayNumericExpression; Expression of type tarray with a numeric type. BooleanExpression; Expression of type tbool. CallExpression; Expression of type tcall. CollectionExpression; Expression of type tarray or tset. DictExpression; Expression of type tdict. IntervalExpression; Expression of type tinterval. LocusExpression; Expression of type tlocus. NumericExpression; Expression of numeric type. Int32Expression; Expression of type tint32. Int64Expression; Expression of type tint64. Float32Expression; Expression of type tfloat32. Float64Expression; Expression of type tfloat64. SetExpression; Expression of type tset. StringExpression; Expression of type tstr. StructExpression; Expression of type tstruct. TupleExpression; Expression of type ttuple. NDArrayExpression; Expression of type tndarray. NDArrayNumericExpression; Expression of type tndarray with a numeric element type. hail.expr.eval(expression)[source]; Evaluate a Hail expression, returning the result.; This method is extremely useful for learning about Hail expressions and; understanding how to compose them.; The expression must have no indices, but can refer to the globals; of a Table or MatrixTable.; Examples; Evaluate a conditional:; >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters:; expression (Expression) – Any expression, or a Python value that can be implicitly interpreted as an expression. Returns:; Any. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/expressions.html
Usability,learn,learning," experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions. View page source. Expressions. eval(expression); Evaluate a Hail expression, returning the result. Expression; Base class for Hail expressions. ArrayExpression; Expression of type tarray. ArrayNumericExpression; Expression of type tarray with a numeric type. BooleanExpression; Expression of type tbool. CallExpression; Expression of type tcall. CollectionExpression; Expression of type tarray or tset. DictExpression; Expression of type tdict. IntervalExpression; Expression of type tinterval. LocusExpression; Expression of type tlocus. NumericExpression; Expression of numeric type. Int32Expression; Expression of type tint32. Int64Expression; Expression of type tint64. Float32Expression; Expression of type tfloat32. Float64Expression; Expression of type tfloat64. SetExpression; Expression of type tset. StringExpression; Expression of type tstr. StructExpression; Expression of type tstruct. TupleExpression; Expression of type ttuple. NDArrayExpression; Expression of type tndarray. NDArrayNumericExpression; Expression of type tndarray with a numeric element type. hail.expr.eval(expression)[source]; Evaluate a Hail expression, returning the result.; This method is extremely useful for learning about Hail expressions and; understanding how to compose them.; The expression must have no indices, but can refer to the globals; of a Table or MatrixTable.; Examples; Evaluate a conditional:; >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters:; expression (Expression) – Any expression, or a Python value that can be implicitly interpreted as an expression. Returns:; Any. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/expressions.html
Availability,error,error,"ata/LCR.interval_list') . Notes; If you are copying a file just to then load it into Python, you can use; open() instead. For example:; >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: ; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hailtop.fs.exists(path, *, requester_pays_config=None)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hailtop.fs.is_dir(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hailtop.fs.is_file(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hailtop.fs.ls(path, *, requester_pays_config=None)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one element. If path is a; directory, returns an element for each file contained in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hailtop.fs.mkdir(path, *, requester_pays_config=None)[source]; Ensure files can be created whose dirname is path. Warning; On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does nothing. hailtop.fs.open(path, mode='r', buffer_size=8192, *, requester_pays_config=None)[source]; Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS.; Examp",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
Deployability,update,updated,"bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_pays_config=None)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
Performance,load,load,"uester_pays_config]); Returns True if path both exists and is a file. ls(path, *[, requester_pays_config]); Returns information about files at path. mkdir(path, *[, requester_pays_config]); Ensure files can be created whose dirname is path. open(path[, mode, buffer_size, ...]); Open a file from the local filesystem of from blob storage. remove(path, *[, requester_pays_config]); Removes the file at path. rmtree(path, *[, requester_pays_config]); Recursively remove all files under the given path. stat(path, *[, requester_pays_config]); Returns information about the file or directory at a given path. hailtop.fs.copy(src, dest, *, requester_pays_config=None)[source]; Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; If you are copying a file just to then load it into Python, you can use; open() instead. For example:; >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: ; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hailtop.fs.exists(path, *, requester_pays_config=None)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hailtop.fs.is_dir(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hailtop.fs.is_file(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hailtop.fs.ls(path, *, requester_pays_config=None)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one elem",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
Security,access,access,"﻿. Hail | ; hailtop.fs Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; Top-Level Functions; copy(); exists(); is_dir(); is_file(); ls(); mkdir(); open(); remove(); rmtree(); stat(). hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.fs Python API. View page source. hailtop.fs Python API; This is the API documentation for Hail’s cloud-agnostic file system implementation in hailtop.fs.; Use import hailtop.fs as hfs to access this functionality. Top-Level Functions. copy(src, dest, *[, requester_pays_config]); Copy a file between filesystems. exists(path, *[, requester_pays_config]); Returns True if path exists. is_dir(path, *[, requester_pays_config]); Returns True if path both exists and is a directory. is_file(path, *[, requester_pays_config]); Returns True if path both exists and is a file. ls(path, *[, requester_pays_config]); Returns information about files at path. mkdir(path, *[, requester_pays_config]); Ensure files can be created whose dirname is path. open(path[, mode, buffer_size, ...]); Open a file from the local filesystem of from blob storage. remove(path, *[, requester_pays_config]); Removes the file at path. rmtree(path, *[, requester_pays_config]); Recursively remove all files under the given path. stat(path, *[, requester_pays_config]); Returns information about the file or directory at a given path. hailtop.fs.copy(src, dest, *, requester_pays_config=None)[source]; Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file",MatchSource.WIKI,docs/0.2/fs_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/fs_api.html
Availability,checkpoint,checkpoint, hail.expr.functions). bit_and() (in module hail.expr.functions). bit_count() (in module hail.expr.functions). bit_lshift() (in module hail.expr.functions). bit_not() (in module hail.expr.functions). bit_or() (in module hail.expr.functions). bit_rshift() (in module hail.expr.functions). bit_xor() (in module hail.expr.functions). block_size (hail.linalg.BlockMatrix property). BlockMatrix (class in hail.linalg). bool() (in module hail.expr.functions). BooleanExpression (class in hail.expr). C. cache() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). calculate_phenotypes() (in module hail.experimental.ldscsim). Call (class in hail.genetics). call() (in module hail.expr.functions). call_stats() (in module hail.expr.aggregators). CallExpression (class in hail.expr). case() (in module hail.expr.functions). CaseBuilder (class in hail.expr.builders). cdf() (in module hail.plot). ceil() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). checkpoint() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). chi_squared_test() (in module hail.expr.functions). choose_cols() (hail.MatrixTable method). citation() (in module hail). coalesce() (in module hail.expr.functions). cochran_mantel_haenszel_test() (in module hail.expr.functions). col (hail.MatrixTable property). col_key (hail.MatrixTable property). col_value (hail.MatrixTable property). collect() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.BooleanExpression method). (hail.expr.CallExpression method). (hail.expr.CollectionExpression method). (hail.expr.DictExpression method). (hail.expr.Expression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
Deployability,update,updated,".functions). UNKNOWN (hail.genetics.AlleleType attribute). unpersist() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). unphase() (hail.expr.CallExpression method). unphased_diploid_gt_index() (hail.expr.CallExpression method). (hail.genetics.Call method). unphased_diploid_gt_index_call() (in module hail.expr.functions). upper() (hail.expr.StringExpression method). V. validate() (hail.vds.VariantDataset method). values() (hail.expr.DictExpression method). (hail.expr.StructExpression method). variant_qc() (in module hail.methods). variant_str() (in module hail.expr.functions). VariantDataset (class in hail.vds). VariantDatasetCombiner (class in hail.vds.combiner). vars() (in module hail.ggplot). VDSMetadata (class in hail.vds.combiner). vep() (in module hail.methods). VEPConfig (class in hail.methods). VEPConfigGRCh37Version85 (class in hail.methods). VEPConfigGRCh38Version95 (class in hail.methods). version() (in module hail). visualize_missingness() (in module hail.plot). vstack() (in module hail.nd). W. when() (hail.expr.builders.CaseBuilder method). (hail.expr.builders.SwitchBuilder method). when_missing() (hail.expr.builders.SwitchBuilder method). window() (hail.expr.LocusExpression method). write() (hail.genetics.Pedigree method). (hail.genetics.ReferenceGenome method). (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). write_expression() (in module hail.experimental). write_from_entry_expr() (hail.linalg.BlockMatrix static method). write_image() (hail.ggplot.GGPlot method). write_many() (hail.Table method). X. x_contigs (hail.genetics.ReferenceGenome property). xlab() (in module hail.ggplot). Y. y_contigs (hail.genetics.ReferenceGenome property). ylab() (in module hail.ggplot). Z. zeros() (in module hail.nd). zip() (in module hail.expr.functions). zip_with_index() (in module hail.expr.functions). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
Modifiability,extend,extend,on method). (hail.expr.Expression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.NumericExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). (hail.expr.StructExpression method). (hail.expr.TupleExpression method). (hail.linalg.BlockMatrix static method). (hail.Table method). export_bgen() (in module hail.methods). export_blocks() (hail.linalg.BlockMatrix method). export_elasticsearch() (in module hail.methods). export_entries_by_col() (in module hail.experimental). export_gen() (in module hail.methods). export_plink() (in module hail.methods). export_rectangles() (hail.linalg.BlockMatrix method). export_vcf() (in module hail.methods). Expression (class in hail.expr). extend() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). eye() (in module hail.nd). F. facet_wrap() (in module hail.ggplot). fam_id (hail.genetics.Trio property). FigureAttribute (class in hail.ggplot). fill() (hail.linalg.BlockMatrix class method). filter() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.CollectionExpression method). (hail.expr.SetExpression method). (hail.linalg.BlockMatrix method). (hail.Table method). (in module hail.expr.aggregators). (in module hail.expr.functions). filter_alleles() (in module hail.methods). filter_alleles_hts() (in module hail.methods). filter_chromosomes() (in module hail.vds). filter_cols() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). filter_entries() (hail.MatrixTable method). filter_intervals() (in module hail.methods). (in module hail.vds). filter_rows() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). filter_samples() (in module hail.vds). filter_,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
Performance,cache,cache,r.aggregators). array_windows() (in module hail.linalg.utils). ArrayExpression (class in hail.expr). ArrayNumericExpression (class in hail.expr). asc() (in module hail). ascertainment_bias() (in module hail.experimental.ldscsim). available_datasets (hail.experimental.DB property). B. balding_nichols_model() (in module hail.methods). binarize() (in module hail.experimental.ldscsim). binary_search() (in module hail.expr.functions). bind() (in module hail.expr.functions). binom_test() (in module hail.expr.functions). bit_and() (in module hail.expr.functions). bit_count() (in module hail.expr.functions). bit_lshift() (in module hail.expr.functions). bit_not() (in module hail.expr.functions). bit_or() (in module hail.expr.functions). bit_rshift() (in module hail.expr.functions). bit_xor() (in module hail.expr.functions). block_size (hail.linalg.BlockMatrix property). BlockMatrix (class in hail.linalg). bool() (in module hail.expr.functions). BooleanExpression (class in hail.expr). C. cache() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). calculate_phenotypes() (in module hail.experimental.ldscsim). Call (class in hail.genetics). call() (in module hail.expr.functions). call_stats() (in module hail.expr.aggregators). CallExpression (class in hail.expr). case() (in module hail.expr.functions). CaseBuilder (class in hail.expr.builders). cdf() (in module hail.plot). ceil() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). checkpoint() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). chi_squared_test() (in module hail.expr.functions). choose_cols() (hail.MatrixTable method). citation() (in module hail). coalesce() (in module hail.expr.functions). cochran_mantel_haenszel_test() (in module hail.expr.functions). col (hail.MatrixTable property). col_key (hail.MatrixTable property). col_value (hail.MatrixTable property). collect() (hail.expr.ArrayExpression method).,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
Security,validat,validate,n module hail.methods). trios (hail.genetics.Pedigree property). truncate_reference_blocks() (in module hail.vds). tset (class in hail.expr.types). tstr (in module hail.expr.types). tstruct (class in hail.expr.types). ttuple (class in hail.expr.types). tuple() (in module hail.expr.functions). TupleExpression (class in hail.expr). U. unfilter_entries() (hail.MatrixTable method). union() (hail.expr.SetExpression method). (hail.Table method). union_cols() (hail.MatrixTable method). union_rows() (hail.MatrixTable method). (hail.vds.VariantDataset method). uniroot() (in module hail.expr.functions). UNKNOWN (hail.genetics.AlleleType attribute). unpersist() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). unphase() (hail.expr.CallExpression method). unphased_diploid_gt_index() (hail.expr.CallExpression method). (hail.genetics.Call method). unphased_diploid_gt_index_call() (in module hail.expr.functions). upper() (hail.expr.StringExpression method). V. validate() (hail.vds.VariantDataset method). values() (hail.expr.DictExpression method). (hail.expr.StructExpression method). variant_qc() (in module hail.methods). variant_str() (in module hail.expr.functions). VariantDataset (class in hail.vds). VariantDatasetCombiner (class in hail.vds.combiner). vars() (in module hail.ggplot). VDSMetadata (class in hail.vds.combiner). vep() (in module hail.methods). VEPConfig (class in hail.methods). VEPConfigGRCh37Version85 (class in hail.methods). VEPConfigGRCh38Version95 (class in hail.methods). version() (in module hail). visualize_missingness() (in module hail.plot). vstack() (in module hail.nd). W. when() (hail.expr.builders.CaseBuilder method). (hail.expr.builders.SwitchBuilder method). when_missing() (hail.expr.builders.SwitchBuilder method). window() (hail.expr.LocusExpression method). write() (hail.genetics.Pedigree method). (hail.genetics.ReferenceGenome method). (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
Testability,log,log,expr.StringExpression method). lengths (hail.genetics.ReferenceGenome property). lgt_to_gt() (in module hail.vds). liftover() (in module hail.expr.functions). linear_mixed_model() (in module hail.methods). linear_mixed_regression_rows() (in module hail.methods). linear_regression_rows() (in module hail.methods). LinearMixedModel (class in hail.stats). linreg() (in module hail.expr.aggregators). literal() (in module hail.expr.functions). load() (hail.vds.combiner.VariantDatasetCombiner static method). load_combiner() (in module hail.vds.combiner). load_dataset() (in module hail.experimental). local_to_global() (in module hail.vds). localize_entries() (hail.MatrixTable method). Locus (class in hail.genetics). locus() (in module hail.expr.functions). locus_from_global_position() (hail.genetics.ReferenceGenome method). (in module hail.expr.functions). locus_interval() (in module hail.expr.functions). locus_windows() (in module hail.linalg.utils). LocusExpression (class in hail.expr). log() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). log10() (in module hail.expr.functions). logistic_regression_rows() (in module hail.methods). logit() (in module hail.expr.functions). loop() (in module hail.experimental). lower() (hail.expr.StringExpression method). ls() (in module hailtop.fs). M. make_betas() (in module hail.experimental.ldscsim). make_table() (hail.MatrixTable method). manhattan() (in module hail.plot). map() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.CollectionExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.SetExpression method). (in module hail.expr.functions). map2() (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). map_values() (hail.expr.DictExpression method). mat_id (hail.genetics.Trio property). matches() (hail.expr.StringExpression method). MatrixTable (class in hail). max() (in module hail.expr.a,MatchSource.WIKI,docs/0.2/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genindex.html
Deployability,install,installation,"﻿. Hail | ; Installing Hail. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail. View page source. Installing Hail. Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started.html
Deployability,install,installation,"﻿. Hail | ; For Software Developers. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Bu",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
Integrability,depend,dependencies,"te: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass a",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
Modifiability,variab,variable,"the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Ch",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
Testability,test,tests,"﻿. Hail | ; For Software Developers. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Bu",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
Usability,guid,guide,"te: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass a",MatchSource.WIKI,docs/0.2/getting_started_developing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html
Deployability,update,updated,"Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides. View page source. How-To Guides. Note; Hail’s How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you’d like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; One aggregation; Multiple aggregations. Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Row Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; From a table of intervals; From a UCSC BED file; Using hl.filter_intervals; Declaring intervals with hl.parse_locus_interval. Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression; Single Phenotype; Multiple Phenotypes; Using Variants (SNPs) as Covariates; Stratified by Group. PLINK Conversions; Polygenic Score Calculation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides.html
Usability,guid,guides,"﻿. Hail | ; How-To Guides. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides. View page source. How-To Guides. Note; Hail’s How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you’d like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; One aggregation; Multiple aggregations. Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Row Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; From a table of intervals; From a UCSC BED file; Using hl.filter_intervals; Declaring intervals with hl.parse_locus_interval. Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression; Single Phenotype; Multiple Phenotypes; Using Variants (SNPs) as Covariates; Stratified by Group. PLINK Conversions; Polygenic Score Calculation. Previous;",MatchSource.WIKI,docs/0.2/guides.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides.html
Deployability,update,updated,"﻿. Hail | ; Hadoop Glob Patterns. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Hadoop Glob Patterns. Change Log And Version Policy. menu; Hail. Other Resources; Hadoop Glob Patterns. View page source. Hadoop Glob Patterns. Pattern; Description. ?. Matches any single character. *. Matches zero or more characters. [abc]. Matches a single character from character set {a,b,c}. [a-b]. Matches a single character from the character range {a…b}. Note that the “^”; character must occur immediately to the right of the opening bracket. [^a]. Matches a single character that is not from character set or range {a}. Note that; the “^”character must occur immediately to the right of the opening bracket. \c. Removes (escapes) any special meaning of character c. {ab,cd}. Matches a string from the string set {ab, cd}. {ab,c{de, fh}}. Matches a string from the string set {ab, cde, cfh}. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hadoop_glob_patterns.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hadoop_glob_patterns.html
Availability,error,error,"e', 'Bob', 'Charlie']). See also; CollectionExpression. Attributes. dtype; The data type of the expression. Methods. aggregate; Uses the aggregator library to compute a summary from an array. append; Append an element to the array and return the result. contains; Returns a boolean indicating whether item is found in the array. extend; Concatenate two arrays and return the result. first; Returns the first element of the array, or missing if empty. grouped; Partition an array into fixed size subarrays. head; Deprecated in favor of first(). index; Returns the first index of x, or missing. last; Returns the last element of the array, or missing if empty. scan; Map each element of the array to cumulative value of function f, with initial value zero. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; ot",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
Deployability,pipeline,pipeline,"d return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
Energy Efficiency,efficient,efficient,"d return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
Integrability,depend,dependencies," Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
Modifiability,extend,extend,"Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; ArrayExpression. View page source. ArrayExpression. class hail.expr.ArrayExpression[source]; Expression of type tarray.; >>> names = hl.literal(['Alice', 'Bob', 'Charlie']). See also; CollectionExpression. Attributes. dtype; The data type of the expression. Methods. aggregate; Uses the aggregator library to compute a summary from an array. append; Append an element to the array and return the result. contains; Returns a boolean indicating whether item is found in the array. extend; Concatenate two arrays and return the result. first; Returns the first element of the array, or missing if empty. grouped; Partition an array into fixed size subarrays. head; Deprecated in favor of first(). index; Returns the first index of x, or missing. last; Returns the last element of the array, or missing if empty. scan; Map each element of the array to cumulative value of function f, with initial value zero. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
Security,access,accessing,"Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. aggregate(f)[source]; Uses the aggregator library to compute a summary from an array.; This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, call_stats(). Parameters:; f – Aggregation function. Returns:; Expression. all(f); Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any elemen",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
Testability,test,test,". Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. append(item)[source]; Append an element to the array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv",MatchSource.WIKI,docs/0.2/hail.expr.ArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html
Availability,error,error,"h of each array; is identical, and will apply the operation positionally (a1 * a2 will; multiply the first element of a1 by the first element of a2, the; second element of a1 by the second element of a2, and so on).; Arithmetic with a scalar will apply the operation to each element of the; array.; >>> a1 = hl.literal([0, 1, 2, 3, 4, 5]). >>> a2 = hl.literal([1, -1, 1, -1, 1, -1]). Attributes. dtype; The data type of the expression. Methods. __add__(other)[source]; Positionally add an array or a scalar.; Examples; >>> hl.eval(a1 + 5); [5, 6, 7, 8, 9, 10]. >>> hl.eval(a1 + a2); [1, 0, 3, 2, 5, 4]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to add. Returns:; ArrayNumericExpression – Array of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by an array or a scalar using floor division.; Examples; >>> hl.eval(a1 // 2); [0, 0, 1, 1, 2, 2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __ge__(other); Return self>=value. __getitem__(item); Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __mod__(other)[source]; Positionally compute the left modulo the right.",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
Deployability,pipeline,pipeline,"array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
Energy Efficiency,power,power,"ession. __mul__(other)[source]; Positionally multiply by an array or a scalar.; Examples; >>> hl.eval(a2 * 5); [5, -5, 5, -5, 5, -5]. >>> hl.eval(a1 * a2); [0, -1, 2, -3, 4, -5]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to multiply by. Returns:; ArrayNumericExpression – Array of positional products. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the array.; Examples; >>> hl.eval(-a1); [0, -1, -2, -3, -4, -5]. Returns:; ArrayNumericExpression – Array expression of the same type. __pow__(other)[source]; Positionally raise to the power of an array or a scalar.; Examples; >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __sub__(other)[source]; Positionally subtract an array or a scalar.; Examples; >>> hl.eval(a2 - 1); [0, -2, 0, -2, 0, -2]. >>> hl.eval(a1 - a2); [-1, 2, 1, 4, 3, 6]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to subtract. Returns:; ArrayNumericExpression – Array of positional differences. __truediv__(other)[source]; Positionally divide by an array or a scalar.; Examples; >>> hl.eval(a1 / 10) ; [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]. >>> hl.eval(a2 / a1) ; [inf, -1.0, 0.5, -0.3333333333333333, 0.25, -0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to divide by. Returns:; ArrayNumericExpression – Array of positional quotients. aggregate(f); ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
Integrability,depend,dependencies,"ng item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
Modifiability,extend,extend,"t"":[1,1,1],""fst"":[0.1,0.1,0.1],""mixture"":false}. Notes; For entry-indexed expressions, if there is one column key field, the; result of calling str() on that field is used as; the column header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:; >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles {""s"":0,""family"":""fam1""} {""s"":1,""family"":""fam1""} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. extend(a); Concatenate two arrays and return the result.; Examples; >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters:; a (ArrayExpression) – Array to concatenate, same type as the callee. Returns:; ArrayExpression. filter(f); Returns a new collection containing elements where f returns True.; Examples; >>> hl.eval(a.filter(lambda x: x % 2 == 0)); [2, 4]. >>> hl.eval(s3.filter(lambda x: ~(x[-1] == 'e'))) ; {'Bob'}. Notes; Returns a same-type expression; evaluated on a SetExpression, returns a; SetExpression. Evaluated on an ArrayExpression,; returns an ArrayExpression. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; CollectionExpression – Expression of the same type as the callee. find(f); Returns the first element where f returns True.; Examples; >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Notes; ",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
Security,access,accessing,", 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __sub__(other)[source]; Positionally subtract an array or a scalar.; Examples; >>> hl.eval(a2 - 1); [0, -2, 0, -2, 0, -2]. >>> hl.eval(a1 - a2); [-1, 2, 1, 4, 3, 6]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to subtract. Returns:; ArrayNumericExpression – Array of positional differences. __truediv__(other)[source]; Positionally divide by an array or a scalar.; Examples; >>> hl.eval(a1 / 10) ; [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]. >>> hl.eval(a2 / a1) ; [inf, -1.0, 0.5, -0.3333333333333333, 0.25, -0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to divide by. Returns:; ArrayNumericExpression – Array of positional quotients. aggregate(f); Uses the aggregator library to compute a summary from an array.; This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, call_stats(). Parameters:; f – Aggregation function. Returns:; Expression. all(f); Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any elemen",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
Testability,test,test,"f the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. append(item); Append an element to the array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv",MatchSource.WIKI,docs/0.2/hail.expr.ArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html
Availability,error,error,"one. Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __and__(other)[source]; Return True if the left and right arguments are True.; Examples; >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval(f & na); False. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) & (x > 2)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if both left and right are True. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __invert__()[source]; Return the boolean negation.; Examples; >>> hl.eval",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
Deployability,update,updated,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
Energy Efficiency,power,power,"; >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
Integrability,depend,dependencies,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
Testability,log,logging,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.BooleanExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html
Availability,error,error,"e call includes two different alleles. is_het_non_ref; Evaluate whether the call includes two different alleles, neither of which is reference. is_het_ref; Evaluate whether the call includes two different alleles, one of which is reference. is_hom_ref; Evaluate whether the call includes two reference alleles. is_hom_var; Evaluate whether the call includes two identical alternate alleles. is_non_ref; Evaluate whether the call includes one or more non-reference alleles. n_alt_alleles; Returns the number of non-reference alleles. one_hot_alleles; Returns an array containing the summed one-hot encoding of the alleles. unphase; Returns an unphased version of this call. unphased_diploid_gt_index; Return the genotype index for unphased, diploid calls. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the i*th* allele.; Examples; Index with a single integer:; >>> hl.eval(call[0]); 0. >>> hl.eval(call[1]); 1. Parameters:; item (int or Expression of type tint32) – Allele index. Returns:; Expression of type tint32. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two express",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
Deployability,update,updated,"l.eval(call.ploidy); 2. Notes; Currently only ploidy 1 and 2 are supported. Returns:; Expression of type tint32. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. unphase()[source]; Returns an unphased version of this call. Returns:; CallExpression. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls.; Examples; >>> hl.eval(call.unphased_diploid_gt_index()); 1. Returns:; Expression of type tint32. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
Integrability,depend,dependencies,"ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains_allele(allele)[source]; Returns true if the call has one or more called alleles of the given index.; >>> c = hl.call(0, 3). >>> hl.eval(c.contains_allele(3)); True. >>> hl.eval(c.contains_allele(1)); False. Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
Testability,log,logging,"if the call is phased.; Examples; >>> hl.eval(call.phased); False. Returns:; BooleanExpression. property ploidy; Return the number of alleles of this call.; Examples; >>> hl.eval(call.ploidy); 2. Notes; Currently only ploidy 1 and 2 are supported. Returns:; Expression of type tint32. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. unphase()[source]; Returns an unphased version of this call. Returns:; CallExpression. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls.; Examples; ",MatchSource.WIKI,docs/0.2/hail.expr.CallExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html
Availability,error,error,"', 'Bob', 'Charlie'}). Attributes. dtype; The data type of the expression. Methods. all; Returns True if f returns True for every element. any; Returns True if f returns True for any element. filter; Returns a new collection containing elements where f returns True. find; Returns the first element where f returns True. flatmap; Map each element of the collection to a new collection, and flatten the results. fold; Reduces the collection with the given function f, provided the initial value zero. group_by; Group elements into a dict according to a lambda function. length; Returns the size of a collection. map; Transform each element of a collection. size; Returns the size of a collection. starmap; Transform each element of a collection of tuples. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. all(f)[source]; Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanEx",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
Deployability,update,updated,"ession refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
Integrability,depend,dependencies,"(arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f)[source]; Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
Testability,log,logging," 8.0, 27.0, 64.0, 125.0]. >>> hl.eval(s3.map(lambda x: x.length())); {3, 5, 7}. Parameters:; f (function ( (arg) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This ",MatchSource.WIKI,docs/0.2/hail.expr.CollectionExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html
Availability,error,error,"ession. View page source. DictExpression. class hail.expr.DictExpression[source]; Expression of type tdict.; >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}). Attributes. dtype; The data type of the expression. Methods. contains; Returns whether a given key is present in the dictionary. get; Returns the value associated with key k or a default value if that key is not present. items; Returns an array of tuples containing key/value pairs in the dictionary. key_set; Returns the set of keys in the dictionary. keys; Returns an array with all keys in the dictionary. map_values; Transform values of the dictionary according to a function. size; Returns the size of the dictionary. values; Returns an array with all values in the dictionary. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) ",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
Deployability,update,updated,"urns:; DictExpression – Dictionary with transformed values. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of the dictionary.; Examples; >>> hl.eval(d.size()); 3. Returns:; Expression of type tint32 – Size of the dictionary. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; Returns an array with all values in the dictionary.; Examples; >>> hl.eval(d.values()) ; [33, 44, 43]. Returns:; ArrayExpression – All values in the dictionary. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
Integrability,depend,dependencies,">>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns whether a given key is present in the dictionary.; Examples; >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters:; item (Expression) – Key to test for inclusion. Returns:; BooleanExpression – True if item is a key of the dictionary, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
Testability,test,test,"Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns whether a given key is present in the dictionary.; Examples; >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters:; item (Expression) – Key to test for inclusion. Returns:; BooleanExpression – True if item is a key of the dictionary, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>>",MatchSource.WIKI,docs/0.2/hail.expr.DictExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html
Availability,error,error,"verview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Ext",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
Deployability,update,updated,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
Integrability,depend,dependencies,"﻿. Hail | ; Expression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
Testability,log,logging,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html
Availability,error,error,"verview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Ext",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
Deployability,update,updated,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
Integrability,depend,dependencies,"﻿. Hail | ; Expression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
Testability,log,logging,"} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None)[source]; Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None)[source]; Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True)[source]; Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression.html
Availability,error,error,ods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Float32Expression. View page source. Float32Expression. class hail.expr.Float32Expression[source]; Expression of type tfloat32.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
Deployability,update,updated,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
Energy Efficiency,power,power,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
Integrability,depend,dependencies,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
Testability,log,logging,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html
Availability,error,error,ods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Float64Expression. View page source. Float64Expression. class hail.expr.Float64Expression[source]; Expression of type tfloat64.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
Deployability,update,updated,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
Energy Efficiency,power,power,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
Integrability,depend,dependencies,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
Testability,log,logging,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Float64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html
Availability,error,error,ns; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Int32Expression. View page source. Int32Expression. class hail.expr.Int32Expression[source]; Expression of type tint32.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
Deployability,update,updated,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
Energy Efficiency,power,power,"hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
Integrability,depend,dependencies,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
Testability,log,logging,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int32Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html
Availability,error,error,ns; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Int64Expression. View page source. Int64Expression. class hail.expr.Int64Expression[source]; Expression of type tint64.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
Deployability,update,updated,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
Energy Efficiency,power,power,"es; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records ",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
Integrability,depend,dependencies,"** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
Testability,log,logging,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.Int64Expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html
Availability,error,error,"Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; IntervalExpression. View page source. IntervalExpression. class hail.expr.IntervalExpression[source]; Expression of type tinterval.; >>> interval = hl.interval(3, 11); >>> locus_interval = hl.parse_locus_interval(""1:53242-90543""). Attributes. dtype; The data type of the expression. end; Returns the end point. includes_end; True if the interval includes the end point. includes_start; True if the interval includes the start point. start; Returns the start point. Methods. contains; Tests whether a value is contained in the interval. overlaps; True if the the supplied interval contains any value in common with this one. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of record",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
Deployability,update,updated,"lue in common with this one.; Examples; >>> hl.eval(interval.overlaps(hl.interval(5, 9))); True. >>> hl.eval(interval.overlaps(hl.interval(11, 20))); False. Parameters:; interval (Expression with type tinterval) – Interval object with the same point type. Returns:; BooleanExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. property start; Returns the start point.; Examples; >>> hl.eval(interval.start); 3. Returns:; Expression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
Integrability,depend,dependencies,"eral(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(value)[source]; Tests whether a value is contained in the interval.; Examples; >>> hl.eval(interval.contains(3)); True. >>> hl.eval(interval.contains(11)); False. Parameters:; value – Object with type matching the interval point type. Returns:; BooleanExpression – True if value is contained in the interval, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. property end; Returns the end point.; Examples; >>> hl.eval(interval.end); 11. Returns:; Expression. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... p",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
Testability,log,logging,"alue in common with this one.; Examples; >>> hl.eval(interval.overlaps(hl.interval(5, 9))); True. >>> hl.eval(interval.overlaps(hl.interval(11, 20))); False. Parameters:; interval (Expression with type tinterval) – Interval object with the same point type. Returns:; BooleanExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. property start; Returns the start point.; Examples; >>> hl.eval(interval.start); 3. Returns:; Expression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.IntervalExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html
Availability,error,error,"long the reference genome. in_autosome; Returns True if the locus is on an autosome. in_autosome_or_par; Returns True if the locus is on an autosome or a pseudoautosomal region of chromosome X or Y. in_mito; Returns True if the locus is on mitochondrial DNA. in_x_nonpar; Returns True if the locus is in a non-pseudoautosomal region of chromosome X. in_x_par; Returns True if the locus is in a pseudoautosomal region of chromosome X. in_y_nonpar; Returns True if the locus is in a non-pseudoautosomal region of chromosome Y. in_y_par; Returns True if the locus is in a pseudoautosomal region of chromosome Y. sequence_context; Return the reference genome sequence at the locus. window; Returns an interval of a specified number of bases around the locus. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of record",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
Deployability,update,updated,"; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. window(before, after)[source]; Returns an interval of a specified number of bases around the locus.; Examples; Create a window of two megabases centered at a locus:; >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; The returned interval is inclusive of both the start and end; endpoints. Parameters:. before (Expression of type tint32) – Number of bases to include before the locus. Truncates at 1.; after (Expression of type tint32) – Number of bases to include after the locus. Truncates at; contig length. Returns:; IntervalExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
Integrability,depend,dependencies,".; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. property contig; Returns the chromosome.; Examples; >>> hl.eval(locus.contig); '1'. Returns:; StringExpression – The chromosome for this locus. property contig_idx; Returns the chromosome.; Examples; >>> hl.eval(locus.contig_idx); 0. Returns:; StringExpression – The index of the chromosome for this locus. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
Performance,load,load,"; Returns True if the locus is in a pseudoautosomal region; of chromosome Y.; Examples; >>> hl.eval(locus.in_y_par()); False. Note; Many variant callers only generate variants on chromosome X for the; pseudoautosomal region. In this case, all loci mapped to chromosome; Y are non-pseudoautosomal. Returns:; BooleanExpression. property position; Returns the position along the chromosome.; Examples; >>> hl.eval(locus.position); 1034245. Returns:; Expression of type tint32 – This locus’s position along its chromosome. sequence_context(before=0, after=0)[source]; Return the reference genome sequence at the locus.; Examples; Get the reference allele at a locus:; >>> hl.eval(locus.sequence_context()) ; ""G"". Get the reference sequence at a locus including the previous 5 bases:; >>> hl.eval(locus.sequence_context(before=5)) ; ""ACTCGG"". Notes; This function requires that this locus’ reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome. Parameters:. before (Expression of type tint32, optional) – Number of bases to include before the locus. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus. Truncates at; contig boundary. Returns:; StringExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handl",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
Testability,log,logging,"Parameters:. before (Expression of type tint32, optional) – Number of bases to include before the locus. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus. Truncates at; contig boundary. Returns:; StringExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. window(before, after)[source]; Returns an interval of a specified number of bases around the locus.; Examples; Create a window of two megabases centered at a locus:; >>> locus = hl.locus('16'",MatchSource.WIKI,docs/0.2/hail.expr.LocusExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html
Availability,error,error,"hail.expr.NDArrayExpression[source]; Expression of type tndarray.; >>> nd = hl.nd.array([[1, 2], [3, 4]]). Attributes. T; Reverse the dimensions of this ndarray. dtype; The data type of the expression. ndim; The number of dimensions of this ndarray. shape; The shape of this ndarray. Methods. map; Applies an element-wise operation on an NDArray. map2; Applies an element-wise binary operation on two NDArrays. reshape; Reshape this ndarray to a new shape. transpose; Permute the dimensions of this ndarray according to the ordering of axes. property T; Reverse the dimensions of this ndarray. For an n-dimensional array a,; a[i_0, …, i_n-1, i_n] = a.T[i_n, i_n-1, …, i_0].; Same as self.transpose().; See also transpose(). Returns:; NDArrayExpression. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of record",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
Deployability,update,updated,"=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None)[source]; Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
Integrability,depend,dependencies,"ression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
Testability,log,logging,"ession of type tint64 or) – :obj: tuple of Expression of type tint64. Examples; >>> v = hl.nd.array([1, 2, 3, 4]) ; >>> m = v.reshape((2, 2)) . Returns:; NDArrayExpression. property shape; The shape of this ndarray.; Examples; >>> hl.eval(nd.shape); (2, 2). Returns:; TupleExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None)[source]; Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimen",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html
Availability,error,error,"on). Arithmetic with a scalar will; apply the operation to each element of the ndarray.; Attributes. T; Reverse the dimensions of this ndarray. dtype; The data type of the expression. ndim; The number of dimensions of this ndarray. shape; The shape of this ndarray. Methods. sum; Sum out one or more axes of an ndarray. property T; Reverse the dimensions of this ndarray. For an n-dimensional array a,; a[i_0, …, i_n-1, i_n] = a.T[i_n, i_n-1, …, i_0].; Same as self.transpose().; See also transpose(). Returns:; NDArrayExpression. __add__(other)[source]; Positionally add an array or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to add. Returns:; NDArrayNumericExpression – NDArray of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
Deployability,update,updated," to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None); Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
Integrability,depend,dependencies,"on. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the ndarray. Returns:; NDArrayNumericExpression – Array expression of the same type. __sub__(other)[source]; Positionally subtract a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to subtract. Returns:; NDArrayNumericExpression – NDArray of positional differences. __truediv__(other)[source]; Positionally divide by a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to divide by. Returns:; NDArrayNumericExpression – NDArray of positional quotients. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
Performance,perform,performed,"darray to add. Returns:; NDArrayNumericExpression – NDArray of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if a has shape (3, 4, 5) and b has shape (3, 5, 6), a is treated; as a stack of three matrices of shape (4, 5) and b as a stack of three matrices of shape (5, 6). a @ b; would then have shape (3, 4, 6).; Notes; The last dimension of a and the second to last dimension of b (or only dimension if b is a vector); must have the same length. The dimensions to the left of the last two dimensions of a and b (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters:; other (numpy.ndarray NDArrayNumericExpression). Returns:; NDArrayNumericExpression or NumericExpression. __",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
Testability,log,logging,"ession of type tint64 or) – :obj: tuple of Expression of type tint64. Examples; >>> v = hl.nd.array([1, 2, 3, 4]) ; >>> m = v.reshape((2, 2)) . Returns:; NDArrayExpression. property shape; The shape of this ndarray.; Examples; >>> hl.eval(nd.shape); (2, 2). Returns:; TupleExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose",MatchSource.WIKI,docs/0.2/hail.expr.NDArrayNumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html
Availability,error,error,vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; NumericExpression. View page source. NumericExpression. class hail.expr.NumericExpression[source]; Expression of numeric type.; >>> x = hl.literal(3). >>> y = hl.literal(4.5). Attributes. dtype; The data type of the expression. Methods. __add__(other)[source]; Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other)[source]; Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other)[source]; Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other)[source]; Less-than-or-equals compar,MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
Deployability,update,updated,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
Energy Efficiency,power,power," x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
Integrability,depend,dependencies,"1111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 45.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The left number divided by the left. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
Testability,log,logging,"} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.NumericExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html
Availability,error,error,"= hl.literal({1, 3, 5}). See also; CollectionExpression. Attributes. dtype; The data type of the expression. Methods. add; Returns a new set including item. contains; Returns True if item is in the set. difference; Return the set of elements in the set that are not present in set s. intersection; Return the intersection of the set and set s. is_subset; Returns True if every element is contained in set s. remove; Returns a new set excluding item. union; Return the union of the set and set s. __and__(other)[source]; Return the intersection of the set and other.; Examples; >>> hl.eval(s1 & s2); {1, 3}. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in both the set and other. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Test whether every element in other is in the set. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if every element in other is in the set. False otherwise. __gt__(other)[source]; Test whether other is a proper subset of the set (other <= set and other != set). Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if other is a proper subset of the set. False otherwise. __le__(other)[source]; Test whether every element in the set is in other. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if every element in the set is in other. False otherwise. __lt__(other)[source]; Test whether the set is a proper subset of o",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
Deployability,update,updated,"| str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. union(s)[source]; Return the union of the set and set s.; Examples; >>> hl.eval(s1.union(s2)); {1, 2, 3, 5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either set. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
Integrability,depend,dependencies,"or any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns True if item is in the set.; Examples; >>> hl.eval(s1.contains(1)); True. >>> hl.eval(s1.contains(10)); False. Parameters:; item (Expression) – Value for inclusion test. Returns:; BooleanExpression – True if item is in the set. describe(handler=<built-in function print>); Print information about type, index, and dependencies. difference(s)[source]; Return the set of elements in the set that are not present in set s.; Examples; >>> hl.eval(s1.difference(s2)); {2}. >>> hl.eval(s2.difference(s1)); {5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements not in s. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""]",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
Testability,test,test,"ssion. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns True if item is in the set.; Examples; >>> hl.eval(s1.contains(1)); True. >>> hl.eval(s1.contains(10)); False. Parameters:; item (Expression) – Value for inclusion test. Returns:; BooleanExpression – True if item is in the set. describe(handler=<built-in function print>); Print information about type, index, and dependencies. difference(s)[source]; Return the set of elements in the set that are not present in set s.; Examples; >>> hl.eval(s1.difference(s2)); {2}. >>> hl.eval(s2.difference(s1)); {5}. Parameters:; s (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements not in s. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> ",MatchSource.WIKI,docs/0.2/hail.expr.SetExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html
Availability,error,error,"se. replace; Replace substrings matching pattern1 with pattern2 using regex. reverse; Returns the reversed value. split; Returns an array of strings generated by splitting the string at delim. startswith; Returns whether substr is a prefix of the string. strip; Returns a copy of the string with whitespace removed from the start and end. translate; Translates characters of the string using mapping. upper; Returns a copy of the string, but with lower case letters converted to upper case. __add__(other)[source]; Concatenate strings.; Examples; >>> hl.eval(s + ' jumped over the lazy dog'); 'The quick brown fox jumped over the lazy dog'. Parameters:; other (StringExpression) – String to concatenate. Returns:; StringExpression – Concatenated string. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Slice or index into the string.; Examples; >>> hl.eval(s[:15]); 'The quick brown'. >>> hl.eval(s[0]); 'T'. Parameters:; item (slice or Expression of type tint32) – Slice or character index. Returns:; StringExpression – Substring or character at index item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; Boole",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
Deployability,update,updated,"val(s.split('\s+')); ['The', 'quick', 'brown', 'fox']. >>> hl.eval(s.split('\s+', 2)); ['The', 'quick brown fox']. Notes; The delimiter is a regex using the; Java regex syntax; delimiter. To split on special characters, escape them with double; backslash (\\). Parameters:. delim (str or StringExpression) – Delimiter regex.; n (Expression of type tint32, optional) – Maximum number of splits. Returns:; ArrayExpression – Array of split strings. startswith(substr)[source]; Returns whether substr is a prefix of the string.; Examples; >>> hl.eval(s.startswith('The')); True. >>> hl.eval(s.startswith('the')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. strip()[source]; Returns a copy of the string with whitespace removed from the start; and end.; Examples; >>> s2 = hl.str(' once upon a time\n'); >>> hl.eval(s2.strip()); 'once upon a time'. Returns:; StringExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. translate(mapping)[source]; Translates characters of the string using mapping.; Examples; >>> string = hl.literal('ATTTGCA'); >>> hl.eval(string.translate({'T': 'U'})); 'AUUUGCA'. Parameters:; mapping (DictExpression) – Dictionary of character-character translations. Returns:; StringExpression. See also; replace(). upper()[source]; Returns a copy of the string, but with lower case letters converted; to upper case.; Examples; >>> hl.eval(s.upper()); 'THE QUICK BROWN FOX'. Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
Integrability,depend,dependencies," if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(substr)[source]; Returns whether substr is contained in the string.; Examples; >>> hl.eval(s.contains('fox')); True. >>> hl.eval(s.contains('dog')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. endswith(substr)[source]; Returns whether substr is a suffix of the string.; Examples; >>> hl.eval(s.endswith('fox')); True. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; StringExpression. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
Testability,log,logging,"roup, not the canonical \1. Parameters:. pattern1 (str or StringExpression); pattern2 (str or StringExpression). reverse()[source]; Returns the reversed value.; .. rubric:: Examples; >>> string = hl.literal('ATGCC'); >>> hl.eval(string.reverse()); 'CCGTA'. Returns:; StringExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. split(delim, n=None)[source]; Returns an array of strings generated by splitting the string at delim.; Examples; >>> hl.eval(s.split('\s+')); ['The', 'quick', 'brown', 'fox']. >>> hl.eval(s.split('\s+', 2)); ['The', 'quick brown fox']. Notes; The delimiter is a regex using the; Java regex syntax; delimiter. To split on special characters, escape them with double; backslash (\\). Parameters:. delim (str or StringExpression) – Delimiter regex.; n (Expression of type tint32, optional) – Maximum number of splits. Returns:; ArrayExpression – Array of split strings. startswith(substr)[source]; Returns whether substr is a prefix of the string.; Examples; >",MatchSource.WIKI,docs/0.2/hail.expr.StringExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html
Deployability,update,updated,"__eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expression of the same type. __ge__(other); Return self>=value. __getitem__(item)[source]; Access a field of the struct by name or index.; Examples; >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters:; item (str) – Field name. Returns:; Expression – Struct field. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other)[source]; Return self!=value. annotate(**named_exprs)[source]; Add new fields or recompute existing fields.; Examples; >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; If an expression in named_exprs shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters:; named_exprs (keyword args of Expression) – Fields to add. Returns:; StructExpression – Struct with new or updated fields. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. drop(*fields)[source]; Drop fields from the struct.; Examples; >>> hl.eval(struct.drop('b')); Struct(a=5). Parameters:; fields (varargs of str) – Fields to drop. Returns:; StructExpression – Struct without certain fields. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 ",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
Integrability,depend,dependencies,"ns:; Expression – Struct field. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other)[source]; Return self!=value. annotate(**named_exprs)[source]; Add new fields or recompute existing fields.; Examples; >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; If an expression in named_exprs shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters:; named_exprs (keyword args of Expression) – Fields to add. Returns:; StructExpression – Struct with new or updated fields. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. drop(*fields)[source]; Drop fields from the struct.; Examples; >>> hl.eval(struct.drop('b')); Struct(a=5). Parameters:; fields (varargs of str) – Fields to drop. Returns:; StructExpression – Struct without certain fields. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
Safety,safe,safer," . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.StructExpression'>>. __eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expre",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
Security,access,accessible,"﻿. Hail | ; StructExpression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
Testability,log,logging,"n the resulting struct in the order they appear in; fields.; The named_exprs arguments are new field expressions. Parameters:. fields (varargs of str) – Field names to keep.; named_exprs (keyword args of Expression) – New field expressions. Returns:; StructExpression – Struct containing specified existing fields and computed fields. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; A list of expressions for each field. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.StructExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html
Availability,error,error,"s; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; TupleExpression. View page source. TupleExpression. class hail.expr.TupleExpression[source]; Expression of type ttuple.; >>> tup = hl.literal((""a"", 1, [1, 2, 3])). Attributes. dtype; The data type of the expression. Methods. count; Do not use this method. index; Do not use this method. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.TupleExpression'>>. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into the tuple.; Examples; >>> hl.eval(tup[1]); 1. Parameters:; item (int) – Element index. Returns:; Expression. __gt__(other); Return self>value. __le__(other); Return self<=value. __len__()[source]; Returns the length of the tuple.; Examples; >>> len(tup); 3. Returns:; int. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expression",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
Deployability,update,updated,"A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. index(value, start=0, stop=None)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
Integrability,depend,dependencies,"; Returns the length of the tuple.; Examples; >>> len(tup); 3. Returns:; int. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. count(value)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.pop.export('output/pops.tsv'); >>> with open('output/pops.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); sample_idx pop; 0 1; 1 2; 2 2; 3 2. >>> small_mt.ancestral_af.export('output/ances",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
Testability,log,logging,"A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. index(value, start=0, stop=None)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.expr.TupleExpression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html
Availability,down,downstream,"passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:; >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean)",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
Deployability,pipeline,pipeline,"rs (keyword args of Expression) – Column-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix, can be used to call GroupedMatrixTable.aggregate(). group_rows_by(*exprs, **named_exprs)[source]; Group rows.; Examples; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result =",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
Energy Efficiency,reduce,reduces,"passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:; >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean)",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
Performance,optimiz,optimizer,"s; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix w",MatchSource.WIKI,docs/0.2/hail.GroupedMatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html
Availability,down,downstream,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
Deployability,pipeline,pipeline,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
Energy Efficiency,reduce,reduces,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
Performance,optimiz,optimizer,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail.GroupedTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html
Availability,checkpoint,checkpoint," fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Expl",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
Deployability,pipeline,pipeline,"regate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection – the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using key_cols_by() with no arguments. Warning; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns:; Table – Table with all non-global fields from the matrix, with one row per entry of the matrix. property entry; Returns a struct expression including all row-and-column-indexed fields.; Examples; Get all entry field names:; >>> list(dataset.entry); ['GT', 'AD', 'DP', 'GQ', 'PL']. Returns:; StructExpression – Struct of all entry fields. explode_cols(field_expr)[source]; Explodes a column field of type array or set, copying the entire column for each element.; Examples; Explode columns by annotated cohorts:; >>> dataset_result = dataset.explode_cols(dataset.cohorts). Notes; The new matrix table will have N copies of each column, where N is the; number of elements that column contains for the field denoted by field_expr.; The field referenced in field_expr is replaced in the sequence of duplicated; columns by the sequence of elements in the array or set. All other fields remain; the same, includ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
Energy Efficiency,efficient,efficient," fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Expl",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
Integrability,depend,dependent," row index field. Returns:; MatrixTable – Dataset with new field. aggregate_cols(expr, _localize=True)[source]; Aggregate over columns to a local value.; Examples; Aggregate over columns:; >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; Unlike most MatrixTable methods, this method does not support; meaningful references to fields that are not global or indexed by column.; This method should be thought of as a more convenient alternative to; the following:; >>> cols_table = dataset.cols(); >>> cols_table.aggregate(; ... hl.struct(fraction_female=hl.agg.fraction(cols_table.pheno.is_female),; ... case_ratio=hl.agg.count_where(cols_table.is_case) / hl.agg.count())). Note; This method supports (and expects!) aggregation over columns. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. aggregate_entries(expr, _localize=True)[source]; Aggregate over entries to a local value.; Examples; Aggregate over entries:; >>> dataset.aggregate_entries(hl.struct(global_gq_mean=hl.agg.mean(dataset.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(dataset.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). Notes; This method should be thought of as a more convenient alternative to; the following:; >>> entries_table = dataset.entries(); >>> entries_table.aggregate(hl.struct(global_gq_mean=hl.agg.mean(entries_table.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(entries_table.GT)))). Note; This method supports (and expects!) aggregation over entries. Parameters:; expr (Expression) – Aggregation expressions. Returns:; any – Aggregated value dependent on expr. aggregate_rows(expr, _localize=True)[source]; Aggregate over rows to a local value.; Examples; Aggregate over rows:; >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
Modifiability,variab,variables," in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Explodes a row field of type array or set, copying the entire row for each element. filter_cols; Filter columns of the matrix. filter_entries; Filter entries of the matrix. filter_rows; Filter rows of the matrix. from_parts; Create a MatrixTable from its component parts. from_rows_table; Construct matrix table with no columns from a table. globals_table; Returns a table with a single row with the globals of the matrix table. group_cols_by; Group columns, used with GroupedMatrixTable.aggregate(). group_rows_by; Group rows, used with GroupedMatrixTable.aggregate(). head; Subset matrix to first n_rows rows and n_cols cols. index_cols; Expose the column values as if looked up in a dictionary, indexing with exprs. index_entries; Expose the entries as if looked up in a dictionary, indexing with exprs. index_globals; Return this matrix table's global variables for use in another expression context. index_rows; Expose the row values as if looked up in a dictionary, indexing with exprs. key_cols_by; Key columns by a new set of fields. key_rows_by; Key rows by a new set of fields. localize_entries; Convert the matrix table to a table with entries localized as an array of structs. make_table; Make a table from a matrix table with one field per sample. n_partitions; Number of partitions. naive_coalesce; Naively decrease the number of partitions. persist; Persist this table in memory or on disk. rename; Rename fields of a matrix table. repartition; Change the number of partitions. rows; Returns a table with all row fields in the matrix. sample_cols; Downsample the matrix table by keeping each column with probability p. sample_rows; Downsample the matrix table by keeping each row with probability p. select_cols; Select existing column fields or create new fields by name, dropping the rest. select_entries; Select existing entry fields or ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
Performance,cache,cache,"umn-indexed fields. globals; Returns a struct expression including all global fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of typ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
Safety,avoid,avoid,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
Testability,test,tested,"in their corresponding arrays. Note; The order of the columns is not guaranteed. Returns:; MatrixTable. cols()[source]; Returns a table with all column fields in the matrix.; Examples; Extract the column table:; >>> cols_table = dataset.cols(). Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the column key (which becomes the table key). To preserve the original; column order as the table row order, first unkey the columns using; key_cols_by() with no arguments. Returns:; Table – Table with all column fields from the matrix, with one row per column of the matrix. compute_entry_filter_stats(row_field='entry_stats_row', col_field='entry_stats_col')[source]; Compute statistics about the number and fraction of filtered entries. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. row_field (str) – Name for computed row field (default: entry_stats_row.; col_field (str) – Name for computed column field (default: entry_stats_col. Returns:; MatrixTable. Notes; Adds a new row field, row_field, and a new column field, col_field,; each of which are structs with the following fields:. n_filtered (tint64) - Number of filtered entries per row; or column.; n_remaining (tint64) - Number of entries not filtered per; row or column.; fraction_filtered (tfloat32) - Number of filtered entries; divided by the total number of filtered and remaining entries. See also; filter_entries(), unfilter_entries(). count()[source]; Count the number of rows and columns in the matrix.; Examples; >>> dataset.count(). Returns:; int, int – Number of rows, number of cols. count_cols(_localize=True)[source]; Count the number of columns in the matrix.; Examples; Count the number of columns:; >>> n_cols = dataset.count_cols(). Returns:; ",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
Usability,simpl,simply,"Then the result of; make_table():; >>> ht = mt.make_table() . has the original row fields along with 6 additional fields,; one for each sample and entry field:; Global fields:; 'batch': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'A.GT': call; 'A.GQ': int32; 'B.GT': call; 'B.GQ': int32; 'C.GT': call; 'C.GQ': int32; Key:; 'locus': locus<GRCh37>; 'alleles': array<str>. n_partitions()[source]; Number of partitions.; Notes; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see here; for details. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use",MatchSource.WIKI,docs/0.2/hail.MatrixTable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html
Availability,checkpoint,checkpoint," (table1.group_by(table1.SEX); ... .aggregate(mean_height_data = hl.agg.mean(table1.HT))); >>> table3.show(). Join tables together inside an annotation expression:; >>> table2 = table2.key_by('ID'); >>> table1 = table1.annotate(B = table2[table1.ID].B); >>> table1.show(). Attributes. globals; Returns a struct expression including all global fields. key; Row key struct. row; Returns a struct expression of all row-indexed fields, including keys. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_index; Add the integer index of each row as a new row field. aggregate; Aggregate over rows into a local value. all; Evaluate whether a boolean expression is true for all rows. annotate; Add new fields. annotate_globals; Add new global fields. anti_join; Filters the table to rows whose key does not appear in other. any; Evaluate whether a Boolean expression is true for at least one row. cache; Persist this table in memory. checkpoint; Checkpoint the table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. index_globals; Return this table's global variables",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
Deployability,pipeline,pipeline,"xploded table.; Missing arrays or sets are treated as empty.; Currently, the name argument may not be used if field is not a; top-level field of the table (e.g. name may be used with ht.foo; but not ht.foo.bar). Parameters:. field (str or Expression) – Top-level field name or expression.; name (str or None) – If not None, rename the exploded field to name. Returns:; Table. export(output, types_file=None, header=True, parallel=None, delimiter='\t')[source]; Export to a text file.; Examples; Export to a tab-separated file:; >>> table1.export('output/table1.tsv.bgz'). Note; It is highly recommended to export large files with a .bgz extension,; which will use a block gzipped compression codec. These files can be; read natively with any Hail method, as well as with Python’s gzip.open; and R’s read.table.; Nested structures will be exported as JSON. In order to export nested struct; fields as separate fields in the resulting table, use flatten() first. Warning; Do not export to a path that is being read from in the same pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is p",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
Integrability,depend,dependent,"-------+-------+-------+. Notes; This method returns a table with a new field whose name is given by; the name parameter, with type tint64. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like Table.take() or Table.export()) will; return rows in order.; This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters:; name (str) – Name of index field. Returns:; Table – Table with a new index field. aggregate(expr, _localize=True)[source]; Aggregate over rows into a local value.; Examples; Aggregate over rows:; >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. all(expr)[source]; Evaluate whether a boolean expression is true for all rows.; Examples; Test whether C1 is greater than 5 in all rows of the table:; >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters:; expr (BooleanExpression) – Expression to test. Returns:; bool. annotate(**named_exprs)[source]; Add new fields.; New Table fields may be defined in several ways:. In terms of constant values. Every row will have the same value.; In terms of other fields in the table.; In terms of fields in other tables, this is called “joining”. Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
Modifiability,variab,variables,"e table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. index_globals; Return this table's global variables for use in another expression context. join; Join two tables together. key_by; Key table by a new set of fields. multi_way_zip_join; Combine many tables in a zip join. n_partitions; Returns the number of partitions in the table. naive_coalesce; Naively decrease the number of partitions. order_by; Sort by the specified fields, defaulting to ascending order. parallelize; Parallelize a local array of structs into a distributed table. persist; Persist this table in memory or on disk. rename; Rename fields of the table. repartition; Change the number of partitions. sample; Downsample the table by keeping each row with probability p. select; Select existing fields or create new fields by name, dropping the rest. select_globals; Select existing global fields or create new fields by name, dropping the rest. semi_join; Filters the table to rows whose key appears in other. show; Print the first few rows of the table to the console. summarize; Compute and print summary information about th",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
Performance,cache,cache,"and aggregate to produce a new table:; >>> table3 = (table1.group_by(table1.SEX); ... .aggregate(mean_height_data = hl.agg.mean(table1.HT))); >>> table3.show(). Join tables together inside an annotation expression:; >>> table2 = table2.key_by('ID'); >>> table1 = table1.annotate(B = table2[table1.ID].B); >>> table1.show(). Attributes. globals; Returns a struct expression including all global fields. key; Row key struct. row; Returns a struct expression of all row-indexed fields, including keys. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_index; Add the integer index of each row as a new row field. aggregate; Aggregate over rows into a local value. all; Evaluate whether a boolean expression is true for all rows. annotate; Add new fields. annotate_globals; Add new global fields. anti_join; Filters the table to rows whose key does not appear in other. any; Evaluate whether a Boolean expression is true for at least one row. cache; Persist this table in memory. checkpoint; Checkpoint the table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
Safety,avoid,avoid,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
Security,access,accessed,"False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (struct.foo) or string indexing (struct['foo']). Warning; Using this method can cause out of memory errors. Only collect small tables. Returns:; list of Struct – List of rows. collect_by_key(name='values')[source]; Collect values for each unique key into an array. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([; ... {'t': 'foo', 'x': 4, 'y': 'A'},; ... {'t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar', 'x': -3, 'y': 'C'},; ... {'t': 'quam', 'x': 0, 'y': 'D'}],; ... hl.tstruct(t=hl.tstr, x=hl.tint32, y=hl.tstr),; ... key='t'). >>> t1.show(); +--------+-------+-----+; | t | x | y |; +--------+-------+-----+; | str | int32 | str |; +--------+-------+-----+; | ""bar"" | 2 | ""B"" |; | ""bar"" | -3 | ""C"" |; | ""foo"" | 4 | ""A"" |; | ""quam"" | 0 | ""D"" |; +--------+-------+-----+. >>> t1.collect_by_key().show(); +--------+---------------------------------+; | t | values |; +--------+---------------------------------+; | str | array<struc",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
Testability,test,test," will; return rows in order.; This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters:; name (str) – Name of index field. Returns:; Table – Table with a new index field. aggregate(expr, _localize=True)[source]; Aggregate over rows into a local value.; Examples; Aggregate over rows:; >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. all(expr)[source]; Evaluate whether a boolean expression is true for all rows.; Examples; Test whether C1 is greater than 5 in all rows of the table:; >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters:; expr (BooleanExpression) – Expression to test. Returns:; bool. annotate(**named_exprs)[source]; Add new fields.; New Table fields may be defined in several ways:. In terms of constant values. Every row will have the same value.; In terms of other fields in the table.; In terms of fields in other tables, this is called “joining”. Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Add field Y containing the square of field X; >>> ht = ht.annotate(Y = ht.X ** 2); >>> ht.show(); +-------+-------+-----+-------+-------+----------+; | ID | HT | SEX | X | Z | Y |; +-------+-------+-----+-------+-------+----------+; | int32 | int32 | str | int32 | ",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
Usability,learn,learning,"﻿. Hail | ; Table. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Table; GroupedTable; MatrixTable; GroupedMatrixTable. Modules; Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Table. View page source. Table. class hail.Table[source]; Hail’s distributed implementation of a dataframe or SQL table.; Use read_table() to read a table that was written with; Table.write(). Use to_spark() and Table.from_spark(); to inter-operate with PySpark’s; SQL and; machine learning; functionality.; Examples; The examples below use table1 and table2, which are imported; from text files using import_table().; >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). +-------+-------+--------+; | ID | A | B |; +-------+-------+--------+; | int32 | int32 | str |; +-------+-------+--------+; | 1 | 65 | cat |; | 2 | 72 | dog |; | 3 | 70 | mouse |; | 4 | 60 | rabbit |; +-------+-------+--------+. Define new annotations:; >>> height_mean_m = 68; >>> height_sd_m = 3; >>> he",MatchSource.WIKI,docs/0.2/hail.Table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail.Table.html
Deployability,update,updated,"﻿. Hail | ; Hail on the Cloud. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud. View page source. Hail on the Cloud; Public clouds are a natural place to run Hail, offering the ability to run on-demand workloads with; high elasticity. Microsoft Azure, Google Cloud Platform, Databricks and Amazon Web Services make it; possible to rent Spark clusters with thousands of cores on-demand, providing for the elastic compute; requirements of scientific research without an up-front capital investment in hardware. General Advice; Start Small; Estimating time; Estimating cost. Query-on-Batch; Getting Started; Variant Effect Predictor (VEP). Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/hail_on_the_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/hail_on_the_cloud.html
Deployability,install,installation,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
Performance,scalab,scalable,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
Testability,test,tests,"ial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; Version 0.2.123; Version 0.2.122; Version 0.2.121; Version 0.2.120; Version 0.2.119; Version 0.2.118; Version 0.2.117; Version 0.2.116; Version 0.2.115; Version 0.2.114; Version 0.2.113; Version 0.2.112; Version 0.2.111; Version 0.2.110; Version 0.2.109; Version 0.2.108; Version 0.2.107; Version 0.2.106; Version 0.2.105; Version 0.2.104; Version 0.2.103; Version 0.2.102; Version 0.2.101; Version 0.2.100; Version 0.2.99; Version 0.2.98; Version 0.2.97; Version 0.2.96; Version 0.2.95; Version 0.2.94; Version 0.2.93; Version 0.2.92; Version 0.2.91; Version 0.2.90; Version 0.2.89; Version 0.2.88; Version 0.2.87; Version 0.2.86; Ver",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
Usability,simpl,simple,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,docs/0.2/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/index.html
Deployability,install,install,"﻿. Hail | ; Libraries. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Libraries. View page source. Libraries; This pages lists any external libraries we are aware of that are built on top of Hail. These libraries are not developed by the Hail team so we cannot necessarily answer; questions about them, but they may provide useful functions not included in base Hail. gnomad (Hail Utilities for gnomAD); This repo contains a number of Hail utility functions and scripts for the gnomAD project and the Translational Genomics Group.; Install with pip install gnomad.; More info can be found in the documentation or on the PyPI project page. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/libraries.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/libraries.html
Deployability,update,updated,"﻿. Hail | ; Other Resources. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Hadoop Glob Patterns. Change Log And Version Policy. menu; Hail. Other Resources. View page source. Other Resources. Hadoop Glob Patterns. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/other_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/other_resources.html
Availability,avail,available,"ins=10, range=((0, 1), None)). Parameters:. x (NumericExpression) – Expression for x-axis (from a Hail table).; y (NumericExpression) – Expression for y-axis (from the same Hail table as x).; bins (int or [int, int]) – The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range (None or ((float, float), (float, float))) – The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width (int) – Plot width (default 600px).; height (int) – Plot height (default 600px).; title (str) – Title of the plot.; colors (Sequence[str]) – List of colors (hex codes, or strings as described; here). Compatible with one of the many; built-in palettes available here.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.scatter(x, y, label=None, title=None, xlabel=None, ylabel=None, size=4, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create an interactive scatter plot.; x and y must both be either:; - a NumericExpression from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (othe",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
Deployability,continuous,continuous,"plot.scatter(x, y, label=None, title=None, xlabel=None, ylabel=None, size=4, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create an interactive scatter plot.; x and y must both be either:; - a NumericExpression from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. x (NumericExpression or (str, NumericExpression)) – List of x-values to be plotted.; y (NumericExpression or (str, NumericExpression)) – List of y-values to be plotted.; label (Expression or Dict[str, Expression]], optional) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be ",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
Modifiability,extend,extend,"﻿. Hail | ; Plot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Plot. View page source. Plot. Warning; Plotting functionality is in early stages and is experimental. Interfaces will change regularly. Plotting in Hail is easy. Hail’s plot functions utilize Bokeh plotting libraries to create attractive,; interactive figures. Plotting functions in this module return a Bokeh Figure, so you can call; a method to plot your data and then choose to extend the plot however you like by interacting; directly with Bokeh. See the GWAS tutorial for examples.; Plot functions in Hail accept data in the form of either Python objects or Table and MatrixTable fields. cdf; Create a cumulative density plot. pdf. smoothed_pdf; Create a density plot. histogram; Create a histogram. cumulative_histogram; Create a cumulative histogram. histogram2d; Plot a two-dimensional histogram. scatter; Create an interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; leg",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
Testability,log,log,"ot functions utilize Bokeh plotting libraries to create attractive,; interactive figures. Plotting functions in this module return a Bokeh Figure, so you can call; a method to plot your data and then choose to extend the plot however you like by interacting; directly with Bokeh. See the GWAS tutorial for examples.; Plot functions in Hail accept data in the form of either Python objects or Table and MatrixTable fields. cdf; Create a cumulative density plot. pdf. smoothed_pdf; Create a density plot. histogram; Create a histogram. cumulative_histogram; Create a cumulative histogram. histogram2d; Plot a two-dimensional histogram. scatter; Create an interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (",MatchSource.WIKI,docs/0.2/plot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/plot.html
Deployability,update,updated,"﻿. Hail | ; Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API. View page source. Python API; This is the Python API documentation for all Hail Python libraries including Query (hail), a cloud-agnostic; file system implementation (hailtop.fs), and Batch (hailtop.batch). hail; hailtop.fs; hailtop.batch. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/root_api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/root_api.html
Deployability,rolling,rolling,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,docs/0.2/scans.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/scans.html
Performance,perform,perform,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,docs/0.2/scans.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/scans.html
Security,expose,exposed,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,docs/0.2/scans.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/scans.html
Deployability,update,updated,"﻿. Hail | ; Search. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Search. Please activate JavaScript to enable the search functionality.; . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/search.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/search.html
Availability,down,downloading,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html
Deployability,install,install,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html
Testability,test,test,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials-landing.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html
Deployability,update,updated,"ct type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique.; Structs are very common in Hail. Each component of a Table and MatrixTable; is a struct:. Table.row(); Table.globals(); MatrixTable.row(); MatrixTable.col(); MatrixTable.entry(); MatrixTable.globals(). Structs appear below the top-level component types as well. Consider the following join:; >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field to table1 called table2_fields. In the new table,; table2_fields will be a struct containing all the non-key fields from table2. Parameters:; field_types (keyword args of HailType) – Fields. See also; StructExpression, Struct. class hail.expr.types.ttuple(*types)[source]; Hail type for tuples.; In Python, these are represented as tuple. Parameters:; types (varargs of HailType) – Element types. See also; TupleExpression. hail.expr.types.tcall = dtype('call'); Hail type for a diploid genotype.; In Python, these are represented by Call. See also; CallExpression, Call, call(), parse_call(), unphased_diploid_gt_index_call(). class hail.expr.types.tlocus(reference_genome='default')[source]; Hail type for a genomic coordinate with a contig and a position.; In Python, these are represented by Locus. Parameters:; reference_genome (ReferenceGenome or str) – Reference genome to use. See also; LocusExpression, locus(), parse_locus(), Locus. reference_genome; Reference genome. Returns:; ReferenceGenome – Reference genome. class hail.expr.types.tinterval(point_type)[source]; Hail type for intervals of ordered values.; In Python, these are represented by Interval. Parameters:; point_type (HailType) – Interval point type. See also; IntervalExpression, Interval, interval(), parse_locus_interval(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
Integrability,interface,interface,"bit floating point numbers.; In Python, these are represented as float. See also; Float64Expression, float(), float64(). hail.expr.types.tstr = dtype('str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType)",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
Modifiability,variab,variable-length,"e numbers, strings, or; numpy arrays.; Types are very important in Hail, because the fields of Table and; MatrixTable objects have data types. Primitive types; Hail’s primitive data types for boolean, numeric and string objects are:. tint; Alias for tint32. tint32; Hail type for signed 32-bit integers. tint64; Hail type for signed 64-bit integers. tfloat; Alias for tfloat64. tfloat32; Hail type for 32-bit floating point numbers. tfloat64; Hail type for 64-bit floating point numbers. tstr; Hail type for text strings. tbool; Hail type for Boolean (True or False) values. Container types; Hail’s container types are:. tarray - Ordered collection of homogenous objects.; tndarray - Ordered n-dimensional arrays of homogenous objects.; tset - Unordered collection of distinct homogenous objects.; tdict - Key-value map. Keys and values are both homogenous.; ttuple - Tuple of heterogeneous values.; tstruct - Structure containing named fields, each with its own; type. tarray; Hail type for variable-length arrays of elements. tndarray; Hail type for n-dimensional arrays. tset; Hail type for collections of distinct elements. tdict; Hail type for key-value maps. ttuple; Hail type for tuples. tinterval; Hail type for intervals of ordered values. tstruct; Hail type for structured groups of heterogeneous fields. Genetics types; Hail has two genetics-specific types:. tlocus; Hail type for a genomic coordinate with a contig and a position. tcall; Hail type for a diploid genotype. When to work with types; In general, you won’t need to mention types explicitly.; There are a few situations where you may want to specify types explicitly:. To specify column types in import_table() if the impute flag does not; infer the type you want.; When converting a Python value to a Hail expression with literal(),; if you don’t wish to rely on the inferred type.; With functions like missing() and empty_array(). Viewing an object’s type; Hail objects have a dtype field that will print their type.; >>",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
Testability,test,tested,"bit floating point numbers.; In Python, these are represented as float. See also; Float64Expression, float(), float64(). hail.expr.types.tstr = dtype('str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType)",MatchSource.WIKI,docs/0.2/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/types.html
Availability,failure,failure,"tches.; Example: name = pca_pipeline; Example: name =~ pca. Predefined Keyword Expression; The left hand side of the statement is a special Batch-specific keyword which can be one of the values; listed in the tables below. Allowed operators are dependent on the type of the value expected for each; keyword, but can be one of =, ==, !=, >, >=, <, <=, =~, !~.; The right hand side is the value to search against. Keywords. Keyword; Value Type; Allowed Operators; Extra. cost; float; =, ==, !=, >, >=, <, <=. duration; float; =, ==, !=, >, >=, <, <=; Values are rounded to the millisecond. start_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. end_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. Example: cost >= 1.00; Example: duration > 5; Example: start_time >= 2023-02-24T17:15:25Z. Keywords specific to searching for batches. Keyword; Value Type; Allowed Operators; Extra. batch_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are running, complete, success, failure, cancelled, open, closed. user; str; =, ==, !=, =~, !~. billing_project; str; =, ==, !=, =~, !~. Example: state = running; Example: user = johndoe; Example: billing_project = johndoe-trial. Keywords specific to searching for jobs in a batch. Keyword; Value Type; Allowed Operators; Extra. job_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are pending, ready, creating, running, live, cancelled, error, failed, bad, success, done. instance; str; =, ==, !=, =~, !~; use this to search for all jobs that ran on a given worker. instance_collection; str; =, ==, !=, =~, !~; use this to search for all jobs in a given pool. Example: user = johndoe; Example: billing_project = johndoe-trial; Example: instance_collection = standard. Combining Multiple Statements; Example: Searching for batches in a time window; start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-07-01T12:35:00Z. Example: Searching for batches that have run since June 2023 that cos",MatchSource.WIKI,docs/batch/advanced_search_help.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/advanced_search_help.html
Integrability,depend,dependent,"artial match, keyword, or predefined keyword) listed below. When the query is run, each; statement will be joined to the next with the AND operator. Exact Match Expression; A single word enclosed with double quotes that is an exact match for either the name or; value of an attribute.; Example: ""pca_pipeline"". Partial Match Expression; A single word without any quotes that is a partial match for either the name or the value; of an attribute.; Example: pipe. Keyword Expression; The left hand side of the statement is the name of the attribute and the right hand side; is the value to search against. Allowed operators are =, ==, !=, =~, and; !~ where the operators with tildes are looking for partial matches.; Example: name = pca_pipeline; Example: name =~ pca. Predefined Keyword Expression; The left hand side of the statement is a special Batch-specific keyword which can be one of the values; listed in the tables below. Allowed operators are dependent on the type of the value expected for each; keyword, but can be one of =, ==, !=, >, >=, <, <=, =~, !~.; The right hand side is the value to search against. Keywords. Keyword; Value Type; Allowed Operators; Extra. cost; float; =, ==, !=, >, >=, <, <=. duration; float; =, ==, !=, >, >=, <, <=; Values are rounded to the millisecond. start_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. end_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. Example: cost >= 1.00; Example: duration > 5; Example: start_time >= 2023-02-24T17:15:25Z. Keywords specific to searching for batches. Keyword; Value Type; Allowed Operators; Extra. batch_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are running, complete, success, failure, cancelled, open, closed. user; str; =, ==, !=, =~, !~. billing_project; str; =, ==, !=, =~, !~. Example: state = running; Example: user = johndoe; Example: billing_project = johndoe-trial. Keywords specific to searching for jobs in a batch. Keyword; Value Type; ",MatchSource.WIKI,docs/batch/advanced_search_help.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/advanced_search_help.html
Deployability,install,installed,"of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.build_python_image; Build a new Python image with dill and the specified pip packages installed. utils.concatenate; Concatenate files using tree aggregation. utils.plink_merge; Merge binary PLINK files using tree aggregation. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
Integrability,interface,interface,". Python API — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; PythonJob. Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; BatchPoolExecutor; BatchPoolFuture. Backends; RunningBatchType; Backend; LocalBackend; ServiceBackend. Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API. View page source. Python API; This is the API documentation for Batch, and provides detailed information; on the Python programming interface.; Use import hailtop.batch to access this functionality. Batches; A Batch is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each Job has; an image in which to execute commands and settings for storage,; memory, and CPU. A BashJob is a subclass of Job; that runs bash commands while a PythonJob executes Python; functions. batch.Batch; Object representing the distributed acyclic graph (DAG) of jobs to run. job.Job; Object representing a single job to execute. job.BashJob; Object representing a single bash job to execute. job.PythonJob; Object representing a single Python job to execute. Resources; A Resource is an abstract class that represents files in a Batch and; has two subtypes: ResourceFile and ResourceGroup.; A single file is represented by a ResourceFile which has two subtypes:; InputResourceFile and JobResourceFile. An InputResourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represent",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
Performance,concurren,concurrent,"esourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
Security,access,access,". Python API — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; PythonJob. Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; BatchPoolExecutor; BatchPoolFuture. Backends; RunningBatchType; Backend; LocalBackend; ServiceBackend. Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API. View page source. Python API; This is the API documentation for Batch, and provides detailed information; on the Python programming interface.; Use import hailtop.batch to access this functionality. Batches; A Batch is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each Job has; an image in which to execute commands and settings for storage,; memory, and CPU. A BashJob is a subclass of Job; that runs bash commands while a PythonJob executes Python; functions. batch.Batch; Object representing the distributed acyclic graph (DAG) of jobs to run. job.Job; Object representing a single job to execute. job.BashJob; Object representing a single bash job to execute. job.PythonJob; Object representing a single Python job to execute. Resources; A Resource is an abstract class that represents files in a Batch and; has two subtypes: ResourceFile and ResourceGroup.; A single file is represented by a ResourceFile which has two subtypes:; InputResourceFile and JobResourceFile. An InputResourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represent",MatchSource.WIKI,docs/batch/api.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api.html
Availability,error,error,"d_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle argume",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
Deployability,release,released,". Python Version Compatibility Policy — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python Version Compatibility Policy. View page source. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility policy on Python; versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
Energy Efficiency,allocate,allocated," Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fi",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
Integrability,message,message," the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fixed the documentation for job memory and storage requests to have default units in bytes. Previous. © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
Modifiability,config,configuration,"n released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
Performance,queue,queued,"on Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python Version Compatibility Policy. View page source. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility policy on Python; versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memo",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
Safety,avoid,avoid,"g only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
Security,authoriz,authorization,"king directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the Lo",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
Testability,log,login,"n released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) ",MatchSource.WIKI,docs/batch/change_log.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/change_log.html
Availability,down,download," focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Docke",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
Deployability,install,install,". Docker Resources — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Docker Resources. View page source. Docker Resources. What is Docker?; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker i",MatchSource.WIKI,docs/batch/docker_resources.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/docker_resources.html
Safety,timeout,timeout,"iltop.batch.job). JobResourceFile (class in hailtop.batch.resource). L. LocalBackend (class in hailtop.batch.backend). M. map() (hailtop.batch.batch_pool_executor.BatchPoolExecutor method). memory() (hailtop.batch.job.Job method). N. new_bash_job() (hailtop.batch.batch.Batch method). new_job() (hailtop.batch.batch.Batch method). new_python_job() (hailtop.batch.batch.Batch method). P. plink_merge() (in module hailtop.batch.utils). PythonJob (class in hailtop.batch.job). PythonResult (class in hailtop.batch.resource). R. read_input() (hailtop.batch.batch.Batch method). read_input_group() (hailtop.batch.batch.Batch method). regions() (hailtop.batch.job.Job method). requester_pays_fs() (hailtop.batch.backend.Backend method). Resource (class in hailtop.batch.resource). ResourceFile (class in hailtop.batch.resource). ResourceGroup (class in hailtop.batch.resource). result() (hailtop.batch.batch_pool_executor.BatchPoolFuture method). run() (hailtop.batch.batch.Batch method). running() (hailtop.batch.batch_pool_executor.BatchPoolFuture method). RunningBatchType (class in hailtop.batch.backend). S. select_jobs() (hailtop.batch.batch.Batch method). ServiceBackend (class in hailtop.batch.backend). shutdown() (hailtop.batch.batch_pool_executor.BatchPoolExecutor method). source() (hailtop.batch.resource.InputResourceFile method). (hailtop.batch.resource.JobResourceFile method). (hailtop.batch.resource.PythonResult method). (hailtop.batch.resource.Resource method). (hailtop.batch.resource.ResourceGroup method). spot() (hailtop.batch.job.Job method). storage() (hailtop.batch.job.Job method). submit() (hailtop.batch.batch_pool_executor.BatchPoolExecutor method). supported_regions() (hailtop.batch.backend.ServiceBackend static method). T. timeout() (hailtop.batch.job.Job method). V. validate_file() (hailtop.batch.backend.Backend method). W. write_output() (hailtop.batch.batch.Batch method). © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/genindex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/genindex.html
Availability,avail,available,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/getting_started.html
Deployability,install,install,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/getting_started.html
Integrability,depend,depends,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/getting_started.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/getting_started.html
Deployability,pipeline,pipelines,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch. View page source. Batch; Batch is a Python module for creating and executing jobs. A job consists of a bash; command to run as well as a specification of the resources required and some metadata.; Batch allows you to easily build complicated computational pipelines with many jobs and numerous; dependencies. Batches can either be executed locally or with the Batch Service. Contents. Getting Started; Installation. Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Clumping GWAS Results; Random Forest. Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Exact Match Expression; Partial Match Expression; Keyword Expression; Predefined Keyword Expression; Combining Multiple Statements. Python Version Compatibility Policy; Change Log. Indices and tables. Index; Search Page. Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/index.html
Integrability,depend,dependencies,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch. View page source. Batch; Batch is a Python module for creating and executing jobs. A job consists of a bash; command to run as well as a specification of the resources required and some metadata.; Batch allows you to easily build complicated computational pipelines with many jobs and numerous; dependencies. Batches can either be executed locally or with the Batch Service. Contents. Getting Started; Installation. Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Clumping GWAS Results; Random Forest. Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Exact Match Expression; Partial Match Expression; Keyword Expression; Predefined Keyword Expression; Combining Multiple Statements. Python Version Compatibility Policy; Change Log. Indices and tables. Index; Search Page. Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/index.html
Availability,avail,available,"﻿. Batch Service — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Loca",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
Deployability,install,installed,"﻿. Batch Service — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Loca",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
Energy Efficiency,schedul,scheduler,"tch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
Integrability,depend,dependent," users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gclo",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
Modifiability,config,configuration,"ore/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK ",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
Safety,avoid,avoid,"ill be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-proj",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
Security,access,access," API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; fil",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
Testability,log,logs," API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; fil",MatchSource.WIKI,docs/batch/service.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/service.html
Availability,echo,echo,"e resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
Deployability,install,installed,". Tutorial — Batch documentation. Batch; . Getting Started; Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Tutorial. View page source. Tutorial; This tutorial goes through the basic concepts of Batch with examples. Import; Batch is located inside the hailtop module, which can be installed; as described in the Getting Started section.; >>> import hailtop.batch as hb. f-strings; f-strings were added to Python in version 3.6 and are denoted by the ‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states th",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
Integrability,depend,dependencies,"rrent variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two job",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
Modifiability,variab,variable,". Tutorial — Batch documentation. Batch; . Getting Started; Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Tutorial. View page source. Tutorial; This tutorial goes through the basic concepts of Batch with examples. Import; Batch is located inside the hailtop module, which can be installed; as described in the Getting Started section.; >>> import hailtop.batch as hb. f-strings; f-strings were added to Python in version 3.6 and are denoted by the ‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states th",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
Security,access,accessed,"s_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo """,MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
Usability,learn,learn,"‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('e",MatchSource.WIKI,docs/batch/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/tutorial.html
Energy Efficiency,reduce,reduced,"butes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the ",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
Modifiability,polymorphi,polymorphism,"﻿. . AltAllele — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; AltAllele. View page source. AltAllele¶. class hail.representation.AltAllele(ref, alt)[source]¶; An object that represents an allele in a polymorphism deviating from the reference allele. Parameters:; ref (str) – reference allele; alt (str) – alternate allele. Attributes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphis",MatchSource.WIKI,docs/0.1/representation/hail.representation.AltAllele.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html
Modifiability,variab,variables,"; True if the call contains two different alternate alleles. Return type:bool. is_het_ref()[source]¶; True if the call contains one reference and one alternate allele. Return type:bool. is_hom_ref()[source]¶; True if the call is 0/0. Return type:bool. is_hom_var()[source]¶; True if the call contains two identical alternate alleles. Return type:bool. is_not_called()[source]¶; True if the call is missing. Return type:bool. num_alt_alleles()[source]¶; Returns the count of non-reference alleles.; This function returns None if the genotype call is missing. Return type:int or None. one_hot_alleles(num_alleles)[source]¶; Returns a list containing the one-hot encoded representation of the called alleles.; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Call.html
Modifiability,variab,variables,"ol. is_hom_ref()[source]¶; True if the genotype call is 0/0. Return type:bool. is_hom_var()[source]¶; True if the genotype call contains two identical alternate alleles. Return type:bool. is_not_called()[source]¶; True if the genotype call is missing. Return type:bool. num_alt_alleles()[source]¶; Returns the count of non-reference alleles.; This function returns None if the genotype call is missing. Return type:int or None. od()[source]¶; Returns the difference between the total depth and the allelic depth sum.; Equivalent to:; g.dp - sum(g.ad). Return type:int or None. one_hot_alleles(num_alleles)[source]¶; Returns a list containing the one-hot encoded representation of the called alleles.; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotyp",MatchSource.WIKI,docs/0.1/representation/hail.representation.Genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html
Testability,test,test,".; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. Parameters:num_genotypes (int) – number of possible genotypes. Return type:list of int or None. p_ab(theta=0.5)[source]¶; Returns the p-value associated with finding the given allele depth ratio.; This function uses a one-tailed binomial test.; This function returns None if the allelic depth (ad) is missing. Parameters:theta (float) – null reference probability for binomial model. Return type:float. pl¶; Returns the phred-scaled genotype posterior likelihoods. Return type:list of int or None. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html
Testability,test,test,"rio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. complete_trios; List of trio objects that have a defined father, mother, and sex. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]¶; List of trio objects that have a defined father, mother, and sex. Return type:list of Trio. filter_to(samples)[source]¶; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, the father is set to None.; If the mother is not in the list of samples provided, the mother is set to None. Parameters:samples (list of str) – list of sample IDs to keep. Return type:Pedigree. static read(fam_path, delimiter='\\s+')[source]¶; Read a .fam file and return a pedigree object.; Examples; >>> ped = Pedigree.read('data/test.fam'). Notes; This method reads a PLINK .fam file.; Hail expects a file in the same spec as PLINK outlines. Parameters:; fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:Pedigree. trios¶; List of trio objects in this pedigree. Return type:list of Trio. write(path)[source]¶; Write a .fam file to the given path.; Examples; >>> ped = Pedigree.read('data/test.fam'); >>> ped.write('out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data structure in Hail.; Reading and writing a PLINK .fam file will result in loss of this information.; Use the key table method import_fam() to manipulate this; information. Parameters:path (str) – output path. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Pedigree.html
Security,access,accessing,"﻿. . Struct — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Struct. View page source. Struct¶. class hail.representation.Struct(attributes)[source]¶; Nested annotation structure.; >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both ‘items’ and ‘attributes’, which; allows either syntax for accessing the element “foo” of struct “bar”:; >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name “1kg”, for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:; >>> getattr(bar, '1kg'); >>> bar['1kg']. The pprint module can be used to print nested Structs in a more; human-readable fashion:; >>> from pprint import pprint; >>> pprint(bar). Parameters:attributes (dict) – struct members. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. get; Get an item, or return a default value if the item is not found. get(item, default=None)[source]¶; Get an item, or return a default value if the item is not found. Parameters:; item (str) – Name of attribute.; default – Default value. Returns:Value of item if found, or default value if not. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/hail.representation.Struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Struct.html
Modifiability,polymorphi,polymorphism,"﻿. . Variant — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Variant. View page source. Variant¶. class hail.representation.Variant(contig, start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial D",MatchSource.WIKI,docs/0.1/representation/hail.representation.Variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html
Modifiability,polymorphi,polymorphism,"﻿. . representation — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation. View page source. representation¶. Classes. hail.representation.Variant; An object that represents a genomic polymorphism. hail.representation.AltAllele; An object that represents an allele in a polymorphism deviating from the reference allele. hail.representation.Genotype; An object that represents an individual’s genotype at a genomic locus. hail.representation.Call; An object that represents an individual’s call at a genomic locus. hail.representation.Locus; An object that represents a location in the genome. hail.representation.Interval; A genomic interval marked by start and end loci. hail.representation.Trio; Class containing information about nuclear family relatedness and sex. hail.representation.Pedigree; Class containing a list of trios, with extra functionality. hail.representation.Struct; Nested annotation structure. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/representation/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/representation/index.html
Availability,down,download,"﻿. . Using the expression language to slice, dice, and query genetic data — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Introduction to the expression language; Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.is",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
Energy Efficiency,power,powerful,"',; u'HG02282',; u'HG02477']},; {u'at': u'8:95863909:A:T', u'homvars': []},; {u'at': u'8:97172671:C:T', u'homvars': []}]. takeBy¶; takeBy is an aggregator that takes elements of an aggregable ordered; by a lambda function (smallest to largest). We can easily select the; variants with the lowest p-values after regression:. In [42]:. top_5_pvals = (vds.linreg('sa.metadata.CaffeineConsumption'); .query_variants('variants.map(v => {at: str(v), pval: va.linreg.pval}).takeBy(x => x.pval, 5)')); pprint(top_5_pvals). 2018-10-18 01:26:07 Hail: INFO: Running linear regression on 1000 samples with 1 covariate including intercept... [{u'at': u'10:56025604:A:C', u'pval': 5.595049078641033e-05},; {u'at': u'20:55431571:A:C', u'pval': 0.00010899661736561121},; {u'at': u'10:91099630:T:C', u'pval': 0.00013497679316886596},; {u'at': u'4:149350527:T:C', u'pval': 0.00017786066989195366},; {u'at': u'7:152600817:G:A', u'pval': 0.0002252314501866726}]. Aggregating by key¶; The; aggregate_by_key; method is likely the most powerful piece of query functionality in Hail.; It’s a method on KeyTable.; You can produce key tables from a; VariantDataset with; three methods:. variants_table():; a key table with the variant and variant annotations as columns.; There is one row per variant.; samples_table():; a key table with the sample and sample annotations as columns. There; is one row per sample.; genotypes_table():; a key table that is the coordinate representation of the genetic; matrix. The columns are the variant, variant annotations, sample,; sample annotations, and genotype. There is one row per variant/sample; combination: (N * M) total rows!. Using; aggregate_by_key; with; genotypes_table; can produce counts of loss of function variants in cases and controls; per gene, compute the mean depth per sample per exon, and much more. You; define the aggregation keys, and you define how to combine the rows.; This method produces another; KeyTable.; We use it here to compute the mean depth and quali",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
Integrability,wrap,wraps,"xpression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
Modifiability,variab,variable,"0961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. Types in action¶; We’ll produce some sample annotations with the; sample_qc; method, then use these annotations to demonstrate some of the expression; language features. In [5]:. vds = vds.variant_qc().cache().sample_qc(). In [6]:. pprint(vds.sample_schema). Struct{; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; }; }. Filtering with expressions¶; The schema printed above is the type of the sample annotations, which; are given the variable name ‘sa’ wherever they appear. Here, we use the; filter_samples_expr method to filter samples based on these; annotations. If we want to filter on the “dpMean” above, we need to; select the ‘qc’ field from the ‘sa’ struct, then select the ‘dpMean’; field from the ‘qc’ struct. These selections are done with dots.; There are four Hail methods that use the expression language to filter a; dataset: -; filter_variants_expr; -; filter_samples_expr; -; filter_genotypes; -; filter_alleles; All these methods take a Hail expression as a string argument, and; return a filtered dataset. In [7]:. # unfiltered; vds.num_samples. Out[7]:. 1000. In [8]:. vds.filter_samples_expr('sa.qc.dpMean > 5', keep=True).num_samples. Out[8]:. 699. In [9]:. vds.filter_samples_expr('sa.qc.dpMean <= 5', keep=False).num_samples. Out[9]:. 699. In [10]:. vds.filter_samples_expr('sa.qc.callRate > 0.95', keep=True).num_samples. Out[10]:. 928. In [11]:. vds.filter_samples_expr(",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
Performance,cache,cache,"tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). Downloading data (~50M) from Google Storage...; Download finished!; Extracting...; Done!. We will read a dataset from disk, and print some summary statistics; about it to re-familiarize ourselves. In [4]:. vds = hc.read('data/1kg.vds'); vds.summarize().report(). Samples: 1000; Variants: 10961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. Types in action¶; We’ll produce some sample annotations with the; sample_qc; method, then use these annotations to demonstrate some of the expression; language features. In [5]:. vds = vds.variant_qc().cache().sample_qc(). In [6]:. pprint(vds.sample_schema). Struct{; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; }; }. Filtering with expressions¶; The schema printed above is the type of the sample annotations, which; are given the variable name ‘sa’ wherever they appear. Here, we use the; filter_samples_expr method to filter samples based on these; annotations. If we want to filter on the “dpMean” above, we need to; select the ‘qc’ field from the ‘sa’ struct, then select the ‘dpMean’; field from the ‘qc’ struct. These selections are done with dots.; There are four Hail methods that use the expression language to filter a; dataset: -; filter_variants_expr; -; filter_samples_expr;",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
Security,access,accessed,"xpression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
Usability,simpl,simple,"ds.filter_variants_expr('va.qc.AF > 0.1', keep=True).count_variants(). Out[16]:. 7993L. In [17]:. # Filter on allele frequency and GQ mean; vds.filter_variants_expr('va.qc.AF > 0.1 && va.qc.gqMean > 20').count_variants(). Out[17]:. 7879L. In [18]:. # Genotype call rate across the entire dataset; vds.summarize().call_rate. Out[18]:. 0.9831634887327798. As we can see in the previous cell, the overall call rate of this; dataset is 98.7%. In [19]:. vds.filter_genotypes('g.gq >= 20', keep=True).summarize().call_rate. Out[19]:. 0.5495507709150625. However, 40% of those called genotypes are called with GQ 20 or less!; This corresponds to less than 99% confidence in the call. Annotating with expressions¶; It is also possible to produce new annotations with the expression; language. These take an expression of the form:; <new annotation name> = <expression>. To annotate samples, the new annotation name must also start with; sa. To annotate variants, it must always begin with va.; Here are some simple examples. In [20]:. (vds.annotate_samples_expr('sa.keepThisSample = sa.qc.callRate > 0.95 && sa.qc.dpMean > 5'); .filter_samples_expr('sa.keepThisSample', keep=True).num_samples). Out[20]:. 696. In [21]:. (vds.annotate_variants_expr('va.keepThisVariant = va.qc.AF > 0.1 && va.qc.gqMean > 20'); .filter_variants_expr('va.keepThisVariant').count_variants()). Out[21]:. 7879L. Key tables also have an; annotate; method. We can use this to produce new columns or redefine old ones:. In [22]:. kt.to_dataframe().show(5). +-------+----------+---------------+--------+----------+-------------------+; | Sample|Population|SuperPopulation|isFemale|PurpleHair|CaffeineConsumption|; +-------+----------+---------------+--------+----------+-------------------+; |NA19784| MXL| AMR| false| false| 8|; |NA19102| YRI| AFR| true| false| 6|; |HG00141| GBR| EUR| false| false| 6|; |HG01890| ACB| AFR| false| false| 8|; |HG00263| GBR| EUR| true| true| 6|; +-------+----------+---------------+--------+----------+",MatchSource.WIKI,docs/0.1/tutorials/expression-language-part-2.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html
Availability,down,download,"﻿. . Overview — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
Deployability,patch,patches,"w¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
Energy Efficiency,consumption,consumption,".01'); .ld_prune(memory_per_core=256, num_cores=4)). 2018-10-18 01:26:50 Hail: INFO: Running LD prune with nSamples=843, nVariants=9085, nPartitions=4, and maxQueueSize=257123.; 2018-10-18 01:26:50 Hail: INFO: LD prune step 1 of 3: nVariantsKept=8478, nPartitions=4, time=351.375ms; 2018-10-18 01:26:51 Hail: INFO: LD prune step 2 of 3: nVariantsKept=8478, nPartitions=12, time=1.184s; 2018-10-18 01:26:52 Hail: INFO: Coerced sorted dataset; 2018-10-18 01:26:52 Hail: INFO: LD prune step 3 of 3: nVariantsKept=8478, time=481.478ms. In [44]:. common_vds.count(). Out[44]:. (843L, 8555L). These filters removed about 15% of sites (we started with a bit over; 10,000). This is NOT representative of most sequencing datasets! We; have already downsampled the full thousand genomes dataset to include; more common variants than we’d expect by chance.; In Hail, the association tests accept sample annotations for the sample; phenotype and covariates. Since we’ve already got our phenotype of; interest (caffeine consumption) in the dataset, we are good to go:. In [45]:. gwas = common_vds.linreg('sa.CaffeineConsumption'); pprint(gwas.variant_schema). 2018-10-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; },; linreg: S",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
Integrability,interface,interface,"utations, but is probably the most complex part of Hail. See the; pair of tutorials on the expression language to learn more!; Here, we can use query_variants to pull out 5 variants to see what; they look like. In [7]:. vds.query_variants('variants.take(5)'). Out[7]:. [Variant(contig=1, start=904165, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=909917, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=986963, ref=C, alts=[AltAllele(ref=C, alt=T)]),; Variant(contig=1, start=1563691, ref=T, alts=[AltAllele(ref=T, alt=G)]),; Variant(contig=1, start=1707740, ref=T, alts=[AltAllele(ref=T, alt=G)])]. There are often several ways to do something in Hail. Here are two ways; to peek at the first few sample IDs:. In [8]:. vds.query_samples('samples.take(5)'). Out[8]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. In [9]:. vds.sample_ids[:5]. Out[9]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. There’s a similar interface for looking at the genotypes in a dataset.; We use; query_genotypes; to look at the first few genotype calls. In [10]:. vds.query_genotypes('gs.take(5)'). Out[10]:. [Genotype(GT=0, AD=[4, 0], DP=4, GQ=12, PL=[0, 12, 194]),; Genotype(GT=1, AD=[4, 3], DP=7, GQ=85, PL=[85, 0, 109]),; Genotype(GT=0, AD=[1, 0], DP=1, GQ=3, PL=[0, 3, 42]),; Genotype(GT=0, AD=[14, 0], DP=14, GQ=42, PL=[0, 42, 533]),; Genotype(GT=0, AD=[12, 0], DP=12, GQ=36, PL=[0, 36, 420])]. Integrate sample annotations¶; Hail treats variant and sample annotations as first-class citizens.; Annotations are usually a critical part of any genetic study. Sample; annotations are where you’ll store information about sample phenotypes,; ancestry, sex, and covariates. Variant annotations can be used to store; information like gene membership and functional impact for use in QC or; analysis.; In this tutorial, we demonstrate how to take a text file and use it to; annotate the samples in a VDS.; iPython supports various cell “magics”. The %%",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
Performance,cache,cache,"ndition_ab = '''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantg",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
Safety,recover,recover," with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Double; }; }. Now that we’ve got principal components per sample, we may as well plot; them! Human history exerts a strong effect in genetic datasets. Even; with a 50MB sequencing dataset, we can recover the major human; populations. In [51]:. pca_table = pca.samples_table().to_pandas(); colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}; plt.scatter(pca_table[""sa.pca.PC1""], pca_table[""sa.pca.PC2""],; c = pca_table[""sa.SuperPopulation""].map(colors),; alpha = .5); plt.xlim(-0.6, 0.6); plt.xlabel(""PC1""); plt.ylabel(""PC2""); legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]; plt.legend(handles=legend_entries, loc=2); plt.show(). Now we can rerun our linear regression, controlling for the first few; principal components and sample sex. In [52]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including inter",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
Testability,test,test,"﻿. . Overview — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
Usability,learn,learn,"ng a public 1000 genomes VCF to; about 50 MB. In [5]:. vds = hc.read('data/1kg.vds'). Getting to know our data¶; It’s important to have easy ways to slice, dice, query, and summarize a; dataset. Some of these methods are demonstrated below.; The; summarize; method is useful for providing a broad overview of the data contained in; a dataset. In [6]:. vds.summarize().report(). Samples: 1000; Variants: 10961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. The; query_variants; method is the first time we’ll see the Hail expression; language. The expression; language allows for a variety of incredibly expressive queries and; computations, but is probably the most complex part of Hail. See the; pair of tutorials on the expression language to learn more!; Here, we can use query_variants to pull out 5 variants to see what; they look like. In [7]:. vds.query_variants('variants.take(5)'). Out[7]:. [Variant(contig=1, start=904165, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=909917, ref=G, alts=[AltAllele(ref=G, alt=A)]),; Variant(contig=1, start=986963, ref=C, alts=[AltAllele(ref=C, alt=T)]),; Variant(contig=1, start=1563691, ref=T, alts=[AltAllele(ref=T, alt=G)]),; Variant(contig=1, start=1707740, ref=T, alts=[AltAllele(ref=T, alt=G)])]. There are often several ways to do something in Hail. Here are two ways; to peek at the first few sample IDs:. In [8]:. vds.query_samples('samples.take(5)'). Out[8]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. In [9]:. vds.sample_ids[:5]. Out[9]:. [u'HG00096', u'HG00097', u'HG00099', u'HG00100', u'HG00101']. There’s a similar interface for looking at the genotypes in a dataset.; We use; query_genotypes; to look at the first few genotype calls. In [10]:. vds.query_genotypes",MatchSource.WIKI,docs/0.1/tutorials/hail-overview.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html
Availability,avail,available,"guage. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to programming are able to use it to explore; genetic data, even if this means copying motifs and expressions found on; places like Hail discussion forum.; For learning purposes, HailContext contains the method; eval_expr_ty",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
Integrability,wrap,wraps,"ed types; Learn more!; Exercises. Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Introduction to the Expression Language. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to pro",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
Modifiability,variab,variables,"﻿. . Introduction to the Expression Language — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Introduction to the Expression Language. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
Safety,safe,safe,"ssociated type.; Hail defines the following types:; Primitives: - Int -; Double -; Float -; Long -; Boolean -; String; Compound Types: - Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; Genetic Types: - Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call. Primitive Types¶; Let’s start with simple primitive types. Primitive types are a basic; building block for any programming language - these are things like; numbers and strings and boolean values.; Hail expressions are passed as Python strings to Hail methods. In [2]:. # the Boolean literals are 'true' and 'false'; hc.eval_expr_typed('true'). Out[2]:. (True, Boolean). The return value is True, not true. Why? When values are; returned by Hail methods, they are returned as the corresponding Python; value. In [3]:. hc.eval_expr_typed('123'). Out[3]:. (123, Int). In [4]:. hc.eval_expr_typed('123.45'). Out[4]:. (123.45, Double). String literals are denoted with double-quotes. The ‘u’ preceding the; printed result denotes a unicode string, and is safe to ignore. In [5]:. hc.eval_expr_typed('""Hello, world""'). Out[5]:. (u'Hello, world', String). Primitive types support all the usual operations you’d expect. For; details, refer to the documentation on; operators and; types. Here are some examples. In [6]:. hc.eval_expr_typed('3 + 8'). Out[6]:. (11, Int). In [7]:. hc.eval_expr_typed('3.2 * 0.5'). Out[7]:. (1.6, Double). In [8]:. hc.eval_expr_typed('3 ** 3'). Out[8]:. (27.0, Double). In [9]:. hc.eval_expr_typed('25 ** 0.5'). Out[9]:. (5.0, Double). In [10]:. hc.eval_expr_typed('true || false'). Out[10]:. (True, Boolean). In [11]:. hc.eval_expr_typed('true && false'). Out[11]:. (False, Boolean). Missingness¶; Like R, all values in Hail can be missing. Most operations, like; addition, return missing if any of their inputs is missing. There are a; few special operations for manipulating missing values. There is also a; missing literal, but you have to specify it’s type. Missing Hail values; are converted to ",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
Security,access,accessed,"ed types; Learn more!; Exercises. Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Introduction to the Expression Language. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to pro",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
Testability,test,test,". In [6]:. hc.eval_expr_typed('3 + 8'). Out[6]:. (11, Int). In [7]:. hc.eval_expr_typed('3.2 * 0.5'). Out[7]:. (1.6, Double). In [8]:. hc.eval_expr_typed('3 ** 3'). Out[8]:. (27.0, Double). In [9]:. hc.eval_expr_typed('25 ** 0.5'). Out[9]:. (5.0, Double). In [10]:. hc.eval_expr_typed('true || false'). Out[10]:. (True, Boolean). In [11]:. hc.eval_expr_typed('true && false'). Out[11]:. (False, Boolean). Missingness¶; Like R, all values in Hail can be missing. Most operations, like; addition, return missing if any of their inputs is missing. There are a; few special operations for manipulating missing values. There is also a; missing literal, but you have to specify it’s type. Missing Hail values; are converted to None in Python. In [12]:. hc.eval_expr_typed('NA: Int') # missing Int. Out[12]:. (None, Int). In [13]:. hc.eval_expr_typed('NA: Dict[String, Int]'). Out[13]:. (None, Dict[String,Int]). In [14]:. hc.eval_expr_typed('1 + NA: Int'). Out[14]:. (None, Int). You can test missingness with isDefined and isMissing. In [15]:. hc.eval_expr_typed('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. ",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
Usability,learn,learning,"ark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to programming are able to use it to explore; genetic data, even if this means copying motifs and expressions found on; places like Hail discussion forum.; For learning purposes, HailContext contains the method; eval_expr_typed.; This method takes a Python string of Hail expr code, evaluates it, and; returns a tuple with the result and the type. We’ll be using this method; throughout the expression language tutorial. Hail Types¶; The Hail expression language is strongly typed, meaning that every; expression has an associated type.; Hail defines the following types:; Primitives: - Int -; Double -; Float -; Long -; Boolean -; String; Compound Types: - Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; Genetic Types: - Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call. Primitive Types¶; Let’s start with simple primitive types. Primitive types are a basic; building block for any programming language - these are things like; numbers and strings and boolean values.; Hail expressions are passed as Python strings to Hail methods. In [2]:. # the Boolean literals are 'true' and 'false'; hc.eval_expr_typed('true'). Out[2]:. (True, Boolean). The return value is T",MatchSource.WIKI,docs/0.1/tutorials/introduction-to-the-expression-language.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html
Availability,avail,available,"﻿. . Overview: module code — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Overview: module code. All modules for which code is available; hail.context; hail.dataset; hail.expr; hail.keytable; hail.kinshipMatrix; hail.ldMatrix; hail.representation.annotations; hail.representation.genotype; hail.representation.interval; hail.representation.pedigree; hail.representation.variant; hail.utils. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/index.html
Availability,error,error,"om pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change conf",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
Deployability,configurat,configuration,". HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.context. Source code for hail.context; from __future__ import print_function # Python 2 and 3 print compatibility. from hail.typecheck import *; from pyspark import SparkContext; from pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=s",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
Integrability,rout,routed,"ry for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmetho",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
Modifiability,config,configuration,". HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.context. Source code for hail.context; from __future__ import print_function # Python 2 and 3 print compatibility. from hail.typecheck import *; from pyspark import SparkContext; from pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=s",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
Performance,load,load,"ance=numeric,; sample_file=nullable(strlike),; min_partitions=nullable(integral)); def import_bgen(self, path, tolerance=0.2, sample_file=None, min_partitions=None):; """"""Import .bgen file(s) as variant dataset. **Examples**. Importing a BGEN file as a VDS (assuming it has already been indexed). >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). **Notes**. Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. .. _gpfilters:. **Genotype probability (``gp``) representation**:. The following modifications are made to genotype probabilities in BGEN v1.1 files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*St",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
Safety,recover,recovery,"r, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
Testability,log,log,"import *; from pyspark import SparkContext; from pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart s",MatchSource.WIKI,docs/0.1/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html
Availability,avail,available,"enotype(self):; return self._jvds.isGenericGenotype(). @property; @handle_py4j; def sample_ids(self):; """"""Return sampleIDs. :return: List of sample IDs.; :rtype: list of str; """""". if self._sample_ids is None:; self._sample_ids = jiterable_to_list(self._jvds.sampleIds()); return self._sample_ids. @property; @handle_py4j; def sample_annotations(self):; """"""Return a dict of sample annotations. The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. :return: dict; """""". if self._sample_annotations is None:; zipped_annotations = Env.jutils().iterableToArrayList(; self._jvds.sampleIdsAndAnnotations(); ); r = {}; for element in zipped_annotations:; r[element._1()] = self.sample_schema._convert_to_py(element._2()); self._sample_annotations = r; return self._sample_annotations. [docs] @handle_py4j; def num_partitions(self):; """"""Number of partitions. **Notes**. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. :rtype: int; """""". return self._jvds.nPartitions(). @property; @handle_py4j; def num_samples(self):; """"""Number of samples. :rtype: int; """""". if self._num_samples is None:; self._num_samples = self._jvds.nSamples(); return self._num_samples. [docs] @handle_py4j; def count_variants(self):; """"""Count number of variants in variant dataset. :rtype: long; """""". return self._jvds.countVariants(). [docs] @handle_py4j; def was_split(self):; """"""True if multiallelic variants have been split into multiple biallelic variants. Result is True if :py:meth:`~hail.VariantDataset.split_multi` or :py:meth:`~hail.VariantDataset.filter_multi` has been called on this variant dataset,; or if the variant dataset was imported w",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Deployability,pipeline,pipeline,"econd index is the state on the right.; For example, ``concordance[1][4]`` is the number of ""no call"" genotypes on the left that were called ; homozygous variant on the right. ; ; :param right: right hand variant dataset for concordance; :type right: :class:`.VariantDataset`. :return: The global concordance statistics, a key table with sample concordance; statistics, and a key table with variant concordance statistics.; :rtype: (list of list of int, :py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.concordance(right._jvds); j_global_concordance = r._1(); sample_kt = KeyTable(self.hc, r._2()); variant_kt = KeyTable(self.hc, r._3()); global_concordance = [[j_global_concordance.apply(j).apply(i) for i in xrange(5)] for j in xrange(5)]. return global_concordance, sample_kt, variant_kt. [docs] @handle_py4j; def count(self):; """"""Returns number of samples and variants in the dataset.; ; **Examples**; ; >>> samples, variants = vds.count(); ; **Notes**; ; This is also the fastest way to force evaluation of a Hail pipeline.; ; :returns: The sample and variant counts.; :rtype: (int, int); """""". r = self._jvds.count(). return r._1(), r._2(). [docs] @handle_py4j; def deduplicate(self):; """"""Remove duplicate variants. :return: Deduplicated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.deduplicate()). [docs] @handle_py4j; @typecheck_method(fraction=numeric,; seed=integral); def sample_variants(self, fraction, seed=1):; """"""Downsample variants to a given fraction of the dataset.; ; **Examples**; ; >>> small_vds = vds.sample_variants(0.01); ; **Notes**; ; This method may not sample exactly ``(fraction * n_variants)``; variants from the dataset. :param float fraction: (Expected) fraction of variants to keep. :param int seed: Random seed. :return: Downsampled variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.sampleVariants(fraction, seed)). [docs] @handle_py4j; @re",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Energy Efficiency,efficient,efficiently,"iants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Integrability,depend,depends,"ment expects a list of Hail expressions whose types match, in order, the ; table's key types.; ; Each expression in the list ``vds_key`` has the following symbols in; scope:. - ``s`` (*String*): sample ID; - ``sa``: sample annotations; ; **The** ``root`` **and** ``expr`` **arguments**; ; .. note::; ; One of ``root`` or ``expr`` is required, but not both. ; ; The ``expr`` parameter expects an annotation expression involving ``sa`` (the existing ; sample annotations in the dataset) and ``table`` (a struct containing the columns in ; the table), like ``sa.col1 = table.col1, sa.col2 = table.col2`` or ``sa = merge(sa, table)``.; The ``root`` parameter expects an annotation path beginning in ``sa``, like ``sa.annotations``.; Passing ``root='sa.annotations'`` is exactly the same as passing ``expr='sa.annotations = table'``. ``expr`` has the following symbols in scope:. - ``sa``: sample annotations; - ``table``: See note. .. note:: ; ; The value of ``table`` inside root/expr depends on the number of values in the key table, ; as well as the ``product`` argument. There are three behaviors based on the number of values; and one branch for ``product`` being true and false, for a total of six modes:; ; +-------------------------+-------------+--------------------+-----------------------------------------------+; | Number of value columns | ``product`` | Type of ``table`` | Value of ``table`` |; +=========================+=============+====================+===============================================+; | More than 2 | False | ``Struct`` | Struct with an element for each column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 1 | False | ``T`` | The value column. |; +-------------------------+-------------+--------------------+-----------------------------------------------+; | 0 | False | ``Boolean`` | Existence of any matching key. |; +-------------------------+-------------+--------------------+-----------",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Modifiability,variab,variable,"expr: Annotation expression.; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = "","".join(expr). jvds = self._jvdf.annotateGenotypesExpr(expr); vds = VariantDataset(self.hc, jvds); if isinstance(vds.genotype_schema, TGenotype):; return VariantDataset(self.hc, vds._jvdf.toVDS()); else:; return vds. [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_global_expr(self, expr):; """"""Annotate global with expression. **Example**. Annotate global with an array of populations:. >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:. >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. - ``global``: global annotations. :param expr: Annotation expression; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateGlobalExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(path=strlike,; annotation=anytype,; annotation_type=Type); def annotate_global(self, path, annotation, annotation_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotatio",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Performance,load,load,"c, vds._jvdf.toVDS()); return func(coerced_vds, *args, **kwargs); else:; raise TypeError(""genotype signature must be Genotype, but found '%s'"" % type(vds.genotype_schema)). return func(vds, *args, **kwargs). @decorator; def convertVDS(func, vds, *args, **kwargs):; if vds._is_generic_genotype:; if isinstance(vds.genotype_schema, TGenotype):; vds = VariantDataset(vds.hc, vds._jvdf.toVDS()). return func(vds, *args, **kwargs). vds_type = lazy(). [docs]class VariantDataset(object):; """"""Hail's primary representation of genomic data, a matrix keyed by sample and variant. Variant datasets may be generated from other formats using the :py:class:`.HailContext` import methods,; constructed from a variant-keyed :py:class:`KeyTable` using :py:meth:`.VariantDataset.from_table`,; and simulated using :py:meth:`~hail.HailContext.balding_nichols_model`. Once a variant dataset has been written to disk with :py:meth:`~hail.VariantDataset.write`,; use :py:meth:`~hail.HailContext.read` to load the variant dataset into the environment. >>> vds = hc.read(""data/example.vds""). :ivar hc: Hail Context.; :vartype hc: :class:`.HailContext`; """""". def __init__(self, hc, jvds):; self.hc = hc; self._jvds = jvds. self._globals = None; self._sample_annotations = None; self._colkey_schema = None; self._sa_schema = None; self._rowkey_schema = None; self._va_schema = None; self._global_schema = None; self._genotype_schema = None; self._sample_ids = None; self._num_samples = None; self._jvdf_cache = None. [docs] @staticmethod; @handle_py4j; @typecheck(table=KeyTable); def from_table(table):; """"""Construct a sites-only variant dataset from a key table. **Examples**. Import a text table and construct a sites-only VDS:. >>> table = hc.import_table('data/variant-lof.tsv', types={'v': TVariant()}).key_by('v'); >>> sites_vds = VariantDataset.from_table(table). **Notes**. The key table must be keyed by one column of type :py:class:`.TVariant`. All columns in the key table become variant annotations in the result.;",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Safety,predict,predicting,"tDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::.",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Security,access,accessed,"e_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. - ``global``: global annotations. :param expr: Annotation expression; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateGlobalExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(path=strlike,; annotation=anytype,; annotation_type=Type); def annotate_global(self, path, annotation, annotation_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotation path starting in 'global'. :param annotation: annotation to add to global. :param annotation_type: Hail type of annotation; :type annotation_type: :py:class:`.Type`. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". annotation_type._typecheck(annotation). annotated = self._jvds.annotateGlobal(annotation_type._convert_to_j(annotation), annotation_type._jtype, path); assert annotated.globalSignature().typeCheck(annotated.globalAnnotation()), 'error in java type checking'; return VariantDataset(self.hc, annotated). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_samples_expr(self, expr):; """"""Annotate samples with expression. **Examples**. Compute per-sample GQ statistics for hets:. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.i",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Testability,test,test," the variant dataset was imported with :py:meth:`~hail.HailContext.import_plink`, :py:meth:`~hail.HailContext.import_gen`,; or :py:meth:`~hail.HailContext.import_bgen`, or if the variant dataset was simulated with :py:meth:`~hail.HailContext.balding_nichols_model`. :rtype: bool; """""". return self._jvds.wasSplit(). [docs] @handle_py4j; def file_version(self):; """"""File version of variant dataset. :rtype: int; """""". return self._jvds.fileVersion(). [docs] @handle_py4j; @typecheck_method(key_exprs=oneof(strlike, listof(strlike)),; agg_exprs=oneof(strlike, listof(strlike))); def aggregate_by_key(self, key_exprs, agg_exprs):; """"""Aggregate by user-defined key and aggregation expressions to produce a KeyTable.; Equivalent to a group-by operation in SQL. **Examples**. Compute the number of LOF heterozygote calls per gene per sample:. >>> kt_result = (vds; ... .aggregate_by_key(['Sample = s', 'Gene = va.gene'],; ... 'nHet = g.filter(g => g.isHet() && va.consequence == ""LOF"").count()'); ... .export(""test.tsv"")). This will produce a :class:`KeyTable` with 3 columns (`Sample`, `Gene`, `nHet`). :param key_exprs: Named expression(s) for which fields are keys.; :type key_exprs: str or list of str. :param agg_exprs: Named aggregation expression(s).; :type agg_exprs: str or list of str. :rtype: :class:`.KeyTable`; """""". if isinstance(key_exprs, list):; key_exprs = "","".join(key_exprs); if isinstance(agg_exprs, list):; agg_exprs = "","".join(agg_exprs). return KeyTable(self.hc, self._jvds.aggregateByKey(key_exprs, agg_exprs)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=oneof(strlike, listof(strlike)),; propagate_gq=bool); def annotate_alleles_expr(self, expr, propagate_gq=False):; """"""Annotate alleles with expression. .. include:: requireTGenotype.rst. **Examples**. To create a variant annotation ``va.nNonRefSamples: Array[Int]`` where the ith entry of; the array is the number of samples carrying the ith alternate allele:. >>> vds_result = vds.annotate_alleles_expr('va.nNo",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Usability,guid,guide,"mple_ids = jiterable_to_list(self._jvds.sampleIds()); return self._sample_ids. @property; @handle_py4j; def sample_annotations(self):; """"""Return a dict of sample annotations. The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. :return: dict; """""". if self._sample_annotations is None:; zipped_annotations = Env.jutils().iterableToArrayList(; self._jvds.sampleIdsAndAnnotations(); ); r = {}; for element in zipped_annotations:; r[element._1()] = self.sample_schema._convert_to_py(element._2()); self._sample_annotations = r; return self._sample_annotations. [docs] @handle_py4j; def num_partitions(self):; """"""Number of partitions. **Notes**. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. :rtype: int; """""". return self._jvds.nPartitions(). @property; @handle_py4j; def num_samples(self):; """"""Number of samples. :rtype: int; """""". if self._num_samples is None:; self._num_samples = self._jvds.nSamples(); return self._num_samples. [docs] @handle_py4j; def count_variants(self):; """"""Count number of variants in variant dataset. :rtype: long; """""". return self._jvds.countVariants(). [docs] @handle_py4j; def was_split(self):; """"""True if multiallelic variants have been split into multiple biallelic variants. Result is True if :py:meth:`~hail.VariantDataset.split_multi` or :py:meth:`~hail.VariantDataset.filter_multi` has been called on this variant dataset,; or if the variant dataset was imported with :py:meth:`~hail.HailContext.import_plink`, :py:meth:`~hail.HailContext.import_gen`,; or :py:meth:`~hail.HailContext.import_bgen`, or if the variant dataset was simulated with :py:meth:`~hail.HailContext.balding_ni",MatchSource.WIKI,docs/0.1/_modules/hail/dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html
Deployability,patch,patch,"; return annotation. def _typecheck(self, annotation):; if annotation and not isinstance(annotation, Locus):; raise TypeCheckError('TLocus expected type hail.representation.Locus, but found %s' %; type(annotation)). [docs]class TInterval(Type):; """"""; Hail type corresponding to :class:`hail.representation.Interval`. .. include:: hailType.rst. - `expression language documentation <types.html#interval>`__; - in Python, values are instances of :class:`hail.representation.Interval`. """"""; __metaclass__ = SingletonType. def __init__(self):; super(TInterval, self).__init__(scala_object(Env.hail().expr, 'TInterval')). def _convert_to_py(self, annotation):; if annotation:; return Interval._from_java(annotation); else:; return annotation. def _convert_to_j(self, annotation):; if annotation is not None:; return annotation._jrep; else:; return annotation. def _typecheck(self, annotation):; if annotation and not isinstance(annotation, Interval):; raise TypeCheckError('TInterval expected type hail.representation.Interval, but found %s' %; type(annotation)). __singletons__ = {'is.hail.expr.TInt$': TInt,; 'is.hail.expr.TLong$': TLong,; 'is.hail.expr.TFloat$': TFloat,; 'is.hail.expr.TDouble$': TDouble,; 'is.hail.expr.TBoolean$': TBoolean,; 'is.hail.expr.TString$': TString,; 'is.hail.expr.TVariant$': TVariant,; 'is.hail.expr.TAltAllele$': TAltAllele,; 'is.hail.expr.TLocus$': TLocus,; 'is.hail.expr.TGenotype$': TGenotype,; 'is.hail.expr.TCall$': TCall,; 'is.hail.expr.TInterval$': TInterval}. import pprint. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, Type):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/expr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html
Integrability,message,message,"﻿. . hail.expr — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.expr. Source code for hail.expr; import abc; from hail.java import scala_object, Env, jset; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call. class TypeCheckError(Exception):; """"""; Error thrown at mismatch between expected and supplied python types. :param str message: Error message; """""". def __init__(self, message):; self.msg = message; super(TypeCheckError).__init__(TypeCheckError). def __str__(self):; return self.msg. [docs]class Type(object):; """"""; Hail type superclass used for annotations and expression language.; """"""; __metaclass__ = abc.ABCMeta. def __init__(self, jtype):; self._jtype = jtype. def __repr__(self):; return str(self). def __str__(self):; return self._jtype.toPrettyString(0, True, False). def __eq__(self, other):; return self._jtype.equals(other._jtype). def __hash__(self):; return self._jtype.hashCode(). [docs] def pretty(self, indent=0, attrs=False):; """"""Returns a prettily formatted string representation of the type. :param int indent: Number of spaces to indent. :param bool attrs: Print struct field attributes. :rtype: str; """""". return self._jtype.toPrettyString(indent, False, attrs). @classmethod; def _from_java(cls, jtype):; # FIXME string matching is pretty hacky; class_name = jtype.getClass().getCanonicalName(). if class_name in __singletons__:; return __singletons__[class_name](); elif class_name == 'is.hail.expr.TArray':; return TArray._from_java(jtype); elif class_name == 'is.hail.expr.TSet':; return TSet._from_java(jtype); elif class_name == 'is.hail.expr.TDict':; return TDict._from_java(jtype); elif class_name == 'is.hail.expr.TStruct':; return TStruct._from_java(jtype); else:; raise TypeError(""unknown type class: '%s'"" % cla",MatchSource.WIKI,docs/0.1/_modules/hail/expr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html
Security,hash,hashCode," Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.expr. Source code for hail.expr; import abc; from hail.java import scala_object, Env, jset; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call. class TypeCheckError(Exception):; """"""; Error thrown at mismatch between expected and supplied python types. :param str message: Error message; """""". def __init__(self, message):; self.msg = message; super(TypeCheckError).__init__(TypeCheckError). def __str__(self):; return self.msg. [docs]class Type(object):; """"""; Hail type superclass used for annotations and expression language.; """"""; __metaclass__ = abc.ABCMeta. def __init__(self, jtype):; self._jtype = jtype. def __repr__(self):; return str(self). def __str__(self):; return self._jtype.toPrettyString(0, True, False). def __eq__(self, other):; return self._jtype.equals(other._jtype). def __hash__(self):; return self._jtype.hashCode(). [docs] def pretty(self, indent=0, attrs=False):; """"""Returns a prettily formatted string representation of the type. :param int indent: Number of spaces to indent. :param bool attrs: Print struct field attributes. :rtype: str; """""". return self._jtype.toPrettyString(indent, False, attrs). @classmethod; def _from_java(cls, jtype):; # FIXME string matching is pretty hacky; class_name = jtype.getClass().getCanonicalName(). if class_name in __singletons__:; return __singletons__[class_name](); elif class_name == 'is.hail.expr.TArray':; return TArray._from_java(jtype); elif class_name == 'is.hail.expr.TSet':; return TSet._from_java(jtype); elif class_name == 'is.hail.expr.TDict':; return TDict._from_java(jtype); elif class_name == 'is.hail.expr.TStruct':; return TStruct._from_java(jtype); else:; raise TypeError(""unknown type class: '%s'"" % class_name). @abc.abstractmethod; def _typecheck(self, annotation):; """"""; Raise an exception if the given annotation is not the appropriate type. :pa",MatchSource.WIKI,docs/0.1/_modules/hail/expr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html
Availability,redundant,redundant,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
Deployability,pipeline,pipelines,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
Modifiability,config,config,"DB. .. warning::. :py:meth:`~.export_mongodb` is EXPERIMENTAL. """""". (scala_package_object(self.hc._hail.driver); .exportMongoDB(self.hc._jsql_context, self._jkt, mode)). [docs] @handle_py4j; @typecheck_method(zk_host=strlike,; collection=strlike,; block_size=integral); def export_solr(self, zk_host, collection, block_size=100):; """"""Export to Solr.; ; .. warning::. :py:meth:`~.export_solr` is EXPERIMENTAL. """""". self._jkt.exportSolr(zk_host, collection, block_size). [docs] @handle_py4j; @typecheck_method(address=strlike,; keyspace=strlike,; table=strlike,; block_size=integral,; rate=integral); def export_cassandra(self, address, keyspace, table, block_size=100, rate=1000):; """"""Export to Cassandra. .. warning::. :py:meth:`~.export_cassandra` is EXPERIMENTAL. """""". self._jkt.exportCassandra(address, keyspace, table, block_size, rate). [docs] @handle_py4j; @typecheck_method(host=strlike,; port=integral,; index=strlike,; index_type=strlike,; block_size=integral,; config=nullable(dictof(strlike, strlike)),; verbose=bool); def export_elasticsearch(self, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export to Elasticsearch. .. warning::. :py:meth:`~.export_elasticsearch` is EXPERIMENTAL. """""". self._jkt.exportElasticsearch(host, port, index, index_type, block_size, config, verbose). [docs] @handle_py4j; @typecheck_method(column_names=oneof(strlike, listof(strlike))); def explode(self, column_names):; """"""Explode columns of this key table. The explode operation unpacks the elements in a column of type ``Array`` or ``Set`` into its own row.; If an empty ``Array`` or ``Set`` is exploded, the entire row is removed from the :py:class:`.KeyTable`. **Examples**. Assume ``kt3`` is a :py:class:`.KeyTable` with three columns: c1, c2 and; c3. >>> kt3 = hc.import_table('data/kt_example3.tsv', impute=True,; ... types={'c1': TString(), 'c2': TArray(TInt()), 'c3': TArray(TArray(TInt()))}). The types of each column are ``String``, ``Array[Int]``, and ``Array[Array[I",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
Performance,cache,cache,"= {row.ID : row.SEX for row in kt1.collect()}. **Notes**. This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-dep",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
Safety,avoid,avoid,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
Testability,assert,assert,"; @handle_py4j; @typecheck(hc=anytype,; rows_py=oneof(listof(Struct), listof(dictof(strlike, anytype))),; schema=TStruct,; key_names=listof(strlike),; num_partitions=nullable(integral)); def from_py(hc, rows_py, schema, key_names=[], num_partitions=None):; return KeyTable(; hc,; Env.hail().keytable.KeyTable.parallelize(; hc._jhc, [schema._convert_to_j(r) for r in rows_py],; schema._jtype, key_names, joption(num_partitions))). @property; @handle_py4j; def num_columns(self):; """"""Number of columns. >>> kt1.num_columns; 8. :rtype: int; """""". if self._num_columns is None:; self._num_columns = self._jkt.nFields(); return self._num_columns. @property; @handle_py4j; def schema(self):; """"""Table schema. **Examples**. >>> print(kt1.schema). The ``pprint`` module can be used to print the schema in a more human-readable format:. >>> from pprint import pprint; >>> pprint(kt1.schema). :rtype: :class:`.TStruct`; """""". if self._schema is None:; self._schema = Type._from_java(self._jkt.signature()); assert (isinstance(self._schema, TStruct)); return self._schema. @property; @handle_py4j; def key(self):; """"""List of key columns. >>> kt1.key; [u'ID']. :rtype: list of str; """""". if self._key is None:; self._key = list(self._jkt.key()); return self._key. @property; @handle_py4j; def columns(self):; """"""Names of all columns. >>> kt1.columns; [u'ID', u'HT', u'SEX', u'X', u'Z', u'C1', u'C2', u'C3']. :rtype: list of str; """""". if self._column_names is None:; self._column_names = list(self._jkt.fieldNames()); return self._column_names. [docs] @handle_py4j; def count(self):; """"""Count the number of rows. **Examples**; ; >>> kt1.count(); ; :rtype: int; """""". return self._jkt.count(). [docs] @handle_py4j; @typecheck_method(other=kt_type); def same(self, other):; """"""Test whether two key tables are identical. **Examples**. >>> if kt1.same(kt2):; ... print(""KeyTables are the same!""). :param other: key table to compare against; :type other: :class:`.KeyTable` . :rtype: bool; """""". return self._jkt.same(other",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
Usability,guid,guide,"utput, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist; a table when you are done with it.; """"""; self._jkt.unpersist(). [docs] @handle_py4j; @typecheck_method(cols=tupleof(oneof(strlike, Ascending, Descending))); ",MatchSource.WIKI,docs/0.1/_modules/hail/keytable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html
Integrability,depend,depending,"s()]. [docs] def matrix(self):; """"""; Gets the matrix backing this kinship matrix. :return: Matrix of kinship values.; :rtype: `IndexedRowMatrix <https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix>`__; """"""; from pyspark.mllib.linalg.distributed import IndexedRowMatrix. return IndexedRowMatrix(self._jkm.matrix()). [docs] @typecheck_method(output=strlike); def export_tsv(self, output):; """"""; Export kinship matrix to tab-delimited text file with sample list as header.; ; :param str output: File path for output. ; """"""; self._jkm.exportTSV(output). [docs] @typecheck_method(output=strlike); def export_rel(self, output):; """"""; Export kinship matrix as .rel file. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output. ; """"""; self._jkm.exportRel(output). [docs] @typecheck_method(output=strlike); def export_gcta_grm(self, output):; """"""; Export kinship matrix as .grm file. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output.; """"""; self._jkm.exportGctaGrm(output). [docs] @typecheck_method(output=strlike,; opt_n_file=nullable(strlike)); def export_gcta_grm_bin(self, output, opt_n_file=None):; """"""; Export kinship matrix as .grm.bin file or as .grm.N.bin file, depending on whether an N file is specified. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output. ; ; :param opt_n_file: The file path to the N file. ; :type opt_n_file: str or None; """"""; self._jkm.exportGctaGrmBin(output, joption(opt_n_file)). [docs] @typecheck_method(output=strlike); def export_id_file(self, output):; """"""; Export samples as .id file. See `PLINK formats <https://www.cog-genomics.org/plink2/formats>`_.; ; :param str output: File path for output.; """"""; self._jkm.exportIdFile(output). © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/kinshipMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/kinshipMatrix.html
Testability,assert,assert,"st of variants.; :rtype: list of Variant; """"""; jvars = self._jldm.variants(); return list(map(lambda jrep: Variant._from_java(jrep), jvars)). [docs] def matrix(self):; """"""; Gets the distributed matrix backing this LD matrix. :return: Matrix of Pearson correlation values.; :rtype: `IndexedRowMatrix <https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix>`__; """"""; from pyspark.mllib.linalg.distributed import IndexedRowMatrix. return IndexedRowMatrix(self._jldm.matrix()). [docs] def to_local_matrix(self):; """"""; Converts the LD matrix to a local Spark matrix.; ; .. caution::; ; Only call this method when the LD matrix is small enough to fit in local memory on the driver. ; ; :return: Matrix of Pearson correlation values.; :rtype: `Matrix <https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrix>`__; """"""; from pyspark.mllib.linalg import DenseMatrix. j_local_mat = self._jldm.toLocalMatrix(); assert j_local_mat.majorStride() == j_local_mat.rows(); assert j_local_mat.offset() == 0; assert j_local_mat.isTranspose() == False; return DenseMatrix(j_local_mat.rows(), j_local_mat.cols(), list(j_local_mat.data()), False). [docs] def write(self, path):; """"""; Writes the LD matrix to a file. **Examples**. Write an LD matrix to a file. >>> vds.ld_matrix().write('output/ld_matrix'). :param path: the path to which to write the LD matrix; :type path: str; """""". self._jldm.write(path). [docs] @staticmethod; def read(path):; """"""; Reads the LD matrix from a file. **Examples**. Read an LD matrix from a file. >>> ld_matrix = LDMatrix.read('data/ld_matrix'). :param path: the path from which to read the LD matrix; :type path: str; """""". jldm = Env.hail().methods.LDMatrix.read(Env.hc()._jhc, path); return LDMatrix(jldm). [docs] @typecheck_method(path=strlike,; column_delimiter=strlike,; header=nullable(strlike),; parallel_write=bool,; entries=enumeration('full', 'lower', 'strict_lower', 'upper', 'st",MatchSource.WIKI,docs/0.1/_modules/hail/ldMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/ldMatrix.html
Usability,clear,clear,"iants(); summary.call_rate = jrep.callRate().get() if jrep.callRate().isDefined() else float('nan'); summary.contigs = [str(x) for x in jiterable_to_list(jrep.contigs())]; summary.multiallelics = jrep.multiallelics(); summary.snps = jrep.snps(); summary.mnps = jrep.mnps(); summary.insertions = jrep.insertions(); summary.deletions = jrep.deletions(); summary.complex = jrep.complex(); summary.star = jrep.star(); summary.max_alleles = jrep.maxAlleles(); return summary. def __repr__(self):; return 'Summary(samples=%d, variants=%d, call_rate=%f, contigs=%s, multiallelics=%d, snps=%d, ' \; 'mnps=%d, insertions=%d, deletions=%d, complex=%d, star=%d, max_alleles=%d)' % (; self.samples, self.variants, self.call_rate,; self.contigs, self.multiallelics, self.snps,; self.mnps, self.insertions, self.deletions,; self.complex, self.star, self.max_alleles). def __str__(self):; return repr(self). [docs] def report(self):; """"""Print the summary information.""""""; print('') # clear out pesky progress bar if necessary; print('%16s: %d' % ('Samples', self.samples)); print('%16s: %d' % ('Variants', self.variants)); print('%16s: %f' % ('Call Rate', self.call_rate)); print('%16s: %s' % ('Contigs', self.contigs)); print('%16s: %d' % ('Multiallelics', self.multiallelics)); print('%16s: %d' % ('SNPs', self.snps)); print('%16s: %d' % ('MNPs', self.mnps)); print('%16s: %d' % ('Insertions', self.insertions)); print('%16s: %d' % ('Deletions', self.deletions)); print('%16s: %d' % ('Complex Alleles', self.complex)); print('%16s: %d' % ('Star Alleles', self.star)); print('%16s: %d' % ('Max Alleles', self.max_alleles)). class FunctionDocumentation(object):; @handle_py4j; def types_rst(self, file_name):; Env.hail().utils.FunctionDocumentation.makeTypesDocs(file_name). @handle_py4j; def functions_rst(self, file_name):; Env.hail().utils.FunctionDocumentation.makeFunctionsDocs(file_name). [docs]@handle_py4j; @typecheck(path=strlike,; buffer_size=integral); def hadoop_read(path, buffer_size=8192):; """"""Open a",MatchSource.WIKI,docs/0.1/_modules/hail/utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/utils.html
Deployability,patch,patch,"ll not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, default=None):; """"""Get an item, or return a default value if the item is not found.; ; :param str item: Name of attribute.; ; :param default: Default value.; ; :returns: Value of item if found, or default value if not.; """"""; return self._attrs.get(item, default). @typecheck(struct=Struct); def to_dict(struct):; d = {}; for k, v in struct._attrs.iteritems():; if isinstance(v, Struct):; d[k] = to_dict(v); else:; d[k] = v; return d. import pprint. _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, *args, **kwargs):; if isinstance(obj, Struct):; obj = to_dict(obj); return _old_printer._format(self, obj, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html
Security,access,accessing,"﻿. . hail.representation.annotations — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.annotations. Source code for hail.representation.annotations; from hail.typecheck import *. [docs]class Struct(object):; """"""; Nested annotation structure. >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name ""1kg"", for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html
Testability,assert,assert,"ons. Source code for hail.representation.annotations; from hail.typecheck import *. [docs]class Struct(object):; """"""; Nested annotation structure. >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name ""1kg"", for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, default=None):; """"""Get an item, or return a default value if the item is not found.; ; :param str item: Name of attribute.; ; :param default: Default value.; ; :returns: Value of item if found, or default value if not.; """"""; return self._attrs.get(item, default). @typecheck(struct=Struct); def to_dict(struct):; d = ",MatchSource.WIKI,docs/0.1/_modules/hail/representation/annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html
Modifiability,variab,variables," reference and one alternate allele. :rtype: bool; """""". return self._jrep.isHetRef(). [docs] def is_not_called(self):; """"""True if the genotype call is missing. :rtype: bool; """""". return self._jrep.isNotCalled(). [docs] def is_called(self):; """"""True if the genotype call is non-missing. :rtype: bool; """""". return self._jrep.isCalled(). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return from_option(self._jrep.nNonRefAlleles()). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; ho",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
Security,hash,hashCode,"elf, gt, ad=None, dp=None, gq=None, pl=None):; """"""Initialize a Genotype object."""""". jvm = Env.jvm(); jgt = joption(gt); if ad:; jad = jsome(jarray(jvm.int, ad)); else:; jad = jnone(); jdp = joption(dp); jgq = joption(gq); if pl:; jpl = jsome(jarray(jvm.int, pl)); else:; jpl = jnone(). jrep = scala_object(Env.hail().variant, 'Genotype').apply(; jgt, jad, jdp, jgq, jpl, False, False); self._gt = gt; self._ad = ad; self._dp = dp; self._gq = gq; self._pl = pl; self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; fake_ref = 'FakeRef=True' if self._jrep.fakeRef() else ''; if self._jrep.isLinearScale():; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, GP=%s%s)' %\; (self.gt, self.ad, self.dp, self.gq, self.gp, fake_ref); else:; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, PL=%s%s)' % \; (self.gt, self.ad, self.dp, self.gq, self.pl, fake_ref). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jrep):; g = Genotype.__new__(cls); g._init_from_java(jrep); g._gt = from_option(jrep.gt()); g._ad = jarray_to_list(from_option(jrep.ad())); g._dp = from_option(jrep.dp()); g._gq = from_option(jrep.gq()); g._pl = jarray_to_list(from_option(jrep.pl())); return g. @property; def gt(self):; """"""Returns the hard genotype call. :rtype: int or None; """""". return self._gt. @property; def ad(self):; """"""Returns the allelic depth. :rtype: list of int or None; """""". return self._ad. @property; def dp(self):; """"""Returns the total depth. :rtype: int or None; """""". return self._dp. @property; def gq(self):; """"""Returns the phred-scaled genotype quality. :return: int or None; """""". return self._gq. @property; def pl(self):; """"""Returns the phred-scaled genotype posterior likelihoods. :rtype: list of int or None; """""". return self._pl. [docs] def od(self):; """"""Returns the difference between the total depth and the alle",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
Testability,test,testsetup,"﻿. . hail.representation.genotype — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.genotype. Source code for hail.representation.genotype; from hail.java import *; from hail.typecheck import *. [docs]class Genotype(object):; """"""; An object that represents an individual's genotype at a genomic locus. .. testsetup::. g = Genotype(0, ad=[9,1], dp=11, gq=20, pl=[0,100,1000]). :param gt: Genotype hard call; :type gt: int or None; :param ad: allelic depth (1 element per allele including reference); :type ad: list of int or None; :param dp: total depth; :type dp: int or None; :param gq: genotype quality; :type gq: int or None; :param pl: phred-scaled posterior genotype likelihoods (1 element per possible genotype); :type pl: list of int or None; """""". @handle_py4j; def __init__(self, gt, ad=None, dp=None, gq=None, pl=None):; """"""Initialize a Genotype object."""""". jvm = Env.jvm(); jgt = joption(gt); if ad:; jad = jsome(jarray(jvm.int, ad)); else:; jad = jnone(); jdp = joption(dp); jgq = joption(gq); if pl:; jpl = jsome(jarray(jvm.int, pl)); else:; jpl = jnone(). jrep = scala_object(Env.hail().variant, 'Genotype').apply(; jgt, jad, jdp, jgq, jpl, False, False); self._gt = gt; self._ad = ad; self._dp = dp; self._gq = gq; self._pl = pl; self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; fake_ref = 'FakeRef=True' if self._jrep.fakeRef() else ''; if self._jrep.isLinearScale():; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, GP=%s%s)' %\; (self.gt, self.ad, self.dp, self.gq, self.gp, fake_ref); else:; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, PL=%s%s)' % \; (self.gt, self.ad, self.dp, self.gq, self.pl, fake_ref). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.",MatchSource.WIKI,docs/0.1/_modules/hail/representation/genotype.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html
Security,hash,hashCode," import Locus; from hail.typecheck import *. interval_type = lazy(). [docs]class Interval(object):; """"""; A genomic interval marked by start and end loci. .. testsetup::. interval1 = Interval.parse('X:100005-X:150020'); interval2 = Interval.parse('16:29500000-30200000'). :param start: inclusive start locus; :type start: :class:`.Locus`; :param end: exclusive end locus; :type end: :class:`.Locus`; """""". @handle_py4j; def __init__(self, start, end):; if not (isinstance(start, Locus) and isinstance(end, Locus)):; raise TypeError('expect arguments of type (Locus, Locus) but found (%s, %s)' %; (str(type(start)), str(type(end)))); jrep = scala_object(Env.hail().variant, 'Locus').makeInterval(start._jrep, end._jrep); self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Interval(start=%s, end=%s)' % (repr(self.start), repr(self.end)). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._start = Locus._from_java(self._jrep.start()). @classmethod; def _from_java(cls, jrep):; interval = Interval.__new__(cls); interval._init_from_java(jrep); return interval. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a genomic interval from string representation. **Examples**:. >>> interval_1 = Interval.parse('X:100005-X:150020'); >>> interval_2 = Interval.parse('16:29500000-30200000'); >>> interval_3 = Interval.parse('16:29.5M-30.2M') # same as interval_2; >>> interval_4 = Interval.parse('16:30000000-END'); >>> interval_5 = Interval.parse('16:30M-END') # same as interval_4; >>> interval_6 = Interval.parse('1-22') # autosomes; >>> interval_7 = Interval.parse('X') # all of chromosome X. There are several acceptable representations. ``CHR1:POS1-CHR2:POS2`` is the fully specified representation, and; we use this to define the various shortcut representations. In a ``POS`` field",MatchSource.WIKI,docs/0.1/_modules/hail/representation/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/interval.html
Testability,test,testsetup,"﻿. . hail.representation.interval — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.interval. Source code for hail.representation.interval; from hail.java import *; from hail.representation.variant import Locus; from hail.typecheck import *. interval_type = lazy(). [docs]class Interval(object):; """"""; A genomic interval marked by start and end loci. .. testsetup::. interval1 = Interval.parse('X:100005-X:150020'); interval2 = Interval.parse('16:29500000-30200000'). :param start: inclusive start locus; :type start: :class:`.Locus`; :param end: exclusive end locus; :type end: :class:`.Locus`; """""". @handle_py4j; def __init__(self, start, end):; if not (isinstance(start, Locus) and isinstance(end, Locus)):; raise TypeError('expect arguments of type (Locus, Locus) but found (%s, %s)' %; (str(type(start)), str(type(end)))); jrep = scala_object(Env.hail().variant, 'Locus').makeInterval(start._jrep, end._jrep); self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Interval(start=%s, end=%s)' % (repr(self.start), repr(self.end)). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._start = Locus._from_java(self._jrep.start()). @classmethod; def _from_java(cls, jrep):; interval = Interval.__new__(cls); interval._init_from_java(jrep); return interval. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a genomic interval from string representation. **Examples**:. >>> interval_1 = Interval.parse('X:100005-X:150020'); >>> interval_2 = Interval.parse('16:29500000-30200000'); >>> interval_3 = Interval.parse('16:29.5M-30.2M') # same as interval_2; >>> i",MatchSource.WIKI,docs/0.1/_modules/hail/representation/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/interval.html
Security,hash,hashCode,"; if is_female is not None:; jsex = jsome(jobject.Female()) if is_female else jsome(jobject.Male()); else:; jsex = jnone(). self._jrep = Env.hail().methods.BaseTrio(proband, joption(fam), joption(father), joption(mother), jsex); self._fam = fam; self._proband = proband; self._father = father; self._mother = mother; self._is_female = is_female. @classmethod; def _from_java(cls, jrep):; trio = Trio.__new__(cls); trio._jrep = jrep; return trio. def __repr__(self):; return 'Trio(proband=%s, fam=%s, father=%s, mother=%s, is_female=%s)' % (; repr(self.proband), repr(self.fam), repr(self.father),; repr(self.mother), repr(self.is_female)). def __str__(self):; return 'Trio(proband=%s, fam=%s, father=%s, mother=%s, is_female=%s)' % (; str(self.proband), str(self.fam), str(self.father),; str(self.mother), str(self.is_female)). def __eq__(self, other):; if not isinstance(other, Trio):; return False; else:; return self._jrep == other._jrep. @handle_py4j; def __hash__(self):; return self._jrep.hashCode(). @property; @handle_py4j; def proband(self):; """"""ID of proband in trio, never missing. :rtype: str; """"""; if not hasattr(self, '_proband'):; self._proband = self._jrep.kid(); return self._proband. @property; @handle_py4j; def father(self):; """"""ID of father in trio, may be missing. :rtype: str or None; """""". if not hasattr(self, '_father'):; self._father = from_option(self._jrep.dad()); return self._father. @property; @handle_py4j; def mother(self):; """"""ID of mother in trio, may be missing. :rtype: str or None; """""". if not hasattr(self, '_mother'):; self._mother = from_option(self._jrep.mom()); return self._mother. @property; @handle_py4j; def fam(self):; """"""Family ID. :rtype: str or None; """""". if not hasattr(self, '_fam'):; self._fam = from_option(self._jrep.fam()); return self._fam. @property; @handle_py4j; def is_male(self):; """"""Returns True if the proband is a reported male, False if reported female, and None if no sex is defined. :rtype: bool or None; """"""; if not hasattr(self, '",MatchSource.WIKI,docs/0.1/_modules/hail/representation/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html
Testability,test,test," in a complete trio. :rtype: bool; """""". if not hasattr(self, '_complete'):; self._complete = self._jrep.isComplete(); return self._complete. [docs]class Pedigree(object):; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """""". @handle_py4j; def __init__(self, trios):. self._jrep = Env.hail().methods.Pedigree(jindexed_seq([t._jrep for t in trios])); self._trios = trios. @classmethod; def _from_java(cls, jrep):; ped = Pedigree.__new__(cls); ped._jrep = jrep; ped._trios = None; return ped. def __eq__(self, other):; if not isinstance(other, Pedigree):; return False; else:; return self._jrep == other._jrep. @handle_py4j; def __hash__(self):; return self._jrep.hashCode(). [docs] @staticmethod; @handle_py4j; @typecheck(fam_path=strlike,; delimiter=strlike); def read(fam_path, delimiter='\\s+'):; """"""Read a .fam file and return a pedigree object. **Examples**. >>> ped = Pedigree.read('data/test.fam'). **Notes**. This method reads a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. Hail expects a file in the same spec as PLINK outlines. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """""". jrep = Env.hail().methods.Pedigree.read(fam_path, Env.hc()._jhc.hadoopConf(), delimiter); return Pedigree._from_java(jrep). @property; @handle_py4j; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """""". if not self._trios:; self._trios = [Trio._from_java(t) for t in jiterable_to_list(self._jrep.trios())]; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father, mother, and sex. :rtype: list of :class:`.Trio`; """"""; return filter(lambda t: t.is_complete(), self.trios). [docs] @handle_py4j; @typecheck_method(samples=listof(strlike)); def filter_to(self, samples):; """"""Filter the pedigree to a given list of sample IDs. **",MatchSource.WIKI,docs/0.1/_modules/hail/representation/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html
Energy Efficiency,reduce,reduced," alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate al",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
Modifiability,polymorphi,polymorphism,"﻿. . hail.representation.variant — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.variant. Source code for hail.representation.variant; from hail.java import scala_object, Env, handle_py4j; from hail.typecheck import *. [docs]class Variant(object):; """"""; An object that represents a genomic polymorphism. .. testsetup::. v_biallelic = Variant.parse('16:20012:A:TT'); v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :param contig: chromosome identifier; :type contig: str or int; :param int start: chromosomal position (1-based); :param str ref: reference allele; :param alts: single alternate allele, or list of alternate alleles; :type alts: str or list of str; """""". @handle_py4j; def __init__(self, contig, start, ref, alts):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Variant').apply(contig, start, ref, alts); self._init_from_java(jrep); self._contig = contig; self._start = start; self._ref = ref. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Variant(contig=%s, start=%s, ref=%s, alts=%s)' % (self.contig, self.start, self.ref, self._alt_alleles). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._alt_alleles = map(AltAllele._from_java, [jrep.altAlleles().apply(i) for i in xrange(jrep.nAltAlleles())]). @classmethod; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT,",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
Security,hash,hashCode,"""; An object that represents a genomic polymorphism. .. testsetup::. v_biallelic = Variant.parse('16:20012:A:TT'); v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :param contig: chromosome identifier; :type contig: str or int; :param int start: chromosomal position (1-based); :param str ref: reference allele; :param alts: single alternate allele, or list of alternate alleles; :type alts: str or list of str; """""". @handle_py4j; def __init__(self, contig, start, ref, alts):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Variant').apply(contig, start, ref, alts); self._init_from_java(jrep); self._contig = contig; self._start = start; self._ref = ref. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Variant(contig=%s, start=%s, ref=%s, alts=%s)' % (self.contig, self.start, self.ref, self._alt_alleles). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._alt_alleles = map(AltAllele._from_java, [jrep.altAlleles().apply(i) for i in xrange(jrep.nAltAlleles())]). @classmethod; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,...ALTN. Below is an example of; each:. >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :rtype: :class:`.Variant`; """"""; jrep = scala_object(Env.hail().variant, 'Variant').parse(string); return Variant._from_java(jrep). @property; def contig(self):; """"""; Chromosome identifier. :rtype: str; """"""; return self._contig. @property; def start(self):; """"""; Chromosomal position (1-based). :rtype",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
Testability,test,testsetup,"﻿. . hail.representation.variant — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.variant. Source code for hail.representation.variant; from hail.java import scala_object, Env, handle_py4j; from hail.typecheck import *. [docs]class Variant(object):; """"""; An object that represents a genomic polymorphism. .. testsetup::. v_biallelic = Variant.parse('16:20012:A:TT'); v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :param contig: chromosome identifier; :type contig: str or int; :param int start: chromosomal position (1-based); :param str ref: reference allele; :param alts: single alternate allele, or list of alternate alleles; :type alts: str or list of str; """""". @handle_py4j; def __init__(self, contig, start, ref, alts):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Variant').apply(contig, start, ref, alts); self._init_from_java(jrep); self._contig = contig; self._start = start; self._ref = ref. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Variant(contig=%s, start=%s, ref=%s, alts=%s)' % (self.contig, self.start, self.ref, self._alt_alleles). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._alt_alleles = map(AltAllele._from_java, [jrep.altAlleles().apply(i) for i in xrange(jrep.nAltAlleles())]). @classmethod; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT,",MatchSource.WIKI,docs/0.1/_modules/hail/representation/variant.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html
Deployability,update,updated,"﻿. Hail | ; Amazon Web Services. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Amazon Web Services. View page source. Amazon Web Services; While Hail does not have any built-in tools for working with Amazon EMR, there are two approaches maintained by third parties:. AWS maintains a Hail on AWS quickstart.; The Avillach Lab at Harvard Medical School maintains an open-source tool. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/amazon_web_services.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/amazon_web_services.html
Availability,down,down,"ht.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of tr",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
Deployability,install,installations,"﻿. Hail | ; Microsoft Azure. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list run",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
Modifiability,config,configured,"﻿. Hail | ; Microsoft Azure. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list run",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
Security,password,password,"nstallation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT R",MatchSource.WIKI,docs/0.2/cloud/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html
Availability,avail,available,"Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Databricks. View page source. Databricks; The docker images described below are maintained by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) . Display Bokeh plots; Hail uses the Bokeh library to create plots. The show; function built into Bokeh does not work in Databricks. T",MatchSource.WIKI,docs/0.2/cloud/databricks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html
Deployability,install,installed,"﻿. Hail | ; Databricks. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Databricks. View page source. Databricks; The docker images described below are maintained by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_lo",MatchSource.WIKI,docs/0.2/cloud/databricks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html
Testability,log,logs,"ed by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) . Display Bokeh plots; Hail uses the Bokeh library to create plots. The show; function built into Bokeh does not work in Databricks. To display a Bokeh plot generated by Hail,; you can run a command like:; >>> from bokeh.embed import components, file_html; >>> from bokeh.resources import CDN; >>> plot = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); >>> html = file_html(plot, CDN, ""Chart""). And then call the Databricks function displayHTML with html as its argument.; See Databricks’ Bokeh docs for; more information. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/databricks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html
Deployability,update,updated," This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general, once you know the wall clock time of your job, you can enter your cluster parameters into the; Google Cloud Pricing Calculator. and get a precise estimate; of cost using the latest prices. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
Energy Efficiency,charge,charges," simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general,",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
Modifiability,config,configurable,"ets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; General Advice. View page source. General Advice. Start Small; The cloud has a reputation for easily burning lots of money. You don’t want to be the person who; spent ten thousand dollars one night without thinking about it. Luckily, it’s easy to not be that person!; Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with filter_rows.; Hail will make sure not to load data for other chromosomes.; import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail’s hl.balding_nichols_model creates a random genotype dataset with configurable numbers of rows and columns.; You can use these datasets for experimentation.; As you’ll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
Performance,load,load,"ng time; Estimating cost. Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; General Advice. View page source. General Advice. Start Small; The cloud has a reputation for easily burning lots of money. You don’t want to be the person who; spent ten thousand dollars one night without thinking about it. Luckily, it’s easy to not be that person!; Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with filter_rows.; Hail will make sure not to load data for other chromosomes.; import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail’s hl.balding_nichols_model creates a random genotype dataset with configurable numbers of rows and columns.; You can use these datasets for experimentation.; As you’ll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. Howe",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
Usability,simpl,simple,"ing about it. Luckily, it’s easy to not be that person!; Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with filter_rows.; Hail will make sure not to load data for other chromosomes.; import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail’s hl.balding_nichols_model creates a random genotype dataset with configurable numbers of rows and columns.; You can use these datasets for experimentation.; As you’ll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed fr",MatchSource.WIKI,docs/0.2/cloud/general_advice.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html
Availability,down,down,"b to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rac",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
Deployability,install,installations,"﻿. Hail | ; Google Cloud Platform. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be co",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
Energy Efficiency,charge,charges,"a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
Integrability,interface,interface,"e; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is t",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
Modifiability,config,configured,"form. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to re",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
Safety,avoid,avoid,". The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow-annotation-db to enable the specific list of buckets that the annotation database; relies on. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument --vep GRCh38. A cluster started without the --vep argument is; unable to run VE",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
Security,access,accessing,"a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that ",MatchSource.WIKI,docs/0.2/cloud/google_cloud.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html
Availability,avail,available,"ure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our dis",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
Deployability,deploy,deploying,". Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Getting Started; Variant Effect Predictor (VEP). Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spar",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
Modifiability,config,config,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
Performance,load,loaded,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
Testability,log,login,"tion Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on",MatchSource.WIKI,docs/0.2/cloud/query_on_batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html
Deployability,update,updated,"_exome_C_ALL_Rec; giant_whr_exome_C_EUR_Add; giant_whr_exome_C_EUR_Rec; giant_whr_exome_M_ALL_Add; giant_whr_exome_M_ALL_Rec; giant_whr_exome_M_EUR_Add; giant_whr_exome_M_EUR_Rec; giant_whr_exome_W_ALL_Add; giant_whr_exome_W_ALL_Rec; giant_whr_exome_W_EUR_Add; giant_whr_exome_W_EUR_Rec; gnomad_annotation_pext; gnomad_base_pext; gnomad_chrM_coverage; gnomad_chrM_sites; gnomad_exome_coverage; gnomad_exome_sites; gnomad_genome_coverage; gnomad_genome_sites; gnomad_hgdp_1kg_subset_dense; gnomad_hgdp_1kg_subset_sample_metadata; gnomad_hgdp_1kg_subset_sparse; gnomad_hgdp_1kg_subset_variant_annotations; gnomad_ld_scores_afr; gnomad_ld_scores_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas.html
Availability,avail,available,".DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Not",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
Deployability,configurat,configuration,"﻿. Hail | ; DB. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
Modifiability,config,configuration,"﻿. Hail | ; DB. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add",MatchSource.WIKI,docs/0.2/experimental/hail.experimental.DB.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html
Availability,avail,available,"entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'bin",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
Deployability,continuous,continuous,"ions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+0",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
Integrability,depend,depend,"asses; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental. View page source. Experimental; This module serves two functions: as a staging area for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions.; At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. Warning; The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. ldscsim. Contribution Guidelines; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. Function docstrings are required. Hail uses; NumPy style docstrings.; Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on Tests in; python/tests/experimental/test_experimental.py; Code style is not strictly enforced, aside from egregious violations. We do; recommend using autopep8 though!. Annotation Database; Classes. hail.experimental.DB; An annotation database instance. Genetics Methods. load_dataset(name, version, reference_genome); Load a genetic dataset from Hail's repository. ld_score(entry_expr, locus_expr, radius[, ...]); Calculate LD scores. ld_score_regression(weight_expr, ...[, ...]); Estimate S",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
Modifiability,variab,variable-length,"20e-01; 7 9.7540e-01; 8 8.4848e-01; 9 3.7423e-01. Due to overhead and file system limits related to having large numbers; of open files, this function will iteratively export groups of columns.; The batch_size parameter can control the size of these groups. Parameters:. mt (MatrixTable); path (int) – Path (directory to write to.; batch_size (int) – Number of columns to write per iteration.; bgzip (bool) – BGZip output files.; header_json_in_file (bool) – Include JSON header in each component file (if False, only written to index.tsv). hail.experimental.gather(ht, key, value, *fields)[source]; Collapse fields into key-value pairs.; gather() mimics the functionality of the gather() function found in R’s; tidyr package. This is a way to turn “wide” format data into “long”; format data. Parameters:. ht (Table) – A Hail table.; key (str) – The name of the key field in the gathered table.; value (str) – The name of the value field in the gathered table.; fields (variable-length args of obj:str) – Names of fields to gather in ht. Returns:; Table – Table with original fields gathered into key and value fields. hail.experimental.separate(ht, field, into, delim)[source]; Separate a field into multiple fields by splitting on a delimiter; character or position.; separate() mimics the functionality of the separate() function in R’s; tidyr package.; This function will create a new table where field has been split into; multiple new fields, whose names are given by into.; If delim is a str (including regular expression strings), field; will be separated into columns by that string. In this case, the length; of into must match the number of resulting fields.; If delim is an int, field will be separated into two row fields,; where the first field contains the first delim characters of field; and the second field contains the remaining characters. Parameters:. ht (Table) – A Hail table.; field (str) – The name of the field to separate in ht.; into (list of str) – The names of the fi",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
Performance,load,load,"ls of genes or transcripts. export_entries_by_col(mt, path[, ...]); Export entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> h",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
Testability,test,tests," page source. Experimental; This module serves two functions: as a staging area for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions.; At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. Warning; The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. ldscsim. Contribution Guidelines; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. Function docstrings are required. Hail uses; NumPy style docstrings.; Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on Tests in; python/tests/experimental/test_experimental.py; Code style is not strictly enforced, aside from egregious violations. We do; recommend using autopep8 though!. Annotation Database; Classes. hail.experimental.DB; An annotation database instance. Genetics Methods. load_dataset(name, version, reference_genome); Load a genetic dataset from Hail's repository. ld_score(entry_expr, locus_expr, radius[, ...]); Calculate LD scores. ld_score_regression(weight_expr, ...[, ...]); Estimate SNP-heritability and level of confounding biases from genome-wide association study (GWAS) summary statistics. write_expression(expr, path[, overwrite]); Write an Expression. read_expression(path[, _assert_type]); Read an Expression written with experimental.write_expression(). filtering_allele_frequency(ac, an, ci); Computes a filtering allele frequency (described below) for ac and an with confidence ci. hail_metadata(t_path); Create",MatchSource.WIKI,docs/0.2/experimental/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/index.html
Deployability,update,updated,"tb, coef_dict=None, str_expr=None, axis='rows')[source]; Aggregates by linear combination fields matching either keys in coef_dict; or str_expr. Outputs the aggregation in a MatrixTable or Table; as a new row field “agg_annot” or a new column field “agg_cov”. Parameters:. tb (MatrixTable or Table) – MatrixTable or Table containing fields to be aggregated.; coef_dict (dict, optional) – Coefficients to multiply each field. The coefficients are specified by; coef_dict value, the row (or col) field name is specified by coef_dict key.; If not included, coefficients are assumed to be 1.; str_expr (str, optional) – String expression to match against row (or col) field names.; axis (str) – Either ‘rows’ or ‘cols’. If ‘rows’, this aggregates across row fields.; If ‘cols’, this aggregates across col fields. If tb is a Table, axis = ‘rows’. Returns:; MatrixTable or Table – MatrixTable or Table containing aggregation field. hail.experimental.ldscsim.get_coef_dict(tb, str_expr=None, ref_coef_dict=None, axis='rows')[source]; Gets either col or row fields matching str_expr and take intersection; with keys in coefficient reference dict. Parameters:. tb (MatrixTable or Table) – MatrixTable or Table containing row (or col) for coef_dict.; str_expr (str, optional) – String expression pattern to match against row (or col) fields. If left; unspecified, the intersection of field names is only between existing; row (or col) fields in mt and keys of ref_coef_dict.; ref_coef_dict (dict, optional) – Reference coefficient dictionary with keys that are row (or col) field; names from which to subset. If not included, coefficients are assumed to be 1.; axis (str) – Field type in which to search for field names. Options: ‘rows’, ‘cols’. Returns:; coef_dict (dict) – Coefficients to multiply each field. The coefficients are specified by; coef_dict value, the row (or col) field name is specified by coef_dict key. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html
Integrability,depend,depending,"alent of _annotate_all, but checks source MatrixTable of exprs. ascertainment_bias(mt, y, P); Adds ascertainment bias to a binary phenotype to give it a sample prevalence of P = cases/(cases+controls). binarize(mt, y, K[, exact]); Binarize phenotype y such that it has prevalence K = cases/(cases+controls) Uses inverse CDF of Gaussian to set binarization threshold when exact = False, otherwise uses ranking to determine threshold. agg_fields(tb[, coef_dict, str_expr, axis]); Aggregates by linear combination fields matching either keys in coef_dict or str_expr. get_coef_dict(tb[, str_expr, ref_coef_dict, ...]); Gets either col or row fields matching str_expr and take intersection with keys in coefficient reference dict. hail.experimental.ldscsim.simulate_phenotypes(mt, genotype, h2, pi=None, rg=None, annot=None, popstrat=None, popstrat_var=None, exact_h2=False)[source]; Simulate phenotypes for testing LD score regression.; Simulates betas (SNP effects) under the infinitesimal, spike & slab, or; annotation-informed models, depending on parameters passed. Optionally adds; population stratification. Parameters:. mt (MatrixTable) – MatrixTable containing genotypes to be used. Also should contain; variant annotations as row fields if running the annotation-informed; model or covariates as column fields if adding population stratification.; genotype (Expression or CallExpression) – Entry field containing genotypes of individuals to be used for the; simulation.; h2 (float or int or list or numpy.ndarray) – SNP-based heritability of simulated trait.; pi (float or int or list or numpy.ndarray, optional) – Probability of SNP being causal when simulating under the spike & slab; model.; rg (float or int or list or numpy.ndarray, optional) – Genetic correlation between traits.; annot (Expression, optional) – Row field to use as our aggregated annotations.; popstrat (Expression, optional) – Column field to use as our aggregated covariates for adding population; stratification.; exa",MatchSource.WIKI,docs/0.2/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html
Testability,test,testing,"methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; ldscsim. View page source. ldscsim. Models for SNP effects:; Infinitesimal (can simulate n correlated traits); Spike & slab (can simulate 2 correlated traits); Annotation-informed. Features:; Field aggregation tools for annotation-informed model and; population stratification with many covariates.; Automatic adjustment of genetic correlation parameters; to allow for the joint simulation of up to 100 randomly; correlated phenotypes.; Methods for binarizing phenotypes to have a certain prevalence; and for adding ascertainment bias to binarized phenotypes. simulate_phenotypes(mt, genotype, h2[, pi, ...]); Simulate phenotypes for testing LD score regression. make_betas(mt, h2[, pi, annot, rg]); Generates betas under different models. multitrait_inf(mt[, h2, rg, cov_matrix, seed]); Generates correlated betas for multi-trait infinitesimal simulations for any number of phenotypes. multitrait_ss(mt, h2, pi[, rg, seed]); Generates spike & slab betas for simulation of two correlated phenotypes. get_cov_matrix(h2, rg[, psd_rg]); Creates covariance matrix for simulating correlated SNP effects. calculate_phenotypes(mt, genotype, beta, h2); Calculates phenotypes by multiplying genotypes and betas. normalize_genotypes(genotype); Normalizes genotypes to have mean 0 and variance 1 at each SNP. annotate_all(mt[, row_exprs, col_exprs, ...]); Equivalent of _annotate_all, but checks source MatrixTable of exprs. ascertainment_bias(mt, y, P); Adds ascertainment bias to a binary phenotype to give it a sample prevalence of P = cases/(cases+controls). binarize(mt, y, K[, exact]); Binarize phenotype y such that it has prevalence K = ",MatchSource.WIKI,docs/0.2/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html
Availability,down,downstream,"onary keyed by results of f. hail.expr.functions.fold(f, zero, collection)[source]; Reduces a collection with the given function f, provided the initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; collection (ArrayExpression or SetExpression). Returns:; Expression. hail.expr.functions.array_scan(f, zero, a)[source]; Map each element of a to cumulative value of function f, with initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; a (ArrayExpression). Returns:; ArrayExpression. hail.expr.functions.reversed(x)[source]; Reverses the elements of a collection.; Examples; >>> a = ['The', 'quick', 'brown', 'fox']; >>> hl.eval(hl.reversed(a)); ['fox', 'brown', 'quick', 'The']. Parameters:; x (ArrayExpression or StringExpression) – Array or string expression. Returns:; Expression. hail.expr.functions.keyed_intersection(*arrays, key)[source]; Compute the intersection of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. arrays; key. Returns:; ArrayExpression. hail.expr.functions.keyed_union(*arrays, key)[source]; Compute the distinct union of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. exprs; key. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/collections.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/collections.html
Deployability,update,updated,"onary keyed by results of f. hail.expr.functions.fold(f, zero, collection)[source]; Reduces a collection with the given function f, provided the initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; collection (ArrayExpression or SetExpression). Returns:; Expression. hail.expr.functions.array_scan(f, zero, a)[source]; Map each element of a to cumulative value of function f, with initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; a (ArrayExpression). Returns:; ArrayExpression. hail.expr.functions.reversed(x)[source]; Reverses the elements of a collection.; Examples; >>> a = ['The', 'quick', 'brown', 'fox']; >>> hl.eval(hl.reversed(a)); ['fox', 'brown', 'quick', 'The']. Parameters:; x (ArrayExpression or StringExpression) – Array or string expression. Returns:; Expression. hail.expr.functions.keyed_intersection(*arrays, key)[source]; Compute the intersection of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. arrays; key. Returns:; ArrayExpression. hail.expr.functions.keyed_union(*arrays, key)[source]; Compute the distinct union of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. exprs; key. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/collections.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/collections.html
Modifiability,variab,variable-length,">>> a = [(1, 5), (3, 2), (7, 8)]. >>> hl.eval(hl.starmap(lambda x, y: hl.if_else(x < y, x, y), a)); [1, 2, 7]. Parameters:. f (function ( (*args) -> Expression)) – Function to transform each element of the collection.; collection (ArrayExpression or SetExpression) – Collection expression. Returns:; ArrayExpression or SetExpression. – Collection where each element has been transformed by f. hail.expr.functions.zip(*arrays, fill_missing=False)[source]; Zip together arrays into a single array.; Examples; >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the fill_missing parameter.; >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; The element type of the resulting array is a ttuple with a field; for each array. Parameters:. arrays (: variable-length args of ArrayExpression) – Array expressions.; fill_missing (bool) – If False, return an array with length equal to the shortest length; of the arrays. If True, return an array equal to the longest; length of the arrays, by extending the shorter arrays with missing; values. Returns:; ArrayExpression. hail.expr.functions.enumerate(a, start=0, *, index_first=True)[source]; Returns an array of (index, element) tuples.; Examples; >>> hl.eval(hl.enumerate(['A', 'B', 'C'])); [(0, 'A'), (1, 'B'), (2, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], start=3)); [(3, 'A'), (4, 'B'), (5, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], index_first=False)); [('A', 0), ('B', 1), ('C', 2)]. Parameters:. a (ArrayExpression); start (Int32Expression) – The index value from which the counter is started, 0 by default.; index_first (bool) – If True, the index is the first value of the element tuples. If; False, the index is the second value. Returns:; ArrayExpression – Array of (index, element) or (element, index",MatchSource.WIKI,docs/0.2/functions/collections.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/collections.html
Deployability,update,updated," '3'). >>> hl.eval(t[2]); '3'. Parameters:; iterable (an iterable of Expression) – Tuple elements. Returns:; TupleExpression. hail.expr.functions.array(collection)[source]; Construct an array expression.; Examples; >>> s = {'Bob', 'Charlie', 'Alice'}. >>> hl.eval(hl.array(s)); ['Alice', 'Bob', 'Charlie']. Parameters:; collection (ArrayExpression or SetExpression or DictExpression). Returns:; ArrayExpression. hail.expr.functions.empty_array(t)[source]; Returns an empty array of elements of a type t.; Examples; >>> hl.eval(hl.empty_array(hl.tint32)); []. Parameters:; t (str or HailType) – Type of the array elements. Returns:; ArrayExpression. hail.expr.functions.set(collection)[source]; Convert a set expression.; Examples; >>> s = hl.set(['Bob', 'Charlie', 'Alice', 'Bob', 'Bob']); >>> hl.eval(s) ; {'Alice', 'Bob', 'Charlie'}. Returns:; SetExpression – Set of all unique elements. hail.expr.functions.empty_set(t)[source]; Returns an empty set of elements of a type t.; Examples; >>> hl.eval(hl.empty_set(hl.tstr)); set(). Parameters:; t (str or HailType) – Type of the set elements. Returns:; SetExpression. hail.expr.functions.dict(collection)[source]; Creates a dictionary.; Examples; >>> hl.eval(hl.dict([('foo', 1), ('bar', 2), ('baz', 3)])); {'bar': 2, 'baz': 3, 'foo': 1}. Notes; This method expects arrays or sets with elements of type ttuple; with 2 fields. The first field of the tuple becomes the key, and the second; field becomes the value. Parameters:; collection (DictExpression or ArrayExpression or SetExpression). Returns:; DictExpression. hail.expr.functions.empty_dict(key_type, value_type)[source]; Returns an empty dictionary with key type key_type and value type; value_type.; Examples; >>> hl.eval(hl.empty_dict(hl.tstr, hl.tint32)); {}. Parameters:. key_type (str or HailType) – Type of the keys.; value_type (str or HailType) – Type of the values. Returns:; DictExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/constructors.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/constructors.html
Deployability,update,updated,".or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (BooleanExpression); value (Expression) – Value to return if predicate is True. Returns:; Expression – This expression has the same type as b. hail.expr.functions.range(start, stop=None, step=1)[source]; Returns an array of integers from start to stop by step.; Examples; >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; The range includes start, but excludes stop.; If provided exactly one argument, the argument is interpreted as stop and; start is set to zero. This matches the behavior of Python’s range. Parameters:. start (int or Expression of type tint32) – Start of range.; stop (int or Expression of type tint32) – End of range.; step (int or Expression of type tint32) – Step of range. Returns:; ArrayNumericExpression. hail.expr.functions.query_table(path, point_or_interval)[source]; Query records from a table corresponding to a given point or range of keys.; Notes; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in Table.annotate(). Warning; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters:. path (str) – Table path.; point_or_interval – Point or interval to query. Returns:; ArrayExpression. CaseBuilder; Class for chaining multiple if-else statements. SwitchBuilder; Class for generating conditional trees based on value of an expression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
Modifiability,variab,variable,"﻿. Hail | ; Core language functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions. View page source. Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query rec",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
Safety,safe,safeguards,".or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (BooleanExpression); value (Expression) – Value to return if predicate is True. Returns:; Expression – This expression has the same type as b. hail.expr.functions.range(start, stop=None, step=1)[source]; Returns an array of integers from start to stop by step.; Examples; >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; The range includes start, but excludes stop.; If provided exactly one argument, the argument is interpreted as stop and; start is set to zero. This matches the behavior of Python’s range. Parameters:. start (int or Expression of type tint32) – Start of range.; stop (int or Expression of type tint32) – End of range.; step (int or Expression of type tint32) – Step of range. Returns:; ArrayNumericExpression. hail.expr.functions.query_table(path, point_or_interval)[source]; Query records from a table corresponding to a given point or range of keys.; Notes; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in Table.annotate(). Warning; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters:. path (str) – Table path.; point_or_interval – Point or interval to query. Returns:; ArrayExpression. CaseBuilder; Class for chaining multiple if-else statements. SwitchBuilder; Class for generating conditional trees based on value of an expression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
Testability,test,tests,"| ; Core language functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions. View page source. Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records fr",MatchSource.WIKI,docs/0.2/functions/core.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/core.html
Availability,down,downcode," functions. locus(contig, pos[, reference_genome]); Construct a locus expression from a chromosome and position. locus_from_global_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig n",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
Deployability,update,updated," filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_reference_genome. hail.expr.functions.min_rep(locus, alleles)[source]; Computes the minimal representation of a (locus, alleles) polymorphism.; Examples; >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['TAA', 'TA'])); Struct(locus=Locus(contig=1, position=100000, reference_genome=GRCh37), alleles=['TA', 'T']). >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['AATAA', 'AACAA'])); Struct(locus=Locus(contig=1, position=100002, reference_genome=GRCh37), alleles=['T', 'C']). Notes; Computing the minimal representation can cause the locus shift right (the; position can increase). Parameters:. locus (LocusExpression); alleles (ArrayExpression of type tstr). Returns:; StructExpression – A tstruct expression with two fields, locus; (LocusExpression) and alleles; (ArrayExpression of type tstr). hail.expr.functions.reverse_complement(s, rna=False)[source]; Reverses the string and translates base pairs into their complements; .. rubric:: Examples; >>> bases = hl.literal('NNGATTACA'); >>> hl.eval(hl.reverse_complement(bases)); 'TGTAATCNN'. Parameters:. s (StringExpression) – Base string.; rna (bool) – If True, pair adenine (A) with uracil (U) instead of thymine (T). Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
Modifiability,polymorphi,polymorphism,"_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_g",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
Performance,load,load,"3 array of bi-allelic Phred-scaled genotype likelihoods. Returns:; Expression of type tfloat64. hail.expr.functions.gp_dosage(gp)[source]; Return expected genotype dosage from array of genotype probabilities.; Examples; >>> hl.eval(hl.gp_dosage([0.0, 0.5, 0.5])); 1.5. Notes; This function is only defined for bi-allelic variants. The gp argument; must be length 3. The value is gp[1] + 2 * gp[2]. Parameters:; gp (Expression of type tarray of tfloat64) – Length 3 array of bi-allelic genotype probabilities. Returns:; Expression of type tfloat64. hail.expr.functions.get_sequence(contig, position, before=0, after=0, reference_genome='default')[source]; Return the reference sequence at a given locus.; Examples; Return the reference allele for 'GRCh37' at the locus '1:45323':; >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) ; ""T"". Notes; This function requires reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome.; Returns None if contig and position are not valid coordinates in; reference_genome. Parameters:. contig (Expression of type tstr) – Locus contig.; position (Expression of type tint32) – Locus position.; before (Expression of type tint32, optional) – Number of bases to include before the locus of interest. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome (str or ReferenceGenome) – Reference genome to use. Must have a reference sequence available. Returns:; StringExpression. hail.expr.functions.mendel_error_code(locus, is_female, father, mother, child)[source]; Compute a Mendelian violation code for genotypes.; >>> father = hl.call(0, 0); >>> mother = hl.call(1, 1); >>> child1 = hl.call(0, 1) # consistent; >>> child2 = hl.call(0, 0) # Mendel error; >>> locus = hl.locus('2', 2000000). >>> hl.eval(hl.mendel_error_cod",MatchSource.WIKI,docs/0.2/functions/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html
Availability,error,error,"p-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; CaseBuilder. View page source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that e",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
Deployability,update,updated," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
Integrability,message,message,"p-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; CaseBuilder. View page source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that e",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
Testability,test,test," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html
Availability,error,error,"o Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missin",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
Deployability,update,updated,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
Integrability,message,message,"o Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missin",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
Testability,test,test,"g And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression);",MatchSource.WIKI,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html
Availability,down,downcode," logit(); floor(); ceil(); sqrt(); sign(); min(); nanmin(); max(); nanmax(); mean(); median(); product(); sum(); cumulative_sum(); argmin(); argmax(); corr(); uniroot(); binary_search(). String functions; format(); json(); parse_json(); hamming(); delimit(); entropy(); parse_int(); parse_int32(); parse_int64(); parse_float(); parse_float32(); parse_float64(). Statistical functions; chi_squared_test(); fisher_exact_test(); contingency_table_test(); cochran_mantel_haenszel_test(); dbeta(); dchisq(); dnorm(); dpois(); hardy_weinberg_test(); binom_test(); pchisqtail(); pgenchisq(); pnorm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expre",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
Deployability,update,updated,"etting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
Modifiability,variab,variable,"(). Statistical functions; chi_squared_test(); fisher_exact_test(); contingency_table_test(); cochran_mantel_haenszel_test(); dbeta(); dchisq(); dnorm(); dpois(); hardy_weinberg_test(); binom_test(); pchisqtail(); pgenchisq(); pnorm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an arra",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
Security,expose,exposed,"﻿. Hail | ; Functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions. View page source. Functions; These functions are exposed at the top level of the module, e.g. hl.case. Core language functions; literal(); cond(); if_else(); switch(); case(); bind(); rbind(); missing(); null(); str(); is_missing(); is_defined(); coalesce(); or_else(); or_missing(); range(); query_table(); CaseBuilder; SwitchBuilder. Constructor functions; bool(); float(); float32(); float64(); int(); int32(); int64(); interval(); struct(); tuple(); array(); empty_array(); set(); empty_set(); dict(); empty_dict(). Collection functions; len(); map(); flatmap(); starmap(); zip(); enumerate(); zip_with_index(); flatten(); any(); all(); filter(); sorted(); find(); group_by(); fold(); array_scan(); reversed(); keyed_intersection(); keyed_union(). Numeric functions; abs(); approx_equal(); bit_and(); bit_or(); bit_xor(); bit_lshift(); bit_rshift(); bit_not(); bit_count(); exp(); expit(); is_nan(); is_finite(); is_infinite(); log(); log10(); logit(); floor(); ceil(); sqrt(); sign(); min(); nanmin(); max(); nanmax(); mean(); median(); product(); sum(); cumulative_sum(); argmin(); argmax(); corr(); uniroot(); binary_search(). String functions; format(); json(); parse_json(); hamming(); delimit(); entropy(); parse_int(); parse_int32(); parse_int64(); parse_float(); parse_float32(); parse_float64(). Statistical functions; chi_squared_test",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
Testability,log,log,"enu; Hail. Python API; Hail Query Python API; Functions. View page source. Functions; These functions are exposed at the top level of the module, e.g. hl.case. Core language functions; literal(); cond(); if_else(); switch(); case(); bind(); rbind(); missing(); null(); str(); is_missing(); is_defined(); coalesce(); or_else(); or_missing(); range(); query_table(); CaseBuilder; SwitchBuilder. Constructor functions; bool(); float(); float32(); float64(); int(); int32(); int64(); interval(); struct(); tuple(); array(); empty_array(); set(); empty_set(); dict(); empty_dict(). Collection functions; len(); map(); flatmap(); starmap(); zip(); enumerate(); zip_with_index(); flatten(); any(); all(); filter(); sorted(); find(); group_by(); fold(); array_scan(); reversed(); keyed_intersection(); keyed_union(). Numeric functions; abs(); approx_equal(); bit_and(); bit_or(); bit_xor(); bit_lshift(); bit_rshift(); bit_not(); bit_count(); exp(); expit(); is_nan(); is_finite(); is_infinite(); log(); log10(); logit(); floor(); ceil(); sqrt(); sign(); min(); nanmin(); max(); nanmax(); mean(); median(); product(); sum(); cumulative_sum(); argmin(); argmax(); corr(); uniroot(); binary_search(). String functions; format(); json(); parse_json(); hamming(); delimit(); entropy(); parse_int(); parse_int32(); parse_int64(); parse_float(); parse_float32(); parse_float64(). Statistical functions; chi_squared_test(); fisher_exact_test(); contingency_table_test(); cochran_mantel_haenszel_test(); dbeta(); dchisq(); dnorm(); dpois(); hardy_weinberg_test(); binom_test(); pchisqtail(); pgenchisq(); pnorm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(",MatchSource.WIKI,docs/0.2/functions/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/index.html
Availability,toler,tolerance,"﻿. Hail | ; Numeric functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum elem",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
Deployability,update,updated," type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
Modifiability,extend,extended,"Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_xor(x, y)[source]; Bitwise exclusive-or x and y.; Examples; >>> hl.eval(hl.bit_xor(5, 3)); 6. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_lshift(x, y)[source]; Bitwise left-shift x by y.; Examples; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:; >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for mo",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
Performance,perform,perform," type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
Testability,log,logical," Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Retur",MatchSource.WIKI,docs/0.2/functions/numeric.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html
Deployability,pipeline,pipeline,"6938964441,; 0.5550464170615771]. In fact, in this case we are getting the tail of; >>> table = hl.utils.range_table(7, 1).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; The values of a random function are fully determined by three things:. The seed set on the function itself. If not specified, these are simply; generated sequentially.; Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a range_table(),; this data is simply the row id, as suggested by the previous examples.; The global seed. This is fixed for the entire session, and can only be set; using the global_seed argument to init(). To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; reset_global_randomness() at the start of a pipeline, which resets the; counter used to generate seeds.; >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition specify the; global_seed in init(). If not specified, the global seed is chosen; randomly. All documentation examples were computed using global_seed=0.; >>> hl.stop() ; >>> hl.init(global_seed=0) ; >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) ; [0.9828239225846387, 0.49094525115847415]. rand_bool(p[, seed]); Returns True with probability p. rand_beta(a, b[, lower, upper, seed]); Samples from a beta distribution with parameters a (alpha) and b (beta). rand_cat(prob[, seed]); Samples from a categorical ",MatchSource.WIKI,docs/0.2/functions/random.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/random.html
Usability,simpl,simply,"nge_table(5, 5).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876]. However, moving it to a sufficiently different context will produce different; results:; >>> table = hl.utils.range_table(7, 1); >>> table = table.filter(table.idx >= 2).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. In fact, in this case we are getting the tail of; >>> table = hl.utils.range_table(7, 1).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; The values of a random function are fully determined by three things:. The seed set on the function itself. If not specified, these are simply; generated sequentially.; Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a range_table(),; this data is simply the row id, as suggested by the previous examples.; The global seed. This is fixed for the entire session, and can only be set; using the global_seed argument to init(). To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; reset_global_randomness() at the start of a pipeline, which resets the; counter used to generate seeds.; >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition specify the; global_seed in init(",MatchSource.WIKI,docs/0.2/functions/random.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/random.html
Availability,error,error,"Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
Deployability,release,released,"> hl.eval(hl.pgenchisq(40 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0. >>> hl.eval(hl.pgenchisq(-80, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of ti",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
Energy Efficiency,efficient,efficient,"e parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
Integrability,integrat,integrate," lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Exp",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
Modifiability,variab,variables,"qtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the CDF.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Noncentrality parameter, defaults to 0 if unspecified.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pgenchisq(x, w, k, lam, mu, sigma, *, max_iterations=None, min_accuracy=None)[source]; The cumulative probability function of a generalized chi-squared distribution.; The generalized chi-squared distribution has many interpretations. We share here four; interpretations of the values of this distribution:. A linear combination of normal variables and squares of normal variables.; A weighted sum of sums of squares of normally distributed values plus a normally distributed; value.; A weighted sum of chi-squared distributed values plus a normally distributed value.; A “quadratic form” in a vector; of uncorrelated standard normal values. The parameters of this function correspond to the parameters of the third interpretation. \[\begin{aligned}; w &: R^n \quad k : Z^n \quad lam : R^n \quad mu : R \quad sigma : R \\; \\; x &\sim N(mu, sigma^2) \\; y_i &\sim \mathrm{NonCentralChiSquared}(k_i, lam_i) \\; \\; Z &= x + w y^T \\; &= x + \sum_i w_i y_i \\; Z &\sim \mathrm{GeneralizedNonCentralChiSquared}(w, k, lam, mu, sigma); \end{aligned}\]; The generalized chi-squared distribution often arises when working on linear models with standard; normal noise because the sum of the squares of the residuals should follow a generalized; chi-squared distribution.; Examples; The following plot shows three examples of the generalized chi-squared",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
Performance,perform,performs,"e parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
Safety,abort,aborting,"rray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probabili",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
Security,access,accessible,"971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterati",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
Testability,test,test,"﻿. Hail | ; Statistical functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Statistical functions. View page source. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dchisq(x, df[, ncp, log_p]); Compute the probability density at x of a chi-squared distribution with df degrees of freedom. dnorm(x[, mu, sigma, log_p]); Compute the probability density at x of a normal distribution with mean mu and standard deviation sigma. dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. binom_test(x, n, p, alternative); Performs a binomial test on p given x successes in n trials. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for ",MatchSource.WIKI,docs/0.2/functions/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/stats.html
Deployability,update,updated,"type tint32. hail.expr.functions.parse_int32(x)[source]; Parse a string as a 32-bit integer.; Examples; >>> hl.eval(hl.parse_int32('154')); 154. >>> hl.eval(hl.parse_int32('15.4')); None. >>> hl.eval(hl.parse_int32('asdf')); None. Notes; If the input is an invalid integer, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tint32. hail.expr.functions.parse_int64(x)[source]; Parse a string as a 64-bit integer.; Examples; >>> hl.eval(hl.parse_int64('154')); 154. >>> hl.eval(hl.parse_int64('15.4')); None. >>> hl.eval(hl.parse_int64('asdf')); None. Notes; If the input is an invalid integer, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tint64. hail.expr.functions.parse_float(x)[source]; Parse a string as a 64-bit floating point number.; Examples; >>> hl.eval(hl.parse_float('1.1')); 1.1. >>> hl.eval(hl.parse_float('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat64. hail.expr.functions.parse_float32(x)[source]; Parse a string as a 32-bit floating point number.; Examples; >>> hl.eval(hl.parse_float32('1.1')); 1.100000023841858. >>> hl.eval(hl.parse_float32('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat32. hail.expr.functions.parse_float64(x)[source]; Parse a string as a 64-bit floating point number.; Examples; >>> hl.eval(hl.parse_float64('1.1')); 1.1. >>> hl.eval(hl.parse_float64('asdf')); None. Notes; If the input is an invalid floating point number, then result of this call will be missing. Parameters:; x (StringExpression). Returns:; NumericExpression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/functions/string.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/string.html
Modifiability,variab,variable-length,"nt(x); Parse a string as a 32-bit integer. parse_int32(x); Parse a string as a 32-bit integer. parse_int64(x); Parse a string as a 64-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. hail.expr.functions.format(f, *args)[source]; Returns a formatted string using a specified format string and arguments.; Examples; >>> hl.eval(hl.format('%.3e', 0.09345332)); '9.345e-02'. >>> hl.eval(hl.format('%.4f', hl.missing(hl.tfloat64))); 'null'. >>> hl.eval(hl.format('%s %s %s', 'hello', hl.tuple([3, hl.locus('1', 2453)]), True)); 'hello (3, 1:2453) true'. Notes; See the Java documentation; for valid format specifiers and arguments.; Missing values are printed as 'null' except when using the; format flags ‘b’ and ‘B’ (printed as 'false' instead). Parameters:. f (StringExpression) – Java format string.; args (variable-length arguments of Expression) – Arguments to format. Returns:; StringExpression. hail.expr.functions.json(x)[source]; Convert an expression to a JSON string expression.; Examples; >>> hl.eval(hl.json([1,2,3,4,5])); '[1,2,3,4,5]'. >>> hl.eval(hl.json(hl.struct(a='Hello', b=0.12345, c=[1,2], d={'hi', 'bye'}))); '{""a"":""Hello"",""b"":0.12345,""c"":[1,2],""d"":[""bye"",""hi""]}'. Parameters:; x – Expression to convert. Returns:; StringExpression – String expression with JSON representation of x. hail.expr.functions.parse_json(x, dtype)[source]; Convert a JSON string to a structured expression.; Examples; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters:. x (StringExpression) – JSON string.; dtype – Type of value to parse. Returns:; Expression. hail.expr.functions.hamming(s1, s2)[source]; Returns the Hamming distance between the two strings.; Examples; >>> hl.eval(hl.hamming('ATATA', 'ATGCA')); 2.",MatchSource.WIKI,docs/0.2/functions/string.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/functions/string.html
Deployability,update,updated,"eatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; AlleleType. View page source. AlleleType. class hail.genetics.AlleleType[source]; An enumeration for allele type.; Notes; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; Attributes. UNKNOWN; Unknown Allele Type. SNP; Single-nucleotide Polymorphism (SNP). MNP; Multi-nucleotide Polymorphism (MNP). INSERTION; Insertion. DELETION; Deletion. COMPLEX; Complex Polymorphism. STAR; Star Allele (alt=*). SYMBOLIC; Symbolic Allele. TRANSITION; Transition SNP. TRANSVERSION; Transversion SNP. pretty_name; A formatted (as opposed to uppercase) version of the member's name, to match allele_type(). Methods. strings; Returns the names of the allele types, for use with literal(). COMPLEX = 5; Complex Polymorphism. DELETION = 4; Deletion. INSERTION = 3; Insertion. MNP = 2; Multi-nucleotide Polymorphism (MNP). SNP = 1; Single-nucleotide Polymorphism (SNP). STAR = 6; Star Allele (alt=*). SYMBOLIC = 7; Symbolic Allele; e.g. alt=<INS>. TRANSITION = 8; Transition SNP; e.g. ref=A alt=G. Note; This is only really used internally in hail.vds.sample_qc() and; hail.methods.sample_qc(). TRANSVERSION = 9; Transversion SNP; e.g. ref=A alt=C. Note; This is only really used internally in hail.vds.sample_qc() and; hail.methods.sample_qc(). UNKNOWN = 0; Unknown Allele Type. property pretty_name; A formatted (as opposed to uppercase) version of the member’s name,; to match allele_type(); Examples; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True. static strings()[source]; Returns the names of the allele types, for use with; literal(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.AlleleType.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.AlleleType.html
Deployability,update,updated,"ation of the called alleles. unphased_diploid_gt_index; Return the genotype index for unphased, diploid calls. property alleles; Get the alleles of this call. Returns:; list of int. is_diploid()[source]; True if the ploidy == 2. Return type:; bool. is_haploid()[source]; True if the ploidy == 1. Return type:; bool. is_het()[source]; True if the call contains two different alleles. Return type:; bool. is_het_non_ref()[source]; True if the call contains two different alternate alleles. Return type:; bool. is_het_ref()[source]; True if the call contains one reference and one alternate allele. Return type:; bool. is_hom_ref()[source]; True if the call has no alternate alleles. Return type:; bool. is_hom_var()[source]; True if the call contains identical alternate alleles. Return type:; bool. is_non_ref()[source]; True if the call contains any non-reference alleles. Return type:; bool. n_alt_alleles()[source]; Returns the count of non-reference alleles. Return type:; int. one_hot_alleles(n_alleles)[source]; Returns a list containing the one-hot encoded representation of the; called alleles.; Examples; >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters:; n_alleles (int) – Number of total alleles, including the reference. Returns:; list of int. property phased; True if the call is phased. Returns:; bool. property ploidy; The number of alleles for this call. Returns:; int. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls. Returns:; int. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Call.html
Deployability,update,updated,". Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; Locus. View page source. Locus. class hail.genetics.Locus[source]; An object that represents a location in the genome. Parameters:. contig (str) – Chromosome identifier.; position (int) – Chromosomal position (1-indexed).; reference_genome (str or ReferenceGenome) – Reference genome to use. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.locus.take(5). This is rare; it is much; more common to manipulate the LocusExpression object, which is; constructed using the following functions:. locus(); parse_locus(); locus_from_global_position(). Attributes. contig; Chromosome identifier. position; Chromosomal position (1-based). reference_genome; Reference genome. Methods. parse; Parses a locus object from a CHR:POS string. property contig; Chromosome identifier.; :rtype: str. classmethod parse(string, reference_genome='default')[source]; Parses a locus object from a CHR:POS string.; Examples; >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). Parameters:. string (str) – String to parse.; reference_genome (str or ReferenceGenome) – Reference genome to use. Default is default_reference(). Return type:; Locus. property position; Chromosomal position (1-based).; :rtype: int. property reference_genome; Reference genome. Returns:; ReferenceGenome. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Locus.html
Deployability,update,updated,". class hail.genetics.Pedigree[source]; Class containing a list of trios, with extra functionality. Parameters:; trios (list of Trio) – list of trio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. complete_trios; List of trio objects that have a defined father and mother. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a PLINK .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]; List of trio objects that have a defined father and mother. Return type:; list of Trio. filter_to(samples)[source]; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, pat_id is set to None.; If the mother is not in the list of samples provided, mat_id is set to None. Parameters:; samples (list [str]) – Sample IDs to keep. Returns:; Pedigree. classmethod read(fam_path, delimiter='\\s+')[source]; Read a PLINK .fam file and return a pedigree object.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'). Notes; See PLINK .fam file for; the required format. Parameters:. fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:; Pedigree. property trios; List of trio objects in this pedigree. Return type:; list of Trio. write(path)[source]; Write a .fam file to the given path.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use import_fam() to; manipulate this information. Parameters:; path (str) – output path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html
Testability,test,test,". class hail.genetics.Pedigree[source]; Class containing a list of trios, with extra functionality. Parameters:; trios (list of Trio) – list of trio objects to include in pedigree. Attributes. trios; List of trio objects in this pedigree. Methods. complete_trios; List of trio objects that have a defined father and mother. filter_to; Filter the pedigree to a given list of sample IDs. read; Read a PLINK .fam file and return a pedigree object. write; Write a .fam file to the given path. complete_trios()[source]; List of trio objects that have a defined father and mother. Return type:; list of Trio. filter_to(samples)[source]; Filter the pedigree to a given list of sample IDs.; Notes; For any trio, the following steps will be applied:. If the proband is not in the list of samples provided, the trio is removed.; If the father is not in the list of samples provided, pat_id is set to None.; If the mother is not in the list of samples provided, mat_id is set to None. Parameters:; samples (list [str]) – Sample IDs to keep. Returns:; Pedigree. classmethod read(fam_path, delimiter='\\s+')[source]; Read a PLINK .fam file and return a pedigree object.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'). Notes; See PLINK .fam file for; the required format. Parameters:. fam_path (str) – path to .fam file.; delimiter (str) – Field delimiter. Return type:; Pedigree. property trios; List of trio objects in this pedigree. Return type:; list of Trio. write(path)[source]; Write a .fam file to the given path.; Examples; >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). Notes; This method writes a PLINK .fam file. Caution; Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use import_fam() to; manipulate this information. Parameters:; path (str) – output path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Pedigree.html
Availability,avail,available,"s (list of str) – Contig names.; lengths (dict of str to int) – Dict of contig names to contig lengths.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Attributes. contigs; Contig names. global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. lengths; Dict of contig name to contig length. mt_contigs; Mitochondrial contigs. name; Name of reference genome. par; Pseudoautosomal regions. x_contigs; X contigs. y_contigs; Y contigs. Methods. add_liftover; Register a chain file for liftover. add_sequence; Load the reference sequence from a FASTA file. contig_length; Contig length. from_fasta_file; Create reference genome from a FASTA file. has_liftover; True if a liftover chain file is available from this reference genome to the destination reference. has_sequence; True if the reference sequence has been loaded. locus_from_global_position; "". read; Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
Deployability,update,updated,"ondrial contigs. Returns:; list of str. property name; Name of reference genome. Returns:; str. property par; Pseudoautosomal regions. Returns:; list of Interval. classmethod read(path)[source]; Load reference genome from a JSON file.; Notes; The JSON file must have the following format:; {""name"": ""my_reference_genome"",; ""contigs"": [{""name"": ""1"", ""length"": 10000000},; {""name"": ""2"", ""length"": 20000000},; {""name"": ""X"", ""length"": 19856300},; {""name"": ""Y"", ""length"": 78140000},; {""name"": ""MT"", ""length"": 532}],; ""xContigs"": [""X""],; ""yContigs"": [""Y""],; ""mtContigs"": [""MT""],; ""par"": [{""start"": {""contig"": ""X"",""position"": 60001},""end"": {""contig"": ""X"",""position"": 2699521}},; {""start"": {""contig"": ""Y"",""position"": 10001},""end"": {""contig"": ""Y"",""position"": 2649521}}]; }. name must be unique and not overlap with Hail’s pre-instantiated; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'.; The contig names in xContigs, yContigs, and mtContigs must be; present in contigs. The intervals listed in par must have contigs in; either xContigs or yContigs and must have positions between 0 and; the contig length given in contigs. Parameters:; path (str) – Path to JSON file. Returns:; ReferenceGenome. remove_liftover(dest_reference_genome)[source]; Remove liftover to dest_reference_genome. Parameters:; dest_reference_genome (str or ReferenceGenome). remove_sequence()[source]; Remove the reference sequence. write(output)[source]; “Write this reference genome to a file in JSON format.; Examples; >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; Use read() to reimport the exported; reference genome in a new HailContext session. Parameters:; output (str) – Path of JSON file to write. property x_contigs; X contigs. Returns:; list of str. property y_contigs; Y contigs. Returns:; list of str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
Performance,load,loaded,"contig lengths.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Attributes. contigs; Contig names. global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. lengths; Dict of contig name to contig length. mt_contigs; Mitochondrial contigs. name; Name of reference genome. par; Pseudoautosomal regions. x_contigs; X contigs. y_contigs; Y contigs. Methods. add_liftover; Register a chain file for liftover. add_sequence; Load the reference sequence from a FASTA file. contig_length; Contig length. from_fasta_file; Create reference genome from a FASTA file. has_liftover; True if a liftover chain file is available from this reference genome to the destination reference. has_sequence; True if the reference sequence has been loaded. locus_from_global_position; "". read; Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
Security,access,access,"lot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; ReferenceGenome. View page source. ReferenceGenome. class hail.genetics.ReferenceGenome[source]; An object that represents a reference genome.; Examples; >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; Hail comes with predefined reference genomes (case sensitive!):. GRCh37, Genome Reference Consortium Human Build 37; GRCh38, Genome Reference Consortium Human Build 38; GRCm38, Genome Reference Consortium Mouse Build 38; CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using get_reference():; >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using read will add the reference genome to the list of; known references; it is possible to access the reference genome using; get_reference() anytime afterwards. Note; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; Hail allows setting a default reference so that the reference_genome; argument of import_vcf() does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the default_reference argument of; init(). In order to set a custom reference genome as default,; pass the reference as an argument to default_reference() after; initializing Hail. Parameters:. name (str) – Name of reference. Must b",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
Testability,test,test,"ence sequence from a FASTA file. contig_length; Contig length. from_fasta_file; Create reference genome from a FASTA file. has_liftover; True if a liftover chain file is available from this reference genome to the destination reference. has_sequence; True if the reference sequence has been loaded. locus_from_global_position; "". read; Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-common/references/grch38_to_grch37.over.chain.gz; Public download links are available; here. Parameters:. chain_file (str) – Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to. add_sequence(fasta_file, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/referen",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html
Deployability,update,updated,"uration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; Trio. View page source. Trio. class hail.genetics.Trio[source]; Class containing information about nuclear family relatedness and sex. Parameters:. s (str) – Sample ID of proband.; fam_id (str or None) – Family ID.; pat_id (str or None) – Sample ID of father.; mat_id (str or None) – Sample ID of mother.; is_female (bool or None) – Sex of proband. Attributes. fam_id; Family ID. is_female; Returns True if the proband is a reported female, False if reported male, and None if no sex is defined. is_male; Returns True if the proband is a reported male, False if reported female, and None if no sex is defined. mat_id; ID of mother in trio, may be missing. pat_id; ID of father in trio, may be missing. s; ID of proband in trio, never missing. Methods. is_complete; Returns True if the trio has a defined mother and father. property fam_id; Family ID. Return type:; str or None. is_complete()[source]; Returns True if the trio has a defined mother and father.; The considered fields are mat_id() and pat_id().; Recall that s may never be missing. The fam_id(); and is_female() fields may be missing in a complete trio. Return type:; bool. property is_female; Returns True if the proband is a reported female,; False if reported male, and None if no sex is defined. Return type:; bool or None. property is_male; Returns True if the proband is a reported male,; False if reported female, and None if no sex is defined. Return type:; bool or None. property mat_id; ID of mother in trio, may be missing. Return type:; str or None. property pat_id; ID of father in trio, may be missing. Return type:; str or None. property s; ID of proband in trio, never missing. Return type:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/hail.genetics.Trio.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Trio.html
Deployability,update,updated,"﻿. Hail | ; genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics. View page source. genetics. Classes. AlleleType; An enumeration for allele type. Call; An object that represents an individual's call at a genomic locus. Locus; An object that represents a location in the genome. Pedigree; Class containing a list of trios, with extra functionality. ReferenceGenome; An object that represents a reference genome. Trio; Class containing information about nuclear family relatedness and sex. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/genetics/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/genetics/index.html
Deployability,continuous,continuous,", color=None)[source]; Creates a line plot with the area between the line and the x-axis filled in.; Supported aesthetics: x, y, fill, color, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining non x-axis facing side, none by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete f",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
Integrability,interface,interface,"ation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Plotting With hail.ggplot Overview. View page source. Plotting With hail.ggplot Overview. Warning; Plotting functionality is in early stages and is experimental. The hl.ggplot module is designed based on R’s tidyverse ggplot2 library. This module provides a subset of ggplot2’s; functionality to allow users to generate plots in much the same way they would in ggplot2.; This module is intended to be a new, more flexible way of plotting compared to the hl.plot module. This module; currently uses plotly to generate plots, as opposed to hl.plot, which uses bokeh. Core functions. ggplot; Create the initial plot object. aes; Create an aesthetic mapping. coord_cartesian; Set the boundaries of the plot. hail.ggplot.ggplot(table, mapping={})[source]; Create the initial plot object.; This function is the beginning of all plots using the hail.ggplot interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result.; Examples; Create a y = x^2 scatter plot; >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters:. table – The table containing the data to plot.; mapping – Default list of aesthetic mappings from table data to plot attributes. Returns:; GGPlot. hail.ggplot.aes(**kwargs)[source]; Create an aesthetic mapping. Parameters:; kwargs – Map aesthetic names to hail expressions based on table’s plot. Returns:; Aesthetic – The aesthetic mapping to be applied. hail.ggplot.coord_cartesian(xlim=None, ylim=None)[source]; Set the boundaries of the plot. Parameters:. xlim (tuple with two int) – The minimum and maximum x value to show on the plot.; ylim (tuple with two int) – The minimum and maximum y value to show on the plot. Returns:; FigureAttribu",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
Modifiability,flexible,flexible," Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Plotting With hail.ggplot Overview. View page source. Plotting With hail.ggplot Overview. Warning; Plotting functionality is in early stages and is experimental. The hl.ggplot module is designed based on R’s tidyverse ggplot2 library. This module provides a subset of ggplot2’s; functionality to allow users to generate plots in much the same way they would in ggplot2.; This module is intended to be a new, more flexible way of plotting compared to the hl.plot module. This module; currently uses plotly to generate plots, as opposed to hl.plot, which uses bokeh. Core functions. ggplot; Create the initial plot object. aes; Create an aesthetic mapping. coord_cartesian; Set the boundaries of the plot. hail.ggplot.ggplot(table, mapping={})[source]; Create the initial plot object.; This function is the beginning of all plots using the hail.ggplot interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result.; Examples; Create a y = x^2 scatter plot; >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters:. table – The table containing the data to plot.; mapping – Default list of aesthetic mappings from table data to plot attributes. Returns:; GGPlot. hail.ggplot.aes(**kwargs)[source]; Create an aesthetic mapp",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
Testability,log,log,"p. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining non x-axis facing side, none by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns s",MatchSource.WIKI,docs/0.2/ggplot/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html
Deployability,update,updated,"p. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expression passed to; aggregate. Other column fields and entry fields are dropped. Aggregate Per Row Group. description:; Compute the number of calls with one or more non-reference; alleles per gene group. code:; >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). dependencies:; MatrixTable.group_rows_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the rows of the matrix table by the row-indexed field gene; using MatrixTable.group_rows_by(), which returns a; GroupedMatrixTable. Then use GroupedMatrixTable.aggregate(); to compute an aggregation per grouped row.; The result is a matrix table with an entry field n_non_ref that contains; the result of the aggregation. This new matrix table has a row schema; equal to the fields passed to group_rows_by, a column schema equal to the; column schema of the original matrix table, and an entry schema determined; by the expression passed to aggregate. Other row fields and entry fields; are dropped. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
Integrability,depend,dependencies,"ence (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; Aggregate Row Values Into a Local Value; Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Aggregation. View page source. Aggregation; For a full list of aggregators, see the aggregators; section of the API reference. Table Aggregations. Aggregate Over Rows Into A Local Value. One aggregation. description:; Compute the fraction of rows where SEX == 'M' in a table. code:; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'M')); 0.5. dependencies:; Table.aggregate(), aggregators.fraction(). Multiple aggregations. description:; Compute two aggregation statistics, the fraction of rows where; SEX == 'M' and the mean value of X, from the rows of a table. code:; >>> ht.aggregate(hl.struct(fraction_male = hl.agg.fraction(ht.SEX == 'M'),; ... mean_x = hl.agg.mean(ht.X))); Struct(fraction_male=0.5, mean_x=6.5). dependencies:; Table.aggregate(), aggregators.fraction(), aggregators.mean(), StructExpression. Aggregate Per Group. description:; Group the table ht by ID and compute the mean value of X per group. code:; >>> result_ht = ht.group_by(ht.ID).aggregate(mean_x=hl.agg.mean(ht.X)). dependencies:; Table.group_by(), GroupedTable.aggregate(), aggregators.mean(). Matrix Table Aggregations. Aggregate Entries Per Row (Over Columns). description:; Count the number of occurrences of each unique GT field per row, i.e.; aggregate over the columns of the matrix table.; Methods MatrixTable.filter_rows(), MatrixTable.select_rows(),; and Ma",MatchSource.WIKI,docs/0.2/guides/agg.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/agg.html
Deployability,update,updated,"﻿. Hail | ; Annotation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Annotation. View page source. Annotation; Annotations are Hail’s way of adding data fields to Hail’s tables and matrix; tables. Create a nested annotation. description:; Add a new field gq_mean as a nested field inside info. code:; >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). dependencies:; StructExpression.annotate(), MatrixTable.annotate_rows(). understanding:. To add a new field gq_mean as a nested field inside info,; instead of a top-level field, we need to annotate the info field itself.; Construct an expression mt.info.annotate(gq_mean=...) which adds the field; to info. Then, reassign this expression to info using; MatrixTable.annotate_rows(). Remove a nested annotation. description:; Drop a field AF, which is nested inside the info field. To drop a nested field AF, construct an expression mt.info.drop('AF'); which drops the field from its parent field, info. Then, reassign this; expression to info using MatrixTable.annotate_rows(). code:; >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). dependencies:; StructExpression.drop(), MatrixTable.annotate_rows(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/annotation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/annotation.html
Integrability,depend,dependencies,"﻿. Hail | ; Annotation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Annotation. View page source. Annotation; Annotations are Hail’s way of adding data fields to Hail’s tables and matrix; tables. Create a nested annotation. description:; Add a new field gq_mean as a nested field inside info. code:; >>> mt = mt.annotate_rows(info=mt.info.annotate(gq_mean=hl.agg.mean(mt.GQ))). dependencies:; StructExpression.annotate(), MatrixTable.annotate_rows(). understanding:. To add a new field gq_mean as a nested field inside info,; instead of a top-level field, we need to annotate the info field itself.; Construct an expression mt.info.annotate(gq_mean=...) which adds the field; to info. Then, reassign this expression to info using; MatrixTable.annotate_rows(). Remove a nested annotation. description:; Drop a field AF, which is nested inside the info field. To drop a nested field AF, construct an expression mt.info.drop('AF'); which drops the field from its parent field, info. Then, reassign this; expression to info using MatrixTable.annotate_rows(). code:; >>> mt = mt.annotate_rows(info=mt.info.drop('AF')). dependencies:; StructExpression.drop(), MatrixTable.annotate_rows(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/guides/annotation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/annotation.html
Deployability,pipeline,pipelines,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression. PLINK Conversions; Polygenic Score Calculation. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Genetics. View page source. Genetics; This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the genetics methods page. Formatting. Convert variants in string format to separate locus and allele fields. code:; >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). dependencies:; parse_variant(), key_by(). understanding:. If your variants are strings of the format ‘chr:pos:ref:alt’, you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles.; hl.parse_variant(ht.variant) constructs a StructExpression; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, locus and; alleles. Liftover variants from one coordinate system to another. tags:; liftover. description:; Liftover a Table or MatrixTable from one reference genome to another. code:; First, we need to set up",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
Energy Efficiency,efficient,efficient,"; ... [hl.parse_locus_interval(x, reference_genome='GRCh37') for x in intervals]). dependencies:; methods.filter_intervals(), parse_locus_interval(). Pruning Variants in Linkage Disequilibrium. tags:; LD Prune. description:; Remove correlated variants from a matrix table. code:; >>> biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(mt.GT, r2=0.2, bp_window_size=500000); >>> filtered_mt = mt.filter_rows(; ... hl.is_defined(pruned_variant_table[mt.row_key])). dependencies:; ld_prune(). understanding:. Hail’s ld_prune() method takes a matrix table and returns a table; with a subset of variants which are uncorrelated with each other. The method; requires a biallelic dataset, so we first filter our dataset to biallelic; variants. Next, we get a table of independent variants using ld_prune(),; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistic",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
Integrability,depend,dependencies," How-To Guides; Aggregation; Annotation (Adding Fields); Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression. PLINK Conversions; Polygenic Score Calculation. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Genetics. View page source. Genetics; This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the genetics methods page. Formatting. Convert variants in string format to separate locus and allele fields. code:; >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). dependencies:; parse_variant(), key_by(). understanding:. If your variants are strings of the format ‘chr:pos:ref:alt’, you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles.; hl.parse_variant(ht.variant) constructs a StructExpression; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, locus and; alleles. Liftover variants from one coordinate system to another. tags:; liftover. description:; Liftover a Table or MatrixTable from one reference genome to another. code:; First, we need to set up the two reference genomes (source and destination):; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') ; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Then we can liftover ",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
Modifiability,flexible,flexible,"; which we can use to filter the rows of our original dataset.; Note that it is more efficient to do the final filtering step on the original; dataset, rather than on the biallelic dataset, so that the biallelic dataset; does not need to be recomputed. Analysis. Linear Regression. Single Phenotype. tags:; Linear Regression. description:; Compute linear regression statistics for a single phenotype. code:; Approach #1: Use the linear_regression_rows() method; >>> ht = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(linreg=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator. However, the aggregators.linreg() aggregator is more flexible (multiple covariates; can vary by entry) and returns a richer set of statistics. Multiple Phenotypes. tags:; Linear Regression. description:; Compute linear regression statistics for multiple phenotypes. code:; Approach #1: Use the linear_regression_rows() method for all phenotypes simultaneously; >>> ht_result = hl.linear_regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). depen",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
Usability,guid,guides,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression. PLINK Conversions; Polygenic Score Calculation. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides; Genetics. View page source. Genetics; This page tailored how-to guides for small but commonly-used patterns; appearing in genetics pipelines. For documentation on the suite of; genetics functions built into Hail, see the genetics methods page. Formatting. Convert variants in string format to separate locus and allele fields. code:; >>> ht = ht.key_by(**hl.parse_variant(ht.variant)). dependencies:; parse_variant(), key_by(). understanding:. If your variants are strings of the format ‘chr:pos:ref:alt’, you may want; to convert them to separate locus and allele fields. This is useful if; you have imported a table with variants in string format and you would like to; join this table with other Hail tables that are keyed by locus and; alleles.; hl.parse_variant(ht.variant) constructs a StructExpression; containing two nested fields for the locus and alleles. The ** syntax unpacks; this struct so that the resulting table has two new fields, locus and; alleles. Liftover variants from one coordinate system to another. tags:; liftover. description:; Liftover a Table or MatrixTable from one reference genome to another. code:; First, we need to set up",MatchSource.WIKI,docs/0.2/guides/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html
Deployability,install,installation,"﻿. Hail | ; Use Hail on Azure HDInsight. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Next Steps. Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect ",MatchSource.WIKI,docs/0.2/install/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/azure.html
Security,password,password,". ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Next Steps. Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter; When you are finished with the cluster stop it:; hailctl h",MatchSource.WIKI,docs/0.2/install/azure.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/azure.html
Deployability,install,installation,"﻿. Hail | ; Use Hail on Google Dataproc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Next Steps. Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Google Dataproc. View page source. Use Hail on Google Dataproc; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl dataproc which starts, stops, and; manipulates Hail-enabled Dataproc clusters.; Start a dataproc cluster named “my-first-cluster”. Cluster names may only; contain a mix lowercase letters and dashes. Starting a cluster can take as long; as two minutes.; hailctl dataproc start my-first-cluster. Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl dataproc submit my-first-cluster hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also start a Jupyter Notebook running on the cluster:; hailctl dataproc connect my-first-cluster notebook. When you are finished with the cluster stop it:; hailctl dataproc stop my-first-cluster. Next",MatchSource.WIKI,docs/0.2/install/dataproc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/dataproc.html
Deployability,install,installation,"﻿. Hail | ; Install Hail on GNU/Linux. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on GNU/Linux. View page source. Install Hail on GNU/Linux. Install Java 11.; Install Python 3.9 or later.; Install a recent version of the C and C++ standard libraries. GCC 5.0, LLVM; version 3.4, or any later versions suffice.; Install BLAS and LAPACK.; Install Hail using pip. On a recent Debian-like system, the following should suffice:; apt-get install -y \; openjdk-11-jre-headless \; g++ \; python3.9 python3-pip \; libopenblas-base liblapack3; python3.9 -m pip install hail. Now let’s take Hail for a spin!. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/linux.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/linux.html
Deployability,install,installation,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
Integrability,message,message,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
Modifiability,config,config,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/install/macosx.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/macosx.html
Availability,down,downloads,"n API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a ",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
Deployability,install,installation,"﻿. Hail | ; Install Hail on a Spark Cluster. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like sy",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
Modifiability,config,configuration,"tem, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
Usability,simpl,simpler,"﻿. Hail | ; Install Hail on a Spark Cluster. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like sy",MatchSource.WIKI,docs/0.2/install/other-cluster.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html
Deployability,install,installation,"﻿. Hail | ; Your First Hail Query. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+---",MatchSource.WIKI,docs/0.2/install/try.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/try.html
Energy Efficiency,power,powered,"﻿. Hail | ; Your First Hail Query. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+---",MatchSource.WIKI,docs/0.2/install/try.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/install/try.html
Availability,resilien,resilience,"gh the same; effect can be achieved for * by using @. Warning; For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for + and *, place the; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matri",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
Deployability,pipeline,pipelines," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
Energy Efficiency,power,power,"Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; BlockMatrix. View page source. BlockMatrix. class hail.linalg.BlockMatrix[source]; Hail’s block-distributed matrix of tfloat64 elements. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. A block matrix is a distributed analogue of a two-dimensional; NumPy ndarray with; shape (n_rows, n_cols) and NumPy dtype float64.; Import the class with:; >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; default_block_size().; Operations and broadcasting; The core operations are consistent with NumPy: +, -, *, and; / for element-wise addition, subtraction, multiplication, and division;; @ for matrix multiplication; T for transpose; and ** for; element-wise exponentiation to a scalar power.; For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (int or float). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size.; To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to float64. One-dimensional ndarrays; of shape (n) are promoted to two-dimensional ndarrays of shape (1,; n), i.e. a single row.; Block matrices support broadcasting of +, -, *, and /; between matrices of different shapes, consistent with the NumPy; broadcasting rules.; There is one exception: block matrices do not currently support element-wise; “outer product” of a single row and a single column",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
Integrability,interface,interface,"﻿. Hail | ; BlockMatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; BlockMatrix. View page source. BlockMatrix. class hail.linalg.BlockMatrix[source]; Hail’s block-distributed matrix of tfloat64 elements. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. A block matrix is a distributed analogue of a two-dimensional; NumPy ndarray with; shape (n_rows, n_cols) and NumPy dtype float64.; Import the class with:; >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; default_block_size().; Operations and broadcasting; The core operations are consistent with NumPy: +, -, *, and; / for element-wise addition, subtraction, multiplication, and division;; @ for matrix multiplication; T for transpose; and ** for; element-wise exponentiation to a scalar power.; For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (int or float). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
Modifiability,config,configuration,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
Performance,perform,performance,"gh the same; effect can be achieved for * by using @. Warning; For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for + and *, place the; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matri",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
Safety,avoid,avoiding,", statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing “linkage disequilibrium” between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra.; To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a block-sparse matrix. Otherwise, we say the matrix; is block-dense. The property is_sparse() encodes this state.; Dropped blocks are not stored in memory or on write(). In fact,; blocks that are dropped prior to an action like export() or; to_numpy() are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes.; Block-sparse matrices may be created with; sparsify_band(),; sparsify_rectangles(),; sparsify_row_intervals(),; and sparsify_triangle().; The following methods naturally propagate block-sparsity:. Addition and subtraction “union” realized blocks.; Element-wise multiplication “intersects” realized blocks.; Transpose “transposes” realized blocks.; abs() and sqrt() preserve the realized blocks.; sum() along an axis realizes those blocks for which at least one; block summand is realized.; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify()",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
Testability,test,tested,"﻿. Hail | ; BlockMatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; BlockMatrix. View page source. BlockMatrix. class hail.linalg.BlockMatrix[source]; Hail’s block-distributed matrix of tfloat64 elements. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. A block matrix is a distributed analogue of a two-dimensional; NumPy ndarray with; shape (n_rows, n_cols) and NumPy dtype float64.; Import the class with:; >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; default_block_size().; Operations and broadcasting; The core operations are consistent with NumPy: +, -, *, and; / for element-wise addition, subtraction, multiplication, and division;; @ for matrix multiplication; T for transpose; and ** for; element-wise exponentiation to a scalar power.; For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (int or float). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block ",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
Usability,clear,clear,"tion.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_ex",MatchSource.WIKI,docs/0.2/linalg/hail.linalg.BlockMatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html
Deployability,pipeline,pipelines,"﻿. Hail | ; linalg. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg. View page source. linalg; File formats and interface for numeric matrices are experimental.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. Classes. BlockMatrix; Hail's block-distributed matrix of tfloat64 elements. Modules. utils. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/linalg/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/index.html
Integrability,interface,interface,"﻿. Hail | ; linalg. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg. View page source. linalg; File formats and interface for numeric matrices are experimental.; Improvements to Hail 0.2 may necessitate re-writing pipelines and files; to maintain compatibility. Classes. BlockMatrix; Hail's block-distributed matrix of tfloat64 elements. Modules. utils. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/linalg/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/index.html
Availability,error,errors,"ter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. tran",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Deployability,configurat,configuration,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Energy Efficiency,reduce,reduced," with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of \(Q\) is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call \(Z Z^T\):. \[\begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}\]; The eigenvalues of \(Z Z^T\) and \(Z^T Z\) are the squared singular values of \(Z\);; therefore, we instead focus on \(Z^T Z\). In the expressions below, we elide transpositions; of symmetric matrices:. \[\begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}\]; Before substituting the definition of \(P_0\), simplify it using the reduced QR; decomposition:. \[\begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}\]; Substitute this simplified expression into \(Z\):. \[\begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}\]; Split this symmetric matrix by observing that \(I - Q Q^T\) is idempotent:. \[\begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}\]; Finally, the squared singular values of \(Z\) are the eigenvalues of \(Z^T Z\), so; \(Q\) should be distributed as follows:. \[\begin{align*}; U S V^T ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Integrability,depend,depend,"llowing:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Modifiability,config,configuring,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Performance,throughput,throughput,"ated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; da",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Safety,avoid,avoid,"result of row_correlation() using; linalg.utils.locus_windows() and; BlockMatrix.sparsify_row_intervals(); in order to only compute linkage disequilibrium between nearby; variants. Use row_correlation() directly to calculate correlation; without windowing.; More precisely, variants are 0-indexed by their order in the matrix table; (see add_row_index()). Each variant is regarded as a vector of; elements defined by entry_expr, typically the number of alternate alleles; or genotype dosage. Missing values are mean-imputed within variant.; The method produces a symmetric block-sparse matrix supported in a; neighborhood of the diagonal. If variants \(i\) and \(j\) are on the; same contig and within radius base pairs (inclusive) then the; \((i, j)\) element is their; Pearson correlation coefficient.; Otherwise, the \((i, j)\) element is 0.0.; Rows with a constant value (i.e., zero variance) will result in nan; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; aggregators.stats()).; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that’s; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; See the warnings in row_correlation(). In particular, for large; matrices it may be preferable to run its stages separately.; entry_expr and locus_expr are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Para",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Security,validat,validation,"{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Testability,log,logistic,"s matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(datase",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Usability,simpl,simply,"ion, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In; PLINK/GCTA the denominator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. ",MatchSource.WIKI,docs/0.2/methods/genetics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html
Availability,toler,tolerance," data in textual representations into a Hail; MatrixTable. Finally, it is possible to create a Hail Table; from a pandas DataFrame with Table.from_pandas(). import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_lines(paths[, min_partitions, ...]); Import lines of file(s) as a Table of strings. Genetics; Hail has several functions to import genetics-specific file formats into Hail; MatrixTable or Table objects:. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_bed(path[, reference_genome, ...]); Import a UCSC BED file as a Table. import_bgen(path, entry_fields[, ...]); Import BGEN file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_fam(path[, quant_pheno, delimiter, ...]); Import a PLINK FAM file into a Table. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. Export. General purpose; Some of the most widely-used export functionality is found as class methods; on the Table and Expression objects:. Table.export(): Used to write a Table to a text table (TSV).; Expression.export(): Used to write an expression to a text file. For; one-dimensional expressions (table row fields, matrix table row or column fields),; this is very similar to Table.export(). For two-dimensional expressions; (entry expressions on matrix tables), a text matrix representation that can be; imported with import_matrix_table() will be produced.; Table.to_pandas(): Used to convert a Hail table to a pandas; DataFrame. Genetics; Hail can export to some of the genetics-specific file formats:. export_vcf(dataset, output[, ...]); Export a MatrixTable ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
Deployability,pipeline,pipeline,"ence genome of; the dataset.; The output VCF header will not contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; append_to_header parameter. Warning; INFO fields stored at VCF import are not automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating info, downstream; tools which may produce erroneous results. The solution is to create new; fields in info or overwrite existing fields. For example, in order to; produce an accurate AC field, one can run variant_qc() and copy; the variant_qc.AC field to info.AC as shown below.; >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) ; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; Do not export to a path that is being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
Integrability,depend,dependent,"enotype, which; differs from default PLINK behavior. Use missing='-9' to interpret this; value as missing. Parameters:. bed (str) – PLINK BED file.; bim (str) – PLINK BIM file.; fam (str) – PLINK FAM file.; min_partitions (int, optional) – Minimum number of partitions. Useful in conjunction with block_size.; missing (str) – String used to denote missing values only for the phenotype field.; This is in addition to “-9”, “0”, and “N/A” for case-control; phenotypes.; delimiter (str) – FAM file field delimiter regex.; quant_pheno (bool) – If True, FAM phenotype is interpreted as quantitative.; a2_reference (bool) – If True, A2 is treated as the reference allele. If False, A1 is treated; as the reference allele.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome. If None, the; default is dependent on the reference_genome. For “GRCh37”, the default; is {'23': 'X', '24': 'Y', '25': 'X', '26': 'MT'}. For “GRCh38”, the; default is {'1': 'chr1', ..., '22': 'chr22', '23': 'chrX', '24': 'chrY', '25': 'chrX', '26': 'chrM'}.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; n_partitions (int, optional) – Number of partitions. If both n_partitions and block_size; are specified, n_partitions will be used.; block_size (int, optional) – Block size, in MB. Default: 128MB blocks. Returns:; MatrixTable. hail.methods.import_table(paths, key=None, min_partitions=None, impute=False, no_header=False, comment=(), delimiter='\t', missing='NA', types={}, quote=None, skip_blank_lines=False, force_bgz=False, filter=None, find_replace=None, force=False, source_file_field=None)[source]; Import delimited text file (text table) as Table.; The resulting Table will have no key fields. Use; Table.key_by() to specify keys. See also:; import_matrix_table().; Ex",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
Modifiability,parameteriz,parameterized," UCSC BED file as a Table.; Examples; The file formats are; $ cat data/file1.bed; track name=""BedTest""; 20 1 14000000; 20 17000000 18000000; ... $ cat file2.bed; track name=""BedTest""; 20 1 14000000 cnv1; 20 17000000 18000000 cnv2; ... Add the row field cnv_region indicating inclusion in; at least one interval of the three-column BED file:; >>> bed = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(cnv_region = hl.is_defined(bed[dataset.locus])). Add a row field cnv_id with the value given by the; fourth column of a BED file:; >>> bed = hl.import_bed('data/file2.bed'); >>> result = dataset.annotate_rows(cnv_id = bed[dataset.locus].target). Notes; The table produced by this method has one of two possible structures. If; the .bed file has only three fields (chrom, chromStart, and; chromEnd), then the produced table has only one column:. interval (tinterval) - Row key. Genomic interval. If; reference_genome is defined, the point type of the interval will be; tlocus parameterized by the reference_genome. Otherwise,; the point type is a tstruct with two fields: contig with; type tstr and position with type tint32. If the .bed file has four or more columns, then Hail will store the fourth; column as a row field in the table:. interval (tinterval) - Row key. Genomic interval. Same schema as above.; target (tstr) - Fourth column of .bed file. UCSC bed files can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; Intervals in UCSC BED files are 0-indexed and half open.; The line “5 100 105” correpsonds to the interval [5:101-5:106) in Hail’s; 1-indexed notation. Details; here. Parameters:. path (str) – Path to .bed file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_intervals (bool) – If True and reference_genome is not None, skip lines with; intervals that are not consistent with the reference genome.; contig_recodi",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
Performance,perform,performance,"﻿. Hail | ; Import / Export. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Import / Export. View page source. Import / Export. This page describes functionality for moving data in and out of Hail.; Hail has a suite of functionality for importing and exporting data to and from; general-purpose, genetics-specific, and high-performance native file formats. Native file formats; When saving data to disk with the intent to later use Hail, we highly recommend; that you use the native file formats to store Table and; MatrixTable objects. These binary formats not only smaller than other formats; (especially textual ones) in most cases, but also are significantly faster to; read into Hail later.; These files can be created with methods on the Table and; MatrixTable objects:. Table.write(); MatrixTable.write(). These files can be read into a Hail session later using the following methods:. read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Import. General purpose; The import_table() function is widely-used to import textual data; into a Hail Table. import_matrix_table() is used to import; two-dimensional matrix data in textual representations into a Hail; MatrixTable. Finally, it is possible to create a Hail Table; from a pandas DataFrame with Table.from_pandas(). import",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
Safety,avoid,avoid,"(one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True)[source]; Export a Table to Elasticsearch.; By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. Warning; export_elasticsearch() is EXPERIMENTAL. Note; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a config with the; es.mapping.id; option set to a field that contains a unique value for each row. hail.methods.export_bgen(mt, output, gp=None, varid=None, rsid=None, parallel=None, compression_codec='zlib')[source]; Export MatrixTable as MatrixTable as BGEN 1.2 file with 8; bits of per probability. Also writes SAMPLE file.; If parallel is None, the BGEN file is written to output + '.bgen'. Otherwise, output; + '.bgen' will be a directory containing many BGEN files. In either case, the SAMPLE file is; written to output + '.sample'. For example,; >>> hl.export_bgen(mt, '/path/to/dataset') . Will write two files: /path/to/dataset.bgen and /path/to/dataset.sample. In contrast,; >>> hl.export_bgen(mt, '/path/to/dataset', parallel='header_per_shard') . Will create /path/to/dataset.sample and will create mt.n_partitions() files into the; directory /path/to/dataset.bgen/.; Notes; The export_bgen() function requires genotype probabilities, either as an entry; field of mt (of t",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
Testability,test,test,"(0-indexed) and the column keys 0, 1, … N.; force_bgz (bool) – If True, load .gz files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep (str) – This parameter is a deprecated name for delimiter, please use that; instead.; delimiter (str) – A single character string which separates values in the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns:; MatrixTable – MatrixTable constructed from imported data. hail.methods.import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quant_pheno=False, a2_reference=True, reference_genome='default', contig_recoding=None, skip_invalid_loci=False, n_partitions=None, block_size=None)[source]; Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable.; Examples; >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the --make-bed option.; Hail uses the individual ID (column 2 in FAM file) as the sample id (s).; The individual IDs must be unique.; The resulting MatrixTable has the following fields:. Row fields:. locus (tlocus or tstruct) – Row key. The; chromosome and position. If reference_genome is defined, the type; will be tlocus parameterized by reference_genome.; Otherwise, the type will be a tstruct with two fields:; contig with type tstr and position with type; tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference allele (A2; if a2_reference is True) is the first element in the array.; rsid (tstr) – Column 2 in the BIM file.; cm_position (tfloat64) – Column 3 in ",MatchSource.WIKI,docs/0.2/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/impex.html
Availability,toler,tolerance,"edness; identity_by_descent(); king(); pc_relate(); simulate_random_mating(). Miscellaneous; grep(); maximal_independent_set(); rename_duplicates(); segment_intervals(). Import / Export. export_elasticsearch(t, host, port, index, ...); Export a Table to Elasticsearch. export_gen(dataset, output[, precision, gp, ...]); Export a MatrixTable as GEN and SAMPLE files. export_bgen(mt, output[, gp, varid, rsid, ...]); Export MatrixTable as MatrixTable as BGEN 1.2 file with 8 bits of per probability. export_plink(dataset, output[, call, ...]); Export a MatrixTable as PLINK2 BED, BIM and FAM files. export_vcf(dataset, output[, ...]); Export a MatrixTable or Table as a VCF file. get_vcf_metadata(path); Extract metadata from VCF header. import_bed(path[, reference_genome, ...]); Import a UCSC BED file as a Table. import_bgen(path, entry_fields[, ...]); Import BGEN file(s) as a MatrixTable. import_fam(path[, quant_pheno, delimiter, ...]); Import a PLINK FAM file into a Table. import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association ",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
Deployability,update,update,"gression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
Modifiability,variab,variable,"le into a Table. import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call conc",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
Performance,throughput,throughput,"arly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_d",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
Testability,test,test,"le into a Table. import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call conc",MatchSource.WIKI,docs/0.2/methods/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/index.html
Deployability,update,updated,"ction must satisfy the following property:; tie_breaker(l, r) == -tie_breaker(r, l).; When multiple nodes have the same degree, this algorithm will order the; nodes according to tie_breaker and remove the largest node.; If keyed is False, then a node may appear twice in the resulting; table. Parameters:. i (Expression) – Expression to compute one endpoint of an edge.; j (Expression) – Expression to compute another endpoint of an edge.; keep (bool) – If True, return vertices in set. If False, return vertices removed.; tie_breaker (function) – Function used to order nodes with equal degree.; keyed (bool) – If True, key the resulting table by the node field, this requires; a sort. Returns:; Table – Table with the set of independent vertices. The table schema is one row; field node which has the same type as input expressions i and j. hail.methods.rename_duplicates(dataset, name='unique_id')[source]; Rename duplicate column keys. Note; Requires the column key to be one field of type tstr. Examples; >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; This method produces a new column field from the string column key by; appending a unique suffix _N as necessary. For example, if the column; key “NA12878” appears three times in the dataset, the first will produce; “NA12878”, the second will produce “NA12878_1”, and the third will produce; “NA12878_2”. The name of this new field is parameterized by name. Parameters:. dataset (MatrixTable) – Dataset.; name (str) – Name of new field. Returns:; MatrixTable. hail.methods.segment_intervals(ht, points)[source]; Segment the interval keys of ht at a given set of points. Parameters:. ht (Table) – Table with interval keys.; points (Table or ArrayExpression) – Points at which to segment the intervals, a table or an array. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/misc.html
Modifiability,sandbox,sandbox,"ep, ...]); Return a table containing the vertices in a near maximal independent set of an undirected graph whose edges are given by a two-column table. rename_duplicates(dataset[, name]); Rename duplicate column keys. segment_intervals(ht, points); Segment the interval keys of ht at a given set of points. hail.methods.grep(regex, path, max_count=100, *, show=True, force=False, force_bgz=False)[source]; Searches given paths for all lines containing regex matches.; Examples; Print all lines containing the string hello in file.txt:; >>> hl.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hl.grep('\\d', ['data/file1.txt','data/file2.txt']). Notes; grep() mimics the basic functionality of Unix grep in; parallel, printing results to the screen. This command is provided as a; convenience to those in the statistical genetics community who often; search enormous text files like VCFs. Hail uses Java regular expression; patterns.; The RegExr sandbox may be helpful. Parameters:. regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return; show (bool) – When True, show the values on stdout. When False, return a; dictionary mapping file names to lines.; force_bgz (bool) – If True, read files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, read gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; dict of str to list of str. hail.methods.maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True)[source]; Return a table containing the vertices in a near; maximal independent set; of an undirecte",MatchSource.WIKI,docs/0.2/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/misc.html
Testability,sandbox,sandbox,"ep, ...]); Return a table containing the vertices in a near maximal independent set of an undirected graph whose edges are given by a two-column table. rename_duplicates(dataset[, name]); Rename duplicate column keys. segment_intervals(ht, points); Segment the interval keys of ht at a given set of points. hail.methods.grep(regex, path, max_count=100, *, show=True, force=False, force_bgz=False)[source]; Searches given paths for all lines containing regex matches.; Examples; Print all lines containing the string hello in file.txt:; >>> hl.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hl.grep('\\d', ['data/file1.txt','data/file2.txt']). Notes; grep() mimics the basic functionality of Unix grep in; parallel, printing results to the screen. This command is provided as a; convenience to those in the statistical genetics community who often; search enormous text files like VCFs. Hail uses Java regular expression; patterns.; The RegExr sandbox may be helpful. Parameters:. regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return; show (bool) – When True, show the values on stdout. When False, return a; dictionary mapping file names to lines.; force_bgz (bool) – If True, read files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, read gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; dict of str to list of str. hail.methods.maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True)[source]; Return a table containing the vertices in a near; maximal independent set; of an undirecte",MatchSource.WIKI,docs/0.2/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/misc.html
Availability,down,down,"ed_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of a sample’s first k; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. an individual’s first k principal component coordinates fully; describe their allele-frequency-relevant ancestry, and; the relationship between ancestry (as descri",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
Deployability,configurat,configurations,"leotide variants for which both; individuals \(i\) and \(j\) have a non-missing genotype.; \(X_{i,s}\) be the genotype score matrix. Each entry corresponds to; the genotype of individual \(i\) at variant; \(s\). Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. \(X_{i,s}\) is calculated by invoking; n_alt_alleles() on the call_expr. The three counts above, \(N^{Aa}\), \(N^{Aa,Aa}\), and; \(N^{AA,aa}\), exclude variants where one or both individuals have; missing genotypes.; In terms of the symbols above, we can define \(d\), the genetic distance; between two samples. We can interpret \(d\) as an unnormalized; measurement of the genetic material not shared identically-by-descent:. \[d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2\]; In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. \((X_{i,s} - X_{j,s})^2\); homref; het; homalt. homref; 0; 1; 4. het; 1; 0; 1. homalt; 4; 1; 0. which leads to this expression for genetic distance:. \[d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}\]; The first term, \(4 N^{AA,aa}_{i,j}\), accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case of a pair of heteroyzgous genotypes. We offset this; with the fourth and final term.; The genetic distance, \(d_{i,j}\), ranges between zero and four times; the number of variants in the dataset. In the supplement to Manichaikul,; et. al, the authors demonstrate that the kinship coefficient,; \(\phi_{i,j}\), between two individuals from the same population is; rel",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
Energy Efficiency,efficient,efficient,"indexed call expression.; block_size (int, optional) – Block size of block matrices used in the algorithm.; Default given by BlockMatrix.default_block_size(). Returns:; MatrixTable – A MatrixTable whose rows and columns are keys are taken from; call-expr’s column keys. It has one entry field, phi. hail.methods.pc_relate(call_expr, min_individual_maf, *, k=None, scores_expr=None, min_kinship=None, statistics='all', block_size=None, include_self_kinship=False)[source]; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Note; Requires the dataset to contain only diploid genotype calls. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) . Only compute the kinship statistic. This is more efficient than; computing all statistics.; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') . Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using Table.filter().; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) . One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:; >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\rig",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
Integrability,depend,depending,"are all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; \(2^{-3} = 12.5 %\) of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that \(g_{is}\) is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first.; The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the statistics parameter:. i (col_key.dtype) – First sample. (key field); j (col_key.dtype) – Second sample. (key field); kin (tfloat64) – Kinship estimate, \(\widehat{\phi_{ij}}\).; ibd2 (tfloat64) – IBD2 estimate, \(\widehat{k^{(2)}_{ij}}\).; ibd0 (tfloat64) – IBD0 estimate, \(\widehat{k^{(0)}_{ij}}\).; ibd1 (tfloat64) – IBD1 estimate, \(\widehat{k^{(1)}_{ij}}\). Here col_key refers to the column key of the source matrix table,; and col_key.dtype is a struct containing the column key fields.; There is one row for each pair of distinct samples (columns), where i; corresponds to the column of smaller column index. In particular, if the; same column key value exists for \(n\) columns, then the resulting; table will have \(\binom{n-1}{2}\) rows with both key fields equal to; that column key value. This may result in unexpected behavior in downstream; processing. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; min_individual_maf (float) ",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
Modifiability,inherit,inherited,"﻿. Hail | ; Relatedness. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Relatedness. View page source. Relatedness; The relatedness of two individuals characterizes their biological; relationship. For example, two individuals might be siblings or; parent-and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles “inherited identically by descent”. Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same “recent,” common ancestor. The term “recent” distinguishes; alleles shared IBD from family members from alleles shared IBD from “distant”; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness.; Relatedness is usually quantified by two quantities: kinship coefficient; (\(\phi\) or PI_HAT) and probability-of-identity-by-descent-zero; (\(\pi_0\) or Z0). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent.; Hail provides three methods for the inference of relatedness: PLINK-style; identity by ",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
Performance,perform,perform,"scent estimates. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:; >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; PI_HAT in \([0.2, 0.9]\), using minor allele frequencies stored in; the row field panel_maf:; >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; The dataset must have a column field named s which is a StringExpression; and which uniquely identifies a column.; The implementation is based on the IBD algorithm described in the PLINK; paper.; identity_by_descent() requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first.; The resulting Table entries have the type: { i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }. The key list is: *i: String, j:; String*.; Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form; i j ibd.Z0 ibd.Z1 ibd.Z2 ibd.PI_HAT ibs0 ibs1 ibs2; sample1 sample2 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample3 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample4 0.6807 0.0000 0.3193 0.3193 ...; sample1 sample5 0.1966 0.0000 0.8034 0.8034 ... Parameters:. dataset (MatrixTable) – Variant-keyed and sample-keyed MatrixTable containing genotype information.; maf (Float64Expression, optional) – Row-indexed expression for the minor allele frequency.; bounded (bool) – Forces the estimations for Z0, Z1, Z2, and PI_HAT to take; on biologically meani",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
Usability,simpl,simply,"hip=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of a sample’s first k; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. an individual’s first k principal component coordinates fully; describe their allele-frequency-relevant ancestry, and; the relationship between ancestry (as described by principal; component coordinates) and population allele frequency is linear. The estimators for kinship, and identity-by-descent zero, one, and two; follow. Let:. \(S",MatchSource.WIKI,docs/0.2/methods/relatedness.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html
Availability,error,error,"taset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; sum_x (tfloat64) – Sum of input values x.; y_transpose_x (tfloat64) – Dot product of response; vector y with the input vector x.; beta (tfloat64) –; Fit effect coefficient of x, \(\hat\beta_1\) below.; standard_error (tfloat64) –; Estimated standard error, \(\widehat{\mathrm{se}}_1\).; t_stat (tfloat64) – \(t\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}_1\).; p_value (tfloat64) – \(p\)-value. If y is a list of expressions, then the last five fields instead have type; tarray of tfloat64, with corresponding indexing of; the list and each array.; If y is a list of lists of expressions, then n and sum_x are of type; array<float64>, and the last five fields are of type; array<array<float64>>. Index into these arrays with; a[index_in_outer_list, index_in_inner_list]. For example, if; y=[[a], [b, c]] then the p-value for b is p_value[1][0].; In the statistical genetics example above, the input variable x encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. \[\mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathr",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
Deployability,update,updated," each row of entries is regarded as a vector with elements; defined by entry_expr and missing values mean-imputed per row.; The (i, j) element of the resulting block matrix is the correlation; between rows i and j (as 0-indexed by order in the matrix table;; see add_row_index()).; The correlation of two vectors is defined as the; Pearson correlation coeffecient; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors.; This method has two stages:. writing the row-normalized block matrix to a temporary file on persistent; disk with BlockMatrix.from_entry_expr(). The parallelism is; n_rows / block_size.; reading and multiplying this block matrix by its transpose. The; parallelism is (n_rows / block_size)^2 if all blocks are computed. Warning; See all warnings on BlockMatrix.from_entry_expr(). In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; BlockMatrix.write_from_entry_expr().; The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; ld_matrix().; To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; BlockMatrix – Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
Energy Efficiency,reduce,reduces,"omplete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association.; The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the Jeffrey’s; invariant prior. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the logreg.fit fields reflect the null model; otherwise,; they reflect the full model.; See; Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert’s notes; Statistical Theory.; Firth introduced his approach in; Bias reduction of maximum likelihood estimates, 1993.; Hei",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
Integrability,depend,depends,"esent values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
Modifiability,variab,variable,"﻿. Hail | ; Statistics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, par",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
Performance,perform,perform,", \sigma^2)\]; Boolean covariates like \(\mathrm{is\_female}\) are encoded as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=d",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
Safety,predict,predicting,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
Testability,test,test,"﻿. Hail | ; Statistics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, par",MatchSource.WIKI,docs/0.2/methods/stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/methods/stats.html
Deployability,update,updated,"l be NaN.; Examples; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.maximum(a, b)); array([2, 5, 4], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.maximum(a, b)); array([nan, 5., nan]). Parameters:. nd1 (NDArrayExpression); nd2 (class:.NDArrayExpression, .ArrayExpression, numpy ndarray, or nested python lists/tuples.) – Nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns:; NDArrayExpression – Element-wise maximums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, the resulting; array will be of that shape. hail.nd.minimum(nd1, nd2)[source]; Compares elements at corresponding indexes in arrays; and returns an array of the minimum element found; at each compared index.; If an array element being compared has the value NaN,; the minimum for that index will be NaN.; Examples; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.minimum(a, b)); array([1, 3, 3], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.minimum(a, b)); array([nan, 3., nan]). Parameters:. nd1 (NDArrayExpression); nd2 (class:.NDArrayExpression, .ArrayExpression, numpy ndarray, or nested python lists/tuples.) – nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns:; min_array (NDArrayExpression) – Element-wise minimums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, resulting array; will be of that shape. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
Energy Efficiency,reduce,reduced,"hape(M, N). Returns:; NDArrayExpression – A 1 dimension NDArray of length min(M, N), containing the diagonal of nd. hail.nd.solve(a, b, no_crash=False)[source]; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the default output of this function, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}\]; The complete QR, has the following properties:. \[m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}\]. Parameters:. nd (NDArrayExpression) – A 2 dimensional ndarray, shape(M, N); mode (str) – One of “reduced”, “complete”, “r”, or “raw”. Defaults to “reduced”. Returns:. - q (ndarray of float64) – A matrix with orthonormal columns.; - r (ndarray of float64) – The upper-triangular matrix R.; - (h, tau) (nd",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
Integrability,interface,interface,"﻿. Hail | ; nd. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; nd. View page source. nd. NDArray Functions. Notes; This is a recently added, experimental module. We would love to hear what use cases you have for this as we expand this functionality.; As much as possible, we try to mimic the numpy array interface. array(input_array[, dtype]); Construct an NDArrayExpression. arange(start[, stop, step]); Returns a 1-dimensions ndarray of integers from start to stop by step. full(shape, value[, dtype]); Creates a hail NDArrayNumericExpression full of the specified value. zeros(shape[, dtype]); Creates a hail NDArrayNumericExpression full of zeros. ones(shape[, dtype]); Creates a hail NDArrayNumericExpression full of ones. diagonal(nd); Gets the diagonal of a 2 dimensional NDArray. solve(a, b[, no_crash]); Solve a linear system. solve_triangular(A, b[, lower, no_crash]); Solve a triangular linear system Ax = b for x. qr(nd[, mode]); Performs a QR decomposition. svd(nd[, full_matrices, compute_uv]); Performs a singular value decomposition. inv(nd); Performs a matrix inversion. concatenate(nds[, axis]); Join a sequence of arrays along an existing axis. hstack(arrs); Stack arrays in sequence horizontally (column wise). vstack(arrs); Stack arrays in sequence vertically (row wise). eye(N[, M, dtype]); Construct a 2-D NDArrayExpression with ones on the main diagonal and zeros elsewhere. identity(N[, dtype]); Constru",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
Modifiability,variab,variables,"NDArrayNumericExpression full of ones.; Examples; Create a 5 by 7 NDArray of type tfloat64 ones.; >>> hl.nd.ones((5, 7)). It is possible to specify a type other than tfloat64 with the dtype argument.; >>> hl.nd.ones((5, 7), dtype=hl.tfloat32). Parameters:. shape (tuple or TupleExpression) – Desired shape.; dtype (HailType) – Desired hail type. Default: float64. See also; full(). Returns:; NDArrayNumericExpression – ndarray of the specified size full of ones. hail.nd.diagonal(nd)[source]; Gets the diagonal of a 2 dimensional NDArray.; Examples; >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters:; nd (NDArrayNumericExpression) – A 2 dimensional NDArray, shape(M, N). Returns:; NDArrayExpression – A 1 dimension NDArray of length min(M, N), containing the diagonal of nd. hail.nd.solve(a, b, no_crash=False)[source]; Solve a linear system. Parameters:. a (NDArrayNumericExpression, (N, N)) – Coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the system Ax = B. Shape is same as shape of B. hail.nd.solve_triangular(A, b, lower=False, no_crash=False)[source]; Solve a triangular linear system Ax = b for x. Parameters:. A (NDArrayNumericExpression, (N, N)) – Triangular coefficient matrix.; b (NDArrayNumericExpression, (N,) or (N, K)) – Dependent variables.; lower (bool:) – If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns:; NDArrayNumericExpression, (N,) or (N, K) – Solution to the triangular system Ax = B. Shape is same as shape of B. hail.nd.qr(nd, mode='reduced')[source]; Performs a QR decomposition.; If K = min(M, N), then:. reduced: returns q and r with dimensions (M, K), (K, N); complete: returns q and r with dimensions (M, M), (M, N); r: returns only r with dimensions (K, N); raw: returns h, tau with dimensions (N, M), (K,). Notes; The reduced QR, the",MatchSource.WIKI,docs/0.2/nd/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/nd/index.html
Deployability,update,updated,"r -1, because the; discriminant, y, is missing. Switch Statements; Finally, Hail has the switch() function to build a conditional tree based; on the value of an expression. In the example below, csq is a; StringExpression representing the functional consequence of a; mutation. If csq does not match one of the cases specified by; when(), it is set to missing with; or_missing(). Other switch statements are documented in the; SwitchBuilder class.; >>> csq = hl.str('nonsense'). >>> (hl.switch(csq); ... .when(""synonymous"", False); ... .when(""intron"", False); ... .when(""nonsense"", True); ... .when(""indel"", True); ... .or_missing()); <BooleanExpression of type bool>. As with case statements, missingness will propagate up through a switch; statement. If we changed the value of csq to the missing value; hl.missing(hl.tstr), then the result of the switch statement above would also; be missing. Missingness; In Hail, all expressions can be missing. An expression representing a missing; value of a given type can be generated with the missing() function, which; takes the type as its single argument.; An example of generating a Float64Expression that is missing is:; >>> hl.missing('float64'); <Float64Expression of type float64>. These can be used with conditional statements to set values to missing if they; don’t satisfy a condition:; >>> hl.if_else(x > 2.0, x, hl.missing(hl.tfloat)); <Float64Expression of type float64>. The Python representation of a missing value is None. For example, if; we define cnull to be a missing value with type tcall, calling; the method is_het will return None and not False.; >>> cnull = hl.missing('call'); >>> hl.eval(cnull.is_het()); None. Functions; In addition to the methods exposed on each Expression, Hail also has; numerous functions that can be applied to expressions, which also return an; expression.; Take a look at the Functions page for full documentation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html
Integrability,wrap,wrapped,"utations into native code, and running them in parallel.; The result of the expression is computed only when it is needed. So z is; an expression representing the computation of x + y, but not the actual; value.; To peek at the value of this computation, there are two options:; eval(), which returns a Python value, and Expression.show(),; which prints a human-readable representation of an expression.; >>> hl.eval(z); 11; >>> z.show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 11 |; +--------+. Hail’s expressions are especially important for interacting with fields in; tables and matrix tables. Throughout Hail documentation and tutorials, you will; see code like this:; >>> ht2 = ht.annotate(C4 = ht.C3 + 3 * ht.C2 ** 2). This snippet of code is adding a field, C4, to a table, ht, and; returning the result as a new table, ht2. The code passed to the; Table.annotate() method is an expression that references the fields; C3 and C2 in ht.; Notice that 3 and 2 are not wrapped in constructor functions like; hl.int32(3). In the same way that Hail expressions can be combined together; via operations like addition and multiplication, they can also be combined with; Python objects.; For example, we can add a Python int to an Int32Expression.; >>> x + 3; <Int32Expression of type int32>. Addition is commutative, so we can also add an Int32Expression to an; int.; >>> 3 + x; <Int32Expression of type int32>. Note that Hail expressions cannot be used in other modules, like numpy; or scipy.; Hail has many subclasses of Expression – one for each Hail type. Each; subclass has its own constructor method. For example, if we have a list of Python; integers, we can convert this to a Hail ArrayNumericExpression with; array():; >>> a = hl.array([1, 2, -3, 0, 5]); >>> a; <ArrayNumericExpression of type array<int32>>. Expression objects keep track of their data type, which is; why we can see that a is of type array<int32> in the output above. An; expression’s type can also be ",MatchSource.WIKI,docs/0.2/overview/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html
Security,access,accessed,"rapped in constructor functions like; hl.int32(3). In the same way that Hail expressions can be combined together; via operations like addition and multiplication, they can also be combined with; Python objects.; For example, we can add a Python int to an Int32Expression.; >>> x + 3; <Int32Expression of type int32>. Addition is commutative, so we can also add an Int32Expression to an; int.; >>> 3 + x; <Int32Expression of type int32>. Note that Hail expressions cannot be used in other modules, like numpy; or scipy.; Hail has many subclasses of Expression – one for each Hail type. Each; subclass has its own constructor method. For example, if we have a list of Python; integers, we can convert this to a Hail ArrayNumericExpression with; array():; >>> a = hl.array([1, 2, -3, 0, 5]); >>> a; <ArrayNumericExpression of type array<int32>>. Expression objects keep track of their data type, which is; why we can see that a is of type array<int32> in the output above. An; expression’s type can also be accessed with Expression.dtype().; >>> a.dtype; dtype('array<int32>'). Hail arrays can be indexed and sliced like Python lists or numpy arrays:; >>> a[1]; <Int32Expression of type int32>. >>> a[1:-1]; <ArrayNumericExpression of type array<int32>>. In addition to constructor methods like array() and bool(),; Hail expressions can also be constructed with the literal() method,; which will impute the type of of the expression.; >>> hl.literal([0,1,2]); <ArrayNumericExpression of type array<int32>>. Boolean Logic; Unlike Python, a Hail BooleanExpression cannot be used with the Python; keywords and, or, and not. The Hail substitutes are &, |,; and ~.; >>> s1 = hl.int32(3) == 4; >>> s2 = hl.int32(3) != 4. >>> s1 & s2; <BooleanExpression of type bool>. >>> s1 | s2; <BooleanExpression of type bool>. >>> ~s1; <BooleanExpression of type bool>. Remember that you can use eval(): to evaluate the expression.; >>> hl.eval(~s1); True. Caution; The operator precedence of & and | is different from ",MatchSource.WIKI,docs/0.2/overview/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html
Deployability,update,updated,"﻿. Hail | ; Hail Overview. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; Expressions; Tables; MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview. View page source. Hail Overview; Hail is a library for analyzing structured tabular and matrix data. Hail; contains a collection of primitives for operating on data in parallel, as well; as a suite of functionality for processing genetic data.; This section of Hail’s documentation contains detailed explanations of Hail’s; architecture, primitives, classes, and libraries. Expressions; What is an Expression?; Boolean Logic; Conditional Expressions; Missingness; Functions. Tables; Import; Global Fields; Keys; Referencing Fields; Updating Fields; Aggregation; Joins; Interacting with Tables Locally. MatrixTables; Keys; Referencing Fields; Import; Common Operations; Aggregation; Group-By; Joins; Interacting with Matrix Tables Locally. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/index.html
Deployability,update,update,"------------------------; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>; ----------------------------------------. Common Operations; Like tables, Hail provides a number of methods for manipulating data in a; matrix table.; Filter; MatrixTable has three methods to filter based on expressions:. MatrixTable.filter_rows(); MatrixTable.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schem",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
Energy Efficiency,efficient,efficiently,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
Integrability,interface,interfaces,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
Performance,perform,performing,"eld case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_n",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
Security,expose,exposes,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
Usability,simpl,simplest,"le.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty());",MatchSource.WIKI,docs/0.2/overview/matrix_table-1.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html
Deployability,update,update,"------------------------; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>; ----------------------------------------. Common Operations; Like tables, Hail provides a number of methods for manipulating data in a; matrix table.; Filter; MatrixTable has three methods to filter based on expressions:. MatrixTable.filter_rows(); MatrixTable.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schem",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
Energy Efficiency,efficient,efficiently,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
Integrability,interface,interfaces,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
Performance,perform,performing,"eld case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_n",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
Security,expose,exposes,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
Usability,simpl,simplest,"le.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty());",MatchSource.WIKI,docs/0.2/overview/matrix_table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table.html
Availability,down,downstream,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
Deployability,update,update,"cussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; To compute an aggregate statistic over the rows of; a dataset, Hail provides an Table.aggregate() method which can be passed; a wide variety of aggregator functions (see Aggregators):; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'F')); 0.5. We also might want to compute the mean value of HT for each sex. This is; possible with a combination of Table.group_by() and; GroupedTable.aggregate():; >>> ht_agg = (ht.group_by(ht.SEX); ... .aggregate(mean = hl.agg.mean(ht.HT))); >>> ht_agg.show(); +-----+----------+; | SEX | mean |; +-----+----------+; | str | float64 |; +-----+----------+; | ""F"" | 6.50e+01 |; | ""M"" | 6.85e+01 |; +-----+----------+. Note that the result of ht.group_by(...).aggregate(...) is a new; Table while the result of ht.aggregate(...) is a Python value. Joi",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
Modifiability,variab,variable,"oins; Interacting with Tables Locally. MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview; Table Overview. View page source. Table Overview; A Table is the Hail equivalent of a SQL table, a Pandas Dataframe, an; R Dataframe, a dyplr Tibble, or a Spark Dataframe. It consists of rows of data; conforming to a given schema where each column (row field) in the dataset is of; a specific type. Import; Hail has functions to create tables from a variety of data sources.; The most common use case is to load data from a TSV or CSV file, which can be; done with the import_table() function.; >>> ht = hl.import_table(""data/kt_example1.tsv"", impute=True). Examples of genetics-specific import methods are; import_locus_intervals(), import_fam(), and import_bed().; Many Hail methods also return tables.; An example of a table is below. We recommend ht as a variable name for; tables, referring to a “Hail table”.; >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Global Fields; In addition to row fields, Hail tables also have global fields. You can think of; globals as extra fields in the table whose values are identical for every row.; For example, the same table above with the global field G = 5 can be thought; of as; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | G |; +-------+-------+-----+-------+----",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
Performance,load,load,"ble Overview. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; Expressions; Tables; Import; Global Fields; Keys; Referencing Fields; Updating Fields; Aggregation; Joins; Interacting with Tables Locally. MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview; Table Overview. View page source. Table Overview; A Table is the Hail equivalent of a SQL table, a Pandas Dataframe, an; R Dataframe, a dyplr Tibble, or a Spark Dataframe. It consists of rows of data; conforming to a given schema where each column (row field) in the dataset is of; a specific type. Import; Hail has functions to create tables from a variety of data sources.; The most common use case is to load data from a TSV or CSV file, which can be; done with the import_table() function.; >>> ht = hl.import_table(""data/kt_example1.tsv"", impute=True). Examples of genetics-specific import methods are; import_locus_intervals(), import_fam(), and import_bed().; Many Hail methods also return tables.; An example of a table is below. We recommend ht as a variable name for; tables, referring to a “Hail table”.; >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Global Fields; In addition to row fields, Hail tables also have global fie",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
Security,access,accessed,"| 5 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 5 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. but the value 5 is only stored once for the entire dataset and NOT once per; row of the table. The output of Table.describe() lists what all of the row; fields and global fields are.; >>> ht.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; Row fields can be specified to be the key of the table with the method; Table.key_by(). Keys are important for joining tables together (discussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
Testability,test,testing,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/overview/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/overview/table.html
Deployability,update,updated,"﻿. Hail | ; LinearMixedModel. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; stats; LinearMixedModel. View page source. LinearMixedModel. class hail.stats.LinearMixedModel[source]; Class representing a linear mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. Attributes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/stats/hail.stats.LinearMixedModel.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/stats/hail.stats.LinearMixedModel.html
Deployability,update,updated,"﻿. Hail | ; stats. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; stats. View page source. stats. Classes. LinearMixedModel; Class representing a linear mixed model. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/stats/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/stats/index.html
Availability,avail,available," Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.s",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
Deployability,integrat,integrate,"t(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
Energy Efficiency,consumption,consumption,"tistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_rows method can help!. Let’s do a GWAS!; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); not so far from Hardy-Weinberg equilibrium as to suggest sequencing error. [35]:. mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). [36]:. mt = mt.filter_rows(mt.variant_qc.p_value_hwe > 1e-6). [37]:. print('Samples: %d Variants: %d' % (mt.count_cols(), mt.count_rows())). [Stage 37:> (0 + 1) / 1]. Samples: 250 Variants: 7774. These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check wheth",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
Integrability,integrat,integrate,"t(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
Modifiability,variab,variable,"et, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.; The rows method can be used to get a table with all the row fields in our MatrixTable.; We can use rows along with select to pull out 5 variants. The select method takes either a string refering to a field name in the table, or a Hail Expression. Here, we leave the arguments blank to keep only the row key fields, locus and alleles.; Use the show method to display the variants. [6]:. mt.rows().select().show(5). locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; showing top 5 rows. Alternatively:. [7]:. mt.row_key.show(5). locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; showing to",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
Performance,load,load,"ing data from VCF; Getting to know our data; Adding column fields; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Epilogue. Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; ",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
Safety,detect,detect," 1) / 1]. Often, these metrics are correlated. [30]:. p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate'); show(p). [Stage 30:> (0 + 1) / 1]. Removing outliers from the dataset will generally improve association results. We can make arbitrary cutoffs and use them to filter:. [31]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); print('After filter, %d/284 samples remain.' % mt.count_cols()). [Stage 32:> (0 + 1) / 1]. After filter, 250/284 samples remain. Next is genotype QC. It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.; In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!. [32]:. ab = mt.AD[1] / hl.sum(mt.AD). filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))). fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab)); print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.'); mt = mt.filter_entries(filter_condition_ab). [Stage 34:> (0 + 1) / 1]. Filtering 3.60% entries out of downstream analysis. [ ]:. Variant QC is a bit more of the same: we can use the variant_qc function to produce a variety of useful statistics, plot them, and filter. [33]:. mt = hl.variant_qc(mt). [34]:. mt.row.describe(). --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; rsid: str,; qual: float",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
Testability,test,test,"; ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Download public 1000 Genomes data; Importing data from VCF; Getting to know our data; Adding column fields; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Epilogue. Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
Usability,simpl,simply,"cs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan(gwas.p_value); show(p). We have found a caffeine consumption locus! Now simply apply Hail’s Nature paper function to publish the result.; Just kidding, that function won’t land until Hail 1.0!. Rare variant analysis; Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT). [49]:. entries = mt.entries(); results = (entries.group_by(pop = entries.pheno.SuperPopulation, chromosome = entries.locus.contig); .aggregate(n_het = hl.agg.count_where(entries.GT.is_het()))). [50]:. results.show(). [Stage 184:> (0 + 1) / 1]. popchromosomen_hetstrstrint64; ""AFR""""1""11039; ""AFR""""10""7123; ""AFR""""11""6777; ""AFR""""12""7016; ""AFR""""13""4650; ""AFR""""14""4262; ""AFR""""15""3847; ""AFR""""16""4564; ""AFR""""17""3607; ""AFR""""18""4133; showing top 10 rows. We use the MatrixTable.entries method to convert our matrix table to a table (with one row for each sample for each variant). In this representation, it is easy to aggregate over any fields we like, which is often ",MatchSource.WIKI,docs/0.2/tutorials/01-genome-wide-association-study.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html
Availability,down,download,"l; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Tutorial. View page source. Table Tutorial; Table is Hail’s distributed analogue of a data frame or SQL table. It will be familiar if you’ve used R or pandas, but Table differs in 3 important ways:. It is distributed. Hail tables can store far more data than can fit on a single computer.; It carries global fields.; It is keyed. A Table has two different kinds of fields:. global fields; row fields. Importing and Reading; Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
Deployability,update,updated,"y: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. You can count the rows of a table. [6]:. users.count(). [6]:. 943. You can access fields of tables with the Python attribute notation table.field, or with index notation table['field']. The latter is useful when the field names are not valid Python identifiers (if a field name includes a space, for example). [7]:. users.occupation.describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. [8]:. users['occupation'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. users.occupation and users['occupation'] are Hail Expressions; Lets peak at their using show. Notice that the key is shown as well!. [9]:. users.occupation.show(). idoccupationint32str; 1""technician""; 2""other""; 3""writer""; 4""technician""; 5""other""; 6""executive""; 7""administrator""; 8""administrator""; 9""student""; 10""lawyer""; showing top 10 rows. Exercise; The movie dataset has two other tables: movies.ht and ratings.ht. Load these tables and have a quick look around. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
Performance,load,load," familiar if you’ve used R or pandas, but Table differs in 3 important ways:. It is distributed. Hail tables can store far more data than can fit on a single computer.; It carries global fields.; It is keyed. A Table has two different kinds of fields:. global fields; row fields. Importing and Reading; Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; ",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
Security,access,access," Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. You can count the rows of a table. [6]:. users.count(). [6]:. 943. You can access fields of tables with the Python attribute notation table.field, or with index notation table['field']. The latter is useful when the field names are not valid Python identifiers (if a field name includes a space, for example). [7]:. users.occupation.describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. [8]:. users['occupation'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. users.occupation and users['occupation'] are Hail Expressions; Lets peak at their using show. Notice that the key is shown as well!. [9]:. users.occupation.show(). idoccupationint32str; 1""technician""; 2""oth",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
Testability,log,logger,"uted. Hail tables can store far more data than can fit on a single computer.; It carries global fields.; It is keyed. A Table has two different kinds of fields:. global fields; row fields. Importing and Reading; Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex",MatchSource.WIKI,docs/0.2/tutorials/03-tables.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html
Availability,avail,available,"ow to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum. [4]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
Deployability,update,updated,"ntheses appropriately. A single ‘&’ denotes logical AND and a single ‘|’ denotes logical OR. [9]:. users.aggregate(hl.agg.filter((users.occupation == 'writer') | (users.occupation == 'executive'), hl.agg.count())). [9]:. 77. [10]:. users.aggregate(hl.agg.filter((users.sex == 'F') | (users.occupation == 'executive'), hl.agg.count())). [10]:. 302. hist; As we saw in the first tutorial, hist can be used to build a histogram over numeric data. [11]:. hist = users.aggregate(hl.agg.hist(users.age, 10, 70, 60)); hist. [11]:. Struct(bin_edges=[10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0], bin_freq=[1, 1, 0, 5, 3, 6, 5, 14, 18, 23, 32, 27, 37, 28, 33, 38, 34, 35, 36, 32, 39, 25, 28, 26, 17, 27, 21, 19, 17, 22, 21, 10, 21, 13, 23, 15, 12, 14, 20, 19, 20, 20, 6, 12, 4, 11, 6, 9, 3, 3, 9, 3, 2, 3, 2, 3, 1, 0, 2, 5], n_smaller=1, n_larger=1). [12]:. p = hl.plot.histogram(hist, legend='Age'); show(p). take and collect; There are a few aggregators for collecting values. take localizes a few values into an array. It has an optional ordering.; collect localizes all values into an array.; collect_as_set localizes all unique values into a set. [13]:. users.aggregate(hl.agg.take(users.occupation, 5)). [13]:. ['technician', 'other', 'writer', 'technician', 'other']. [14]:. users.aggregate(hl.agg.take(users.age, 5, ordering=-users.age)). [14]:. [73, 70, 70, 70, 69]. Warning! Aggregators like collect and counter return Python objects and can fail with out of memory errors if you apply them to collections that are too large (e.g. all 50 trillion genotypes in the UK Biobank dataset). [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
Performance,load,load,"erage age? Youngest age? Oldest age?; What are all the occupations that appear, and how many times does each appear?. We can answer these questions with aggregation. Aggregation combines many values together to create a summary.; To start, we’ll aggregate all the values in a table. (Later, we’ll learn how to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum.",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
Testability,log,logger,"y times does each appear?. We can answer these questions with aggregation. Aggregation combines many values together to create a summary.; To start, we’ll aggregate all the values in a table. (Later, we’ll learn how to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum. [4]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
Usability,learn,learn,"able Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Aggregation Tutorial. View page source. Aggregation Tutorial; In the last section, we inspected the structure of the data and displayed a few example values.; How do we get a deeper feel for the data? One of the most natural things to do is to create a summary of a large number of values. For example, you could ask:. How many women are in the dataset? How many men?; What is the average age? Youngest age? Oldest age?; What are all the occupations that appear, and how many times does each appear?. We can answer these questions with aggregation. Aggregation combines many values together to create a summary.; To start, we’ll aggregate all the values in a table. (Later, we’ll learn how to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ ",MatchSource.WIKI,docs/0.2/tutorials/04-aggregation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html
Availability,avail,available,"ixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:. 66. [4]:. users.aggregate(hl.agg.counter(users.occupation == 'programmer'))[True]. [4]:. 66. Annotate; You can add new fields to a table with annotate. As an example, let’s create a new column called cleaned_occupation that replaces missing entries in the occupation field labeled as ‘other’ with ‘non",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
Deployability,update,updated,"ssing_occupations = hl.set(['other', 'none']). t = users.select(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idcleaned_occupationint32str; 1""technician""; 2NA; 3""writer""; 4""technician""; 5NA; 6""executive""; 7""administrator""; 8""administrator""; 9""student""; 10""lawyer""; showing top 10 rows. [11]:. missing_occupations = hl.set(['other', 'none']). t = users.transmute(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idagesexzipcodecleaned_occupationint32int32strstrstr; 124""M""""85711""""technician""; 253""F""""94043""NA; 323""M""""32067""""writer""; 424""M""""43537""""technician""; 533""F""""15213""NA; 642""M""""98101""""executive""; 757""M""""91344""""administrator""; 836""M""""05201""""administrator""; 929""M""""01002""""student""; 1053""M""""90703""""lawyer""; showing top 10 rows. Global Fields; Finally, you can add global fields with annotate_globals. Globals are useful for storing metadata about a dataset or storing small data structures like sets and maps. [12]:. t = users.annotate_globals(cohort = 5, cloudable = hl.set(['sample1', 'sample10', 'sample15'])); t.describe(). ----------------------------------------; Global fields:; 'cohort': int32; 'cloudable': set<str>; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. [13]:. t.cloudable. [13]:. <SetExpression of type set<str>>. [14]:. hl.eval(t.cloudable). [14]:. {'sample1', 'sample10', 'sample15'}. Exercises. Z-score normalize the age field of users.; Convert zip to an integer. Hint: Not all zipcodes are US zipcodes! Use hl.int32 to convert a string to an integer. Use StringExpression.matches to see if a string matches a regular expression. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
Performance,load,load,"Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Filter; Annotate; Select and Transmute; Global Fields; Exercises. Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
Testability,log,logger," Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Filter; Annotate; Select and Transmute; Global Fields; Exercises. Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:. 66. [4]:. users.aggregate(hl.agg.counter(users.occupation == 'programmer'))[True]. [4]:. 6",MatchSource.WIKI,docs/0.2/tutorials/05-filter-annotate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html
Availability,avail,available,"illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the key.; A table has an ordered list of fields known as the key. Our users table has one key, the id field. We can see all the fields, as well as the keys, of a table by calling describe(). [2]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. key is a struct expression of all of the key fields, so we can refer to the key of a table without explicitly specifying",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
Deployability,update,updated,"ngineer""""Charade (1963)""5.00e+00; ""entertainment""""American in Paris, An (1951)""5.00e+00; ""executive""""A Chef in Love (1996)""5.00e+00; ""healthcare""""39 Steps, The (1935)""5.00e+00; ""homemaker""""Beautiful Girls (1996)""5.00e+00; ""lawyer""""Anastasia (1997)""5.00e+00; showing top 10 rows. Let’s try to get a deeper understanding of this result. Notice that every movie displayed has an average rating of 5, which means that every person gave these movies the highest rating. Is that unlikely? We can determine how many people rated each of these movies by working backwards and filtering our original movie_data table by fields in highest_rated.; Note that in the second line below, we are taking advantage of the fact that Hail tables are keyed. [19]:. highest_rated = highest_rated.key_by(; highest_rated.occupation, highest_rated.movie). counts_temp = movie_data.filter(; hl.is_defined(highest_rated[movie_data.occupation, movie_data.movie])). counts = counts_temp.group_by(counts_temp.occupation, counts_temp.movie).aggregate(; counts = hl.agg.count()). counts.show(). [Stage 108:> (0 + 1) / 1]. occupationmoviecountsstrstrint64; ""administrator""""A Chef in Love (1996)""1; ""artist""""39 Steps, The (1935)""1; ""doctor""""Alien (1979)""1; ""educator""""Aparajito (1956)""2; ""engineer""""Charade (1963)""1; ""entertainment""""American in Paris, An (1951)""1; ""executive""""A Chef in Love (1996)""1; ""healthcare""""39 Steps, The (1935)""1; ""homemaker""""Beautiful Girls (1996)""1; ""lawyer""""Anastasia (1997)""1; showing top 10 rows. So it looks like the highest rated movies, when computed naively, mostly have a single viewer rating them. To get a better understanding of the data, we can recompute this list but only include movies which have more than 1 viewer (left as an exercise). Exercises. What is the favorite movie for each occupation, conditional on there being more than one viewer?; What genres are rated most differently by men and women?. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
Performance,load,load,"tudy (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; The Key to Understanding Joins; Joining Tables; Exercises. MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Joins Tutorial. View page source. Table Joins Tutorial; This tutorial walks through some ways to join Hail tables. We’ll use a simple movie dataset to illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
Testability,log,logger,"ge Log And Version Policy. menu; Hail. Hail Tutorials; Table Joins Tutorial. View page source. Table Joins Tutorial; This tutorial walks through some ways to join Hail tables. We’ll use a simple movie dataset to illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the key.; A table has an ordered list of fields known as the key. Our users table has one key, the id field. We can see all the fields, as well as the keys, of a table by calling describe(). [2]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; -----",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
Usability,simpl,simple,"﻿. Hail | ; Table Joins Tutorial. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; The Key to Understanding Joins; Joining Tables; Exercises. MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Joins Tutorial. View page source. Table Joins Tutorial; This tutorial walks through some ways to join Hail tables. We’ll use a simple movie dataset to illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-",MatchSource.WIKI,docs/0.2/tutorials/06-joins.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html
Availability,avail,available,".; Column fields are stored once per column. These can contain information about the columns, or summary data calculated per column.; Entry fields are the piece that makes this structure a matrix – there is an entry for each (row, column) pair. Importing and Reading; Like tables, matrix tables can be imported from a variety of formats: VCF, (B)GEN, PLINK, TSV, etc. Matrix tables can also be read from a “native” matrix table format. Let’s read a sample of prepared 1KG data. [1]:. import hail as hl; from bokeh.io import output_notebook, show; output_notebook(). hl.utils.get_1kg('data/'). Loading BokehJS ... Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2011-0.2.133-4c60fddb171a.log; 2024-10-04 20:11:52.232 Hail: INFO: 1KG files found. [2]:. mt = hl.read_matrix_table('data/1kg.mt'); mt.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; ----------------------------------------; Entry f",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
Deployability,update,updated,"; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; 'call_rate': float64; ----------------------------------------; Entry fields:; 'GT': call; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'PL': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. [12]:. p = hl.plot.histogram(mt2.call_rate, range=(0,1.0), bins=100,; title='Variant Call Rate Histogram', legend='Call Rate'); show(p). Exercise: GQ vs DP; In this exercise, you’ll use Hail to investigate a strange property of sequencing datasets.; The DP field is the sequencing depth (the number of reads).; Let’s first plot a histogram of DP:. [13]:. p = hl.plot.histogram(mt.DP, range=(0,40), bins=40, title='DP Histogram', legend='DP'); show(p). [Stage 9:> (0 + 1) / 1]. Now, let’s do the same thing for GQ.; The GQ field is the phred-scaled “genotype quality”. The formula to convert to a linear-scale confidence (0 to 1) is 10 ** -(mt.GQ / 10). GQ is truncated to lie between 0 and 99. [14]:. p = hl.plot.histogram(mt.GQ, range=(0,100), bins=100, title='GQ Histogram', legend='GQ'); show(p). [Stage 10:> (0 + 1) / 1]. Whoa! That’s a strange distribution! There’s a big spike at 100. The rest of the values have roughly the same shape as the DP distribution, but form a Dimetrodon. Use Hail to figure out what’s going on!. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
Performance,scalab,scalability,"﻿. Hail | ; MatrixTable Tutorial. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; MatrixTable Anatomy; Importing and Reading; MatrixTable operations; Exercise: GQ vs DP. Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; MatrixTable Tutorial. View page source. MatrixTable Tutorial; If you’ve gotten this far, you’re probably thinking:. “Can’t I do all of this in pandas or R?”; “What does this have to do with biology?”. The two crucial features that Hail adds are scalability and the domain-specific primitives needed to work easily with biological data. Fear not! You’ve learned most of the basic concepts of Hail and now are ready for the bit that makes it possible to represent and compute on genetic matrices: the MatrixTable.; In the last example of the Table Joins Tutorial, the ratings table had a compound key: movie_id and user_id. The ratings were secretly a movie-by-user matrix!; However, since this matrix is very sparse, it is reasonably represented in a so-called “coordinate form” Table, where each row of the table is an entry of the sparse matrix. For large and dense matrices (like sequencing data), the per-row overhead of coordinate reresentations is untenable. That’s why we built MatrixTable, a 2-dimensional generalization of Table. MatrixTable Anatomy; Recall that Table has two kinds of fields:. global fields; row fields. MatrixTable has four kinds of fields:. global fields; row fields; column fields; entry fields. Row fields are fields that are stored once per row. Th",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
Security,access,accessed,"--------------------------------. [4]:. mt.GT.describe(). --------------------------------------------------------; Type:; call; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7efd1f44dc10>; Index:; ['row', 'column']; --------------------------------------------------------. MatrixTable operations; We belabored the operations on tables because they all have natural analogs (sometimes several) on matrix tables. For example:. count => count_{rows, cols} (and count which returns both); filter => filter_{rows, cols, entries}; annotate => annotate_{rows, cols, entries} (and globals for both); select => select_{rows, cols, entries} (and globals for both); transmute => transmute_{rows, cols, entries} (and globals for both); group_by => group_{rows, cols}_by; explode => expode_{rows, cols}; aggregate => aggregate_{rows, cols, entries}. Some operations are unique to MatrixTable:. The row fields can be accessed as a Table with rows; The column fields can be accessed as a Table with cols.; The entire field space of a MatrixTable can be accessed as a coordinate-form Table with entries. Be careful with this! While it’s fast to aggregate or query, trying to write this Table to disk could produce files thousands of times larger than the corresponding MatrixTable. Let’s explore mt using these tools. Let’s get the size of the dataset. [5]:. mt.count() # (rows, cols). [5]:. (10879, 284). Let’s look at the first few row keys (variants) and column keys (sample IDs). [6]:. mt.rows().select().show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; 1:2252970[""C"",""T""]; 1:2284195[""T"",""C""]; 1:2779043[""T"",""C""]; 1:2944527[""G"",""A""]; 1:3761547[",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
Testability,log,logger,"s four kinds of fields:. global fields; row fields; column fields; entry fields. Row fields are fields that are stored once per row. These can contain information about the rows, or summary data calculated per row.; Column fields are stored once per column. These can contain information about the columns, or summary data calculated per column.; Entry fields are the piece that makes this structure a matrix – there is an entry for each (row, column) pair. Importing and Reading; Like tables, matrix tables can be imported from a variety of formats: VCF, (B)GEN, PLINK, TSV, etc. Matrix tables can also be read from a “native” matrix table format. Let’s read a sample of prepared 1KG data. [1]:. import hail as hl; from bokeh.io import output_notebook, show; output_notebook(). hl.utils.get_1kg('data/'). Loading BokehJS ... Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2011-0.2.133-4c60fddb171a.log; 2024-10-04 20:11:52.232 Hail: INFO: 1KG files found. [2]:. mt = hl.read_matrix_table('data/1kg.mt'); mt.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingC",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
Usability,learn,learned,"2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; MatrixTable Anatomy; Importing and Reading; MatrixTable operations; Exercise: GQ vs DP. Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; MatrixTable Tutorial. View page source. MatrixTable Tutorial; If you’ve gotten this far, you’re probably thinking:. “Can’t I do all of this in pandas or R?”; “What does this have to do with biology?”. The two crucial features that Hail adds are scalability and the domain-specific primitives needed to work easily with biological data. Fear not! You’ve learned most of the basic concepts of Hail and now are ready for the bit that makes it possible to represent and compute on genetic matrices: the MatrixTable.; In the last example of the Table Joins Tutorial, the ratings table had a compound key: movie_id and user_id. The ratings were secretly a movie-by-user matrix!; However, since this matrix is very sparse, it is reasonably represented in a so-called “coordinate form” Table, where each row of the table is an entry of the sparse matrix. For large and dense matrices (like sequencing data), the per-row overhead of coordinate reresentations is untenable. That’s why we built MatrixTable, a 2-dimensional generalization of Table. MatrixTable Anatomy; Recall that Table has two kinds of fields:. global fields; row fields. MatrixTable has four kinds of fields:. global fields; row fields; column fields; entry fields. Row fields are fields that are stored once per row. These can contain information about the rows, or summary data calculated per row.; Column fields are stored once per column. These can contain in",MatchSource.WIKI,docs/0.2/tutorials/07-matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html
Availability,avail,available,"; Scatter; 2-D histogram; Q-Q (Quantile-Quantile); Manhattan. GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; 'Population': str; 'SuperPopulation': str; 'isFemale': bool; 'PurpleHair': bool; 'CaffeineConsumption': int32; 'sample_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
Deployability,update,updated," (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None); show(p). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
Modifiability,variab,variables,"ariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None)",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
Performance,load,load,"ence. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; Histogram; Cumulative Histogram; Scatter; 2-D histogram; Q-Q (Quantile-Quantile); Manhattan. GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
Testability,log,logger,"enome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; Histogram; Cumulative Histogram; Scatter; 2-D histogram; Q-Q (Quantile-Quantile); Manhattan. GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; 'Population': st",MatchSource.WIKI,docs/0.2/tutorials/08-plotting.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html
Availability,avail,available,"ighly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = gg",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
Deployability,continuous,continuous,"’s filter the data to 2007 for our first experiments. [10]:. gp_2007 = gp.filter(gp.year == 2007). If we want to see how many countries from each continent we have, we can use geom_bar, which just takes in an x aesthetic and then implicitly counts how many values of each x there are. [11]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(). [11]:. To make it a little prettier, let’s color per continent as well. We use fill to specify color of shapes (as opposed to color for points and lines. color on a bar chart sets the color of the bar outline.). [12]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent)). [12]:. Maybe we instead want to see not the number of countries per continent, but the number of people living on each continent. We can do this with geom_bar as well by specifying a weight. [13]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent, weight=gp_2007.pop)). [13]:. Histograms are similar to bar plots, except they break a continuous x axis into bins. Let’s import the iris dataset for this. [14]:. iris = hl.Table.from_pandas(plotly.data.iris()); iris.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'sepal_length': float64; 'sepal_width': float64; 'petal_length': float64; 'petal_width': float64; 'species': str; 'species_id': int32; ----------------------------------------; Key: []; ----------------------------------------. Let’s make a histogram:. [15]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(). [15]:. By default histogram plots groups stacked on top of each other, which is not always easy to interpret. We can specify the position argument to histogram to get different behavior. ""dodge"" puts the bars next to each other:. [16]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""dodge""). [16]:. And ""identity"" plots them over each other. It he",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
Integrability,interface,interface,"nitializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
Modifiability,flexible,flexible,". 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial; The Grammar of Graphics; Geoms that group; Labels and Axes. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GGPlot Tutorial. View page source. GGPlot Tutorial. [1]:. import hail as hl; from hail.ggplot import *. import plotly. Loading BokehJS ... The Hail team has implemented a plotting module for hail based on the very popular ggplot2 package from R’s tidyverse. That library is very fully featured and we will never be quite as flexible as it, but with just a subset of its functionality we can make highly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI ava",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
Performance,load,load,"t import *. import plotly. Loading BokehJS ... The Hail team has implemented a plotting module for hail based on the very popular ggplot2 package from R’s tidyverse. That library is very fully featured and we will never be quite as flexible as it, but with just a subset of its functionality we can make highly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
Safety,interlock,interlocking,"n x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For that reason, it’s possible to define aesthetics that only apply to certain geoms. [8]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared)) +; geom_line() +; geom_point(aes(color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))); ); fig.show(). All geoms can take in their own aesthetic mapping, which lets them specify aesthetics specific to them. And geom_point still inherits the x and y aesthetics from the mapping defined in ggplot(). Geoms that group; Some geoms implicitly do an aggregation based on the x aesthetic, and so don’t take a y value. Consider this dataset from gapminder with information about countries around the world, with one datapoint taken per country in the years 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, and 2007. [9]:. gp = hl.Table.from_pandas(plotly.data.gapminder()); gp.describe(). ----------------------------------------; Global fields:; None; -------------------------------------",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
Security,interlock,interlocking,"n x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For that reason, it’s possible to define aesthetics that only apply to certain geoms. [8]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared)) +; geom_line() +; geom_point(aes(color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))); ); fig.show(). All geoms can take in their own aesthetic mapping, which lets them specify aesthetics specific to them. And geom_point still inherits the x and y aesthetics from the mapping defined in ggplot(). Geoms that group; Some geoms implicitly do an aggregation based on the x aesthetic, and so don’t take a y value. Consider this dataset from gapminder with information about countries around the world, with one datapoint taken per country in the years 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, and 2007. [9]:. gp = hl.Table.from_pandas(plotly.data.gapminder()); gp.describe(). ----------------------------------------; Global fields:; None; -------------------------------------",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
Testability,log,logger,"le for hail based on the very popular ggplot2 package from R’s tidyverse. That library is very fully featured and we will never be quite as flexible as it, but with just a subset of its functionality we can make highly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or poi",MatchSource.WIKI,docs/0.2/tutorials/09-ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html
Availability,error,error,"er than 50 MB). Examples; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/df.csv', 'w') as f: ; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt') as f: ; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). Caution; These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use hadoop_copy(); to move your file to a distributed file system. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hail.utils.hadoop_copy(src, dest)[source]; Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
Deployability,update,updated,". Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.; See the; MovieLens website; for more information about this dataset. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite existing files/directories at those locations. hail.utils.ANY_REGION; Built-in mutable sequence.; If no argument is given, the constructor creates a new empty list.; The argument must be an iterable if specified. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
Performance,optimiz,optimized,"-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, ov",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
Security,access,accessing,"ions. get_movie_lens(output_dir[, overwrite]); Download public Movie Lens dataset. class hail.utils.Interval(start, end, includes_start=True, includes_end=False, point_type=None)[source]; An object representing a range of values between start and end.; >>> interval2 = hl.Interval(3, 6). Parameters:. start (any type) – Object with type point_type.; end (any type) – Object with type point_type.; includes_start (bool) – Interval includes start.; includes_end (bool) – Interval includes end. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.interval.take(5). This is rare; it is much; more common to manipulate the IntervalExpression object, which is; constructed using the following functions:. interval(); locus_interval(); parse_locus_interval(). class hail.utils.Struct(**kwargs)[source]; Nested annotation structure.; >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both ‘items’ and ‘attributes’, which; allows either syntax for accessing the element “foo” of struct “bar”:; >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:; >>> bar['1kg']. The pprint module can be used to print nested Structs in a more; human-readable fashion:; >>> from pprint import pprint; >>> pprint(bar). Parameters:; attributes – Field names and values. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.info.take(5). This is rare; it is much; more common to manipulate the StructExpression object, which is; constructed using the struct() function. class hail.utils.frozendict(d)[source]; An object representing an immutable dictionary.; >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a frozendict:; >>> dict(frozendict({'a': 1, 'b': 2})). Note; This object refers to the Pyth",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
Testability,log,log,"nd Version Policy. menu; Hail. Python API; Hail Query Python API; utils. View page source. utils. ANY_REGION; Built-in mutable sequence. Interval(start, end[, includes_start, ...]); An object representing a range of values between start and end. Struct(**kwargs); Nested annotation structure. frozendict(d); An object representing an immutable dictionary. hadoop_open(path[, mode, buffer_size]); Open a file through the Hadoop filesystem API. hadoop_copy(src, dest); Copy a file through the Hadoop filesystem API. hadoop_exists(path); Returns True if path exists. hadoop_is_file(path); Returns True if path both exists and is a file. hadoop_is_dir(path); Returns True if path both exists and is a directory. hadoop_stat(path); Returns information about the file or directory at a given path. hadoop_ls(path); Returns information about files at path. hadoop_scheme_supported(scheme); Returns True if the Hadoop filesystem supports URLs with the given scheme. copy_log(path); Attempt to copy the session log to a hadoop-API-compatible location. range_table(n[, n_partitions]); Construct a table with the row index and no other fields. range_matrix_table(n_rows, n_cols[, ...]); Construct a matrix table with row and column indices and no entry fields. get_1kg(output_dir[, overwrite]); Download subset of the 1000 Genomes dataset and sample annotations. get_hgdp(output_dir[, overwrite]); Download subset of the Human Genome Diversity Panel dataset and sample annotations. get_movie_lens(output_dir[, overwrite]); Download public Movie Lens dataset. class hail.utils.Interval(start, end, includes_start=True, includes_end=False, point_type=None)[source]; An object representing a range of values between start and end.; >>> interval2 = hl.Interval(3, 6). Parameters:. start (any type) – Object with type point_type.; end (any type) – Object with type point_type.; includes_start (bool) – Interval includes start.; includes_end (bool) – Interval includes end. Note; This object refers to the Python val",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
Usability,simpl,simpler,"Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). Caution; These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use hadoop_copy(); to move your file to a distributed file system. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hail.utils.hadoop_copy(src, dest)[source]; Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; Try using hadoop_open() first, it’s simpler, but not great; for large data! For example:; >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: ; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hail.utils.hadoop_exists(path)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_file(path)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_dir(path)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_stat(path)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_ti",MatchSource.WIKI,docs/0.2/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/utils/index.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.combiner.load_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.load_combiner. View page source. hail.vds.combiner.load_combiner. hail.vds.combiner.load_combiner(path)[source]; Load a VariantDatasetCombiner from path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.load_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.load_combiner.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.combiner.new_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.new_combiner. View page source. hail.vds.combiner.new_combiner. hail.vds.combiner.new_combiner(*, output_path, temp_path, save_path=None, gvcf_paths=None, vds_paths=None, vds_sample_counts=None, intervals=None, import_interval_size=None, use_genome_default_intervals=False, use_exome_default_intervals=False, gvcf_external_header=None, gvcf_sample_names=None, gvcf_info_to_keep=None, gvcf_reference_entry_fields_to_keep=None, call_fields=['PGT'], branch_factor=100, target_records=24000, gvcf_batch_size=None, batch_size=None, reference_genome='default', contig_recoding=None, force=False)[source]; Create a new VariantDatasetCombiner or load one from save_path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.new_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html
Performance,load,load,"﻿. Hail | ; hail.vds.combiner.new_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.new_combiner. View page source. hail.vds.combiner.new_combiner. hail.vds.combiner.new_combiner(*, output_path, temp_path, save_path=None, gvcf_paths=None, vds_paths=None, vds_sample_counts=None, intervals=None, import_interval_size=None, use_genome_default_intervals=False, use_exome_default_intervals=False, gvcf_external_header=None, gvcf_sample_names=None, gvcf_info_to_keep=None, gvcf_reference_entry_fields_to_keep=None, call_fields=['PGT'], branch_factor=100, target_records=24000, gvcf_batch_size=None, batch_size=None, reference_genome='default', contig_recoding=None, force=False)[source]; Create a new VariantDatasetCombiner or load one from save_path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.new_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html
Availability,failure,failure-tolerant,"﻿. Hail | ; VariantDatasetCombiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VariantDatasetCombiner. View page source. VariantDatasetCombiner. class hail.vds.combiner.VariantDatasetCombiner[source]; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets.; Examples; A Variant Dataset comprises one or more sequences. A new Variant Dataset is constructed from; GVCF files and/or extant Variant Datasets. For example, the following produces a new Variant; Dataset from four GVCF files containing whole genome sequences; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='g",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
Deployability,update,updated,"nfo_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. to_dict()[source]; A serializable representation of this combiner. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
Integrability,depend,depends,"_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: use_exome_default_intervals=True and; use_genome_default_intervals=True.; The combiner serializes itself to save_path so that it can be restarted after failure. Parameters:. save_path (str) – The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path (str) – The location to store the new VariantDataset.; temp_path (str) – The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome (ReferenceGenome) – The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor (int) – The number of Variant Datasets to combine at once.",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
Performance,load,load,"o partition the GVCF files. The same partitioning is used; for all GVCF files. Finer partitioning yields more parallelism but less work per task.; gvcf_info_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. t",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html
Deployability,update,updated,"﻿. Hail | ; VDSMetadata. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VDSMetadata. View page source. VDSMetadata. class hail.vds.combiner.VDSMetadata[source]; The path to a Variant Dataset and the number of samples within. Parameters:. path (str) – Path to the variant dataset.; n_samples (int) – Number of samples contained within the Variant Dataset at path. Attributes. n_samples; Alias for field number 1. path; Alias for field number 0. Methods. __add__(value, /); Return self+value. __class_getitem__(); See PEP 585. __contains__(key, /); Return key in self. __eq__(value, /); Return self==value. __ge__(value, /); Return self>=value. __getitem__(key, /); Return self[key]. __getnewargs__(); Return self as a plain tuple. Used by copy and pickle. __gt__(value, /); Return self>value. __iter__(); Implement iter(self). __le__(value, /); Return self<=value. __len__(); Return len(self). __lt__(value, /); Return self<value. __mul__(value, /); Return self*value. __ne__(value, /); Return self!=value. __rmul__(value, /); Return value*self. count(value, /); Return number of occurrences of value. index(value, start=0, stop=9223372036854775807, /); Return first index of value.; Raises ValueError if the value is not present. n_samples; Alias for field number 1. path; Alias for field number 0. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.combiner.VDSMetadata.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VDSMetadata.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.filter_chromosomes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_chromosomes. View page source. hail.vds.filter_chromosomes. hail.vds.filter_chromosomes(vds, *, keep=None, remove=None, keep_autosomes=False)[source]; Filter chromosomes of a VariantDataset in several possible modes.; Notes; There are three modes for filter_chromosomes(), based on which argument is passed; to the function. Exactly one of the below arguments must be passed by keyword. keep: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a VariantDataset with only those; chromosomes.; remove: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a VariantDataset with those chromosomes; removed.; keep_autosomes: This argument expects the value True, and returns a dataset without; sex and mitochondrial chromosomes. Parameters:. vds (VariantDataset) – Dataset.; keep – Keep a specified list of contigs.; remove – Remove a specified list of contigs; keep_autosomes – If true, keep only autosomal chromosomes. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.filter_chromosomes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_chromosomes.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.filter_intervals. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_intervals. View page source. hail.vds.filter_intervals. hail.vds.filter_intervals(vds, intervals, *, split_reference_blocks=False, keep=True)[source]; Filter intervals in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; intervals (Table or ArrayExpression of type tinterval) – Intervals to filter on.; split_reference_blocks (bool) – If true, remove reference data outside the given intervals by segmenting reference; blocks at interval boundaries. Results in a smaller result, but this filter mode; is more computationally expensive to evaluate.; keep (bool) – Whether to keep, or filter out (default) rows that fall within any; interval in intervals. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.filter_intervals.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_intervals.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.filter_samples. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_samples. View page source. hail.vds.filter_samples. hail.vds.filter_samples(vds, samples, *, keep=True, remove_dead_alleles=False)[source]; Filter samples in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; samples (Table or list of str) – Samples to keep or remove.; keep (bool) – Whether to keep (default), or filter out the samples from samples_table.; remove_dead_alleles (bool) – If true, remove alleles observed in no samples. Alleles with AC == 0 will be; removed, and LA values recalculated. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.filter_samples.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_samples.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.filter_variants. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.filter_variants. View page source. hail.vds.filter_variants. hail.vds.filter_variants(vds, variants_table, *, keep=True)[source]; Filter variants in a VariantDataset, without removing reference; data. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; variants_table (Table) – Variants to filter on.; keep (bool) – Whether to keep (default), or filter out the variants from variants_table. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.filter_variants.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.filter_variants.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.impute_sex_chromosome_ploidy. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.impute_sex_chromosome_ploidy. View page source. hail.vds.impute_sex_chromosome_ploidy. hail.vds.impute_sex_chromosome_ploidy(vds, calling_intervals, normalization_contig, use_variant_dataset=False)[source]; Impute sex chromosome ploidy from depth of reference or variant data within calling intervals.; Returns a Table with sample ID keys, with the following fields:. autosomal_mean_dp (float64): Mean depth on calling intervals on normalization contig.; x_mean_dp (float64): Mean depth on calling intervals on X chromosome.; x_ploidy (float64): Estimated ploidy on X chromosome. Equal to 2 * x_mean_dp / autosomal_mean_dp.; y_mean_dp (float64): Mean depth on calling intervals on chromosome.; y_ploidy (float64): Estimated ploidy on Y chromosome. Equal to 2 * y_mean_db / autosomal_mean_dp. Parameters:. vds (vds: VariantDataset) – Dataset.; calling_intervals (Table or ArrayExpression) – Calling intervals with consistent read coverage (for exomes, trim the capture intervals).; normalization_contig (str) – Autosomal contig for depth comparison.; use_variant_dataset (bool) – Whether to use depth of variant data within calling intervals instead of reference data. Default will use reference data. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chromosome_ploidy.html
Deployability,update,updated,"x_chr_ploidy_from_interval_coverage. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.impute_sex_chr_ploidy_from_interval_coverage. View page source. hail.vds.impute_sex_chr_ploidy_from_interval_coverage. hail.vds.impute_sex_chr_ploidy_from_interval_coverage(mt, normalization_contig)[source]; Impute sex chromosome ploidy from a precomputed interval coverage MatrixTable.; The input MatrixTable must have the following row fields:. interval (interval): Genomic interval of interest.; interval_size (int32): Size of interval, in bases. And the following entry fields:. sum_dp (int64): Sum of depth values by base across the interval. Returns a Table with sample ID keys, with the following fields:. autosomal_mean_dp (float64): Mean depth on calling intervals on normalization contig.; x_mean_dp (float64): Mean depth on calling intervals on X chromosome.; x_ploidy (float64): Estimated ploidy on X chromosome. Equal to 2 * x_mean_dp / autosomal_mean_dp.; y_mean_dp (float64): Mean depth on calling intervals on chromosome.; y_ploidy (float64): Estimated ploidy on Y chromosome. Equal to 2 * y_mean_db / autosomal_mean_dp. Parameters:. mt (MatrixTable) – Interval-by-sample MatrixTable with sum of depth values across the interval.; normalization_contig (str) – Autosomal contig for depth comparison. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.impute_sex_chr_ploidy_from_interval_coverage.html
Deployability,update,updated,"enomic interval of interest.; interval_size (int32): Size of interval, in bases. Computes the following entry fields:. bases_over_gq_threshold (tuple of int64): Number of bases in the interval; over each GQ threshold.; fraction_over_gq_threshold (tuple of float64): Fraction of interval (in bases); above each GQ threshold. Computed by dividing each member of bases_over_gq_threshold; by interval_size.; bases_over_dp_threshold (tuple of int64): Number of bases in the interval; over each DP threshold.; fraction_over_dp_threshold (tuple of float64): Fraction of interval (in bases); above each DP threshold. Computed by dividing each member of bases_over_dp_threshold; by interval_size.; sum_dp (int64): Sum of depth values by base across the interval.; mean_dp (float64): Mean depth of bases across the interval. Computed by dividing; sum_dp by interval_size. If the dp_field parameter is not specified, the DP is used for depth; if present. If no DP field is present, the MIN_DP field is used. If no DP; or MIN_DP field is present, no depth statistics will be calculated. Note; The metrics computed by this method are computed only from reference blocks. Most; variant callers produce data where non-reference calls interrupt reference blocks, and; so the metrics computed here are slight underestimates of the true values (which would; include the quality/depth of non-reference calls). This is likely a negligible difference,; but is something to be aware of, especially as it interacts with samples of; ancestral backgrounds with more or fewer non-reference calls. Parameters:. vds (VariantDataset); intervals (Table) – Table of intervals. Must be start-inclusive, and cannot span contigs.; gq_thresholds (tuple of int) – GQ thresholds.; dp_field (str, optional) – Field for depth calculation. Uses DP or MIN_DP by default (with priority for DP if present). Returns:; MatrixTable – Interval-by-sample matrix. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.interval_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.interval_coverage.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.lgt_to_gt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.lgt_to_gt. View page source. hail.vds.lgt_to_gt. hail.vds.lgt_to_gt(lgt, la)[source]; Transform LGT into GT using local alleles array. Parameters:. lgt (CallExpression) – LGT value.; la (ArrayExpression) – Local alleles array. Returns:; CallExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.lgt_to_gt.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.lgt_to_gt.html
Deployability,update,updated,"t; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.local_to_global. View page source. hail.vds.local_to_global. hail.vds.local_to_global(array, local_alleles, n_alleles, fill_value, number)[source]; Reindex a locally-indexed array to globally-indexed.; Examples; >>> local_alleles = hl.array([0, 2]); >>> local_ad = hl.array([9, 10]); >>> local_pl = hl.array([94, 0, 123]). >>> hl.eval(local_to_global(local_ad, local_alleles, n_alleles=3, fill_value=0, number='R')); [9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; The number parameter matches the VCF specification; number definitions:. A indicates one value per allele, excluding the reference.; R indicates one value per allele, including the reference.; G indicates one value per unique diploid genotype. Warning; Using this function can lead to an enormous explosion in data size, without increasing; information capacity. Its appropriate use is to conform to antiquated and badly-scaling; representations (e.g. pVCF), but even so, caution should be exercised. Reindexing local; PLs (or any G-numbered field) at a site with 1000 alleles will produce an array with; more than 5,000 values per sample – with 100,000 samples, nearly 50GB per variant!. See also; lgt_to_gt(). Parameters:. array (ArrayExpression) – Array to reindex.; local_alleles (ArrayExpression) – Local alleles array.; n_alleles (Int32Expression) – Total number of alleles to reindex to.; fill_value – Value to fill in at global indices with no data.; number (str) – One of ‘A’, ‘R’, ‘G’. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.local_to_global.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.local_to_global.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.merge_reference_blocks. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.merge_reference_blocks. View page source. hail.vds.merge_reference_blocks. hail.vds.merge_reference_blocks(ds, equivalence_function, merge_functions=None)[source]; Merge adjacent reference blocks according to user equivalence criteria.; Examples; Coarsen GQ granularity into bins of 10 and merges blocks with the same GQ in order to; compress reference data.; >>> rd = vds.reference_data ; >>> vds.reference_data = rd.annotate_entries(GQ = rd.GQ - rd.GQ % 10) ; >>> vds2 = hl.vds.merge_reference_blocks(vds,; ... equivalence_function=lambda block1, block2: block1.GQ == block2.GQ),; ... merge_functions={'MIN_DP': 'min'}) . Notes; The equivalence_function argument expects a function from two reference blocks to a; boolean value indicating whether they should be combined. Adjacency checks are builtin; to the method (two reference blocks are ‘adjacent’ if the END of one block is one base; before the beginning of the next).; The merge_functions. Parameters:; ds (VariantDataset or MatrixTable) – Variant dataset or reference block matrix table. Returns:; VariantDataset or MatrixTable. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.merge_reference_blocks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.merge_reference_blocks.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.read_vds. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.read_vds. View page source. hail.vds.read_vds. hail.vds.read_vds(path, *, intervals=None, n_partitions=None, _assert_reference_type=None, _assert_variant_type=None, _warn_no_ref_block_max_length=True)[source]; Read in a VariantDataset written with VariantDataset.write(). Parameters:; path (str). Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.read_vds.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.read_vds.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.sample_qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.sample_qc. View page source. hail.vds.sample_qc. hail.vds.sample_qc(vds, *, gq_bins=(0, 20, 60), dp_bins=(0, 1, 10, 20, 30), dp_field=None)[source]; Compute sample quality metrics about a VariantDataset.; If the dp_field parameter is not specified, the DP is used for depth; if present. If no DP field is present, the MIN_DP field is used. If no DP; or MIN_DP field is present, no depth statistics will be calculated. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; gq_bins (tuple of int) – Tuple containing cutoffs for genotype quality (GQ) scores.; dp_bins (tuple of int) – Tuple containing cutoffs for depth (DP) scores.; dp_field (str) – Name of depth field. If not supplied, DP or MIN_DP will be used, in that order. Returns:; Table – Hail Table of results, keyed by sample. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.sample_qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.sample_qc.html
Availability,error,error,"﻿. Hail | ; hail.vds.split_multi. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.split_multi. View page source. hail.vds.split_multi. hail.vds.split_multi(vds, *, filter_changed_loci=False)[source]; Split the multiallelic variants in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; filter_changed_loci (bool) – If any REF/ALT pair changes locus under min_rep(), filter that; variant instead of throwing an error. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.split_multi.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.split_multi. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.split_multi. View page source. hail.vds.split_multi. hail.vds.split_multi(vds, *, filter_changed_loci=False)[source]; Split the multiallelic variants in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; filter_changed_loci (bool) – If any REF/ALT pair changes locus under min_rep(), filter that; variant instead of throwing an error. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.split_multi.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html
Availability,down,downstream,"﻿. Hail | ; hail.vds.store_ref_block_max_length. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.store_ref_block_max_length. View page source. hail.vds.store_ref_block_max_length. hail.vds.store_ref_block_max_length(vds_path)[source]; Patches an existing VDS file to store the max reference block length for faster interval filters.; This method permits vds.filter_intervals() to remove reference data not overlapping a target interval.; This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; vds.truncate_reference_blocks() to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS.; Examples; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') . See also; vds.filter_intervals(), vds.truncate_reference_blocks(). Parameters:; vds_path (str). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.store_ref_block_max_length.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html
Deployability,patch,patch,"﻿. Hail | ; hail.vds.store_ref_block_max_length. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.store_ref_block_max_length. View page source. hail.vds.store_ref_block_max_length. hail.vds.store_ref_block_max_length(vds_path)[source]; Patches an existing VDS file to store the max reference block length for faster interval filters.; This method permits vds.filter_intervals() to remove reference data not overlapping a target interval.; This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; vds.truncate_reference_blocks() to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS.; Examples; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') . See also; vds.filter_intervals(), vds.truncate_reference_blocks(). Parameters:; vds_path (str). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.store_ref_block_max_length.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.to_dense_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.to_dense_mt. View page source. hail.vds.to_dense_mt. hail.vds.to_dense_mt(vds)[source]; Creates a single, dense MatrixTable from the split; VariantDataset representation. Parameters:; vds (VariantDataset) – Dataset in VariantDataset representation. Returns:; MatrixTable – Dataset in dense MatrixTable representation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.to_dense_mt.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.to_dense_mt.html
Deployability,update,updated,"﻿. Hail | ; hail.vds.to_merged_sparse_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.to_merged_sparse_mt. View page source. hail.vds.to_merged_sparse_mt. hail.vds.to_merged_sparse_mt(vds, *, ref_allele_function=None)[source]; Creates a single, merged sparse MatrixTable from the split; VariantDataset representation. Parameters:; vds (VariantDataset) – Dataset in VariantDataset representation. Returns:; MatrixTable – Dataset in the merged sparse MatrixTable representation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.to_merged_sparse_mt.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.to_merged_sparse_mt.html
Deployability,patch,patch,"Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.truncate_reference_blocks. View page source. hail.vds.truncate_reference_blocks. hail.vds.truncate_reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None)[source]; Cap reference blocks at a maximum length in order to permit faster interval filtering.; Examples; Truncate reference blocks to 5 kilobases:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) . Truncate the longest 1% of reference blocks to the length of the 99th percentile block:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) . Notes; After this function has been run, the reference blocks have a known maximum length ref_block_max_length,; stored in the global fields, which permits vds.filter_intervals() to filter to intervals of the reference; data by reading ref_block_max_length bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work.; It is also possible to patch an existing VDS to store the max reference block length with vds.store_ref_block_max_length(). See also; vds.store_ref_block_max_length(). Parameters:. vds (VariantDataset or MatrixTable); max_ref_block_base_pairs – Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction – Fraction of reference block length distribution to truncate / winsorize. Returns:; VariantDataset or MatrixTable. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.truncate_reference_blocks.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.truncate_reference_blocks.html
Availability,checkpoint,checkpoint,"alg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VariantDataset. View page source. VariantDataset. class hail.vds.VariantDataset[source]; Class for representing cohort-level genomic data.; This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a datas",MatchSource.WIKI,docs/0.2/vds/hail.vds.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html
Deployability,update,updated,"ss facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; Eagerly checks necessary representational properties of the VDS. write(path, **kwargs)[source]; Write to path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/hail.vds.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html
Security,validat,validate,"n API; Hail Query Python API; Variant Dataset; VariantDataset. View page source. VariantDataset. class hail.vds.VariantDataset[source]; Class for representing cohort-level genomic data.; This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; E",MatchSource.WIKI,docs/0.2/vds/hail.vds.VariantDataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html
Availability,failure,failure-tolerant,"me ploidy from a precomputed interval coverage MatrixTable. to_dense_mt(vds); Creates a single, dense MatrixTable from the split VariantDataset representation. to_merged_sparse_mt(vds, *[, ...]); Creates a single, merged sparse MatrixTable from the split VariantDataset representation. truncate_reference_blocks(ds, *[, ...]); Cap reference blocks at a maximum length in order to permit faster interval filtering. merge_reference_blocks(ds, equivalence_function); Merge adjacent reference blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in ",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
Deployability,pipeline,pipelines,"no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; vds.to_dense_mt() and vds.to_merged_sparse_mt(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
Integrability,interface,interfaces,"ssions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset. View page source. Variant Dataset; The VariantDataset is an extra layer of abstraction of the Hail Matrix Table for working; with large sequencing datasets. It was initially developed in response to the gnomAD project’s need; to combine, represent, and analyze 150,000 whole genomes. It has since been used on datasets as; large as 955,000 whole exomes. The VariantDatasetCombiner produces a; VariantDataset by combining any number of GVCF and/or VariantDataset files. Warning; Hail 0.1 also had a Variant Dataset class. Although pieces of the interfaces are similar, they should not; be considered interchangeable and do not represent the same data. Variant Dataset. VariantDataset; Class for representing cohort-level genomic data. read_vds(path, *[, intervals, n_partitions, ...]); Read in a VariantDataset written with VariantDataset.write(). filter_samples(vds, samples, *[, keep, ...]); Filter samples in a VariantDataset. filter_variants(vds, variants_table, *[, keep]); Filter variants in a VariantDataset, without removing reference data. filter_intervals(vds, intervals, *[, ...]); Filter intervals in a VariantDataset. filter_chromosomes(vds, *[, keep, remove, ...]); Filter chromosomes of a VariantDataset in several possible modes. sample_qc(vds, *[, gq_bins, dp_bins, dp_field]); Compute sample quality metrics about a VariantDataset. split_multi(vds, *[, filter_changed_loci]); Split the multiallelic variants in a VariantDataset. interval_coverage(vds, intervals[, ...]); Compute statistics about base coverage by interval. impute_sex_chromosome_ploidy(vds, ...[, ...])",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
Performance,load,load,"taset representation. to_merged_sparse_mt(vds, *[, ...]); Creates a single, merged sparse MatrixTable from the split VariantDataset representation. truncate_reference_blocks(ds, *[, ...]); Cap reference blocks at a maximum length in order to permit faster interval filtering. merge_reference_blocks(ds, equivalence_function); Merge adjacent reference blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference informa",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
Safety,avoid,avoid,"intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, even though the information content; does not change. To avoid this superlinear scaling, the SVCR renames these; fields to their “local” versions: LGT, LAD, LPL, etc, and adds a new field,; LA (local alleles). The information in the local fields refers to the alleles; defined per row of the matrix indirectly through the LA list.; For instance, if a sample has the following information in its GVCF:; Ref=G Alt=T GT=0/1 AD=5,6 PL=102,0,150. If the alternate alleles A,C,T are discovered in the cohort, this sample’s; entry would look like:; LA=0,2 LGT=0/1 LAD=5,6 LPL=102,0,150. The “1” allele referred to in LGT, and the allele to which the reads in the; second position of LAD belong to, is not the allele with absolute index 1; (C), but rather the allele whose index is in position 1 of the LA list.; The index at position 2 of the LA list is 2, and the allele with absolute; index 2 is T. Local alleles make it possible to keep the data small to; match its inherent information content. Component tables; The VariantDataset is made up of two component matrix tables – the; reference_data and the variant_data.; The reference_",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
Testability,test,tests,"no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; vds.to_dense_mt() and vds.to_merged_sparse_mt(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/vds/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/vds/index.html
Availability,avail,available,﻿. Hail | ; Overview: module code. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Overview: module code. All modules for which code is available; hail.context; hail.experimental.datasets; hail.experimental.db; hail.experimental.export_entries_by_col; hail.experimental.expressions; hail.experimental.filtering_allele_frequency; hail.experimental.full_outer_join_mt; hail.experimental.import_gtf; hail.experimental.ld_score_regression; hail.experimental.ldscore; hail.experimental.ldscsim; hail.experimental.loop; hail.experimental.pca; hail.experimental.phase_by_transmission; hail.experimental.plots; hail.experimental.tidyr; hail.experimental.time; hail.expr.aggregators.aggregators; hail.expr.builders; hail.expr.expressions.base_expression; hail.expr.expressions.expression_utils; hail.expr.expressions.typed_expressions; hail.expr.functions; hail.expr.types; hail.genetics.allele_type; hail.genetics.call; hail.genetics.locus; hail.genetics.pedigree; hail.genetics.reference_genome; hail.ggplot.aes; hail.ggplot.coord_cartesian; hail.ggplot.facets; hail.ggplot.geoms; hail.ggplot.ggplot; hail.ggplot.labels; hail.ggplot.scale; hail.linalg.blockmatrix; hail.linalg.utils.misc; hail.matrixtable; hail.methods.family_methods; hail.methods.impex; hail.methods.misc; hail.methods.pca; hail.methods.qc; hail.methods.relatedness.identity_by_descent; hail.methods.relatedness.king; hail.methods.relatedness.mating_simulation; hail.methods.relatedness.pc_relate; hail.methods.statgen; hail.nd.nd; hail.plot.plots; hail.stats.linear_mixed_model; hail.table; hail.utils.hadoop_utils; hail.utils.interval; hail.utils.misc; hail.utils.struct; hail.utils.tutorial; hail.vds.c,MatchSource.WIKI,docs/0.2/_modules/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/index.html
Deployability,update,updated,"onfiguration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Overview: module code. All modules for which code is available; hail.context; hail.experimental.datasets; hail.experimental.db; hail.experimental.export_entries_by_col; hail.experimental.expressions; hail.experimental.filtering_allele_frequency; hail.experimental.full_outer_join_mt; hail.experimental.import_gtf; hail.experimental.ld_score_regression; hail.experimental.ldscore; hail.experimental.ldscsim; hail.experimental.loop; hail.experimental.pca; hail.experimental.phase_by_transmission; hail.experimental.plots; hail.experimental.tidyr; hail.experimental.time; hail.expr.aggregators.aggregators; hail.expr.builders; hail.expr.expressions.base_expression; hail.expr.expressions.expression_utils; hail.expr.expressions.typed_expressions; hail.expr.functions; hail.expr.types; hail.genetics.allele_type; hail.genetics.call; hail.genetics.locus; hail.genetics.pedigree; hail.genetics.reference_genome; hail.ggplot.aes; hail.ggplot.coord_cartesian; hail.ggplot.facets; hail.ggplot.geoms; hail.ggplot.ggplot; hail.ggplot.labels; hail.ggplot.scale; hail.linalg.blockmatrix; hail.linalg.utils.misc; hail.matrixtable; hail.methods.family_methods; hail.methods.impex; hail.methods.misc; hail.methods.pca; hail.methods.qc; hail.methods.relatedness.identity_by_descent; hail.methods.relatedness.king; hail.methods.relatedness.mating_simulation; hail.methods.relatedness.pc_relate; hail.methods.statgen; hail.nd.nd; hail.plot.plots; hail.stats.linear_mixed_model; hail.table; hail.utils.hadoop_utils; hail.utils.interval; hail.utils.misc; hail.utils.struct; hail.utils.tutorial; hail.vds.combiner.variant_dataset_combiner; hail.vds.functions; hail.vds.methods; hail.vds.sample_qc; hail.vds.variant_dataset; hailtop.frozendict; hailtop.fs.fs_utils. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/index.html
Deployability,update,updated,": int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_autosomes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_autosomes.html
Deployability,update,updated,"Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; 1000_Genomes_chrMT. View page source. 1000_Genomes_chrMT. Versions: phase_3; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (phase_3, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: int32,; VT: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_chrMT.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrMT.html
Deployability,update,updated,": int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_chrX.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrX.html
Deployability,update,updated,"ixTable. Schema (phase_3, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; DP: int32,; END: int32,; SVTYPE: str,; AA: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_chrY.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrY.html
Deployability,update,updated,"oat64,; AF_EAS: float64,; AF_AMR: float64,; AF_SAS: float64,; AF_AFR: float64,; HWE_EUR: float64,; HWE_EAS: float64,; HWE_AMR: float64,; HWE_SAS: float64,; HWE_AFR: float64,; HWE: float64,; ExcHet_EUR: float64,; ExcHet_EAS: float64,; ExcHet_AMR: float64,; ExcHet_SAS: float64,; ExcHet_AFR: float64,; ExcHet: float64,; ME: float64,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AC_Hom_EUR_unrel: int32,; AC_Hom_EAS_unrel: int32,; AC_Hom_AMR_unrel: int32,; AC_Hom_SAS_unrel: int32,; AC_Hom_AFR_unrel: int32,; AC_Het_EUR_unrel: int32,; AC_Het_EAS_unrel: int32,; AC_Het_AMR_unrel: int32,; AC_Het_SAS_unrel: int32,; AC_Het_AFR_unrel: int32,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64,; HWE_EUR_unrel: float64,; HWE_EAS_unrel: float64,; HWE_AMR_unrel: float64,; HWE_SAS_unrel: float64,; HWE_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html
Deployability,update,updated,"oat64,; AF_EAS: float64,; AF_AMR: float64,; AF_SAS: float64,; AF_AFR: float64,; HWE_EUR: float64,; HWE_EAS: float64,; HWE_AMR: float64,; HWE_SAS: float64,; HWE_AFR: float64,; HWE: float64,; ExcHet_EUR: float64,; ExcHet_EAS: float64,; ExcHet_AMR: float64,; ExcHet_SAS: float64,; ExcHet_AFR: float64,; ExcHet: float64,; ME: float64,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AC_Hom_EUR_unrel: int32,; AC_Hom_EAS_unrel: int32,; AC_Hom_AMR_unrel: int32,; AC_Hom_SAS_unrel: int32,; AC_Hom_AFR_unrel: int32,; AC_Het_EUR_unrel: int32,; AC_Het_EAS_unrel: int32,; AC_Het_AMR_unrel: int32,; AC_Het_SAS_unrel: int32,; AC_Het_AFR_unrel: int32,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64,; HWE_EUR_unrel: float64,; HWE_EAS_unrel: float64,; HWE_AMR_unrel: float64,; HWE_SAS_unrel: float64,; HWE_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrX.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrX.html
Deployability,update,updated,"32,; AF: float64,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; END: int32,; ExcessHet: float64,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: int32,; MLEAF: float64,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool,; QD: float64,; RAW_MQ: float64,; ReadPosRankSum: float64,; SOR: float64,; VQSLOD: float64,; VariantType: str,; culprit: str,; AN_EAS: int32,; AN_AMR: int32,; AN_EUR: int32,; AN_AFR: int32,; AN_SAS: int32,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EAS: int32,; AC_AMR: int32,; AC_EUR: int32,; AC_AFR: int32,; AC_SAS: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AF_EAS: float64,; AF_AMR: float64,; AF_EUR: float64,; AF_AFR: float64,; AF_SAS: float64,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html
Deployability,update,updated,": array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: array<str>,; RSID_REMOVED: array<str>,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html
Deployability,update,updated,": array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: array<str>,; RSID_REMOVED: array<str>,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrX.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrX.html
Deployability,update,updated,"population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; DP: int32,; END: int32,; SVTYPE: str,; AA: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: str,; RSID_REMOVED: str,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html
Deployability,update,updated,"ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; CADD. View page source. CADD. Versions: 1.4, 1.6; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (1.4, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int64,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'raw_score': float64; 'PHRED_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/CADD.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/CADD.html
Deployability,update,updated,"d_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; clinvar_gene_summary. View page source. clinvar_gene_summary. Versions: 2019-07; Reference genome builds: None; Type: hail.Table. Schema (2019-07, None); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'GeneID': int32; 'Total_submissions': int32; 'Total_alleles': int32; 'Submissions_reporting_this_gene': int32; 'Alleles_reported_Pathogenic_Likely_pathogenic': int32; 'Gene_MIM_number': int32; 'Number_uncertain': int32; 'Number_with_conflicts': int32; 'gene_name': str; ----------------------------------------; Key: ['gene_name']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/clinvar_gene_summary.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/clinvar_gene_summary.html
Deployability,update,updated,"v_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; clinvar_variant_summary. View page source. clinvar_variant_summary. Versions: 2019-07; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (2019-07, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'Type': str; 'Name': str; 'GeneID': int32; 'GeneSymbol': str; 'HGNC_ID': str; 'ClinicalSignificance': str; 'ClinSigSimple': int32; 'LastEvaluated': str; 'RS# (dbSNP)': int32; 'nsv/esv (dbVar)': str; 'RCVaccession': str; 'PhenotypeIDS': str; 'PhenotypeList': str; 'Origin': str; 'OriginSimple': str; 'Assembly': str; 'ChromosomeAccession': str; 'ReferenceAllele': str; 'AlternateAllele': str; 'Cytogenetic': str; 'ReviewStatus': str; 'NumberSubmitters': int32; 'Guidelines': str; 'TestedInGTR': str; 'OtherIDs': str; 'SubmitterCategories': int32; 'VariationID': int32; 'interval': interval<locus<GRCh37>>; 'AlleleID': int32; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/clinvar_variant_summary.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/clinvar_variant_summary.html
Deployability,update,updated,"; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; DANN. View page source. DANN. Versions: None; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (None, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int64,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/DANN.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/DANN.html
Deployability,update,updated,"C_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype': str; 'ZFIN_zebrafish_gene': str; 'ZFIN_zebrafish_structure': str; 'ZFIN_zebrafish_phenotype_quality': str; 'ZFIN_zebrafish_phenotype_tag': str; ----------------------------------------; Key: ['Gene_name']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
Safety,predict,prediction,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_genes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html
Deployability,update,updated,"es_POPMAX_AF': float64; 'gnomAD_genomes_POPMAX_nhomalt': int32; 'gnomAD_genomes_controls_AC': int32; 'gnomAD_genomes_controls_AN': int32; 'gnomAD_genomes_controls_AF': float64; 'gnomAD_genomes_controls_nhomalt': int32; 'gnomAD_genomes_controls_AFR_AC': int32; 'gnomAD_genomes_controls_AFR_AN': int32; 'gnomAD_genomes_controls_AFR_AF': float64; 'gnomAD_genomes_controls_AFR_nhomalt': int32; 'gnomAD_genomes_controls_AMR_AC': int32; 'gnomAD_genomes_controls_AMR_AN': int32; 'gnomAD_genomes_controls_AMR_AF': float64; 'gnomAD_genomes_controls_AMR_nhomalt': int32; 'gnomAD_genomes_controls_ASJ_AC': int32; 'gnomAD_genomes_controls_ASJ_AN': int32; 'gnomAD_genomes_controls_ASJ_AF': float64; 'gnomAD_genomes_controls_ASJ_nhomalt': int32; 'gnomAD_genomes_controls_EAS_AC': int32; 'gnomAD_genomes_controls_EAS_AN': int32; 'gnomAD_genomes_controls_EAS_AF': float64; 'gnomAD_genomes_controls_EAS_nhomalt': int32; 'gnomAD_genomes_controls_FIN_AC': int32; 'gnomAD_genomes_controls_FIN_AN': int32; 'gnomAD_genomes_controls_FIN_AF': float64; 'gnomAD_genomes_controls_FIN_nhomalt': int32; 'gnomAD_genomes_controls_NFE_AC': int32; 'gnomAD_genomes_controls_NFE_AN': int32; 'gnomAD_genomes_controls_NFE_AF': float64; 'gnomAD_genomes_controls_NFE_nhomalt': int32; 'gnomAD_genomes_controls_POPMAX_AC': int32; 'gnomAD_genomes_controls_POPMAX_AN': int32; 'gnomAD_genomes_controls_POPMAX_AF': float64; 'gnomAD_genomes_controls_POPMAX_nhomalt': int32; 'clinvar_id': int32; 'clinvar_clnsig': str; 'clinvar_trait': str; 'clinvar_review': str; 'clinvar_hgvs': str; 'clinvar_var_source': str; 'clinvar_MedGen_id': str; 'clinvar_OMIM_id': str; 'clinvar_Orphanet_id': str; 'Interpro_domain': str; 'GTEx_V7_gene': str; 'GTEx_V7_tissue': str; 'Geuvadis_eQTL_target_gene': str; 'locus': locus<GRCh37>; 'alleles': array<str>; 'chr': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/dbNSFP_variants.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_variants.html
Deployability,update,updated,"quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; dbSNP. View page source. dbSNP. Versions: 154; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (154, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; RS: int32,; GENEINFO: str,; PSEUDOGENEINFO: str,; dbSNPBuildID: int32,; SAO: int32,; SSR: int32,; VC: str,; PM: bool,; NSF: bool,; NSM: bool,; NSN: bool,; SYN: bool,; U3: bool,; U5: bool,; ASS: bool,; DSS: bool,; INT: bool,; R3: bool,; R5: bool,; GNO: bool,; PUB: bool,; FREQ: struct {; _GENOME_DK: float64,; _TWINSUK: float64,; _dbGaP_PopFreq: float64,; _Siberian: float64,; _Chileans: float64,; _FINRISK: float64,; _HapMap: float64,; _Estonian: float64,; _ALSPAC: float64,; _GoESP: float64,; _TOPMED: float64,; _PAGE_STUDY: float64,; _1000Genomes: float64,; _Korea1K: float64,; _ChromosomeY: float64,; _ExAC: float64,; _Qatari: float64,; _GoNL: float64,; _MGP: float64,; _GnomAD: float64,; _Vietnamese: float64,; _GnomAD_exomes: float64,; _PharmGKB: float64,; _KOREAN: float64,; _Daghestan: float64,; _HGDP_Stanford: float64,; _NorthernSweden: float64,; _SGDP_PRJ: float64; },; COMMON: bool,; CLNHGVS: array<str>,; CLNVI: array<str>,; CLNORIGIN: array<str>,; CLNSIG: array<str>,; CLNDISDB: array<str>,; CLNDN: array<str>,; CLNREVSTAT: array<str>,; CLNACC: array<str>; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/dbSNP.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbSNP.html
Deployability,update,updated,"mad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; dbSNP_rsid. View page source. dbSNP_rsid. Versions: 154; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (154, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/dbSNP_rsid.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbSNP_rsid.html
Deployability,update,updated,"es_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; Ensembl_homo_sapiens_low_complexity_regions. View page source. Ensembl_homo_sapiens_low_complexity_regions. Versions: release_95; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (release_95, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html
Deployability,update,updated,"cores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; Ensembl_homo_sapiens_reference_genome. View page source. Ensembl_homo_sapiens_reference_genome. Versions: release_95; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (release_95, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'reference_allele': str; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html
Deployability,update,updated,"ces_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gencode. View page source. gencode. Versions: v19, v31, v35; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (v35, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh38>>; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; 'level': int32; 'gene_id': str; 'gene_type': str; 'ccdsid': str; 'exon_id': str; 'exon_number': int32; 'havana_gene': str; 'transcript_type': str; 'protein_id': str; 'gene_name': str; 'transcript_name': str; 'transcript_id': str; 'transcript_support_level': str; 'hgnc_id': str; 'ont': str; 'havana_transcript': str; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gencode.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gencode.html
Deployability,update,updated,"; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gerp_elements. View page source. gerp_elements. Versions: hg19; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (hg19, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh37>>; 'S': float64; 'p_value': float64; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gerp_elements.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gerp_elements.html
Deployability,update,updated,"_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gerp_scores. View page source. gerp_scores. Versions: hg19; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (hg19, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'N': float64; 'S': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gerp_scores.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gerp_scores.html
Deployability,update,updated,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_AFR. View page source. giant_bmi_exome_AFR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'afr_maf': dict<str, float64>; 'exac_afr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html
Deployability,update,updated,"; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_ALL. View page source. giant_bmi_exome_ALL. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html
Deployability,update,updated,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_AMR. View page source. giant_bmi_exome_AMR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'amr_maf': dict<str, float64>; 'exac_amr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html
Deployability,update,updated,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_EAS. View page source. giant_bmi_exome_EAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eas_maf': dict<str, float64>; 'exac_eas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html
Deployability,update,updated,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_EUR. View page source. giant_bmi_exome_EUR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html
Deployability,update,updated,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_SAS. View page source. giant_bmi_exome_SAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'sas_maf': dict<str, float64>; 'exac_sas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html
Deployability,update,updated,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_AFR. View page source. giant_height_exome_AFR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'afr_maf': dict<str, float64>; 'exac_afr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_AFR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AFR.html
Deployability,update,updated,"ad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_ALL. View page source. giant_height_exome_ALL. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_ALL.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_ALL.html
Deployability,update,updated,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_AMR. View page source. giant_height_exome_AMR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'amr_maf': dict<str, float64>; 'exac_amr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_AMR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AMR.html
Deployability,update,updated,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_EAS. View page source. giant_height_exome_EAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eas_maf': dict<str, float64>; 'exac_eas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_EAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EAS.html
Deployability,update,updated,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_EUR. View page source. giant_height_exome_EUR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_EUR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EUR.html
Deployability,update,updated,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_SAS. View page source. giant_height_exome_SAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'sas_maf': dict<str, float64>; 'exac_sas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_height_exome_SAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_SAS.html
Deployability,update,updated,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_ALL_Add. View page source. giant_whr_exome_C_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html
Deployability,update,updated,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_ALL_Rec. View page source. giant_whr_exome_C_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html
Deployability,update,updated,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_EUR_Add. View page source. giant_whr_exome_C_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html
Deployability,update,updated,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_EUR_Rec. View page source. giant_whr_exome_C_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html
Deployability,update,updated,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_ALL_Add. View page source. giant_whr_exome_M_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html
Deployability,update,updated,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_ALL_Rec. View page source. giant_whr_exome_M_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html
Deployability,update,updated,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_EUR_Add. View page source. giant_whr_exome_M_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html
Deployability,update,updated,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_EUR_Rec. View page source. giant_whr_exome_M_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html
Deployability,update,updated,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_ALL_Add. View page source. giant_whr_exome_W_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html
Deployability,update,updated,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_ALL_Rec. View page source. giant_whr_exome_W_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html
Deployability,update,updated,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_EUR_Add. View page source. giant_whr_exome_W_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html
Deployability,update,updated,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_EUR_Rec. View page source. giant_whr_exome_W_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html
Deployability,update,updated,"4,; over_15: float64,; over_20: float64,; over_25: float64,; over_30: float64,; over_50: float64,; over_100: float64; }; }; 'gerp': float64; 'tx_annotation': array<struct {; ensg: str,; csq: str,; symbol: str,; lof: str,; lof_flag: str,; Cells_Transformedfibroblasts: float64,; Prostate: float64,; Spleen: float64,; Brain_FrontalCortex_BA9_: float64,; SmallIntestine_TerminalIleum: float64,; MinorSalivaryGland: float64,; Artery_Coronary: float64,; Skin_SunExposed_Lowerleg_: float64,; Cells_EBV_transformedlymphocytes: float64,; Brain_Hippocampus: float64,; Esophagus_Muscularis: float64,; Brain_Nucleusaccumbens_basalganglia_: float64,; Artery_Tibial: float64,; Brain_Hypothalamus: float64,; Adipose_Visceral_Omentum_: float64,; Cervix_Ectocervix: float64,; Brain_Spinalcord_cervicalc_1_: float64,; Brain_CerebellarHemisphere: float64,; Nerve_Tibial: float64,; Breast_MammaryTissue: float64,; Liver: float64,; Skin_NotSunExposed_Suprapubic_: float64,; AdrenalGland: float64,; Vagina: float64,; Pancreas: float64,; Lung: float64,; FallopianTube: float64,; Pituitary: float64,; Muscle_Skeletal: float64,; Colon_Transverse: float64,; Artery_Aorta: float64,; Heart_AtrialAppendage: float64,; Adipose_Subcutaneous: float64,; Esophagus_Mucosa: float64,; Heart_LeftVentricle: float64,; Brain_Cerebellum: float64,; Brain_Cortex: float64,; Thyroid: float64,; Brain_Substantianigra: float64,; Kidney_Cortex: float64,; Uterus: float64,; Stomach: float64,; WholeBlood: float64,; Bladder: float64,; Brain_Anteriorcingulatecortex_BA24_: float64,; Brain_Putamen_basalganglia_: float64,; Brain_Caudate_basalganglia_: float64,; Colon_Sigmoid: float64,; Cervix_Endocervix: float64,; Ovary: float64,; Esophagus_GastroesophagealJunction: float64,; Testis: float64,; Brain_Amygdala: float64,; mean_proportion: float64; }>; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_annotation_pext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_annotation_pext.html
Deployability,update,updated,". Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ensg': str; 'locus': locus<GRCh37>; 'symbol': str; 'Cells_Transformedfibroblasts': float64; 'Prostate': float64; 'Spleen': float64; 'Brain_FrontalCortex_BA9_': float64; 'SmallIntestine_TerminalIleum': float64; 'MinorSalivaryGland': float64; 'Artery_Coronary': float64; 'Skin_SunExposed_Lowerleg_': float64; 'Cells_EBV_transformedlymphocytes': float64; 'Brain_Hippocampus': float64; 'Esophagus_Muscularis': float64; 'Brain_Nucleusaccumbens_basalganglia_': float64; 'Artery_Tibial': float64; 'Brain_Hypothalamus': float64; 'Adipose_Visceral_Omentum_': float64; 'Cervix_Ectocervix': float64; 'Brain_Spinalcord_cervicalc_1_': float64; 'Brain_CerebellarHemisphere': float64; 'Nerve_Tibial': float64; 'Breast_MammaryTissue': float64; 'Liver': float64; 'Skin_NotSunExposed_Suprapubic_': float64; 'AdrenalGland': float64; 'Vagina': float64; 'Pancreas': float64; 'Lung': float64; 'FallopianTube': float64; 'Pituitary': float64; 'Muscle_Skeletal': float64; 'Colon_Transverse': float64; 'Artery_Aorta': float64; 'Heart_AtrialAppendage': float64; 'Adipose_Subcutaneous': float64; 'Esophagus_Mucosa': float64; 'Heart_LeftVentricle': float64; 'Brain_Cerebellum': float64; 'Brain_Cortex': float64; 'Thyroid': float64; 'Brain_Substantianigra': float64; 'Kidney_Cortex': float64; 'Uterus': float64; 'Stomach': float64; 'WholeBlood': float64; 'Bladder': float64; 'Brain_Anteriorcingulatecortex_BA24_': float64; 'Brain_Putamen_basalganglia_': float64; 'Brain_Caudate_basalganglia_': float64; 'Colon_Sigmoid': float64; 'Cervix_Endocervix': float64; 'Ovary': float64; 'Esophagus_GastroesophagealJunction': float64; 'Testis': float64; 'Brain_Amygdala': float64; 'mean_proportion': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_base_pext.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_base_pext.html
Deployability,update,updated,"ad_ld_scores_afr; gnomad_ld_scores_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_chrM_coverage. View page source. gnomad_chrM_coverage. Versions: 3.1; Reference genome builds: GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'mean': float64; 'median': int32; 'over_100': float64; 'over_1000': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_chrM_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_coverage.html
Deployability,update,updated,"hen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'rsid': set<str>; 'common_low_heteroplasmy': bool; 'base_qual_hist': array<int64>; 'position_hist': array<int64>; 'strand_bias_hist': array<int64>; 'weak_evidence_hist': array<int64>; 'contamination_hist': array<int64>; 'heteroplasmy_below_min_het_threshold_hist': array<int64>; 'excluded_AC': int64; 'AN': int64; 'AC_hom': int64; 'AC_het': int64; 'hl_hist': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_hist_all': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_hist_alt': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_mean': float64; 'mq_mean': float64; 'tlod_mean': float64; 'AF_hom': float32; 'AF_het': float32; 'max_hl': float64; 'hap_AN': array<int64>; 'hap_AC_het': array<int64>; 'hap_AC_hom': array<int64>; 'hap_AF_hom': array<float32>; 'hap_AF_het': array<float32>; 'hap_hl_hist': array<array<int64>>; 'hap_faf_hom': array<float64>; 'hapmax_AF_hom': str; 'hapmax_AF_het': str; 'faf_hapmax_hom': float64; 'pop_AN': array<int64>; 'pop_AC_het': array<int64>; 'pop_AC_hom': array<int64>; 'pop_AF_hom': array<float32>; 'pop_AF_het': array<float32>; 'pop_hl_hist': array<array<int64>>; 'age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_chrM_sites.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_sites.html
Deployability,update,updated,"d_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_exome_coverage. View page source. gnomad_exome_coverage. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_id': int64; 'locus': locus<GRCh37>; 'mean': float64; 'median': int32; 'over_1': float64; 'over_5': float64; 'over_10': float64; 'over_15': float64; 'over_20': float64; 'over_25': float64; 'over_30': float64; 'over_50': float64; 'over_100': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_exome_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_coverage.html
Deployability,update,updated,"ce: str,; motif_feature_consequences: array<struct {; allele_num: int32,; consequence_terms: array<str>,; high_inf_pos: str,; impact: str,; minimised: int32,; motif_feature_id: str,; motif_name: str,; motif_pos: int32,; motif_score_change: float64,; strand: int32,; variant_allele: str; }>,; regulatory_feature_consequences: array<struct {; allele_num: int32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'allele_info': struct {; BaseQRankSum: float64,; ClippingRankSum: float64,; DB: bool,; DP: int32,; DS: bool,; END: int32,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MQ: float64,; MQRankSum: float64,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool,; QD: float64,; ReadPosRankSum: float64,; SOR: float64,; VQSLOD: float64,; culprit: str; }; 'rsid': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_exome_sites.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_sites.html
Deployability,update,updated,"gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_genome_coverage. View page source. gnomad_genome_coverage. Versions: 2.1, 3.0.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_id': int64; 'locus': locus<GRCh37>; 'mean': float64; 'median': int32; 'over_1': float64; 'over_5': float64; 'over_10': float64; 'over_15': float64; 'over_20': float64; 'over_25': float64; 'over_30': float64; 'over_50': float64; 'over_100': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_genome_coverage.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_coverage.html
Deployability,update,updated,"_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; appris: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'vqsr': struct {; AS_VQSLOD: float64,; AS_culprit: str,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool; }; 'region_flag': struct {; lcr: bool,; segdup: bool; }; 'allele_info': struct {; variant_type: str,; allele_type: str,; n_alt_alleles: int32,; was_mixed: bool; }; 'age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_genome_sites.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_sites.html
Deployability,update,updated," n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_qual_hists': struct {; gq_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'gnomad_age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; ----------------------------------------; Entry fields:; 'DP': int32; 'GQ': int32; 'MIN_DP': int32; 'PID': str; 'RGQ': int32; 'SB': array<int32>; 'GT': call; 'PGT': call; 'AD': array<int32>; 'PL': array<int32>; 'adj': bool; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html
Deployability,update,updated," n_non_ref: int64,; n_snp: int64,; n_transition: int64,; n_transversion: int64,; r_het_hom_var: float64,; r_insertion_deletion: float64,; r_ti_tv: float64; }; 'gnomad_sex_imputation': struct {; chr20_mean_dp: float32,; chrX_mean_dp: float32,; chrY_mean_dp: float32,; chrX_ploidy: float32,; chrY_ploidy: float32,; X_karyotype: str,; Y_karyotype: str,; sex_karyotype: str,; f_stat: float64,; n_called: int64,; expected_homs: float64,; observed_homs: int64; }; 'gnomad_population_inference': struct {; pca_scores: array<float64>,; pop: str,; prob_afr: float64,; prob_ami: float64,; prob_amr: float64,; prob_asj: float64,; prob_eas: float64,; prob_fin: float64,; prob_mid: float64,; prob_nfe: float64,; prob_oth: float64,; prob_sas: float64; }; 'gnomad_sample_qc_residuals': struct {; n_snp_residual: float64,; r_ti_tv_residual: float64,; r_insertion_deletion_residual: float64,; n_insertion_residual: float64,; n_deletion_residual: float64,; r_het_hom_var_residual: float64,; n_transition_residual: float64,; n_transversion_residual: float64; }; 'gnomad_sample_filters': struct {; hard_filters: set<str>,; hard_filtered: bool,; release_related: bool,; qc_metrics_filters: set<str>; }; 'gnomad_high_quality': bool; 'gnomad_release': bool; 'relatedness_inference': struct {; related_samples: set<struct {; s: str,; kin: float64,; ibd0: float64,; ibd1: float64,; ibd2: float64; }>,; related: bool; }; 'hgdp_tgp_meta': struct {; project: str,; study_region: str,; population: str,; genetic_region: str,; latitude: float64,; longitude: float64,; hgdp_technical_meta: struct {; source: str,; library_type: str; },; global_pca_scores: array<float64>,; subcontinental_pca: struct {; pca_scores: array<float64>,; pca_scores_outliers_removed: array<float64>,; outlier: bool; },; gnomad_labeled_subpop: str; }; 'high_quality': bool; ----------------------------------------; Key: ['s']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html
Deployability,update,updated,"ad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_hgdp_1kg_subset_sparse. View page source. gnomad_hgdp_1kg_subset_sparse. Versions: 3.1.2; Reference genome builds: GRCh38; Type: hail.MatrixTable. Schema (3.1.2, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'DP': int32; 'END': int32; 'GQ': int32; 'LA': array<int32>; 'LAD': array<int32>; 'LGT': call; 'LPGT': call; 'LPL': array<int32>; 'MIN_DP': int32; 'PID': str; 'RGQ': int32; 'SB': array<int32>; 'gvcf_info': struct {; ClippingRankSum: float64,; BaseQRankSum: float64,; MQ: float64,; MQRankSum: float64,; MQ_DP: int32,; QUALapprox: int32,; RAW_MQ: float64,; ReadPosRankSum: float64,; VarDP: int32; }; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html
Deployability,update,updated,"ruct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_qual_hists': struct {; gq_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'gnomad_age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; 'AS_lowqual': bool; 'telomere_or_centromere': bool; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html
Deployability,update,updated,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_afr. View page source. gnomad_ld_scores_afr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html
Deployability,update,updated,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_amr. View page source. gnomad_ld_scores_amr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html
Deployability,update,updated,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_asj. View page source. gnomad_ld_scores_asj. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html
Deployability,update,updated,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_eas. View page source. gnomad_ld_scores_eas. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html
Deployability,update,updated,"7). gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_est. View page source. gnomad_ld_scores_est. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_est.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_est.html
Deployability,update,updated,"; Schema (2.1.1, GRCh37). gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_fin. View page source. gnomad_ld_scores_fin. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html
Deployability,update,updated,"; gnomad_ld_scores_nfe; Schema (2.1.1, GRCh37). gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_nfe. View page source. gnomad_ld_scores_nfe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html
Deployability,update,updated,"; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; Schema (2.1.1, GRCh37). gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_nwe. View page source. gnomad_ld_scores_nwe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html
Deployability,update,updated,"; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; Schema (2.1.1, GRCh37). gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_seu. View page source. gnomad_ld_scores_seu. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html
Deployability,update,updated,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_afr. View page source. gnomad_ld_variant_indices_afr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html
Deployability,update,updated,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_amr. View page source. gnomad_ld_variant_indices_amr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html
Deployability,update,updated,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_asj. View page source. gnomad_ld_variant_indices_asj. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html
Deployability,update,updated,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_eas. View page source. gnomad_ld_variant_indices_eas. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html
Deployability,update,updated,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_est. View page source. gnomad_ld_variant_indices_est. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html
Deployability,update,updated,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_fin. View page source. gnomad_ld_variant_indices_fin. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html
Deployability,update,updated,"ant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_nfe. View page source. gnomad_ld_variant_indices_nfe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'min_af': float64; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html
Deployability,update,updated,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_nwe. View page source. gnomad_ld_variant_indices_nwe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html
Deployability,update,updated,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_seu. View page source. gnomad_ld_variant_indices_seu. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html
Deployability,update,updated,".1.1, None); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'gene': str; 'transcript': str; 'obs_mis': int32; 'exp_mis': float64; 'oe_mis': float64; 'mu_mis': float64; 'possible_mis': int32; 'obs_mis_pphen': int32; 'exp_mis_pphen': float64; 'oe_mis_pphen': float64; 'possible_mis_pphen': int32; 'obs_syn': int32; 'exp_syn': float64; 'oe_syn': float64; 'mu_syn': float64; 'possible_syn': int32; 'obs_lof': int32; 'mu_lof': float64; 'possible_lof': int32; 'exp_lof': float64; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': str; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int32; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int32; 'classic_caf': float64; 'max_af': float64; 'no_lofs': int32; 'obs_het_lof': int32; 'obs_hom_lof': int32; 'defined': int32; 'p': float64; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': int32; 'cds_length': int32; 'num_coding_exons': int32; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': str; 'chromosome': str; 'start_position': int32; 'end_position': int32; ----------------------------------------; Key: ['gene']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_lof_metrics.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_lof_metrics.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; Schema (2.1, GRCh37). gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d01. View page source. gnomad_mnv_genome_d01. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; Schema (2.1, GRCh37). gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d02. View page source. gnomad_mnv_genome_d02. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; Schema (2.1, GRCh37). gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d03. View page source. gnomad_mnv_genome_d03. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; Schema (2.1, GRCh37). gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d04. View page source. gnomad_mnv_genome_d04. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; Schema (2.1, GRCh37). gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d05. View page source. gnomad_mnv_genome_d05. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; Schema (2.1, GRCh37). gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d06. View page source. gnomad_mnv_genome_d06. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; Schema (2.1, GRCh37). gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d07. View page source. gnomad_mnv_genome_d07. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; Schema (2.1, GRCh37). gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d08. View page source. gnomad_mnv_genome_d08. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; Schema (2.1, GRCh37). gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d09. View page source. gnomad_mnv_genome_d09. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html
Deployability,update,updated,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; Schema (2.1, GRCh37). gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d10. View page source. gnomad_mnv_genome_d10. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html
Deployability,update,updated,"res_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; Schema (3.1, GRCh38). gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_pca_variant_loadings. View page source. gnomad_pca_variant_loadings. Versions: 2.1, 3.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'loadings': array<float64>; 'pca_af': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html
Performance,load,loadings,"res_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; Schema (3.1, GRCh38). gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_pca_variant_loadings. View page source. gnomad_pca_variant_loadings. Versions: 2.1, 3.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'loadings': array<float64>; 'pca_af': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html
Deployability,update,updated,".1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'gene': str; 'transcript': str; 'obs_mis': int32; 'exp_mis': float64; 'oe_mis': float64; 'mu_mis': float64; 'possible_mis': int32; 'obs_mis_pphen': int32; 'exp_mis_pphen': float64; 'oe_mis_pphen': float64; 'possible_mis_pphen': int32; 'obs_syn': int32; 'exp_syn': float64; 'oe_syn': float64; 'mu_syn': float64; 'possible_syn': int32; 'obs_lof': int32; 'mu_lof': float64; 'possible_lof': int32; 'exp_lof': float64; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': str; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int32; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int32; 'classic_caf': float64; 'max_af': float64; 'no_lofs': int32; 'obs_het_lof': int32; 'obs_hom_lof': int32; 'defined': int32; 'p': float64; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': int32; 'cds_length': int32; 'num_coding_exons': int32; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': str; 'chromosome': str; 'start_position': int32; 'end_position': int32; ----------------------------------------; Key: ['gene']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html
Deployability,update,updated,"al': array<int64>; 'exp_lof_afr': array<float64>; 'obs_lof_afr': array<int64>; 'exp_lof_amr': array<float64>; 'obs_lof_amr': array<int64>; 'exp_lof_eas': array<float64>; 'obs_lof_eas': array<int64>; 'exp_lof_nfe': array<float64>; 'obs_lof_nfe': array<int64>; 'exp_lof_sas': array<float64>; 'obs_lof_sas': array<int64>; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': set<str>; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int64; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int64; 'n_sites_array': array<int64>; 'classic_caf': float64; 'max_af': float64; 'classic_caf_array': array<float64>; 'no_lofs': int64; 'obs_het_lof': int64; 'obs_hom_lof': int64; 'defined': int64; 'pop_no_lofs': dict<str, int64>; 'pop_obs_het_lof': dict<str, int64>; 'pop_obs_hom_lof': dict<str, int64>; 'pop_defined': dict<str, int64>; 'p': float64; 'pop_p': dict<str, float64>; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': str; 'cds_length': int64; 'num_coding_exons': int64; 'interval': interval<locus<GRCh37>>; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': float64; ----------------------------------------; Key: ['gene', 'transcript']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html
Deployability,update,updated,"hema (2.1.1, GRCh37). ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_variant_co-occurrence. View page source. gnomad_variant_co-occurrence. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'max_freq': float64; 'least_consequence': str; 'same_haplotype_em_probability_cutoff': float64; 'different_haplotypes_em_probability_cutoff': float64; 'global_annotation_descriptions': struct {; max_freq: str,; least_consequence: str,; same_haplotype_em_probability_cutoff: str,; different_haplotypes_em_probability_cutoff: str; }; 'row_annotation_descriptions': struct {; locus1: str,; alleles1: str,; locus2: str,; alleles2: str,; phase_info: struct {; description: str,; gt_counts: str,; em: struct {; hap_counts: str,; p_chet: str,; same_haplotype: str,; different_haplotype: str; }; }; }; ----------------------------------------; Row fields:; 'locus1': locus<GRCh37>; 'alleles1': array<str>; 'locus2': locus<GRCh37>; 'alleles2': array<str>; 'phase_info': dict<str, struct {; gt_counts: array<int32>,; em: struct {; hap_counts: array<float64>,; p_chet: float64,; same_haplotype: bool,; different_haplotype: bool; }; }>; ----------------------------------------; Key: ['locus1', 'alleles1', 'locus2', 'alleles2']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html
Deployability,update,updated,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations. View page source. GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html
Deployability,update,updated,"omad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. View page source. GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html
Deployability,update,updated,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations. View page source. GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html
Deployability,update,updated,"d_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_all_snp_gene_associations. View page source. GTEx_eQTL_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.MatrixTable. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'tissue': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'tss_distance': int32; ----------------------------------------; Entry fields:; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Column key: ['tissue']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html
Deployability,update,updated,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Aorta_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html
Deployability,update,updated,"nt_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Coronary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html
Deployability,update,updated,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html
Deployability,update,updated,"iant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html
Deployability,update,updated,"est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html
Deployability,update,updated,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html
Deployability,update,updated,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html
Deployability,update,updated,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html
Deployability,update,updated,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html
Deployability,update,updated,"omad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html
Deployability,update,updated,"ndices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html
Deployability,update,updated,"ices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html
Deployability,update,updated,"t; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html
Deployability,update,updated,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html
Deployability,update,updated,"ant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html
Deployability,update,updated,"; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html
Deployability,update,updated,"as; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations. View page source. GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html
Deployability,update,updated,"_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. View page source. GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html
Deployability,update,updated,"dices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. View page source. GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html
Deployability,update,updated,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations. View page source. GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html
Deployability,update,updated,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Colon_Transverse_all_snp_gene_associations. View page source. GTEx_eQTL_Colon_Transverse_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html
Deployability,update,updated,"s_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html
Deployability,update,updated,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html
Deployability,update,updated,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html
Deployability,update,updated,"; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations. View page source. GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html
Deployability,update,updated,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations. View page source. GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html
Deployability,update,updated,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations. View page source. GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html
Deployability,update,updated,"asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Liver_all_snp_gene_associations. View page source. GTEx_eQTL_Liver_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html
Deployability,update,updated,"s_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Lung_all_snp_gene_associations. View page source. GTEx_eQTL_Lung_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html
Deployability,update,updated,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations. View page source. GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html
Deployability,update,updated,"nt_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations. View page source. GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html
Deployability,update,updated,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations. View page source. GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html
Deployability,update,updated,"asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Ovary_all_snp_gene_associations. View page source. GTEx_eQTL_Ovary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html
Deployability,update,updated,"nomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Pancreas_all_snp_gene_associations. View page source. GTEx_eQTL_Pancreas_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html
Deployability,update,updated,"mad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Pituitary_all_snp_gene_associations. View page source. GTEx_eQTL_Pituitary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html
Deployability,update,updated,"nomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Prostate_all_snp_gene_associations. View page source. GTEx_eQTL_Prostate_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html
Deployability,update,updated,"t_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. View page source. GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html
Deployability,update,updated,"_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. View page source. GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html
Deployability,update,updated,"ant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. View page source. GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html
Deployability,update,updated,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Spleen_all_snp_gene_associations. View page source. GTEx_eQTL_Spleen_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html
Deployability,update,updated," gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Stomach_all_snp_gene_associations. View page source. GTEx_eQTL_Stomach_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html
Deployability,update,updated,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Testis_all_snp_gene_associations. View page source. GTEx_eQTL_Testis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html
Deployability,update,updated," gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Thyroid_all_snp_gene_associations. View page source. GTEx_eQTL_Thyroid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html
Deployability,update,updated,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Uterus_all_snp_gene_associations. View page source. GTEx_eQTL_Uterus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html
Deployability,update,updated,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Vagina_all_snp_gene_associations. View page source. GTEx_eQTL_Vagina_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html
Deployability,update,updated,"ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Whole_Blood_all_snp_gene_associations. View page source. GTEx_eQTL_Whole_Blood_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html
Deployability,update,updated,"e: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'gene_id': str; 'gene_symbol': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'read_count': int32; ----------------------------------------; Column key: ['s']; Row key: ['gene_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html
Deployability,update,updated,"genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'gene_id': str; 'gene_symbol': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'TPM': float64; ----------------------------------------; Column key: ['s']; Row key: ['gene_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html
Deployability,update,updated,"; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'junction_id': str; 'junction_interval': interval<locus<GRCh37>>; 'gene_id': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'gene_symbol': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'TPM': int32; ----------------------------------------; Column key: ['s']; Row key: ['junction_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html
Deployability,update,updated,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations. View page source. GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html
Deployability,update,updated,"n; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. View page source. GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html
Deployability,update,updated,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations. View page source. GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html
Deployability,update,updated,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Aorta_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Aorta_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html
Deployability,update,updated,"variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Coronary_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Coronary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Coronary_all_snp_gene_associations.html
Deployability,update,updated,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Tibial_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html
Deployability,update,updated,"d_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html
Deployability,update,updated,"ices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html
Deployability,update,updated,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html
Deployability,update,updated,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html
Deployability,update,updated,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html
Deployability,update,updated,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cortex_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html
Deployability,update,updated,"n; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html
Deployability,update,updated,"ant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html
Deployability,update,updated,"t_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html
Deployability,update,updated,"es_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html
Deployability,update,updated,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html
Deployability,update,updated,"_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html
Deployability,update,updated,"s_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html
Deployability,update,updated,"ces_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations. View page source. GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html
Deployability,update,updated,"nomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. View page source. GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html
Deployability,update,updated,"nt_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. View page source. GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html
Deployability,update,updated,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations. View page source. GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html
Deployability,update,updated,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Colon_Transverse_all_snp_gene_associations. View page source. GTEx_sQTL_Colon_Transverse_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html
Deployability,update,updated,"ndices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html
Deployability,update,updated,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html
Deployability,update,updated,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html
Deployability,update,updated,"s_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations. View page source. GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html
Deployability,update,updated,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations. View page source. GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html
Deployability,update,updated,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations. View page source. GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html
Deployability,update,updated,"ices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Liver_all_snp_gene_associations. View page source. GTEx_sQTL_Liver_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html
Deployability,update,updated,"ndices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Lung_all_snp_gene_associations. View page source. GTEx_sQTL_Lung_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html
Deployability,update,updated,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations. View page source. GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html
Deployability,update,updated,"variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations. View page source. GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html
Deployability,update,updated,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations. View page source. GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html
Deployability,update,updated,"ices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Ovary_all_snp_gene_associations. View page source. GTEx_sQTL_Ovary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html
Deployability,update,updated,"st; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Pancreas_all_snp_gene_associations. View page source. GTEx_sQTL_Pancreas_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html
Deployability,update,updated,"; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Pituitary_all_snp_gene_associations. View page source. GTEx_sQTL_Pituitary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html
Deployability,update,updated,"st; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Prostate_all_snp_gene_associations. View page source. GTEx_sQTL_Prostate_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html
Deployability,update,updated,"ariant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. View page source. GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html
Deployability,update,updated,"nomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. View page source. GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html
Deployability,update,updated,"_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. View page source. GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html
Deployability,update,updated,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Spleen_all_snp_gene_associations. View page source. GTEx_sQTL_Spleen_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html
Deployability,update,updated,"_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Stomach_all_snp_gene_associations. View page source. GTEx_sQTL_Stomach_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html
Deployability,update,updated,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Testis_all_snp_gene_associations. View page source. GTEx_sQTL_Testis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html
Deployability,update,updated,"_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Thyroid_all_snp_gene_associations. View page source. GTEx_sQTL_Thyroid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html
Deployability,update,updated,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Uterus_all_snp_gene_associations. View page source. GTEx_sQTL_Uterus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html
Deployability,update,updated,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Vagina_all_snp_gene_associations. View page source. GTEx_sQTL_Vagina_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html
Deployability,update,updated,"omad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Whole_Blood_all_snp_gene_associations. View page source. GTEx_sQTL_Whole_Blood_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html
Deployability,update,updated,"L2': float64; 'UTR_5_UCSC.flanking.500L2': float64; 'WeakEnhancer_HoffmanL2': float64; 'WeakEnhancer_Hoffman.flanking.500L2': float64; 'GERP.NSL2': float64; 'GERP.RSsup4L2': float64; 'MAFbin1L2': float64; 'MAFbin2L2': float64; 'MAFbin3L2': float64; 'MAFbin4L2': float64; 'MAFbin5L2': float64; 'MAFbin6L2': float64; 'MAFbin7L2': float64; 'MAFbin8L2': float64; 'MAFbin9L2': float64; 'MAFbin10L2': float64; 'MAF_Adj_Predicted_Allele_AgeL2': float64; 'MAF_Adj_LLD_AFRL2': float64; 'Recomb_Rate_10kbL2': float64; 'Nucleotide_Diversity_10kbL2': float64; 'Backgrd_Selection_StatL2': float64; 'CpG_Content_50kbL2': float64; 'MAF_Adj_ASMCL2': float64; 'GTEx_eQTL_MaxCPPL2': float64; 'BLUEPRINT_H3K27acQTL_MaxCPPL2': float64; 'BLUEPRINT_H3K4me1QTL_MaxCPPL2': float64; 'BLUEPRINT_DNA_methylation_MaxCPPL2': float64; 'synonymousL2': float64; 'non_synonymousL2': float64; 'Conserved_Vertebrate_phastCons46wayL2': float64; 'Conserved_Vertebrate_phastCons46way.flanking.500L2': float64; 'Conserved_Mammal_phastCons46wayL2': float64; 'Conserved_Mammal_phastCons46way.flanking.500L2': float64; 'Conserved_Primate_phastCons46wayL2': float64; 'Conserved_Primate_phastCons46way.flanking.500L2': float64; 'BivFlnkL2': float64; 'BivFlnk.flanking.500L2': float64; 'Human_Promoter_VillarL2': float64; 'Human_Promoter_Villar.flanking.500L2': float64; 'Human_Enhancer_VillarL2': float64; 'Human_Enhancer_Villar.flanking.500L2': float64; 'Ancient_Sequence_Age_Human_PromoterL2': float64; 'Ancient_Sequence_Age_Human_Promoter.flanking.500L2': float64; 'Ancient_Sequence_Age_Human_EnhancerL2': float64; 'Ancient_Sequence_Age_Human_Enhancer.flanking.500L2': float64; 'Human_Enhancer_Villar_Species_Enhancer_CountL2': float64; 'Human_Promoter_Villar_ExACL2': float64; 'Human_Promoter_Villar_ExAC.flanking.500L2': float64; 'locus': locus<GRCh37>; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html
Deployability,update,updated,"_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; Schema (2.2, GRCh37). panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; ldsc_baselineLD_ldscores. View page source. ldsc_baselineLD_ldscores. Versions: 2.2, 1.1; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (2.2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'annotation': str; 'M_5_50': int32; 'M': int32; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'SNP': str; ----------------------------------------; Entry fields:; 'x': float64; ----------------------------------------; Column key: ['annotation']; Row key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html
Deployability,update,updated,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; Schema (0.2, GRCh37). panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_AFR. View page source. panukb_ld_scores_AFR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html
Deployability,update,updated,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; Schema (0.2, GRCh37). panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_AMR. View page source. panukb_ld_scores_AMR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html
Deployability,update,updated,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; Schema (0.2, GRCh37). panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_CSA. View page source. panukb_ld_scores_CSA. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html
Deployability,update,updated,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; Schema (0.2, GRCh37). panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_EAS. View page source. panukb_ld_scores_EAS. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html
Deployability,update,updated,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; Schema (0.2, GRCh37). panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_EUR. View page source. panukb_ld_scores_EUR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html
Deployability,update,updated,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; Schema (0.2, GRCh37). panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_MID. View page source. panukb_ld_scores_MID. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_scores_MID.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_MID.html
Deployability,update,updated,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; Schema (0.2, GRCh37). panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_AFR. View page source. panukb_ld_variant_indices_AFR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html
Deployability,update,updated,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; Schema (0.2, GRCh37). panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_AMR. View page source. panukb_ld_variant_indices_AMR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html
Deployability,update,updated,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; Schema (0.2, GRCh37). panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_CSA. View page source. panukb_ld_variant_indices_CSA. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html
Deployability,update,updated,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; Schema (0.2, GRCh37). panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_EAS. View page source. panukb_ld_variant_indices_EAS. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html
Deployability,update,updated,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; Schema (0.2, GRCh37). panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_EUR. View page source. panukb_ld_variant_indices_EUR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html
Deployability,update,updated,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; Schema (0.2, GRCh37). panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_MID. View page source. panukb_ld_variant_indices_MID. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html
Deployability,update,updated,"t32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'freq': array<struct {; pop: str,; ac: float64,; af: float64,; an: int64,; gnomad_exomes_ac: int32,; gnomad_exomes_af: float64,; gnomad_exomes_an: int32,; gnomad_genomes_ac: int32,; gnomad_genomes_af: float64,; gnomad_genomes_an: int32; }>; 'pass_gnomad_exomes': bool; 'pass_gnomad_genomes': bool; 'n_passing_populations': int32; 'high_quality': bool; 'nearest_genes': str; 'info': float64; ----------------------------------------; Entry fields:; 'meta_analysis': array<struct {; BETA: float64,; SE: float64,; Pvalue: float64,; Q: float64,; Pvalue_het: float64,; N: int32,; N_pops: int32,; AF_Allele2: float64,; AF_Cases: float64,; AF_Controls: float64; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html
Deployability,update,updated,"t32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'freq': array<struct {; pop: str,; ac: float64,; af: float64,; an: int64,; gnomad_exomes_ac: int32,; gnomad_exomes_af: float64,; gnomad_exomes_an: int32,; gnomad_genomes_ac: int32,; gnomad_genomes_af: float64,; gnomad_genomes_an: int32; }>; 'pass_gnomad_exomes': bool; 'pass_gnomad_genomes': bool; 'n_passing_populations': int32; 'high_quality': bool; 'nearest_genes': str; 'info': float64; ----------------------------------------; Entry fields:; 'meta_analysis': array<struct {; BETA: float64,; SE: float64,; Pvalue: float64,; Q: float64,; Pvalue_het: float64,; N: int32,; N_pops: int32,; AF_Allele2: float64,; AF_Cases: float64,; AF_Controls: float64; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html
Deployability,update,updated,"lineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats; Schema (0.3, GRCh37). Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_summary_stats. View page source. panukb_summary_stats. Versions: 0.3; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (0.3, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'trait_type': str; 'phenocode': str; 'pheno_sex': str; 'coding': str; 'modifier': str; 'pheno_data': array<struct {; n_cases: int32,; n_controls: int32,; heritability: float64,; saige_version: str,; inv_normalized: bool,; pop: str; }>; 'description': str; 'description_more': str; 'coding_description': str; 'category': str; 'n_cases_full_cohort_both_sexes': int64; 'n_cases_full_cohort_females': int64; 'n_cases_full_cohort_males': int64; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'gene': str; 'annotation': str; ----------------------------------------; Entry fields:; 'summary_stats': array<struct {; AF_Allele2: float64,; imputationInfo: float64,; BETA: float64,; SE: float64,; `p.value.NA`: float64,; `AF.Cases`: float64,; `AF.Controls`: float64,; Pvalue: float64,; low_confidence: bool; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/panukb_summary_stats.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_summary_stats.html
Deployability,update,updated,"ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_both_sexes. View page source. UK_Biobank_Rapid_GWAS_both_sexes. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html
Deployability,update,updated," panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_female. View page source. UK_Biobank_Rapid_GWAS_female. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html
Deployability,update,updated,"CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_male. View page source. UK_Biobank_Rapid_GWAS_male. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html
Deployability,update,updated,"hl.linalg.utils.locus_windows(ht.locus, 1); (array([0, 0, 2, 3, 3, 5]), array([2, 2, 3, 5, 5, 6])). Windows with 1cm radius:; >>> hl.linalg.utils.locus_windows(ht.locus, 1.0, coord_expr=ht.cm); (array([0, 1, 1, 3, 3, 5]), array([1, 3, 3, 5, 5, 6])). Notes; This function returns two 1-dimensional ndarrays of integers,; starts and stops, each of size equal to the number of rows.; By default, for all indices i, [starts[i], stops[i]) is the maximal; range of row indices j such that contig[i] == contig[j] and; position[i] - radius <= position[j] <= position[i] + radius.; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that has; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the function will fail.; The last example above uses centimorgan coordinates, so; [starts[i], stops[i]) is the maximal range of row indices j such; that contig[i] == contig[j] and; cm[i] - radius <= cm[j] <= cm[i] + radius.; Index ranges are start-inclusive and stop-exclusive. This function is; especially useful in conjunction with; BlockMatrix.sparsify_row_intervals(). Parameters:. locus_expr (LocusExpression) – Row-indexed locus expression on a table or matrix table.; radius (int) – Radius of window for row values.; coord_expr (Float64Expression, optional) – Row-indexed numeric expression for the row value.; Must be on the same table or matrix table as locus_expr.; By default, the row value is given by the locus position. Returns:; (numpy.ndarray of int, numpy.ndarray of int) – Tuple of start indices array and stop indices array. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/linalg/utils/index.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/linalg/utils/index.html
Availability,avail,available,"sh` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
Deployability,configurat,configuration,"))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_con",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
Integrability,message,messages,"; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`s",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
Modifiability,config,configure,"),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=nullable(enumeration(*BUILTIN_REFERENCES)),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; backend=nullable(enumeration(*BackendType.__args__)),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; ",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
Performance,load,loaded," reference genome (``'GRCh37'`` by default).; With an argument, sets the default reference genome to the argument. Returns; -------; :class:`.ReferenceGenome`; """"""; if new_default_reference is not None:; Env.hc().default_reference = new_default_reference; return None; return Env.hc().default_reference. [docs]def get_reference(name) -> ReferenceGenome:; """"""Returns the reference genome corresponding to `name`. Notes; -----. Hail's built-in references are ``'GRCh37'``, ``GRCh38'``, ``'GRCm38'``, and; ``'CanFam3'``.; The contig names and lengths come from the GATK resource bundle:; `human_g1k_v37.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""; Env.hc(); if name == 'default':; return default_reference(); else:; return Env.backend().get_reference(name). [docs]@typecheck(seed=int); def set_global_seed(seed):; """"""Deprecated. Has no effect. To ensure reproducible randomness, use the `global_seed`; argument to :func:`.init` and :func:`.reset_global_randomness`. See the :ref:`random functions <sec-random-functions>` reference docs for more. Parameters; ----------; seed : :obj:`int`; Integer used to seed Hail's random number generator; """""". warning(; 'hl.set_global_seed has no effect. See '; 'https://hail.is/docs/0.2/functions/random.html for details on '; 'ensuring reproducibility of randomness.'; ); pass. [docs]@typecheck(); def reset_global_randomness():; """"""Restore global randomness to initial state for test reproducibility."""""". Env.reset_global_randomness(). def _set_flags(**f",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
Security,access,accessing,"ault arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifyin",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
Testability,log,log,", nullable, oneof, sequenceof, sized_tupleof, typecheck, typecheck_method; from hail.utils import get_env_or_default; from hail.utils.java import BackendType, Env, choose_backend, warning; from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration, get_gcs_requester_pays_configuration; from hailtop.fs.fs import FS; from hailtop.hail_event_loop import hail_event_loop; from hailtop.utils import secret_alnum_string. from . import __resource_str; from .backend.backend import local_jar_information; from .builtin_references import BUILTIN_REFERENCES. def _get_tmpdir(tmpdir):; if tmpdir is None:; tmpdir = '/tmp'; return tmpdir. def _get_local_tmpdir(local_tmpdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nu",MatchSource.WIKI,docs/0.2/_modules/hail/context.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html
Availability,down,downstream,"on(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; if self._row_keys is not None:; raise Not",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
Deployability,pipeline,pipeline,"s); return self._copy(col_keys=col_key, computed_col_key=computed_key). def _check_bindings(self, caller, new_bindings, indices):; empty = []. def iter_option(o):; return o if o is not None else empty. if indices == self._parent._row_indices:; fixed_fields = [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
Energy Efficiency,reduce,reduces,"on(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; if self._row_keys is not None:; raise Not",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
Integrability,depend,dependent,"ields_referenced)). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_rows(self, expr, _localize=True) -> Any:; """"""Aggregate over rows to a local value. Examples; --------; Aggregate over rows:. >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl.agg.count_where(dataset.qual > 40),; ... mean_qual=hl.agg.mean(dataset.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful references to fields that are not global or indexed by row. This method should be thought of as a more convenient alternative to; the following:. >>> rows_table = dataset.rows(); >>> rows_table.aggregate(hl.struct(n_high_quality=hl.agg.count_where(rows_table.qual > 40),; ... mean_qual=hl.agg.mean(rows_table.qual))). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; base, _ = self._process_joins(expr); analyze('MatrixTable.aggregate_rows', expr, self._global_indices, {self._row_axis}); rows_table = ir.MatrixRowsTable(base._mir); subst_query = ir.subst(expr._ir, {}, {'va': ir.Ref('row', rows_table.typ.row_type)}). agg_ir = ir.TableAggregate(rows_table, subst_query); if _localize:; return Env.backend().execute(ir.MakeTuple([agg_ir]))[0]; else:; return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate_cols(self, expr, _localize=True) -> Any:; """"""Aggregate over columns to a local value. Examples; --------; Aggregate over columns:. >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful ref",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
Modifiability,extend,extend,".gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). See Also; --------; :meth:`.aggregate`. Returns; -------; :class:`.MatrixTable`; Aggregated matrix table.; """"""; assert self._row_keys is not None or self._col_keys is not None. defined_exprs = []; for e in [self._row_fields, self._col_fields, self._entry_fields]:; if e is not None:; defined_exprs.append(e); for e in [self._computed_row_key, self._computed_col_key]:; if e is not None:; defined_exprs.extend(e.values()). def promote_none(e):; return hl.struct() if e is None else e. entry_exprs = promote_none(self._entry_fields); if len(entry_exprs) == 0:; warning(""'GroupedMatrixTable.result': No entry fields were defined.""). base, cleanup = self._parent._process_joins(*defined_exprs). if self._col_keys is not None:; cck = self._computed_col_key or {}; computed_key_uids = {k: Env.get_uid() for k in cck}; modified_keys = [computed_key_uids.get(k, k) for k in self._col_keys]; mt = MatrixTable(; ir.MatrixAggregateColsByKey(; ir.MatrixMapCols(; base._mir,; self._parent.col.annotate(**{computed_key_uids[k]: v for k, v in cck.items()})._ir,; modified_keys,; ),; entry_exprs._ir,; promote_none(self._col_fields)._ir,; ); ); if cck:; mt = mt.rename({v: k for k, v in computed_key_uids.items()}); else:; cck = self._computed_row_key or {}; computed_key_uids = {k: Env.get_uid() for k in cck}; modified_keys = [computed_key_uids.get(k, k) for k in self._row_keys]; mt = MatrixTable(; ir.MatrixAggregateRow",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
Performance,optimiz,optimizer," [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----;",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
Safety,unsafe,unsafe,"""Key columns by a new set of fields. See :meth:`.Table.key_by` for more information on defining a key. Parameters; ----------; keys : varargs of :class:`str` or :class:`.Expression`.; Column fields to key by.; named_keys : keyword args of :class:`.Expression`.; Column fields to key by.; Returns; -------; :class:`.MatrixTable`; """"""; key_fields, computed_keys = get_key_by_exprs(""MatrixTable.key_cols_by"", keys, named_keys, self._col_indices). if not computed_keys:; return MatrixTable(ir.MatrixMapCols(self._mir, self._col._ir, key_fields)); else:; new_col = self.col.annotate(**computed_keys); base, cleanup = self._process_joins(new_col). return cleanup(MatrixTable(ir.MatrixMapCols(base._mir, new_col._ir, key_fields))). @typecheck_method(new_key=str); def _key_rows_by_assert_sorted(self, *new_key):; rk_names = list(self.row_key); i = 0; while i < min(len(new_key), len(rk_names)):; if new_key[i] != rk_names[i]:; break; i += 1. if i < 1:; raise ValueError(; f'cannot implement an unsafe sort with no shared key:\n new key: {new_key}\n old key: {rk_names}'; ). return MatrixTable(ir.MatrixKeyRowsBy(self._mir, list(new_key), is_sorted=True)). [docs] @typecheck_method(keys=oneof(str, Expression), named_keys=expr_any); def key_rows_by(self, *keys, **named_keys) -> 'MatrixTable':; """"""Key rows by a new set of fields. Examples; --------. >>> dataset_result = dataset.key_rows_by('locus'); >>> dataset_result = dataset.key_rows_by(dataset['locus']); >>> dataset_result = dataset.key_rows_by(**dataset.row_key.drop('alleles')). All of these expressions key the dataset by the 'locus' field, dropping; the 'alleles' field from the row key. >>> dataset_result = dataset.key_rows_by(contig=dataset['locus'].contig,; ... position=dataset['locus'].position,; ... alleles=dataset['alleles']). This keys the dataset by the newly defined fields, 'contig' and 'position',; and the 'alleles' field. The old row key field, 'locus', is preserved as; a non-key field. Notes; -----; See :meth:`.Table.key_by` fo",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
Testability,assert,assert," expressions to group by. Returns; -------; :class:`.GroupedMatrixTable`; Grouped matrix, can be used to call :meth:`.GroupedMatrixTable.aggregate`.; """"""; if self._row_keys is not None:; raise NotImplementedError(""GroupedMatrixTable is already grouped by rows; cannot also group by cols.""); if self._col_keys is not None:; raise NotImplementedError(""GroupedMatrixTable is already grouped by cols.""). caller = 'group_cols_by'; col_key, computed_key = get_key_by_exprs(; caller,; exprs,; named_exprs,; self._parent._col_indices,; override_protected_indices={self._parent._global_indices, self._parent._row_indices},; ). self._check_bindings(caller, computed_key, self._parent._col_indices); return self._copy(col_keys=col_key, computed_col_key=computed_key). def _check_bindings(self, caller, new_bindings, indices):; empty = []. def iter_option(o):; return o if o is not None else empty. if indices == self._parent._row_indices:; fixed_fields = [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some place",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
Usability,guid,guide,"column order:. >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:. >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters; ----------; indices : :obj:`list` of :obj:`int`; List of old column indices. Returns; -------; :class:`.MatrixTable`; """"""; n_cols = self.count_cols(); for i in indices:; if not 0 <= i < n_cols:; raise ValueError(f""'choose_cols': expect indices between 0 and {n_cols}, found {i}""); return MatrixTable(ir.MatrixChooseCols(self._mir, indices)). [docs] def n_partitions(self) -> int:; """"""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitio",MatchSource.WIKI,docs/0.2/_modules/hail/matrixtable.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html
Availability,down,downstream,"dTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_res",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Deployability,pipeline,pipeline," object is not mutable""); self.__dict__[key] = value. def __getattr__(self, item):; if item in self.__dict__:; return self.__dict__[item]. raise AttributeError(get_nice_attr_error(self, item)). def _copy_fields_from(self, other: 'ExprContainer'):; self._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same group",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Energy Efficiency,reduce,reduces,"dTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_res",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Integrability,depend,dependent," reference expressions.; named_exprs : keyword args of type :class:`.Expression`; Field names and expressions to compute them. Returns; -------; :class:`.GroupedTable`; Grouped table; use :meth:`.GroupedTable.aggregate` to complete the aggregation.; """"""; key, computed_key = get_key_by_exprs(; 'Table.group_by', exprs, named_exprs, self._row_indices, override_protected_indices={self._global_indices}; ); return GroupedTable(self, self.row.annotate(**computed_key).select(*key)). [docs] @typecheck_method(expr=expr_any, _localize=bool); def aggregate(self, expr, _localize=True):; """"""Aggregate over rows into a local value. Examples; --------; Aggregate over rows:. >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; expr = to_expr(expr); base, _ = self._process_joins(expr); analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}). agg_ir = ir.TableAggregate(base._tir, expr._ir). if _localize:; return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]. return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; ) -> 'Table':; """"""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copie",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Modifiability,variab,variable-length,"---; Key: ['idx']; ----------------------------------------; >>> ht = ht.select_globals(ht.pops, target_date='2025-01-01'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'target_date': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------. Fields may also be selected by their name:. >>> ht = ht.select_globals('target_date'); >>> ht.globals.show(); +--------------------+; | <expr>.target_date |; +--------------------+; | str |; +--------------------+; | ""2025-01-01"" |; +--------------------+. Notes; -----; This method creates new global fields. If a created field shares its name; with a row-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.Table`; Table with specified global fields. """"""; caller = 'Table.select_globals'; new_globals = get_select_exprs(caller, exprs, named_exprs, self._global_indices, self._globals). return self._select_globals(caller, new_globals). [docs] @typecheck_method(named_exprs=expr_any); def transmute_globals(self, **named_exprs) -> 'Table':; """"""Similar to :meth:`.Table.annotate_globals`, but drops referenced fields. Notes; -----; Consider a table with global fields `population`, `area`, and `year`:. >>> ht = hl.utils.range_table(1); >>> ht = ht.annotate_globals(population=1000000, area=500, year=2020). Compute a new field, `density` from `population` and `area` and also drop the latter two; fields:. >>> ht = ht.transmute_globals(density=ht.popu",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Performance,optimiz,optimizer,"lf._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.gro",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Safety,avoid,avoid,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Security,expose,expose,"prs`. Examples; --------; In the example below, both `table1` and `table2` are keyed by one; field `ID` of type ``int``. >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""; try:; return self._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mis",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Testability,assert,assert,"xpressions. class Ascending:; def __init__(self, col):; self.col = col. def __eq__(self, other):; return isinstance(other, Ascending) and self.col == other.col. def __ne__(self, other):; return not self == other. class Descending:; def __init__(self, col):; self.col = col. def __eq__(self, other):; return isinstance(other, Descending) and self.col == other.col. def __ne__(self, other):; return not self == other. [docs]@typecheck(col=oneof(Expression, str)); def asc(col):; """"""Sort by `col` ascending."""""". return Ascending(col). [docs]@typecheck(col=oneof(Expression, str)); def desc(col):; """"""Sort by `col` descending."""""". return Descending(col). class ExprContainer:; # this can only grow as big as the object dir, so no need to worry about memory leak; _warned_about: ClassVar = set(). def __init__(self):; self._fields: Dict[str, Expression] = {}; self._fields_inverse: Dict[Expression, str] = {}; self._dir = set(dir(self)); super(ExprContainer, self).__init__(). def _set_field(self, key, value):; assert key not in self._fields_inverse, key; self._fields[key] = value; self._fields_inverse[value] = key. # key is in __dir for methods; # key is in __dict__ for private class fields; if key in self._dir or key in self.__dict__:; if key not in ExprContainer._warned_about:; ExprContainer._warned_about.add(key); warning(; f""Name collision: field {key!r} already in object dict. ""; f""\n This field must be referenced with __getitem__ syntax: obj[{key!r}]""; ); else:; self.__dict__[key] = value. def _get_field(self, item) -> Expression:; if item in self._fields:; return self._fields[item]. raise LookupError(get_nice_field_error(self, item)). def __iter__(self):; raise TypeError(f""'{self.__class__.__name__}' object is not iterable""). def __delattr__(self, item):; if not item[0] == '_':; raise NotImplementedError(f""'{self.__class__.__name__}' object is not mutable""). def __setattr__(self, key, value):; if not key[0] == '_':; raise NotImplementedError(f""'{self.__class__.__name__}' object ",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Usability,guid,guide,"amed_exprs.items():; analyze(f'{caller}: ({name!r})', expr, self._parent._global_indices, {self._parent._row_axis}); check_collisions(caller, list(named_exprs), self._parent._row_indices); if not named_exprs.keys().isdisjoint(set(self._key_expr)):; intersection = set(named_exprs.keys()) & set(self._key_expr); raise ValueError(; f'GroupedTable.aggregate: Group names and aggregration expression names overlap: {intersection}'; ). base, _ = self._parent._process_joins(self._key_expr, *named_exprs.values()). key_struct = self._key_expr; return Table(; ir.TableKeyByAndAggregate(; base._tir, hl.struct(**named_exprs)._ir, key_struct._ir, self._npartitions, self._buffer_size; ); ). [docs]class Table(ExprContainer):; """"""Hail's distributed implementation of a dataframe or SQL table. Use :func:`.read_table` to read a table that was written with; :meth:`.Table.write`. Use :meth:`.to_spark` and :meth:`.Table.from_spark`; to inter-operate with PySpark's; `SQL <https://spark.apache.org/docs/latest/sql-programming-guide.html>`__ and; `machine learning <https://spark.apache.org/docs/latest/ml-guide.html>`__; functionality. Examples; --------. The examples below use ``table1`` and ``table2``, which are imported; from text files using :func:`.import_table`. >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). .. code-",MatchSource.WIKI,docs/0.2/_modules/hail/table.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html
Deployability,update,updated,"﻿. Hail | ; hailtop.frozendict. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.frozendict. Source code for hailtop.frozendict; from collections.abc import Mapping; from typing import Dict, Generic, TypeVar. T = TypeVar(""T""); U = TypeVar(""U""). [docs]class frozendict(Mapping, Generic[T, U]):; """"""; An object representing an immutable dictionary. >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a `frozendict`:. >>> dict(frozendict({'a': 1, 'b': 2})). Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.my_dict.take(5)``. This is rare; it is much; more common to manipulate the :class:`.DictExpression` object, which is; constructed using :func:`.dict`. This class is necessary because hail; supports using dicts as keys to other dicts or as elements in sets, while; python does not. """""". def __init__(self, d: Dict[T, U]):; self.d = d.copy(). def __getitem__(self, k: T) -> U:; return self.d[k]. def __hash__(self) -> int:; return hash(frozenset(self.items())). def __len__(self) -> int:; return len(self.d). def __iter__(self):; return iter(self.d). def __repr__(self):; return f'frozendict({self.d!r})'. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hailtop/frozendict.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html
Security,hash,hash,"﻿. Hail | ; hailtop.frozendict. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.frozendict. Source code for hailtop.frozendict; from collections.abc import Mapping; from typing import Dict, Generic, TypeVar. T = TypeVar(""T""); U = TypeVar(""U""). [docs]class frozendict(Mapping, Generic[T, U]):; """"""; An object representing an immutable dictionary. >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a `frozendict`:. >>> dict(frozendict({'a': 1, 'b': 2})). Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.my_dict.take(5)``. This is rare; it is much; more common to manipulate the :class:`.DictExpression` object, which is; constructed using :func:`.dict`. This class is necessary because hail; supports using dicts as keys to other dicts or as elements in sets, while; python does not. """""". def __init__(self, d: Dict[T, U]):; self.d = d.copy(). def __getitem__(self, k: T) -> U:; return self.d[k]. def __hash__(self) -> int:; return hash(frozenset(self.items())). def __len__(self) -> int:; return len(self.d). def __iter__(self):; return iter(self.d). def __repr__(self):; return f'frozendict({self.d!r})'. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hailtop/frozendict.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html
Availability,avail,available,"> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html
Deployability,update,updated,"={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region]; for dataset in datasets[name]['versions']; if all([dataset['version'] == version, dataset['reference_genome'] == reference_genome]); ]; assert len(path) == 1; path = path[0]; if path.startswith('s3://'):; try:; dataset = _read_dataset(path); except hl.utils.java.FatalError:; dataset = _read_dataset(path.replace('s3://', 's3a://')); else:; dataset = _read_dataset(path); return dataset. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html
Performance,load,load,"hail.experimental.datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.datasets. Source code for hail.experimental.datasets; from typing import Optional, Union. import hail as hl; from hail.matrixtable import MatrixTable; from hail.table import Table. from .datasets_metadata import get_datasets_metadata. def _read_dataset(path: str) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, `",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html
Testability,assert,assert,"={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region]; for dataset in datasets[name]['versions']; if all([dataset['version'] == version, dataset['reference_genome'] == reference_genome]); ]; assert len(path) == 1; path = path[0]; if path.startswith('s3://'):; try:; dataset = _read_dataset(path); except hl.utils.java.FatalError:; dataset = _read_dataset(path.replace('s3://', 's3a://')); else:; dataset = _read_dataset(path); return dataset. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/datasets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html
Availability,avail,available,"rsion` has two constructors: :func:`.from_json` and; :func:`.get_region`. Parameters; ----------; url : :obj:`dict` or :obj:`str`; Nested dictionary of URLs containing key: value pairs, like; ``cloud: {region: url}`` if using :func:`.from_json` constructor,; or a string with the URL from appropriate region if using the; :func:`.get_region` constructor.; version : :obj:`str`, optional; String of dataset version, if not ``None``.; reference_genome : :obj:`str`, optional; String of dataset reference genome, if not ``None``.; """""". @staticmethod; def from_json(doc: dict, cloud: str) -> Optional['DatasetVersion']:; """"""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; assert 'url' in doc, doc; assert 'version' in doc, doc; assert 'reference_genome' in doc, doc; if cloud in doc['url']:; return DatasetVersion(doc['url'][cloud], doc['version'], doc['reference_genome']); else:; return None. @staticmethod; def get_region(name: str, versions: List['DatasetVersion'], region: str) -> List['DatasetVersion']:; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; available_version",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/db.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html
Deployability,configurat,configuration,"rn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; return hl.read_table(self.url)._maybe_flexindex_table_by_expr(indexer_key_expr, all_matches=all_matches). class Dataset:; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """""". @staticmethod; def from_name_and_json(name: str, doc: dict, region: str, cloud: str) -> Optional['Dataset']:; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/db.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html
Integrability,message,message,".; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; current_version = self.version; available_regions = [k for k in self.url.keys()]; valid_region = region in available_regions; if not valid_region:; message = (; f'\nName: {name}\n'; f'Version: {current_version}\n'; f'This dataset exists but is not yet available in the'; f' {region} region bucket.\n'; f'Dataset is currently available in the'; f' {"", "".join(available_regions)} region bucket(s).\n'; f'Reach out to the Hail team at https://discuss.hail.is/'; f' to request this dataset in your region.'; ); warnings.warn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of ex",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/db.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html
Modifiability,config,configuration,"rn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; return hl.read_table(self.url)._maybe_flexindex_table_by_expr(indexer_key_expr, all_matches=all_matches). class Dataset:; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """""". @staticmethod; def from_name_and_json(name: str, doc: dict, region: str, cloud: str) -> Optional['Dataset']:; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/db.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html
Security,access,access,"atasets_metadata; from .lens import MatrixRows, TableRows. class DatasetVersion:; """""":class:`DatasetVersion` has two constructors: :func:`.from_json` and; :func:`.get_region`. Parameters; ----------; url : :obj:`dict` or :obj:`str`; Nested dictionary of URLs containing key: value pairs, like; ``cloud: {region: url}`` if using :func:`.from_json` constructor,; or a string with the URL from appropriate region if using the; :func:`.get_region` constructor.; version : :obj:`str`, optional; String of dataset version, if not ``None``.; reference_genome : :obj:`str`, optional; String of dataset reference genome, if not ``None``.; """""". @staticmethod; def from_json(doc: dict, cloud: str) -> Optional['DatasetVersion']:; """"""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; assert 'url' in doc, doc; assert 'version' in doc, doc; assert 'reference_genome' in doc, doc; if cloud in doc['url']:; return DatasetVersion(doc['url'][cloud], doc['version'], doc['reference_genome']); else:; return None. @staticmethod; def get_region(name: str, versions: List['DatasetVersion'], region: str) -> List['DatasetVersion']:; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/db.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html
Testability,assert,assert,"ary of URLs containing key: value pairs, like; ``cloud: {region: url}`` if using :func:`.from_json` constructor,; or a string with the URL from appropriate region if using the; :func:`.get_region` constructor.; version : :obj:`str`, optional; String of dataset version, if not ``None``.; reference_genome : :obj:`str`, optional; String of dataset reference genome, if not ``None``.; """""". @staticmethod; def from_json(doc: dict, cloud: str) -> Optional['DatasetVersion']:; """"""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; assert 'url' in doc, doc; assert 'version' in doc, doc; assert 'reference_genome' in doc, doc; if cloud in doc['url']:; return DatasetVersion(doc['url'][cloud], doc['version'], doc['reference_genome']); else:; return None. @staticmethod; def get_region(name: str, versions: List['DatasetVersion'], region: str) -> List['DatasetVersion']:; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); retu",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/db.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html
Deployability,update,updated," wheel 712 Jan 25 17:19 index.tsv; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-00.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-01.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-02.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-03.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-04.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-05.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-06.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-07.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-08.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-09.tsv.bgz. $ zcat output/cols_files/part-00.tsv.bgz; #{""col_idx"":0}; row_idx x; 0 6.2501e-02; 1 7.0083e-01; 2 3.6452e-01; 3 4.4170e-01; 4 7.9177e-02; 5 6.2392e-01; 6 5.9920e-01; 7 9.7540e-01; 8 8.4848e-01; 9 3.7423e-01. Due to overhead and file system limits related to having large numbers; of open files, this function will iteratively export groups of columns.; The `batch_size` parameter can control the size of these groups. Parameters; ----------; mt : :class:`.MatrixTable`; path : :obj:`int`; Path (directory to write to.; batch_size : :obj:`int`; Number of columns to write per iteration.; bgzip : :obj:`bool`; BGZip output files.; header_json_in_file : :obj:`bool`; Include JSON header in each component file (if False, only written to index.tsv); """"""; if use_string_key_as_file_name and not (len(mt.col_key) == 1 and mt.col_key[0].dtype == hl.tstr):; raise ValueError(; f'parameter ""use_string_key_as_file_name"" requires a single string column key, found {list(mt.col_key.dtype.values())}'; ); hl.utils.java.Env.backend().execute(; hl.ir.MatrixToValueApply(; mt._mir,; {; 'name': 'MatrixExportEntriesByCol',; 'parallelism': batch_size,; 'path': path,; 'bgzip': bgzip,; 'headerJsonInFile': header_json_in_file,; 'useStringKeyAsFileName': use_string_key_as_file_name,; },; ); ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/export_entries_by_col.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/export_entries_by_col.html
Deployability,update,updated," path=str, overwrite=bool); def write_expression(expr, path, overwrite=False):; """"""Write an Expression. In the same vein as Python's pickle, write out an expression; that does not have a source (such as one that comes from; Table.aggregate with _localize=False). Example; -------; >>> ht = hl.utils.range_table(100).annotate(x=hl.rand_norm()); >>> mean_norm = ht.aggregate(hl.agg.mean(ht.x), _localize=False); >>> mean_norm; >>> hl.eval(mean_norm); >>> hl.experimental.write_expression(mean_norm, 'output/expression.he'). Parameters; ----------. expr : :class:`~.Expression`; Expression to write.; path : :class:`str`; Path to which to write expression.; Suggested extension: .he (hail expression).; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination. Returns; -------; None; """"""; source = expr._indices.source; if source is not None:; analyze('write_expression.expr', expr, source._global_indices); source = source.select_globals(__expr=expr); expr = source.index_globals().__expr; hl.utils.range_table(1).filter(False).key_by().drop('idx').annotate_globals(expr=expr).write(; path, overwrite=overwrite; ). [docs]@typecheck(path=str, _assert_type=nullable(hail_type)); def read_expression(path, _assert_type=None):; """"""Read an :class:`~.Expression` written with :func:`.experimental.write_expression`. Example; -------; >>> hl.experimental.write_expression(hl.array([1, 2]), 'output/test_expression.he'); >>> expression = hl.experimental.read_expression('output/test_expression.he'); >>> hl.eval(expression). Parameters; ----------. path : :class:`str`; File to read. Returns; -------; :class:`~.Expression`; """"""; _assert_table_type = None; _load_refs = True; if _assert_type:; _assert_table_type = ttable(hl.tstruct(expr=_assert_type), row_type=hl.tstruct(), row_key=[]); _load_refs = False; return hl.read_table(path, _assert_type=_assert_table_type, _load_refs=_load_refs).index_globals().expr. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/expressions.html
Deployability,update,updated,"; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.filtering_allele_frequency. Source code for hail.experimental.filtering_allele_frequency; from hail.expr.expressions import Float64Expression, expr_float64, expr_int32; from hail.expr.functions import _func; from hail.expr.types import tfloat64; from hail.typecheck import typecheck. [docs]@typecheck(ac=expr_int32, an=expr_int32, ci=expr_float64); def filtering_allele_frequency(ac, an, ci) -> Float64Expression:; """"""; Computes a filtering allele frequency (described below); for `ac` and `an` with confidence `ci`. The filtering allele frequency is the highest true population allele frequency; for which the upper bound of the `ci` (confidence interval) of allele count; under a Poisson distribution is still less than the variant's observed; `ac` (allele count) in the reference sample, given an `an` (allele number). This function defines a ""filtering AF"" that represents; the threshold disease-specific ""maximum credible AF"" at or below which; the disease could not plausibly be caused by that variant. A variant with; a filtering AF >= the maximum credible AF for the disease under consideration; should be filtered, while a variant with a filtering AF below the maximum; credible remains a candidate. This filtering AF is not disease-specific:; it can be applied to any disease of interest by comparing with a; user-defined disease-specific maximum credible AF. For more details, see: `Whiffin et al., 2017 <https://www.nature.com/articles/gim201726>`__. Parameters; ----------; ac : int or :class:`.Expression` of type :py:data:`.tint32`; an : int or :class:`.Expression` of type :py:data:`.tint32`; ci : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""filtering_allele_frequency"", tfloat64, ac, an, ci). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html
Availability,error,error," = right.localize_entries('right_entries', 'right_cols'). ht = left_t.join(right_t, how='outer'); ht = ht.annotate_globals(; left_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.left_cols.map(lambda x: hl.tuple([x[f] for f in left.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; right_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.right_cols.map(lambda x: hl.tuple([x[f] for f in right.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; ); ht = ht.annotate_globals(; key_indices=hl.array(ht.left_keys.key_set().union(ht.right_keys.key_set())); .map(lambda k: hl.struct(k=k, left_indices=ht.left_keys.get(k), right_indices=ht.right_keys.get(k))); .flatmap(; lambda s: hl.case(); .when(; hl.is_defined(s.left_indices) & hl.is_defined(s.right_indices),; hl.range(0, s.left_indices.length()).flatmap(; lambda i: hl.range(0, s.right_indices.length()).map(; lambda j: hl.struct(k=s.k, left_index=s.left_indices[i], right_index=s.right_indices[j]); ); ),; ); .when(; hl.is_defined(s.left_indices),; s.left_indices.map(lambda elt: hl.struct(k=s.k, left_index=elt, right_index=hl.missing('int32'))),; ); .when(; hl.is_defined(s.right_indices),; s.right_indices.map(lambda elt: hl.struct(k=s.k, left_index=hl.missing('int32'), right_index=elt)),; ); .or_error('assertion error'); ); ); ht = ht.annotate(; __entries=ht.key_indices.map(; lambda s: hl.struct(left_entry=ht.left_entries[s.left_index], right_entry=ht.right_entries[s.right_index]); ); ); ht = ht.annotate_globals(; __cols=ht.key_indices.map(; lambda s: hl.struct(; **{f: s.k[i] for i, f in enumerate(left.col_key)},; left_col=ht.left_cols[s.left_index],; right_col=ht.right_cols[s.right_index],; ); ); ); ht = ht.drop('left_entries', 'left_cols', 'left_keys', 'right_entries', 'right_cols', 'right_keys', 'key_indices'); return ht._unlocalize_entries('__entries', '__cols', list(left.col_key)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html
Deployability,update,updated," = right.localize_entries('right_entries', 'right_cols'). ht = left_t.join(right_t, how='outer'); ht = ht.annotate_globals(; left_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.left_cols.map(lambda x: hl.tuple([x[f] for f in left.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; right_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.right_cols.map(lambda x: hl.tuple([x[f] for f in right.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; ); ht = ht.annotate_globals(; key_indices=hl.array(ht.left_keys.key_set().union(ht.right_keys.key_set())); .map(lambda k: hl.struct(k=k, left_indices=ht.left_keys.get(k), right_indices=ht.right_keys.get(k))); .flatmap(; lambda s: hl.case(); .when(; hl.is_defined(s.left_indices) & hl.is_defined(s.right_indices),; hl.range(0, s.left_indices.length()).flatmap(; lambda i: hl.range(0, s.right_indices.length()).map(; lambda j: hl.struct(k=s.k, left_index=s.left_indices[i], right_index=s.right_indices[j]); ); ),; ); .when(; hl.is_defined(s.left_indices),; s.left_indices.map(lambda elt: hl.struct(k=s.k, left_index=elt, right_index=hl.missing('int32'))),; ); .when(; hl.is_defined(s.right_indices),; s.right_indices.map(lambda elt: hl.struct(k=s.k, left_index=hl.missing('int32'), right_index=elt)),; ); .or_error('assertion error'); ); ); ht = ht.annotate(; __entries=ht.key_indices.map(; lambda s: hl.struct(left_entry=ht.left_entries[s.left_index], right_entry=ht.right_entries[s.right_index]); ); ); ht = ht.annotate_globals(; __cols=ht.key_indices.map(; lambda s: hl.struct(; **{f: s.k[i] for i, f in enumerate(left.col_key)},; left_col=ht.left_cols[s.left_index],; right_col=ht.right_cols[s.right_index],; ); ); ); ht = ht.drop('left_entries', 'left_cols', 'left_keys', 'right_entries', 'right_cols', 'right_keys', 'key_indices'); return ht._unlocalize_entries('__entries', '__cols', list(left.col_key)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html
Testability,assert,assertion," = right.localize_entries('right_entries', 'right_cols'). ht = left_t.join(right_t, how='outer'); ht = ht.annotate_globals(; left_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.left_cols.map(lambda x: hl.tuple([x[f] for f in left.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; right_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.right_cols.map(lambda x: hl.tuple([x[f] for f in right.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; ); ht = ht.annotate_globals(; key_indices=hl.array(ht.left_keys.key_set().union(ht.right_keys.key_set())); .map(lambda k: hl.struct(k=k, left_indices=ht.left_keys.get(k), right_indices=ht.right_keys.get(k))); .flatmap(; lambda s: hl.case(); .when(; hl.is_defined(s.left_indices) & hl.is_defined(s.right_indices),; hl.range(0, s.left_indices.length()).flatmap(; lambda i: hl.range(0, s.right_indices.length()).map(; lambda j: hl.struct(k=s.k, left_index=s.left_indices[i], right_index=s.right_indices[j]); ); ),; ); .when(; hl.is_defined(s.left_indices),; s.left_indices.map(lambda elt: hl.struct(k=s.k, left_index=elt, right_index=hl.missing('int32'))),; ); .when(; hl.is_defined(s.right_indices),; s.right_indices.map(lambda elt: hl.struct(k=s.k, left_index=hl.missing('int32'), right_index=elt)),; ); .or_error('assertion error'); ); ); ht = ht.annotate(; __entries=ht.key_indices.map(; lambda s: hl.struct(left_entry=ht.left_entries[s.left_index], right_entry=ht.right_entries[s.right_index]); ); ); ht = ht.annotate_globals(; __cols=ht.key_indices.map(; lambda s: hl.struct(; **{f: s.k[i] for i, f in enumerate(left.col_key)},; left_col=ht.left_cols[s.left_index],; right_col=ht.right_cols[s.right_index],; ); ); ); ht = ht.drop('left_entries', 'left_cols', 'left_keys', 'right_entries', 'right_cols', 'right_keys', 'key_indices'); return ht._unlocalize_entries('__entries', '__cols', list(left.col_key)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html
Availability,checkpoint,checkpoint," : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; path,; min_partitions=min_partitions,; comment='#',; no_header=True,; types={'f3': hl.tint, 'f4': hl.tint, 'f5': hl.tfloat, 'f7': hl.tint},; missing='.',; delimiter='\t',; force_bgz=force_bgz,; force=force,; ). ht = ht.rename({; 'f0': 'seqname',; 'f1': 'source',; 'f2': 'feature',; 'f3': 'start',; 'f4': 'end',; 'f5': 'score',; 'f6': 'strand',; 'f7': 'frame',; 'f8': 'attribute',; }). def parse_attributes(unparsed_attributes):; def parse_attribute(attribute):; key_and_value = attribute.split(' '); key = key_and_value[0]; value = key_and_value[1]; return (key, value.replace('""|;\\$', '')). return hl.dict(unparsed_attributes.split('; ').map(parse_attribute)). ht = ht.annotate(attribute=parse_attributes(ht['attribute'])). ht = ht.checkpoint(new_temp_file()). attributes = ht.aggregate(hl.agg.explode(lambda x: hl.agg.collect_as_set(x), ht['attribute'].keys())). ht = ht.transmute(**{x: hl.or_missing(ht['attribute'].contains(x), ht['attribute'][x]) for x in attributes if x}). if reference_genome:; if reference_genome.name == 'GRCh37':; ht = ht.annotate(; seqname=hl.case(); .when((ht['seqname'] == 'M') | (ht['seqname'] == 'chrM'), 'MT'); .when(ht['seqname'].startswith('chr'), ht['seqname'].replace('^chr', '')); .default(ht['seqname']); ); else:; ht = ht.annotate(; seqname=hl.case(); .when(ht['seqname'].startswith('HLA'), ht['seqname']); .when(ht['seqname'].startswith('chrHLA'), ht['seqname'].replace('^chr', '')); .when(ht['seqname'].startswith('chr'), ht['seqname']); .default('chr' + ht['seqname']); ); if skip_invalid_contigs:; valid_contigs = hl.literal(set(reference_genome.contigs)); ht = ht.filter(valid_contigs.contains(ht['seqname'])); ht = ht.transmute(; interval=hl.locus_interval(; ht['seqname'],; ht['start'],; ht['end'],; ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/import_gtf.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html
Deployability,update,updated,"ene_symbols)); if gene_ids:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\.')[0]), gene_ids)); if transcript_ids:; criteria.append(; hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\.')[0]), transcript_ids); ). ht = ht.filter(functools.reduce(operator.ior, criteria)); gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interval))); if verbose:; info(; f'get_gene_intervals found {len(gene_info)} entries:\n'; + ""\n"".join(map(lambda x: f'{x[0]}: {x[1]} ({x[2] if x[0] == ""gene"" else x[3]})', gene_info)); ); intervals = list(map(lambda x: x[-1], gene_info)); return intervals. def _load_gencode_gtf(gtf_file=None, reference_genome=None):; """"""; Get Gencode GTF (from file or reference genome). Parameters; ----------; reference_genome : :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :class:`.Table`; """"""; GTFS = {; 'GRCh37': 'gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz',; 'GRCh38': 'gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz',; }; if reference_genome is None:; reference_genome = hl.default_reference().name; else:; reference_genome = reference_genome.name; if gtf_file is None:; gtf_file = GTFS.get(reference_genome); if gtf_file is None:; raise ValueError(; 'get_gene_intervals requires a GTF file, or the reference genome be one of GRCh37 or GRCh38 (when on Google Cloud Platform)'; ); ht = hl.experimental.import_gtf(; gtf_file, reference_genome=reference_genome, skip_invalid_contigs=True, min_partitions=12; ); ht = ht.annotate(gene_id=ht.gene_id.split('\\.')[0], transcript_id=ht.transcript_id.split('\\.')[0]); return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/import_gtf.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html
Energy Efficiency,reduce,reduce," : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :obj:`list` of :class:`.Interval`; """"""; if gene_symbols is None and gene_ids is None and transcript_ids is None:; raise ValueError('get_gene_intervals requires at least one of gene_symbols, gene_ids, or transcript_ids'); ht = _load_gencode_gtf(gtf_file, reference_genome); criteria = []; if gene_symbols:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_name == y), gene_symbols)); if gene_ids:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\.')[0]), gene_ids)); if transcript_ids:; criteria.append(; hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\.')[0]), transcript_ids); ). ht = ht.filter(functools.reduce(operator.ior, criteria)); gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interval))); if verbose:; info(; f'get_gene_intervals found {len(gene_info)} entries:\n'; + ""\n"".join(map(lambda x: f'{x[0]}: {x[1]} ({x[2] if x[0] == ""gene"" else x[3]})', gene_info)); ); intervals = list(map(lambda x: x[-1], gene_info)); return intervals. def _load_gencode_gtf(gtf_file=None, reference_genome=None):; """"""; Get Gencode GTF (from file or reference genome). Parameters; ----------; reference_genome : :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :class:`.Table`; """"""; GTFS = {; 'GRCh37': 'gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz',; 'GRCh38': 'gs://hail-common/references/gencode/gencode.v29.annota",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/import_gtf.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html
Performance,load,load,"Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqname`` is not consistent with the reference genome.; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions (passed to import_table).; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; path,; min_partitions=min_partitions,; comment='#',; no_header=True,; types={'f3': hl.tint, 'f4': hl.tint, 'f5': hl.tfloat, 'f7': hl.tint},; missing='.',; delimiter='\t',; force_bgz=force_bgz,; force=force,; ). ht = ht.rename({; 'f0': 'seqname',; 'f1': 'source',; 'f2': 'feature',; 'f3': 'start',; 'f4': 'end',; 'f5': 'score',; 'f6': 'strand',; 'f7': 'frame',; 'f8': 'attribute',; }). def parse_attributes(unparsed_attributes):; def parse_attribute(attribute):; key_and_value = attribute.split(' '); key = k",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/import_gtf.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html
Testability,test,test,"; 'frame': int32; 'interval': interval<>. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. Note; ----. This function will return an ``interval`` field of type :class:`.tinterval`; constructed from the ``seqname``, ``start``, and ``end`` fields in the; GTF file. This interval is inclusive of both the start and end positions; in the GTF file. If the ``reference_genome`` parameter is specified, the start and end; points of the ``interval`` field will be of type :class:`.tlocus`.; Otherwise, the start and end points of the ``interval`` field will be of; type :class:`.tstruct` with fields ``seqname`` (type :class:`str`) and; ``position`` (type :obj:`.tint32`). Furthermore, if the ``reference_genome`` parameter is specified and; ``skip_invalid_contigs`` is ``True``, this import function will skip; lines in the GTF where ``seqname`` is not consistent with the reference; genome specified. Example; -------. >>> ht = hl.experimental.import_gtf('data/test.gtf',; ... reference_genome='GRCh37',; ... skip_invalid_contigs=True). >>> ht.describe() # doctest: +SKIP_OUTPUT_CHECK; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip l",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/import_gtf.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html
Deployability,continuous,continuous,"l.ldscore; import hail as hl; from hail.expr.expressions import expr_float64, expr_locus, expr_numeric; from hail.linalg import BlockMatrix; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; annotation_exprs=nullable(oneof(expr_numeric, sequenceof(expr_numeric))),; block_size=nullable(int),; ); def ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None) -> Table:; """"""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). .. code-block:: text. +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/ldscore.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html
Availability,error,error,"`dict`; Coefficients to multiply each field. The coefficients are specified by; `coef_dict` value, the row (or col) field name is specified by `coef_dict` key.; """"""; assert str_expr is not None or ref_coef_dict is not None, ""str_expr and ref_coef_dict cannot both be None""; assert axis in {'rows', 'cols'}, ""axis must be 'rows' or 'cols'""; fields_to_search = tb.row if axis == 'rows' or isinstance(tb, Table) else tb.col; # when axis='rows' we're searching for annotations, axis='cols' searching for covariates; axis_field = 'annotation' if axis == 'rows' else 'covariate'; if str_expr is None:; # take all row (or col) fields in mt matching keys in coef_dict; coef_dict = {k: ref_coef_dict[k] for k in ref_coef_dict.keys() if k in fields_to_search}; # if intersect is empty: return error; assert len(coef_dict) > 0, f'None of the keys in ref_coef_dict match any {axis[:-1]} fields'; return coef_dict # return subset of ref_coef_dict; else:; # str_expr search in list of row (or col) fields; fields = [rf for rf in list(fields_to_search) if str_expr in rf]; assert len(fields) > 0, f'No {axis[:-1]} fields matched str_expr search: {str_expr}'; if ref_coef_dict is None:; print(f'Assuming coef = 1 for all {axis_field}s'); return {k: 1 for k in fields}; in_ref_coef_dict = set(fields).intersection(set(ref_coef_dict.keys())) # fields in ref_coef_dict; # if >0 fields returned by search are not in ref_coef_dict; if in_ref_coef_dict != set(fields):; # if none of the fields returned by search are in ref_coef_dict; assert len(in_ref_coef_dict) > 0, f'None of the {axis_field} fields in ref_coef_dict match search results'; fields_to_ignore = set(fields).difference(in_ref_coef_dict); print(f'Ignored fields from {axis_field} search: {fields_to_ignore}'); print('To include ignored fields, change str_expr to match desired fields'); fields = list(in_ref_coef_dict); return {k: ref_coef_dict[k] for k in fields}. [docs]@typecheck(mt=MatrixTable, y=expr_int32, P=oneof(int, float)); def ascertainment_bias(",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html
Deployability,update,updated,"((1 - K) * P); cas = mt.filter_cols(mt.y_w_asc_bias == 1); con = mt.filter_cols(mt.y_w_asc_bias == 0).add_col_index(name='col_idx_' + uid); keep = round(p * n * (1 - K)) * [1] + round((1 - p) * n * (1 - K)) * [0]; con = con.annotate_cols(**{'keep_' + uid: hl.literal(keep)[hl.int32(con['col_idx_' + uid])]}); con = con.filter_cols(con['keep_' + uid] == 1); con = _clean_fields(con, uid); mt = con.union_cols(cas); return mt. [docs]@typecheck(mt=MatrixTable, y=oneof(expr_int32, expr_float64), K=oneof(int, float), exact=bool); def binarize(mt, y, K, exact=False):; r""""""Binarize phenotype `y` such that it has prevalence `K` = cases/(cases+controls); Uses inverse CDF of Gaussian to set binarization threshold when `exact` = False,; otherwise uses ranking to determine threshold. Parameters; ----------; mt : :class:`.MatrixTable`; :class:`.MatrixTable` containing phenotype to be binarized.; y : :class:`.Expression`; Column field of phenotype.; K : :obj:`int` or :obj:`float`; Desired ""population prevalence"" of phenotype.; exact : :obj:`bool`; Whether to get prevalence as close as possible to `K` (does not use inverse CDF). Returns; -------; :class:`.MatrixTable`; :class:`.MatrixTable` containing binary phenotype with prevalence of approx. `K`; """"""; if exact:; key = list(mt.col_key); uid = Env.get_uid(base=100); mt = mt.annotate_cols(**{'y_' + uid: y}); tb = mt.cols().order_by('y_' + uid); tb = tb.add_index('idx_' + uid); n = tb.count(); # ""+ 1"" because of zero indexing; tb = tb.annotate(y_binarized=tb['idx_' + uid] + 1 <= round(n * K)); tb, mt = tb.key_by('y_' + uid), mt.key_cols_by('y_' + uid); mt = mt.annotate_cols(y_binarized=tb[mt['y_' + uid]].y_binarized); mt = mt.key_cols_by(*map(lambda x: mt[x], key)); else: # use inverse CDF; y_stats = mt.aggregate_cols(hl.agg.stats(y)); threshold = stats.norm.ppf(1 - K, loc=y_stats.mean, scale=y_stats.stdev); mt = mt.annotate_cols(y_binarized=y > threshold); return mt. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html
Integrability,depend,depending," @author: nbaya; """""". import numpy as np; import pandas as pd; from scipy import stats. import hail as hl; from hail.expr.expressions import expr_array, expr_call, expr_float64, expr_int32; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils.java import Env. [docs]@typecheck(; mt=MatrixTable,; genotype=oneof(expr_int32, expr_float64, expr_call),; h2=(oneof(float, int, list, np.ndarray)),; pi=nullable(oneof(float, int, list, np.ndarray)),; rg=nullable(oneof(float, int, list, np.ndarray)),; annot=nullable(oneof(expr_float64, expr_int32)),; popstrat=nullable(oneof(expr_int32, expr_float64)),; popstrat_var=nullable(oneof(float, int)),; exact_h2=bool,; ); def simulate_phenotypes(; mt, genotype, h2, pi=None, rg=None, annot=None, popstrat=None, popstrat_var=None, exact_h2=False; ):; r""""""Simulate phenotypes for testing LD score regression. Simulates betas (SNP effects) under the infinitesimal, spike & slab, or; annotation-informed models, depending on parameters passed. Optionally adds; population stratification. Parameters; ----------; mt : :class:`.MatrixTable`; :class:`.MatrixTable` containing genotypes to be used. Also should contain; variant annotations as row fields if running the annotation-informed; model or covariates as column fields if adding population stratification.; genotype : :class:`.Expression` or :class:`.CallExpression`; Entry field containing genotypes of individuals to be used for the; simulation.; h2 : :obj:`float` or :obj:`int` or :obj:`list` or :class:`numpy.ndarray`; SNP-based heritability of simulated trait.; pi : :obj:`float` or :obj:`int` or :obj:`list` or :class:`numpy.ndarray`, optional; Probability of SNP being causal when simulating under the spike & slab; model.; rg : :obj:`float` or :obj:`int` or :obj:`list` or :class:`numpy.ndarray`, optional; Genetic correlation between traits.; annot : :class:`.Expression`, optional; Row field to use as our aggrega",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html
Testability,test,testing,"﻿. Hail | ; hail.experimental.ldscsim. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.ldscsim. Source code for hail.experimental.ldscsim; #!/usr/bin/env python3; # -*- coding: utf-8 -*-; """"""; Simulation framework for testing LDSC. Models for SNP effects:; - Infinitesimal (can simulate n correlated traits); - Spike & slab (can simulate up to 2 correlated traits); - Annotation-informed. Features:; - Field aggregation tools for annotation-informed model and; population stratification with many covariates.; - Automatic adjustment of genetic correlation parameters; to allow for the joint simulation of up to 100 randomly; correlated phenotypes.; - Methods for binarizing phenotypes to have a certain prevalence; and for adding ascertainment bias to binarized phenotypes. @author: nbaya; """""". import numpy as np; import pandas as pd; from scipy import stats. import hail as hl; from hail.expr.expressions import expr_array, expr_call, expr_float64, expr_int32; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils.java import Env. [docs]@typecheck(; mt=MatrixTable,; genotype=oneof(expr_int32, expr_float64, expr_call),; h2=(oneof(float, int, list, np.ndarray)),; pi=nullable(oneof(float, int, list, np.ndarray)),; rg=nullable(oneof(float, int, list, np.ndarray)),; annot=nullable(oneof(expr_float64, expr_int32)),; popstrat=nullable(oneof(expr_int32, expr_float64)),; popstrat_var=nullable(oneof(float, int)),; exact_h2=bool,; ); def simulate_phenotypes(; mt, genotype, h2, pi=None, rg=None, annot=None, popstrat=None, popstrat_var=None, exact",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/ldscsim.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html
Availability,error,error,must be a single entry-indexed field; (not a list of fields).; * ``n_samples_exprs`` must be a single entry-indexed field; (not a list of fields).; * The ``phenotype`` field that keys the table returned by; :func:`.ld_score_regression` will have values corresponding to the; column keys of the input matrix table. This function returns a :class:`~.Table` with one row per set of summary; statistics passed to the ``chi_sq_exprs`` argument. The following; row-indexed fields are included in the table:. * **phenotype** (:py:data:`.tstr`) -- The name of the phenotype. The; returned table is keyed by this field. See the notes below for; details on the possible values of this field.; * **mean_chi_sq** (:py:data:`.tfloat64`) -- The mean chi-squared; test statistic for the given phenotype.; * **intercept** (`Struct`) -- Contains fields:. - **estimate** (:py:data:`.tfloat64`) -- A point estimate of the; intercept :math:`1 + Na`.; - **standard_error** (:py:data:`.tfloat64`) -- An estimate of; the standard error of this point estimate. * **snp_heritability** (`Struct`) -- Contains fields:. - **estimate** (:py:data:`.tfloat64`) -- A point estimate of the; SNP-heritability :math:`h_g^2`.; - **standard_error** (:py:data:`.tfloat64`) -- An estimate of; the standard error of this point estimate. Warning; -------; :func:`.ld_score_regression` considers only the rows for which both row; fields ``weight_expr`` and ``ld_score_expr`` are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters; ----------; weight_expr : :class:`.Float64Expression`; Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr : :class:`.Float64Expression`; Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs : :class:`.Float64Expression` or :obj:`list` of; :class:`.Float64Expression`; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions f,MatchSource.WIKI,docs/0.2/_modules/hail/experimental/ld_score_regression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html
Deployability,update,updated,"as_bias_corrected),; __step2_jackknife_variance=(; hl.sum(mt.__step2_block_betas_bias_corrected**2); - hl.sum(mt.__step2_block_betas_bias_corrected) ** 2 / n_blocks; ); / (n_blocks - 1); / n_blocks,; ). # combine step 1 and step 2 block jackknifes; mt = mt.annotate_entries(; __step2_initial_w=1.0; / (mt.__w_initial_floor * 2.0 * (mt.__initial_betas[0] + +mt.__initial_betas[1] * mt.__x_floor) ** 2); ). mt = mt.annotate_cols(; __final_betas=[mt.__step1_betas[0], mt.__step2_betas[1]],; __c=(hl.agg.sum(mt.__step2_initial_w * mt.__x) / hl.agg.sum(mt.__step2_initial_w * mt.__x**2)),; ). mt = mt.annotate_cols(; __final_block_betas=hl.map(; lambda i: (mt.__step2_block_betas[i] - mt.__c * (mt.__step1_block_betas[i][0] - mt.__final_betas[0])),; hl.range(0, n_blocks),; ); ). mt = mt.annotate_cols(; __final_block_betas_bias_corrected=(n_blocks * mt.__final_betas[1] - (n_blocks - 1) * mt.__final_block_betas); ). mt = mt.annotate_cols(; __final_jackknife_mean=[mt.__step1_jackknife_mean[0], hl.mean(mt.__final_block_betas_bias_corrected)],; __final_jackknife_variance=[; mt.__step1_jackknife_variance[0],; (; hl.sum(mt.__final_block_betas_bias_corrected**2); - hl.sum(mt.__final_block_betas_bias_corrected) ** 2 / n_blocks; ); / (n_blocks - 1); / n_blocks,; ],; ). # convert coefficient to heritability estimate; mt = mt.annotate_cols(; phenotype=mt.__y_name,; mean_chi_sq=hl.agg.mean(mt.__y),; intercept=hl.struct(estimate=mt.__final_betas[0], standard_error=hl.sqrt(mt.__final_jackknife_variance[0])),; snp_heritability=hl.struct(; estimate=(M / hl.agg.mean(mt.__n)) * mt.__final_betas[1],; standard_error=hl.sqrt((M / hl.agg.mean(mt.__n)) ** 2 * mt.__final_jackknife_variance[1]),; ),; ). # format and return results; ht = mt.cols(); ht = ht.key_by(ht.phenotype); ht = ht.select(ht.mean_chi_sq, ht.intercept, ht.snp_heritability). ht_tmp_file = new_temp_file(); ht.write(ht_tmp_file); ht = hl.read_table(ht_tmp_file). return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/ld_score_regression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html
Testability,test,test,"ight_expr=expr_float64,; ld_score_expr=expr_numeric,; chi_sq_exprs=oneof(expr_float64, sequenceof(expr_float64)),; n_samples_exprs=oneof(expr_numeric, sequenceof(expr_numeric)),; n_blocks=int,; two_step_threshold=int,; n_reference_panel_variants=nullable(int),; ); def ld_score_regression(; weight_expr,; ld_score_expr,; chi_sq_exprs,; n_samples_exprs,; n_blocks=200,; two_step_threshold=30,; n_reference_panel_variants=None,; ) -> Table:; r""""""Estimate SNP-heritability and level of confounding biases from genome-wide association study; (GWAS) summary statistics. Given a set or multiple sets of GWAS summary statistics, :func:`.ld_score_regression` estimates the heritability; of a trait or set of traits and the level of confounding biases present in; the underlying studies by regressing chi-squared statistics on LD scores,; leveraging the model:. .. math::. \mathrm{E}[\chi_j^2] = 1 + Na + \frac{Nh_g^2}{M}l_j. * :math:`\mathrm{E}[\chi_j^2]` is the expected chi-squared statistic; for variant :math:`j` resulting from a test of association between; variant :math:`j` and a trait.; * :math:`l_j = \sum_{k} r_{jk}^2` is the LD score of variant; :math:`j`, calculated as the sum of squared correlation coefficients; between variant :math:`j` and nearby variants. See :func:`ld_score`; for further details.; * :math:`a` captures the contribution of confounding biases, such as; cryptic relatedness and uncontrolled population structure, to the; association test statistic.; * :math:`h_g^2` is the SNP-heritability, or the proportion of variation; in the trait explained by the effects of variants included in the; regression model above.; * :math:`M` is the number of variants used to estimate :math:`h_g^2`.; * :math:`N` is the number of samples in the underlying association study. For more details on the method implemented in this function, see:. * `LD Score regression distinguishes confounding from polygenicity in genome-wide association studies (Bulik-Sullivan et al, 2015) <https://www.ncbi",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/ld_score_regression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html
Availability,error,error,"cy. menu; Hail. Module code; hail.experimental.loop. Source code for hail.experimental.loop; from typing import Callable. from hail import ir; from hail.expr.expressions import construct_expr, construct_variable, expr_any, to_expr, unify_all; from hail.expr.types import hail_type; from hail.typecheck import anytype, typecheck; from hail.utils.java import Env. [docs]@typecheck(f=anytype, typ=hail_type, args=expr_any); def loop(f: Callable, typ, *args):; r""""""Define and call a tail-recursive function with given arguments. Notes; -----; The argument `f` must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. .. math::. f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}. we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)). Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called. This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let's; consider two different recursive definitions for the triangle function; :math:`f(x) = 0 + 1 + \dots + x`:. >>> def triangle1(x):; ... if x == 1:; ... return x; ... return x + triangle1(x - 1). >>> def triangle2(x, total):; ... if x == 0:; ... return total; ... return triangle2(x - 1, total + x). The first function definition, `triangle1`, will call itself and then add x.; This is an example of a non-tail recursive function, since `triangle1(9)`; needs to modify the result of the inner recursive call to `triangle1(8)` by; adding 9 to the result. The second function is tail recursive: the result of `triangle2(9, 0)` is; the same as the result of the inner recursive call, `triangle2(8, 9)`. Example; -------; To find the sum of all the numbers from n=1...10:; >>> triangle_f = lambda f, x, total",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/loop.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html
Deployability,update,updated,"(loop_ir, ir.If):; if contains_recursive_call(loop_ir.cond):; raise TypeError(""branch condition can't contain recursive call!""); check_tail_recursive(loop_ir.cnsq); check_tail_recursive(loop_ir.altr); elif isinstance(loop_ir, ir.Let):; if contains_recursive_call(loop_ir.value):; raise TypeError(""bound value used in other expression can't contain recursive call!""); check_tail_recursive(loop_ir.body); elif isinstance(loop_ir, ir.TailLoop):; if any(contains_recursive_call(x) for n, x in loop_ir.params):; raise TypeError(""parameters passed to inner loop can't contain recursive call!""); elif not isinstance(loop_ir, ir.Recur) and contains_recursive_call(loop_ir):; raise TypeError(""found recursive expression outside of tail position!""). @typecheck(recur_exprs=expr_any); def make_loop(*recur_exprs):; if len(recur_exprs) != len(args):; raise TypeError('Recursive call in loop has wrong number of arguments'); err = None; for i, (rexpr, expr) in enumerate(zip(recur_exprs, args)):; if rexpr.dtype != expr.dtype:; if err is None:; err = 'Type error in recursive call,'; err += f'\n at argument index {i}, loop arg type: {expr.dtype}, '; err += f'recur arg type: {rexpr.dtype}'; if err is not None:; raise TypeError(err); irs = [expr._ir for expr in recur_exprs]; indices, aggregations = unify_all(*recur_exprs); return construct_expr(ir.Recur(loop_name, irs, typ), typ, indices, aggregations). uid_irs = []; loop_vars = []. for expr in args:; uid = Env.get_uid(); loop_vars.append(construct_variable(uid, expr._type, expr._indices, expr._aggregations)); uid_irs.append((uid, expr._ir)). loop_f = to_expr(f(make_loop, *loop_vars)); if loop_f.dtype != typ:; raise TypeError(f""requested type {typ} does not match inferred type {loop_f.dtype}""); check_tail_recursive(loop_f._ir); indices, aggregations = unify_all(*args, loop_f). return construct_expr(ir.TailLoop(loop_name, loop_f._ir, uid_irs), loop_f.dtype, indices, aggregations). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/loop.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html
Modifiability,variab,variables,"define the derivative:. >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at :math:`x_0 = 0`, we'll compute the next step :math:`x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}`; until the difference between :math:`x_{i}` and :math:`x_{i+1}` falls below; our convergence threshold:. >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; -------; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters; ----------; f : function ( (marker, \*args) -> :class:`.Expression`; Function of one callable marker, denoting where the recursive call (or calls) is located,; and many `args`, the loop variables.; typ : :class:`str` or :class:`.HailType`; Type the loop returns.; args : variable-length args of :class:`.Expression`; Expressions to initialize the loop values.; Returns; -------; :class:`.Expression`; Result of the loop with `args` as initial loop values.; """""". loop_name = Env.get_uid(). def contains_recursive_call(non_recursive):; if isinstance(non_recursive, ir.Recur) and non_recursive.name == loop_name:; return True; return any([contains_recursive_call(c) for c in non_recursive.children]). def check_tail_recursive(loop_ir):; if isinstance(loop_ir, ir.If):; if contains_recursive_call(loop_ir.cond):; raise TypeError(""branch condition can't contain recursive call!""); check_tail_recursive(loop_ir.cnsq); check_tail_recursive(loop_ir.altr); elif isinstance(loop_ir, ir.Let):; if contains_recursive_call(loop_ir.value):; raise TypeError(""bound value used in other expression can't contain recursive call!""); check_tail_r",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/loop.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html
Deployability,update,updated,"lt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}; ). if isinstance(loadings_source, hl.MatrixTable):; n_variants = loadings_source.count_rows(); else:; n_variants = loadings_source.count(). mt = mt.filter_rows(hl.is_defined(mt._loadings) & hl.is_defined(mt._af) & (mt._af > 0) & (mt._af < 1)). gt_norm = (mt._call.n_alt_alleles() - 2 * mt._af) / hl.sqrt(n_variants * 2 * mt._af * (1 - mt._af)). return mt.select_cols(scores=hl.agg.array_sum(mt._loadings * gt_norm)).cols(). def _get_expr_or_join(expr, source, other_source, loc):; if source != other_source:; if isinstance(source, hl.MatrixTable):; source = source.annotate_rows(**{loc: expr}); else:; source = source.annotate(**{loc: expr}); expr = source[other_source.row_key][loc]; return expr. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html
Performance,load,loadings,"﻿. Hail | ; hail.experimental.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.pca. Source code for hail.experimental.pca; import hail as hl; from hail.expr.expressions import (; expr_array,; expr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html
Deployability,update,updated,"er/mother schema,; and the resulting entry schema is the same as the proband_entry/father_entry/mother_entry schema.; If the `keep_trio_cols` option is set, then an additional `source_trio` column is added with the trio column data.; If the `keep_trio_entries` option is set, then an additional `source_trio_entry` column is added with the trio entry data. Note; ----; This assumes that the input MatrixTable is a trio MatrixTable (similar to; the result of :func:`~.trio_matrix`) Its entry schema has to contain; 'proband_entry`, `father_entry` and `mother_entry` all with the same type.; Its column schema has to contain 'proband`, `father` and `mother` all with; the same type. Parameters; ----------; tm : :class:`.MatrixTable`; Trio MatrixTable (entries have to be a Struct with `proband_entry`, `mother_entry` and `father_entry` present); col_keys : :obj:`list` of str; Column key(s) for the resulting sample MatrixTable; keep_trio_cols: bool; Whether to add a `source_trio` column with the trio column data (default `True`); keep_trio_entries: bool; Whether to add a `source_trio_entries` column with the trio entry data (default `False`). Returns; -------; :class:`.MatrixTable`; Sample MatrixTable; """""". select_entries_expr = {'__trio_entries': hl.array([tm.proband_entry, tm.father_entry, tm.mother_entry])}; if keep_trio_entries:; select_entries_expr['source_trio_entry'] = hl.struct(**tm.entry); tm = tm.select_entries(**select_entries_expr). tm = tm.key_cols_by(); select_cols_expr = {'__trio_members': hl.enumerate(hl.array([tm.proband, tm.father, tm.mother]))}; if keep_trio_cols:; select_cols_expr['source_trio'] = hl.struct(**tm.col); tm = tm.select_cols(**select_cols_expr). mt = tm.explode_cols(tm.__trio_members). mt = mt.transmute_entries(**mt.__trio_entries[mt.__trio_members[0]]). mt = mt.key_cols_by(); mt = mt.transmute_cols(**mt.__trio_members[1]). if col_keys:; mt = mt.key_cols_by(*col_keys). return mt. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/phase_by_transmission.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html
Integrability,wrap,wrapper," Sex chromosomes of male individuals should be haploid to be phased correctly.; - If `proband_call` is diploid on non-par regions of the sex chromosomes, it is assumed to be female. Returns `NA` when genotype calls cannot be phased.; The following genotype calls combinations cannot be phased by transmission:; 1. One of the calls in the trio is missing; 2. The proband genotype cannot be obtained from the parents alleles (Mendelian violation); 3. All individuals of the trio are heterozygous for the same two alleles; 4. Father is diploid on non-PAR region of X or Y; 5. Proband is diploid on non-PAR region of Y. In addition, individual phased genotype calls are returned as missing in the following situations:; 1. All mother genotype calls non-PAR region of Y; 2. Diploid father genotype calls on non-PAR region of X for a male proband (proband and mother are still phased as father doesn't participate in allele transmission). Note; ----; :func:`~.phase_trio_matrix_by_transmission` provides a convenience wrapper for phasing a trio matrix. Parameters; ----------; locus : :class:`.LocusExpression`; Expression for the locus in the trio matrix; alleles : :class:`.ArrayExpression`; Expression for the alleles in the trio matrix; proband_call : :class:`.CallExpression`; Expression for the proband call in the trio matrix; father_call : :class:`.CallExpression`; Expression for the father call in the trio matrix; mother_call : :class:`.CallExpression`; Expression for the mother call in the trio matrix. Returns; -------; :class:`.ArrayExpression`; Array containing: [phased proband call, phased father call, phased mother call]"""""". def call_to_one_hot_alleles_array(; call: hl.expr.CallExpression, alleles: hl.expr.ArrayExpression; ) -> hl.expr.ArrayExpression:; """"""; Get the set of all different one-hot-encoded allele-vectors in a genotype call.; It is returned as an ordered array where the first vector corresponds to the first allele,; and the second vector (only present if het) the seco",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/phase_by_transmission.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html
Deployability,update,updated,"top=row_file_sizes_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ). rows_grid = gridplot([[p_rows_per_partition, p_stats], [p, p_file_size]]). if 'entry_file_sizes' in all_data:; title = f'Statistics for {data_type}: {t_path}'. msg = f""Rows: {sum(all_data['rows_per_partition']):,}<br/>Partitions: {len(all_data['rows_per_partition']):,}<br/>Size: {total_entry_file_size}<br/>""; if success_file[0]:; msg += success_file[0]. source = ColumnDataSource(pd.DataFrame(all_data)); p = figure(tools=tools, width=panel_size, height=panel_size); p.title.text = title; p.xaxis.axis_label = 'Number of rows'; p.yaxis.axis_label = f'File size ({entry_scale}B)'; color_map = factor_cmap('spans_chromosome', palette=Spectral8, factors=list(set(all_data['spans_chromosome']))); p.scatter('rows_per_partition', 'entry_file_sizes', color=color_map, legend='spans_chromosome', source=source); p.legend.location = 'bottom_right'; p.select_one(HoverTool).tooltips = [; (x, f'@{x}') for x in ('rows_per_partition', 'entry_file_sizes_human', 'partition_bounds', 'index'); ]. p_stats = Div(text=msg); p_rows_per_partition = figure(x_range=p.x_range, width=panel_size, height=subpanel_size); p_rows_per_partition.quad(; top=rows_per_partition_hist,; bottom=0,; left=rows_per_partition_edges[:-1],; right=rows_per_partition_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ); p_file_size = figure(y_range=p.y_range, width=subpanel_size, height=panel_size). row_file_sizes_hist, row_file_sizes_edges = np.histogram(all_data['entry_file_sizes'], bins=50); p_file_size.quad(; right=row_file_sizes_hist,; left=0,; bottom=row_file_sizes_edges[:-1],; top=row_file_sizes_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ); entries_grid = gridplot([[p_rows_per_partition, p_stats], [p, p_file_size]]). return Tabs(tabs=[TabPanel(child=entries_grid, title='Entries'), TabPanel(child=rows_grid, title='Rows')]); else:; return rows_grid. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html
Performance,load,load,"t for a Hail Table or MatrixTable. Parameters; ----------; t_path : str; Path to the Hail Table or MatrixTable files. Returns; -------; :class:`bokeh.plotting.figure` or :class:`bokeh.models.layouts.Column`; """""". def get_rows_data(rows_files):; file_sizes = []; partition_bounds = []; parts_file = [x['path'] for x in rows_files if x['path'].endswith('parts')]; if parts_file:; parts = hadoop_ls(parts_file[0]); for i, x in enumerate(parts):; index = x['path'].split(f'{parts_file[0]}/part-')[1].split('-')[0]; if i < len(parts) - 1:; test_index = parts[i + 1]['path'].split(f'{parts_file[0]}/part-')[1].split('-')[0]; if test_index == index:; continue; file_sizes.append(x['size_bytes']); metadata_file = [x['path'] for x in rows_files if x['path'].endswith('metadata.json.gz')]; if metadata_file:; with hadoop_open(metadata_file[0], 'rb') as f:; rows_meta = json.load(f); try:; partition_bounds = [; (; x['start']['locus']['contig'],; x['start']['locus']['position'],; x['end']['locus']['contig'],; x['end']['locus']['position'],; ); for x in rows_meta['jRangeBounds']; ]; except KeyError:; pass; return partition_bounds, file_sizes. def scale_file_sizes(file_sizes):; min_file_size = min(file_sizes) * 1.1; total_file_size = sum(file_sizes); all_scales = [('T', 1e12), ('G', 1e9), ('M', 1e6), ('K', 1e3), ('', 1e0)]; for overall_scale, overall_factor in all_scales:; if total_file_size > overall_factor:; total_file_size /= overall_factor; break; for scale, factor in all_scales:; if min_file_size > factor:; file_sizes = [x / factor for x in file_sizes]; break; total_file_size = f'{total_file_size:.1f} {overall_scale}B'; return total_file_size, file_sizes, scale. files = hadoop_ls(t_path). rows_file = [x['path'] for x in files if x['path'].endswith('rows')]; entries_file = [x['path'] for x in files if x['path'].endswith('entries')]; success_file = [x['modification_time'] for x in files if x['path'].endswith('SUCCESS')]. metadata_file = [x['path'] for x in files if x['path'].endswith('met",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html
Deployability,update,updated,"ggregate(; **{rv: hl.agg.take(ht[rv], 1)[0] for rv in ht.row_value if rv not in set([*key, field, value])},; **{; fv: hl.agg.filter(; ht[field] == fv,; hl.rbind(hl.agg.take(ht[value], 1), lambda take: hl.if_else(hl.len(take) > 0, take[0], 'NA')),; ); for fv in field_vals; },; ). ht_tmp = new_temp_file(); ht.write(ht_tmp). return ht. [docs]@typecheck(ht=Table, field=str, into=sequenceof(str), delim=oneof(str, int)); def separate(ht, field, into, delim) -> Table:; """"""Separate a field into multiple fields by splitting on a delimiter; character or position. :func:`.separate` mimics the functionality of the `separate()` function in R's; ``tidyr`` package. This function will create a new table where ``field`` has been split into; multiple new fields, whose names are given by ``into``. If ``delim`` is a ``str`` (including regular expression strings), ``field``; will be separated into columns by that string. In this case, the length; of ``into`` must match the number of resulting fields. If ``delim`` is an ``int``, ``field`` will be separated into two row fields,; where the first field contains the first ``delim`` characters of ``field``; and the second field contains the remaining characters. Parameters; ----------; ht : :class:`.Table`; A Hail table.; field : :class:`str`; The name of the field to separate in ``ht``.; into : list of :class:`str`; The names of the fields to create by separating ``field``.; delimiter : :class:`str` or :obj:`int`; The character or position by which to separate ``field``. Returns; -------; :class:`.Table`; Table with original ``field`` split into fields whose names are defined; by `into`."""""". if isinstance(delim, int):; ht = ht.annotate(**{into[0]: ht[field][:delim], into[1]: ht[field][delim:]}); else:; split = ht[field].split(delim); ht = ht.annotate(**{into[i]: split[i] for i in range(len(into))}); ht = ht.drop(field). ht_tmp = new_temp_file(); ht.write(ht_tmp). return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/tidyr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html
Modifiability,variab,variable-length,"); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.tidyr. Source code for hail.experimental.tidyr; import hail as hl; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(ht=Table, key=str, value=str, fields=str); def gather(ht, key, value, *fields) -> Table:; """"""Collapse fields into key-value pairs. :func:`.gather` mimics the functionality of the `gather()` function found in R's; ``tidyr`` package. This is a way to turn ""wide"" format data into ""long""; format data. Parameters; ----------; ht : :class:`.Table`; A Hail table.; key : :class:`str`; The name of the key field in the gathered table.; value : :class:`str`; The name of the value field in the gathered table.; fields : variable-length args of obj:`str`; Names of fields to gather in ``ht``. Returns; -------; :class:`.Table`; Table with original ``fields`` gathered into ``key`` and ``value`` fields."""""". ht = ht.annotate(_col_val=hl.array([hl.struct(field_name=field, value=ht[field]) for field in fields])); ht = ht.drop(*fields); ht = ht.explode(ht['_col_val']); ht = ht.annotate(**{key: ht['_col_val'][0], value: ht['_col_val'][1]}); ht = ht.drop('_col_val'). ht_tmp = new_temp_file(); ht.write(ht_tmp). return hl.read_table(ht_tmp). [docs]@typecheck(ht=Table, field=str, value=str, key=nullable(oneof(str, sequenceof(str)))); def spread(ht, field, value, key=None) -> Table:; """"""Spread a key-value pair of fields across multiple fields. :func:`.spread` mimics the functionality of the `spread()` function in R's; `tidyr` package. This is a way to turn ""long"" format data into ""wide""; format data. Given a ``field``, :func:`.spread` will create a new table by grouping; ``ht`` by its row key and, optionally, any additional fields passed to the;",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/tidyr.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html
Deployability,update,updated,"(time=expr_str, format=expr_str, zone_id=expr_str); def strptime(time, format, zone_id):; """"""; Interpret a formatted datetime string as a Unix timestamp (number of seconds since 1970-01-01T00:00Z (ISO)). Examples; --------. >>> hl.eval(hl.experimental.strptime(""07/08/19 3:00:01 AM"", ""%D %l:%M:%S %p"", ""America/New_York"")); 1562569201. >>> hl.eval(hl.experimental.strptime(""Saturday, October 11, 1997. 05:45:23 AM"", ""%A, %B %e, %Y. %r"", ""GMT+2"")); 876541523. Notes; -----; The following formatting characters are supported in format strings: A a B b D d e F H I j k l M m n p R r S s T t U u V v W Y y z; See documentation here: https://linux.die.net/man/3/strftime. A zone id can take one of three forms. It can be an explicit offset, like ""+01:00"", a relative offset, like ""GMT+2"",; or a IANA timezone database (TZDB) identifier, like ""America/New_York"". Wikipedia maintains a list of TZDB identifiers here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones. Currently, the parser implicitly uses the ""en_US"" locale. This function will fail if there is not enough information in the string to determine a particular timestamp.; For example, if you have the string `""07/08/09""` and the format string `""%Y.%m.%d""`, this method will fail, since that's not specific; enough to determine seconds from. You can fix this by adding ""00:00:00"" to your date string and ""%H:%M:%S"" to your format string. Parameters; ----------; time : str or :class:`.Expression` of type :py:data:`.tstr`; The string from which to parse the time.; format : str or :class:`.Expression` of type :py:data:`.tstr`; The format string describing how to parse the time.; zone_id: str or :class:`.Expression` of type :py:data:`.tstr`; An id representing the timezone. See notes above. Returns; -------; :class:`~.Int64Expression`; The Unix timestamp associated with the given time string.; """"""; return _func(""strptime"", hl.tint64, time, format, zone_id). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/experimental/time.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/time.html
Availability,error,error,"expr_any); def default(self, then):; """"""Finish the switch statement by adding a default case. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0 and self._when_missing_case is None:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). [docs]class CaseBuilder(ConditionalBuilder):; """"""Class for chaining multiple if-else statements. Examples; --------. >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.CaseBuilder.when` or; :meth:`~hail.expr.builders.CaseBui",MatchSource.WIKI,docs/0.2/_modules/hail/expr/builders.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html
Deployability,update,updated,"ion` that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a :class:`.CaseBuilder`. Parameters; ----------; condition: :class:`.BooleanExpression`; then : :class:`.Expression`. Returns; -------; :class:`.CaseBuilder`; Mutates and returns `self`.; """"""; self._unify_type(then.dtype); self._cases.append((condition, then)); return self. [docs] @typecheck_method(then=expr_any); def default(self, then):; """"""Finish the case statement by adding a default case. Notes; -----; If no condition from a :meth:`~.CaseBuilder.when` call is ``True``,; then `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the case statement by returning missing. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the case statement by throwing an error with the given message. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; an error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/builders.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html
Integrability,message,message,"expr_any); def default(self, then):; """"""Finish the switch statement by adding a default case. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0 and self._when_missing_case is None:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). [docs]class CaseBuilder(ConditionalBuilder):; """"""Class for chaining multiple if-else statements. Examples; --------. >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.CaseBuilder.when` or; :meth:`~hail.expr.builders.CaseBui",MatchSource.WIKI,docs/0.2/_modules/hail/expr/builders.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html
Testability,assert,assert,"ave same type, found '{}' and '{}'"".format(self._ret_type, t)). [docs]class SwitchBuilder(ConditionalBuilder):; """"""Class for generating conditional trees based on value of an expression. Examples; --------. >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.SwitchBuilder.when` or; :meth:`~hail.expr.builders.SwitchBuilder.default` method calls must be the; same type. See Also; --------; :func:`.case`, :func:`.cond`, :func:`.switch`. Parameters; ----------; expr : :class:`.Expression`; Value to match against.; """""". @typecheck_method(base=expr_any); def __init__(self, base):; self._base = base; self._when_missing_case = None; super(SwitchBuilder, self).__init__(). def _finish(self, default):; assert len(self._cases) > 0 or self._when_missing_case is not None. def f(base):; # build cond chain bottom-up; if default is self._base:; expr = base; else:; expr = default; for value, then in self._cases[::-1]:; expr = hl.if_else(base == value, then, expr); # needs to be on the outside, because upstream missingness would propagate; if self._when_missing_case is not None:; expr = hl.if_else(hl.is_missing(base), self._when_missing_case, expr); return expr. return hl.bind(f, self._base). [docs] @typecheck_method(value=expr_any, then=expr_any); def when(self, value, then) -> 'SwitchBuilder':; """"""Add a value test. If the `base` expression is equal to `value`, then; returns `then`. Warning; -------; Missingness always compares to missing. Both ``NA == NA`` and; ``NA != NA`` return ``NA``. Use :meth:`~SwitchBuilder.when_missing`; to test missingness. Parameters; ----------; value : :class:`.Expression`; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates a",MatchSource.WIKI,docs/0.2/_modules/hail/expr/builders.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html
Availability,error,error,"turn res. return hl.rbind(cdf, compute). @typecheck(raw_cdf=expr_struct()); def _result_from_raw_cdf(raw_cdf):; levels = raw_cdf.levels; item_weights = (; hl._stream_range(hl.len(levels) - 1); .flatmap(; lambda l: hl._stream_range(levels[l], levels[l + 1]).map(; lambda i: hl.struct(level=l, value=raw_cdf['items'][i]); ); ); .aggregate(lambda x: hl.agg.group_by(x.value, hl.agg.sum(hl.bit_lshift(1, x.level)))); ); weights = item_weights.values(); ranks = weights.scan(lambda acc, weight: acc + weight, 0); values = item_weights.keys(); return hl.struct(values=values, ranks=ranks, _compaction_counts=raw_cdf._compaction_counts). @typecheck(k=expr_int32, left=expr_struct(), right=expr_struct()); def _cdf_combine(k, left, right):; t = tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)); return _func('approxCDFCombine', t, k, left, right). @typecheck(cdf=expr_struct(), failure_prob=expr_oneof(expr_float32, expr_float64), all_quantiles=bool); def _error_from_cdf(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """""". def compute_sum(cdf):; s = hl.sum(; hl.range(0, hl.len(cdf._compaction_counts)).map(lambda i: cdf._compaction_counts[i] * (2 ** (2 * i))); ); return s / (cdf.ranks[-1] ** 2). def update_grid_size(p, s):; return 4 * hl.sqrt(hl.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
Deployability,release,release,"ing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """""". if alternative == 'two.sided':; warning(; '""two.sided"" is a deprecated and will be removed in a future '; 'release, please use ""two-sided"" for the `alternative` parameter '; 'to hl.binom_test'; ); alternative = 'two-sided'. alt_enum = {""two-sided"": 0, ""less"": 1, ""greater"": 2}[alternative]; return _func(""binomTest"", tfloat64, x, n, p, to_expr(alt_enum)). [docs]@typecheck(x=expr_float64, df=expr_float64, ncp=nullable(expr_float64), lower_tail=expr_bool, log_p=expr_bool); def pchisqtail(x, df, ncp=None, lower_tail=False, log_p=False) -> Float64Expression:; """"""Returns the probability under the right-tail starting at x for a chi-squared; distribution with df degrees of freedom. Examples; --------. >>> hl.eval(hl.pchisqtail(5, 1)); 0.025347318677468304. >>> hl.eval(hl.pchisqtail(5, 1, ncp=2)); 0.20571085634347097. >>> hl.eval(hl.pchisqtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the CDF.; df : float or :class:`.Expre",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
Energy Efficiency,power,power,"""""""; return _func(""dnorm"", tfloat64, x, mu, sigma, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, log_p=expr_bool); def dpois(x, lamb, log_p=False) -> Float64Expression:; """"""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability density.; """"""; return _func(""dpois"", tfloat64, x, lamb, log_p). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def exp(x) -> Float64Expression:; """"""Computes `e` raised to the power `x`. Examples; --------. >>> hl.eval(hl.exp(2)); 7.38905609893065. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""exp"", tfloat64, x). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32); def fisher_exact_test(c1, c2, c3, c4) -> StructExpression:; """"""Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher's exact test for a 2x2 table. Examples; --------. >>> hl.eval(hl.fisher_exact_test(10, 10, 10, 10)); Struct(p_value=1.0000000000000002, odds_ratio=1.0,; ci_95_lower=0.24385796914260355, ci_95_upper=4.100747675033819). >>> hl.eval(hl.fisher_exact_test(51, 43, 22, 92)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967,; ci_95_lower=2.5659373368",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
Integrability,wrap,wrapper,"ef literal(x: Any, dtype: Optional[Union[HailType, str]] = None):; """"""Captures and broadcasts a Python variable or object as an expression. Examples; --------. >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; -----; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; :class:`.Table` or :class:`.MatrixTable`. Parameters; ----------; x; Object to capture and broadcast as an expression. Returns; -------; :class:`.Expression`; """"""; wrapper = {'has_expr': False, 'has_free_vars': False}. def typecheck_expr(t, x):; if isinstance(x, Expression):; wrapper['has_expr'] = True; wrapper['has_free_vars'] |= (; builtins.len(x._ir.free_vars) > 0; or builtins.len(x._ir.free_agg_vars) > 0; or builtins.len(x._ir.free_scan_vars) > 0; ). if x.dtype != t:; raise TypeError(f""'literal': type mismatch: expected '{t}', found '{x.dtype}'""); elif x._indices.source is not None:; if x._indices.axes:; raise ExpressionException(; f""'literal' can only accept scalar or global expression arguments,""; f"" found indices {x._indices.axes}""; ); return False; elif x is None or x is pd.NA:; return False; else:; t._typecheck_one_level(x); return True. if dtype is None:; dtype = impute_type(x). # Special handling of numpy. Have to extract from numpy scalars, do nothing on numpy arrays; if isinstance(x, np.generic):; x = x.item(); elif isinstance(x, np.ndarray):; pass; else:; try:; dtype._traverse(x, typecheck_expr); except TypeError as e:; raise TypeError(""'literal': ob",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
Modifiability,variab,variable,"-----; t : :class:`str` or :class:`.HailType`; Type of the missing expression. Returns; -------; :class:`.Expression`; A missing expression of type `t`.; """"""; return construct_expr(ir.NA(t), t). [docs]@deprecated(version=""0.2.62"", reason=""Replaced by hl.missing""); @typecheck(t=hail_type); def null(t: Union[HailType, str]):; """"""Deprecated in favor of :func:`.missing`. Creates an expression representing a missing value of a specified type. Examples; --------. >>> hl.eval(hl.null(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.null('array<str>')); None. Notes; -----; This method is useful for constructing an expression that includes missing; values, since :obj:`None` cannot be interpreted as an expression. Parameters; ----------; t : :class:`str` or :class:`.HailType`; Type of the missing expression. Returns; -------; :class:`.Expression`; A missing expression of type `t`.; """"""; return missing(t). [docs]@typecheck(x=anytype, dtype=nullable(hail_type)); def literal(x: Any, dtype: Optional[Union[HailType, str]] = None):; """"""Captures and broadcasts a Python variable or object as an expression. Examples; --------. >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; -----; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; :class:`.Table` or :class:`.MatrixTable`. Parameters; ----------; x; Object to capture and broadcast as an expression. Returns; -------; :class:`.Expression`; """"""; wrapper = {'has_expr': False, 'has_free_vars': False}. def ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
Performance,perform,performs,"oadcasting; def ceil(x):; """"""The smallest integral value that is greater than or equal to `x`. Examples; --------. >>> hl.eval(hl.ceil(3.1)); 4.0. Parameters; ----------; x : :class:`.Float32Expression`,:class:`.Float64Expression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""ceil"", x.dtype, x). [docs]@typecheck(n_hom_ref=expr_int32, n_het=expr_int32, n_hom_var=expr_int32, one_sided=expr_bool); def hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference g",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
Safety,abort,aborting,"d in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
Security,access,accessible,"eight vector (our `w`) as; :math:`\lambda` or `lb` and the non-centrality vector (our `lam`) as `nc`. We use the Davies' algorithm which was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Exp",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
Testability,log,log,"int32)); return _func('approxCDFCombine', t, k, left, right). @typecheck(cdf=expr_struct(), failure_prob=expr_oneof(expr_float32, expr_float64), all_quantiles=bool); def _error_from_cdf(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """""". def compute_sum(cdf):; s = hl.sum(; hl.range(0, hl.len(cdf._compaction_counts)).map(lambda i: cdf._compaction_counts[i] * (2 ** (2 * i))); ); return s / (cdf.ranks[-1] ** 2). def update_grid_size(p, s):; return 4 * hl.sqrt(hl.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure_prob):; return hl.sqrt(hl.log(2 / failure_prob) * s / 2). def compute_global_error(s):; return hl.rbind(compute_grid_size(s), lambda p: 1 / p + compute_single_error(s, failure_prob / p)). if all_quantiles:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_global_error)); else:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_single_error)). def _error_from_cdf_python(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_pr",MatchSource.WIKI,docs/0.2/_modules/hail/expr/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html
Deployability,update,update,"ich_missing_bit = i % 8; if which_missing_bit == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; kwargs[f] = None; else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); kwargs[f] = field_decoded. return Struct(**kwargs). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; keys = list(self.keys()); length = len(keys); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two f",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
Integrability,wrap,wrapper,"eturn isinstance(other, tdict) and self.key_type == other.key_type and self.value_type == other.value_type. def _pretty(self, b, indent, increment):; b.append('dict<'); self.key_type._pretty(b, indent, increment); b.append(', '); self.value_type._pretty(b, indent, increment); b.append('>'). def _parsable_string(self):; return ""Dict[{},{}]"".format(self.key_type._parsable_string(), self.value_type._parsable_string()). def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[dict, frozendict]:; d = {; self.key_type._convert_from_json_na(elt['key'], _should_freeze=True): self.value_type._convert_from_json_na(; elt['value'], _should_freeze=_should_freeze; ); for elt in x; }; if _should_freeze:; return frozendict(d); return d. def _convert_to_json(self, x):; return [; {'key': self.key_type._convert_to_json(k), 'value': self.value_type._convert_to_json(v)}; for k, v in x.items(); ]. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> Union[dict, frozendict]:; # NB: We ensure the key is always frozen with a wrapper on the key_type in the _array_repr.; d = {}; length = byte_reader.read_int32(); for _ in range(length):; element = self._array_repr.element_type._convert_from_encoding(byte_reader, _should_freeze); d[element.key] = element.value. if _should_freeze:; return frozendict(d); return d. def _convert_to_encoding(self, byte_writer: ByteWriter, value):; length = len(value); byte_writer.write_int32(length); for k, v in value.items():; self._array_repr.element_type._convert_to_encoding(byte_writer, {'key': k, 'value': v}). def _propagate_jtypes(self, jtype):; self._key_type._add_jtype(jtype.keyType()); self._value_type._add_jtype(jtype.valueType()). def unify(self, t):; return isinstance(t, tdict) and self.key_type.unify(t.key_type) and self.value_type.unify(t.value_type). def subst(self):; return tdict(self._key_type.subst(), self._value_type.subst()). def clear(self):; self.key_type.clear(); self.value_type.clear(). def _get_context",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
Modifiability,parameteriz,parameterized,"_tbool). def _parsable_string(self):; return ""Boolean"". def unify(self, t):; return t == tbool. def subst(self):; return self. def clear(self):; pass. def to_numpy(self):; return bool. def _byte_size(self):; return 1. def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> bool:; return byte_reader.read_bool(). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; byte_writer.write_bool(value). class _trngstate(HailType):; def __init__(self):; super(_trngstate, self).__init__(). def __str__(self):; return ""rng_state"". def _eq(self, other):; return isinstance(other, _trngstate). def _parsable_string(self):; return ""RNGState"". def unify(self, t):; return t == trngstate. def subst(self):; return self. def clear(self):; pass. [docs]class tndarray(HailType):; """"""Hail type for n-dimensional arrays. .. include:: _templates/experimental.rst. In Python, these are represented as NumPy :obj:`numpy.ndarray`. Notes; -----. NDArrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array.; ndim : int32; Number of dimensions. See Also; --------; :class:`.NDArrayExpression`, :obj:`.nd.array`; """""". @typecheck_method(element_type=hail_type, ndim=oneof(NatBase, int)); def __init__(self, element_type, ndim):; self._element_type = element_type; self._ndim = NatLiteral(ndim) if isinstance(ndim, int) else ndim; super(tndarray, self).__init__(). @property; def element_type(self):; """"""NDArray element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. @property; def ndim(self):; """"""NDArray number of dimensions. Returns; -------; :obj:`int`; Number of dimensions.; """"""; assert isinstance(self._ndim, NatLiteral), ""tndarray must be realized with a concrete number of dimensions""; return self._ndim.n. def _traverse(self, obj, f):; if f(self, obj):; for elt in np.nditer(obj, ['zerosize_ok']):; self.element_type._traverse(elt.",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
Performance,load,loads,"formatted string representation of the type. Parameters; ----------; indent : :obj:`int`; Spaces to indent. Returns; -------; :class:`str`; """"""; b = []; b.append(' ' * indent); self._pretty(b, indent, increment); return ''.join(b). def _pretty(self, b, indent, increment):; b.append(str(self)). @abc.abstractmethod; def _parsable_string(self) -> str:; raise NotImplementedError. def typecheck(self, value):; """"""Check that `value` matches a type. Parameters; ----------; value; Value to check. Raises; ------; :obj:`TypeError`; """""". def check(t, obj):; t._typecheck_one_level(obj); return True. self._traverse(value, check). @abc.abstractmethod; def _typecheck_one_level(self, annotation):; raise NotImplementedError. def _to_json(self, x):; converted = self._convert_to_json_na(x); return json.dumps(converted). def _convert_to_json_na(self, x):; if x is None:; return x; else:; return self._convert_to_json(x). def _convert_to_json(self, x):; return x. def _from_json(self, s):; x = json.loads(s); return self._convert_from_json_na(x). def _convert_from_json_na(self, x, _should_freeze: bool = False):; if x is None:; return x; else:; return self._convert_from_json(x, _should_freeze). def _convert_from_json(self, x, _should_freeze: bool = False):; return x. def _from_encoding(self, encoding):; return self._convert_from_encoding(ByteReader(memoryview(encoding))). def _to_encoding(self, value) -> bytes:; buf = bytearray(); self._convert_to_encoding(ByteWriter(buf), value); return bytes(buf). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; raise ValueError(""Not implemented yet""). def _convert_to_encoding(self, byte_writer, value):; raise ValueError(""Not implemented yet""). @staticmethod; def _missing(value):; return value is None or value is pd.NA. def _traverse(self, obj, f):; """"""Traverse a nested type and object. Parameters; ----------; obj : Any; f : Callable[[HailType, Any], bool]; Function to evaluate on the type and object. Traverse children if; the f",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
Security,hash,hash,"ferences) == 0. def _to_json_context(self):; if self._json is None:; self._json = {'reference_genomes': {r: hl.get_reference(r)._config for r in self.references}}; return self._json. @classmethod; def union(cls, *types):; ctxs = [t.get_context() for t in types if not t.get_context().is_empty]; if len(ctxs) == 0:; return _empty_context; if len(ctxs) == 1:; return ctxs[0]; refs = ctxs[0].references.union(*[ctx.references for ctx in ctxs[1:]]); return HailTypeContext(refs). _empty_context = HailTypeContext(). [docs]class HailType(object):; """"""; Hail type superclass.; """""". def __init__(self):; super(HailType, self).__init__(); self._context = None. def __repr__(self):; s = str(self).replace(""'"", ""\\'""); return ""dtype('{}')"".format(s). @abc.abstractmethod; def _eq(self, other):; raise NotImplementedError. def __eq__(self, other):; return isinstance(other, HailType) and self._eq(other). @abc.abstractmethod; def __str__(self):; raise NotImplementedError. def __hash__(self):; # FIXME this is a bit weird; return 43 + hash(str(self)). def pretty(self, indent=0, increment=4):; """"""Returns a prettily formatted string representation of the type. Parameters; ----------; indent : :obj:`int`; Spaces to indent. Returns; -------; :class:`str`; """"""; b = []; b.append(' ' * indent); self._pretty(b, indent, increment); return ''.join(b). def _pretty(self, b, indent, increment):; b.append(str(self)). @abc.abstractmethod; def _parsable_string(self) -> str:; raise NotImplementedError. def typecheck(self, value):; """"""Check that `value` matches a type. Parameters; ----------; value; Value to check. Raises; ------; :obj:`TypeError`; """""". def check(t, obj):; t._typecheck_one_level(obj); return True. self._traverse(value, check). @abc.abstractmethod; def _typecheck_one_level(self, annotation):; raise NotImplementedError. def _to_json(self, x):; converted = self._convert_to_json_na(x); return json.dumps(converted). def _convert_to_json_na(self, x):; if x is None:; return x; else:; return self._conv",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
Testability,assert,assert,"lf):; pass. [docs]class tndarray(HailType):; """"""Hail type for n-dimensional arrays. .. include:: _templates/experimental.rst. In Python, these are represented as NumPy :obj:`numpy.ndarray`. Notes; -----. NDArrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array.; ndim : int32; Number of dimensions. See Also; --------; :class:`.NDArrayExpression`, :obj:`.nd.array`; """""". @typecheck_method(element_type=hail_type, ndim=oneof(NatBase, int)); def __init__(self, element_type, ndim):; self._element_type = element_type; self._ndim = NatLiteral(ndim) if isinstance(ndim, int) else ndim; super(tndarray, self).__init__(). @property; def element_type(self):; """"""NDArray element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. @property; def ndim(self):; """"""NDArray number of dimensions. Returns; -------; :obj:`int`; Number of dimensions.; """"""; assert isinstance(self._ndim, NatLiteral), ""tndarray must be realized with a concrete number of dimensions""; return self._ndim.n. def _traverse(self, obj, f):; if f(self, obj):; for elt in np.nditer(obj, ['zerosize_ok']):; self.element_type._traverse(elt.item(), f). def _typecheck_one_level(self, annotation):; if annotation is not None and not isinstance(annotation, np.ndarray):; raise TypeError(""type 'ndarray' expected Python 'numpy.ndarray', but found type '%s'"" % type(annotation)). def __str__(self):; return ""ndarray<{}, {}>"".format(self.element_type, self.ndim). def _eq(self, other):; return isinstance(other, tndarray) and self.element_type == other.element_type and self.ndim == other.ndim. def _pretty(self, b, indent, increment):; b.append('ndarray<'); self._element_type._pretty(b, indent, increment); b.append(', '); b.append(str(self.ndim)); b.append('>'). def _parsable_string(self):; return f'NDArray[{self._element_type._parsable_string()},{self.ndim}]'. def _convert_from_json(self, x, ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
Usability,clear,clear,"x, _should_freeze: bool = False):; return x. def _from_encoding(self, encoding):; return self._convert_from_encoding(ByteReader(memoryview(encoding))). def _to_encoding(self, value) -> bytes:; buf = bytearray(); self._convert_to_encoding(ByteWriter(buf), value); return bytes(buf). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; raise ValueError(""Not implemented yet""). def _convert_to_encoding(self, byte_writer, value):; raise ValueError(""Not implemented yet""). @staticmethod; def _missing(value):; return value is None or value is pd.NA. def _traverse(self, obj, f):; """"""Traverse a nested type and object. Parameters; ----------; obj : Any; f : Callable[[HailType, Any], bool]; Function to evaluate on the type and object. Traverse children if; the function returns ``True``.; """"""; f(self, obj). @abc.abstractmethod; def unify(self, t):; raise NotImplementedError. @abc.abstractmethod; def subst(self):; raise NotImplementedError. @abc.abstractmethod; def clear(self):; raise NotImplementedError. def _get_context(self):; return _empty_context. def get_context(self):; if self._context is None:; self._context = self._get_context(); return self._context. def to_numpy(self):; return object. hail_type = oneof(HailType, transformed((str, dtype)), type(None)). class _tvoid(HailType):; def __init__(self):; super(_tvoid, self).__init__(). def __str__(self):; return ""void"". def _eq(self, other):; return isinstance(other, _tvoid). def _parsable_string(self):; return ""Void"". def unify(self, t):; return t == tvoid. def subst(self):; return self. def clear(self):; pass. def _convert_from_encoding(self, *_):; raise ValueError(""Cannot decode void type""). def _convert_to_encoding(self, *_):; raise ValueError(""Cannot encode void type""). class _tint32(HailType):; """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`.; """""". def __init__(s",MatchSource.WIKI,docs/0.2/_modules/hail/expr/types.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html
Deployability,update,updated,"code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True; """"""; return _ALLELE_STRS[self]. @classmethod; def _missing_(cls, value):; if not isinstance(value, str):; return None; return cls.__members__.get(value.upper()). [docs] @staticmethod; def strings():; """"""Returns the names of the allele types, for use with; :func:`~hail.expr.functions.literal`; """"""; return list(_ALLELE_STRS). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/allele_type.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html
Deployability,update,updated," the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded representation of the; called alleles. Examples; --------. >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; -----; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters; ----------; n_alleles : :obj:`int`; Number of total alleles, including the reference. Returns; -------; :obj:`list` of :obj:`int`; """"""; r = [0] * n_alleles; for a in self._alleles:; r[a] += 1; return r. [docs] def unphased_diploid_gt_index(self):; """"""Return the genotype index for unphased, diploid calls. Returns; -------; :obj:`int`; """"""; from hail.utils import FatalError. if self.ploidy != 2 or self.phased:; raise FatalError(; ""'unphased_diploid_gt_index' is only valid for unphased, diploid calls. Found {}."".format(repr(self)); ); a0 = self._alleles[0]; a1 = self._alleles[1]; assert a0 <= a1; return a1 * (a1 + 1) / 2 + a0. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
Security,hash,hash," check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__(self, item):; """"""Get the i*th* allele. Returns; -------; :obj:`int`; """"""; return self._alleles[item]. @property; def alleles(self) -> Sequence[int]:; """"""Get the alleles of this call. Returns; -------; :obj:`list` of :obj:`int`; """"""; return self._alleles. @property; def ploidy(self):; """"""The number of alleles for this call. Returns; -------; :obj:`int`; """"""; return len(self._alleles). @property; def phased(self):; """"""True if the call is phased. Returns; -------; :obj:`bool`; """"""; return self._phased. [docs] def is_haploid(self):; """"""True if the ploidy == 1. :rtype: bool; """"""; return self.ploidy == 1. [docs] def is_diploid(self):; """"""True if the ploidy == 2. :rtype: bool; """"""; return self.ploidy == 2. [docs] def is_hom_ref(self):; """"""True if the call has no alternate alleles. :rtype: bool; """"""; if self.ploidy == 0:; return False. return all(a == 0 for a in self._alleles). [docs] def is_het(self):; """"""True if the ca",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
Testability,assert,assert," Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/call.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html
Deployability,update,updated,"l_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):; """"""Reference genome. :return: :class:`.ReferenceGenome`; """"""; return self._rg. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
Security,hash,hash,":`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
Testability,assert,assert,"ail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/locus.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html
Deployability,update,updated,"}]"".format(; missing_sex_count, missing_sex_values; ); ). return Pedigree(trios). @property; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """"""; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father and mother. :rtype: list of :class:`.Trio`; """"""; return list(filter(lambda t: t.is_complete(), self.trios)). [docs] @typecheck_method(samples=sequenceof(nullable(str))); def filter_to(self, samples):; """"""Filter the pedigree to a given list of sample IDs. **Notes**. For any trio, the following steps will be applied:. - If the proband is not in the list of samples provided, the trio is removed.; - If the father is not in the list of samples provided, `pat_id` is set to ``None``.; - If the mother is not in the list of samples provided, `mat_id` is set to ``None``. Parameters; ----------; samples: :obj:`list` [:obj:`str`]; Sample IDs to keep. Returns; -------; :class:`.Pedigree`; """"""; sample_set = set(samples). filtered_trios = []; for trio in self._trios:; restricted_trio = trio._restrict_to(sample_set); if restricted_trio is not None:; filtered_trios.append(restricted_trio). return Pedigree(filtered_trios). [docs] @typecheck_method(path=str); def write(self, path):; """"""Write a .fam file to the given path. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). **Notes**. This method writes a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. .. caution::. Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use :func:`~.import_fam` to; manipulate this information. :param path: output path; :type path: str; """""". lines = [t._to_fam_file_line() for t in self._trios]. with Env.fs().open(path, mode=""w"") as file:; for line in lines:; file.write(line + ""\n""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
Security,hash,hash,"e: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; return hash((self._s, self._pat_id, self._mat_id, self._fam_id, self._is_female)). @property; def s(self):; """"""ID of proband in trio, never missing. :rtype: str; """""". return self._s. @property; def pat_id(self):; """"""ID of father in trio, may be missing. :rtype: str or None; """""". return self._pat_id. @property; def mat_id(self):; """"""ID of mother in trio, may be missing. :rtype: str or None; """""". return self._mat_id. @property; def fam_id(self):; """"""Family ID. :rtype: str or None; """""". return self._fam_id. @property; def is_male(self):; """"""Returns ``True`` if the proband is a reported male,; ``False`` if reported female, and ``None`` if no sex is defined. :rtype: bool or None; """""". if self._is_female is None:; return None. return self._is_female is False. @property; def is_female(self):; """"""Returns ``True`` if the proband is a reported female,; ``False`` if reported male, and ``None`` if no sex is defined. :rtype: bool or None; """""". if self._is_female is None:; return None. return self",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
Testability,test,test,"se_zero(sample_id):; if sample_id is None:; return ""0""; return sample_id. line_list = [; sample_id_or_else_zero(self._fam_id),; self._s,; sample_id_or_else_zero(self._pat_id),; sample_id_or_else_zero(self._mat_id),; self._sex_as_numeric_string(),; ""0"",; ]; return ""\t"".join(line_list). [docs]class Pedigree(object):; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """""". @typecheck_method(trios=sequenceof(Trio)); def __init__(self, trios):; self._trios = tuple(trios). def __eq__(self, other):; return isinstance(other, Pedigree) and self._trios == other._trios. def __hash__(self):; return hash(self._trios). def __iter__(self):; return self._trios.__iter__(). [docs] @classmethod; @typecheck_method(fam_path=str, delimiter=str); def read(cls, fam_path, delimiter='\\s+') -> 'Pedigree':; """"""Read a PLINK .fam file and return a pedigree object. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'). Notes; -------. See `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_ for; the required format. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """""". trios = []; missing_sex_count = 0; missing_sex_values = set(); with Env.fs().open(fam_path) as file:; for line in file:; split_line = re.split(delimiter, line.strip()); num_fields = len(split_line); if num_fields != 6:; raise FatalError(; ""Require 6 fields per line in .fam, but this line has {}: {}"".format(num_fields, line); ); (fam, kid, dad, mom, sex, _) = tuple(split_line); # 1 is male, 2 is female, 0 is unknown.; is_female = sex == ""2"" if sex in {'1', '2'} else None. if is_female is None:; missing_sex_count += 1; missing_sex_values.add(kid). trio = Trio(; kid,; fam if fam != ""0"" else None,; dad if dad != ""0"" else None,; mom if mom != ""0"" else None,; is_female,; ); trios.append(trio). only_ids = [trio.s for trio in trios]; duplicate_ids = [id fo",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/pedigree.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html
Availability,down,download,"ce genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_files = (fasta_file, index_file). [docs] def has_sequence(self):; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; return self._sequence_files is not None. [docs] def remove_sequence(self):; """"""Remove the reference sequence.""""""; self._sequence_files = None; Env.backend().remove_sequence(self.name). [docs] @classmethod; @typecheck_method(; name=str,; fasta_file=str,; index_file=str,; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
Deployability,update,updated," : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """""". Env.backend().add_liftover(self.name, chain_file, dest_reference_genome.name); if dest_reference_genome.name in self._liftovers:; raise KeyError(f""Liftover already exists from {self.name} to {dest_reference_genome.name}.""); if dest_reference_genome.name == self.name:; raise ValueError(f'Destination reference genome cannot have the same name as this reference {self.name}.'); self._liftovers[dest_reference_genome.name] = chain_file. [docs] @typecheck_method(global_pos=int); def locus_from_global_position(self, global_pos: int) -> 'hl.Locus':; """""" ""; Constructs a locus from a global position in reference genome.; The inverse of :meth:`.Locus.position`. Examples; --------; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters; ----------; global_pos : int; Zero-based global base position along the reference genome. Returns; -------; :class:`.Locus`; """"""; if global_pos < 0:; raise ValueError(f""global_pos must be non-negative, got {global_pos}""). if self._global_positions_list is None:; # dicts are in insertion order as of 3.7; self._global_positions_list = list(self.global_positions_dict.values()). global_positions = self._global_positions_list; contig = self.contigs[bisect_right(global_positions, global_pos) - 1]; contig_pos = self.global_positions_dict[contig]. if global_pos >= contig_pos + self.lengths[contig]:; raise ValueError(f""global_pos {global_pos} exceeds length of reference genome {self}.""). return hl.Locus(contig, global_pos - contig_pos + 1, self). rg_type.set(ReferenceGenome). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
Modifiability,config,config,"`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; '",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
Performance,load,load,"},; {""name"": ""2"", ""length"": 20000000},; {""name"": ""X"", ""length"": 19856300},; {""name"": ""Y"", ""length"": 78140000},; {""name"": ""MT"", ""length"": 532}],; ""xContigs"": [""X""],; ""yContigs"": [""Y""],; ""mtContigs"": [""MT""],; ""par"": [{""start"": {""contig"": ""X"",""position"": 60001},""end"": {""contig"": ""X"",""position"": 2699521}},; {""start"": {""contig"": ""Y"",""position"": 10001},""end"": {""contig"": ""Y"",""position"": 2649521}}]; }. `name` must be unique and not overlap with Hail's pre-instantiated; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``.; The contig names in `xContigs`, `yContigs`, and `mtContigs` must be; present in `contigs`. The intervals listed in `par` must have contigs in; either `xContigs` or `yContigs` and must have positions between 0 and; the contig length given in `contigs`. Parameters; ----------; path : :class:`str`; Path to JSON file. Returns; -------; :class:`.ReferenceGenome`; """"""; with hl.hadoop_open(path) as f:; return ReferenceGenome._from_config(json.load(f)). [docs] @typecheck_method(output=str); def write(self, output):; """""" ""Write this reference genome to a file in JSON format. Examples; --------. >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; -----. Use :meth:`~hail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ...",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
Security,access,access,"hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference genome using; :func:`~hail.get_reference` anytime afterwards. Note; ----; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; ----; Hail allows setting a default reference so that the ``reference_genome``; argument of :func:`~hail.methods.import_vcf` does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the ``default_reference`` argument of; :func:`~hail.init`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~ha",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
Testability,assert,assert,"`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class:`str` to :obj:`int`; Dict of contig names to contig lengths.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end); """""". @classmethod; def _from_config(cls, config, _builtin=False):; def par_tuple(p):; assert p['start']['contig'] == p['end']['contig']; return (p['start']['contig'], p['start']['position'], p['end']['position']). contigs = config['contigs']; return ReferenceGenome(; config['name'],; [c['name'] for c in contigs],; {c['name']: c['length'] for c in contigs},; config['xContigs'],; config['yContigs'],; config['mtContigs'],; [par_tuple(p) for p in config['par']],; _builtin,; ). @typecheck_method(; name=str,; contigs=sequenceof(str),; lengths=dictof(str, int),; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; _builtin=bool,; ); def __init__(self, name, contigs, lengths, x_contigs=[], y_contigs=[], mt_contigs=[], par=[], _builtin=False):; contigs = wrap_to_list(contigs); x_contigs = wrap_to_list(x_contigs); y_contigs = wrap_to_list(y_contigs); mt_contigs = wrap_to_list(mt_contigs). self._config = {; 'name': name,; 'contigs': [{'name': c, 'length': l} for c, l in lengths.items()],; '",MatchSource.WIKI,docs/0.2/_modules/hail/genetics/reference_genome.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html
Deployability,update,updated,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/aes.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html
Deployability,update,updated,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html
Deployability,update,updated,"he facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; _base_scale_mappings: ClassVar = {; ""shared_xaxes"": ""all"",; ""shared_yaxes"": ""all"",; }. _scale_mappings: ClassVar = {; ""fixed"": _base_scale_mappings,; ""free_x"": {; **_base_scale_mappings,; ""shared_xaxes"": False,; },; ""free_y"": {; **_base_scale_mappings,; ""shared_yaxes"": False,; },; ""free"": {; ""shared_xaxes"": False,; ""shared_yaxes"": False,; },; }. def __init__(; self, facets: StructExpression, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ):; if nrow is not None and ncol is not None:; raise ValueError(""Both `nrow` and `ncol` were specified. "" ""Please specify only one of these values.""); if scales not in self._scale_mappings:; raise ValueError(; f""An unsupported value ({scales}) was provided for `scales`. ""; f""Supported values are: {[k for k in self._scale_mappings.keys()]}.""; ); self.nrow = nrow; self.ncol = ncol; self.facets = facets; self.scales = scales. def get_expr_to_group_by(self) -> StructExpression:; return self.facets. def get_facet_nrows_and_ncols(self, num_facet_values: int) -> Tuple[int, int]:; if self.ncol is not None:; return (n_partitions(num_facet_values, self.ncol), self.ncol); elif self.nrow is not None:; return (self.nrow, n_partitions(num_facet_values, self.nrow)); else:; ncol = int(math.ceil(math.sqrt(num_facet_values))); return (n_partitions(num_facet_values, ncol), ncol). def get_shared_axis_kwargs(self) -> Dict[str, str]:; return self._scale_mappings[self.scales]. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/facets.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html
Availability,down,down,"n 0 and 1.; position: :class:`str`; Tells how to deal with different groups of data at same point. Options are ""stack"" and ""dodge"". Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomHistogram(; mapping,; min_val=min_val,; max_val=max_val,; bins=bins,; fill=fill,; color=color,; alpha=alpha,; position=position,; size=size,; ). # Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def poin",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
Deployability,update,updated,"is facing side, none by default. Overrides ``color`` aesthetic. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomArea(mapping, fill=fill, color=color). class GeomRibbon(Geom):; aes_to_arg: ClassVar = {; ""fill"": (""fillcolor"", ""black""),; ""color"": (""line_color"", ""rgba(0, 0, 0, 0)""),; ""tooltip"": (""hovertext"", None),; ""fill_legend"": (""name"", None),; }. def __init__(self, aes, fill, color):; super().__init__(aes); self.fill = fill; self.color = color. def apply_to_fig(; self, grouped_data, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; def plot_group(df):; trace_args_bottom = {; ""x"": df.x,; ""y"": df.ymin,; ""row"": facet_row,; ""col"": facet_col,; ""mode"": ""lines"",; ""showlegend"": False,; }; self._add_aesthetics_to_trace_args(trace_args_bottom, df); self._update_legend_trace_args(trace_args_bottom, legend_cache). trace_args_top = {; ""x"": df.x,; ""y"": df.ymax,; ""row"": facet_row,; ""col"": facet_col,; ""mode"": ""lines"",; ""fill"": 'tonexty',; }; self._add_aesthetics_to_trace_args(trace_args_top, df); self._update_legend_trace_args(trace_args_top, legend_cache). fig_so_far.add_scatter(**trace_args_bottom); fig_so_far.add_scatter(**trace_args_top). for group_df in grouped_data:; plot_group(group_df). def get_stat(self):; return StatIdentity(). [docs]def geom_ribbon(mapping=aes(), fill=None, color=None):; """"""Creates filled in area between two lines specified by x, ymin, and ymax. Supported aesthetics: ``x``, ``ymin``, ``ymax``, ``color``, ``fill``, ``tooltip``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; fill:; Color of fill to draw, black by default. Overrides ``fill`` aesthetic.; color:; Color of line to draw outlining both side, none by default. Overrides ``color`` aesthetic. :return:; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomRibbon(mapping, fill=fill, color=color). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
Energy Efficiency,power,power,"one),; ""tooltip"": (""hovertext"", None),; ""fill_legend"": (""name"", None),; ""alpha"": (""marker_opacity"", None),; }. def __init__(self, aes, k=1000, smoothing=0.5, fill=None, color=None, alpha=None, smoothed=False):; super().__init__(aes); self.k = k; self.smoothing = smoothing; self.fill = fill; self.color = color; self.alpha = alpha; self.smoothed = smoothed. def apply_to_fig(; self, grouped_data, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; from hail.expr.functions import _error_from_cdf_python. def plot_group(df, idx):; data = df.attrs['data']. if self.smoothed:; n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev):; inv_scale = (np.sqrt(n * slope) / self.smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (; (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); ); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). trace_args = {; ""x"": x_d,; ""y"": final,; ""mode"": ""lines"",; ""fill"": ""tozeroy"",; ""row"": facet_row,; ""col"": facet_col,; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_scatter(**trace_args); else:; confidence = 5. y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]). left = np.concatenate([[min_x], x[keep]]); right = np.concatenate([x[keep], [max_x]]); widths = right - left. trace_args = {; ""x"": [min_x, *x[keep]],; ""y"": slopes,; ""row"": facet_row,",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
Integrability,interface,interface,"width / 2; bar_width = bin_width; else:; raise ValueError(f""Histogram does not support position = {self.position}""). right_xs = left_xs + bin_width. trace_args = {; ""x"": x,; ""y"": df.y,; ""row"": facet_row,; ""col"": facet_col,; ""customdata"": list(zip(left_xs, right_xs)),; ""width"": bar_width,; ""hovertemplate"": ""Range: [%{customdata[0]:.3f}-%{customdata[1]:.3f})<br>""; ""Count: %{y}<br>""; ""<extra></extra>"",; }. self._add_aesthetics_to_trace_args(trace_args, df); self._update_legend_trace_args(trace_args, legend_cache). fig_so_far.add_bar(**trace_args). for idx, group_df in enumerate(grouped_data):; plot_group(group_df, idx). fig_so_far.update_layout(barmode=bar_position_plotly_to_gg(self.position)). def get_stat(self):; return StatBin(self.min_val, self.max_val, self.bins). [docs]def geom_histogram(; mapping=aes(),; *,; min_val=None,; max_val=None,; bins=None,; fill=None,; color=None,; alpha=None,; position='stack',; size=None,; ):; """"""Creates a histogram. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; min_val: `int` or `float`; Minimum value to include in histogram; max_val: `int` or `float`; Maximum value to include in histogram; bins: `int`; Number of bins to plot. 30 by default.; fill:; A single fill color for all bars of histogram, overrides ``fill`` aesthetic.; color:; A single outline color for all bars of histogram, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; position: :class:`str`; Tells how to deal with different groups of data at same point. Options are ""stack"" and ""dodge"". Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomHistogram(; mapping,; min_val=min_val,; max_val=max_val,; bins=bins,; fill=fill,; color=color,; alpha=alpha,; position=position,; size=size,; ). # Computes the maximum entropy distribution whose",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
Modifiability,variab,variables," the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper); fx, fy = xi, yi; new_y[i] = fy; keep[i] = True. min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True). # Consider a line l from (fx, fy) to (x[j], y?). As we increase y?, l first; # bumps into the upper staircase at (x[ui], y[ui] + e), and as we decrease; # y?, l first bumps into the lower staircase at (x[li], y[li+1] - e).; # We track the min and max slopes l can have while staying between the; # staircases, as well as the points li and ui where the line must bend if; # forced too high or too low. while True:; lower_slope = slope_from_fixe",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/geoms.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html
Deployability,continuous,continuous,"les.; if aesthetic_str == ""x"":; if is_continuous:; self.scales[""x""] = scale_x_continuous(); elif is_genomic_type(dtype):; self.scales[""x""] = scale_x_genomic(reference_genome=dtype.reference_genome); else:; self.scales[""x""] = scale_x_discrete(); elif aesthetic_str == ""y"":; if is_continuous:; self.scales[""y""] = scale_y_continuous(); elif is_genomic_type(dtype):; raise ValueError(""Don't yet support y axis genomic""); else:; self.scales[""y""] = scale_y_discrete(); elif aesthetic_str == ""color"" and not is_continuous:; self.scales[""color""] = scale_color_discrete(); elif aesthetic_str == ""color"" and is_continuous:; self.scales[""color""] = scale_color_continuous(); elif aesthetic_str == ""fill"" and not is_continuous:; self.scales[""fill""] = scale_fill_discrete(); elif aesthetic_str == ""fill"" and is_continuous:; self.scales[""fill""] = scale_fill_continuous(); elif aesthetic_str == ""shape"" and not is_continuous:; self.scales[""shape""] = scale_shape_auto(); elif aesthetic_str == ""shape"" and is_continuous:; raise ValueError(; ""The 'shape' aesthetic does not support continuous ""; ""types. Specify values of a discrete type instead.""; ); elif is_continuous:; self.scales[aesthetic_str] = ScaleContinuous(aesthetic_str); else:; self.scales[aesthetic_str] = ScaleDiscrete(aesthetic_str). def copy(self):; return GGPlot(self.ht, self.aes, self.geoms[:], self.labels, self.coord_cartesian, self.scales, self.facet). def verify_scales(self):; for aes_key in self.aes.keys():; check_scale_continuity(self.scales[aes_key], self.aes[aes_key].dtype, aes_key); for geom in self.geoms:; aesthetic_dict = geom.aes.properties; for aes_key in aesthetic_dict.keys():; check_scale_continuity(self.scales[aes_key], aesthetic_dict[aes_key].dtype, aes_key). [docs] def to_plotly(self):; """"""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """""". def make_geom_label(geom_idx):; return f""geom{geom_idx}"". def select_table():; fields_to_select = {""figure_mapping",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
Integrability,interface,interface,"ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""; assert isinstance(mapping, Aesthetic); return GGPlot(table, mapping). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
Testability,assert,assert,"eoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartesian = other; elif isinstance(other, Scale):; copied.scales[other.aesthetic_name] = other; elif isinstance(other, Aesthetic):; copied.aes = copied.aes.merge(other); elif isinstance(other, Faceter):; copied.facet = other; else:; raise ValueError(""Not implemented""). return copied. def add_default_scales(self, aesthetic):; for aesthetic_str, mapped_expr in aesthetic.items():; dtype = mapped_expr.dtype; if aesthetic_str not in self.scales:; is_continuous = is_continuous_type(dtype); # We only know how to come up with a few default scales.; if aesthetic_str == ""x"":; if is_continuous:; self.scales[""x""] = scale_x_continuous(); elif is_genomic_type(dtype):; self.scales[""x""] = scale_x_genomic(reference_ge",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/ggplot.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html
Deployability,update,updated,"l if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the x-axis label.; """"""; return Labels(xlabel=label). [docs]def ylab(label):; """"""Sets the y-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired y-axis label of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the y-axis label.; """"""; return Labels(ylabel=label). def labs(**group_labels):; """"""Sets the labels for the legend groups of a plot. Examples; --------. Create a scatterplot and label the legend groups according to their field names:. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared=ht.idx ** 2); >>> ht = ht.annotate(even=hl.if_else(ht.idx % 2 == 0, ""yes"", ""no"")); >>> ht = ht.annotate(threeven=hl.if_else(ht.idx % 3 == 0, ""good"", ""bad"")); >>> fig = (; ... hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)); ... + hl.ggplot.geom_point(hl.ggplot.aes(color=ht.even, shape=ht.threeven)); ... + hl.ggplot.labs(color=""Even"", shape=""Threeven""); ... ). Parameters; ----------; group_labels:; Map names of plotly ``legendgroup``s to the desired replacement labels. Returns; -------; :class:`.FigureAttribute`; Label object to change the legend group labels.; """"""; return Labels(group_labels=group_labels). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/labels.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html
Availability,down,down,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
Deployability,continuous,continuous,"otly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesthetic_name, name, None, None). if isinstance(reference_genome, str):; reference_genome = get_reference(reference_genome); self.reference_genome = reference_genome. def apply_to_fig(self, parent, fig_so_far):; contig_offsets = dict(list(self.reference_genome.global_positions_dict.items())[:24]); breaks = list(contig_offsets.values()); labels = list(contig_offsets.keys()); self.update_axis(fig_so_far)(tickvals=breaks, ticktext=labels). def transform_data(self, field_expr):; return field_expr.global_position(). def is_discrete(self):; r",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
Security,hash,hash,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
Testability,log,log," None, None). if isinstance(reference_genome, str):; reference_genome = get_reference(reference_genome); self.reference_genome = reference_genome. def apply_to_fig(self, parent, fig_so_far):; contig_offsets = dict(list(self.reference_genome.global_positions_dict.items())[:24]); breaks = list(contig_offsets.values()); labels = list(contig_offsets.keys()); self.update_axis(fig_so_far)(tickvals=breaks, ticktext=labels). def transform_data(self, field_expr):; return field_expr.global_position(). def is_discrete(self):; return False. def is_continuous(self):; return False. class PositionScaleContinuous(PositionScale):; def __init__(self, axis=None, name=None, breaks=None, labels=None, transformation=""identity""):; super().__init__(axis, name, breaks, labels); self.transformation = transformation. def apply_to_fig(self, parent, fig_so_far):; super().apply_to_fig(parent, fig_so_far); if self.transformation == ""identity"":; pass; elif self.transformation == ""log10"":; self.update_axis(fig_so_far)(type=""log""); elif self.transformation == ""reverse"":; self.update_axis(fig_so_far)(autorange=""reversed""); else:; raise ValueError(f""Unrecognized transformation {self.transformation}""). def transform_data(self, field_expr):; return field_expr. def is_discrete(self):; return False. def is_continuous(self):; return True. class PositionScaleDiscrete(PositionScale):; def __init__(self, axis=None, name=None, breaks=None, labels=None):; super().__init__(axis, name, breaks, labels). def apply_to_fig(self, parent, fig_so_far):; super().apply_to_fig(parent, fig_so_far). def transform_data(self, field_expr):; return field_expr. def is_discrete(self):; return True. def is_continuous(self):; return False. class ScaleContinuous(Scale):; def __init__(self, aesthetic_name):; super().__init__(aesthetic_name). def transform_data(self, field_expr):; return field_expr. def is_discrete(self):; return False. def is_continuous(self):; return True. def valid_dtype(self, dtype):; return is_continuous_type(dtype",MatchSource.WIKI,docs/0.2/_modules/hail/ggplot/scale.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html
Availability,resilien,resilience,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
Deployability,pipeline,pipelined,"rage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
Energy Efficiency,power,power,"e_level, with_local_temp_file; from hail.utils.java import Env. block_matrix_type = lazy(). [docs]class BlockMatrix(object):; """"""Hail's block-distributed matrix of :py:data:`.tfloat64` elements. .. include:: ../_templates/experimental.rst. A block matrix is a distributed analogue of a two-dimensional; `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html>`__ with; shape ``(n_rows, n_cols)`` and NumPy dtype ``float64``.; Import the class with:. >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; :meth:`default_block_size`. **Operations and broadcasting**. The core operations are consistent with NumPy: ``+``, ``-``, ``*``, and; ``/`` for element-wise addition, subtraction, multiplication, and division;; ``@`` for matrix multiplication; ``T`` for transpose; and ``**`` for; element-wise exponentiation to a scalar power. For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (:obj:`int` or :obj:`float`). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size. To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to ``float64``. One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exceptio",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
Integrability,depend,dependency,". One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exception: block matrices do not currently support element-wise; ""outer product"" of a single row and a single column, although the same; effect can be achieved for ``*`` by using ``@``. Warning; -------. For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read(",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
Modifiability,config,configuration,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
Performance,perform,performance,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
Safety,avoid,avoiding,"cists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following metho",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
Testability,log,logarithm," ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following methods fail if any operand is block-sparse, but can be forced; by first applying :meth:`densify`. - Element-wise division between two block matrices. - Multiplication by a scalar or broadcasted vector which includes an; infinite or ``nan`` value. - Division by a scalar or broadcasted vector which includes a zero, infinite; or ``nan`` value. - Division of a scalar or broadcasted vector by a block matrix. - Element-wise exponentiation by a negative exponent. - Natural logarithm, :meth:`log`.; """""". def __init__(self, bmir):; self._bmir = bmir. [docs] @classmethod; @typecheck_method(path=str, _assert_type=nullable(tblockmatrix)); def read(cls, path, *, _assert_type=None):; """"""Reads a block matrix. Parameters; ----------; path: :class:`str`; Path to input file. Returns; -------; :class:`.BlockMatrix`; """"""; return cls(BlockMatrixRead(BlockMatrixNativeReader(path), _assert_type=_assert_type)). [docs] @classmethod; @typecheck_method(uri=str, n_rows=int, n_cols=int, block_size=nullable(int), _assert_type=nullable(tblockmatrix)); def fromfile(cls, uri, n_rows, n_cols, block_size=None, *, _assert_type=None):; """"""Creates a block matrix from a binary file. Examples; --------; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') # doctest: +SKIP. To create a block matrix of the same dimensions:. >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.fromfile; <https:/",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
Usability,clear,clear,"rix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.bu",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/blockmatrix.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html
Availability,error,errors,"d:int,is_female:bool,fam_id:str}>'). trios_sym = Env.get_uid(); entries_sym = Env.get_uid(); cols_sym = Env.get_uid(). mt = mt.annotate_globals(**{trios_sym: hl.literal(trios, trios_type)}); mt = mt._localize_entries(entries_sym, cols_sym); mt = mt.annotate_globals(**{; cols_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; id=mt[cols_sym][t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four ta",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
Deployability,configurat,configurations,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
Integrability,depend,depends,"rents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid d)` and :math:`\mathrm{P}(x \mid m)`; are computed from the PL (genotype likelihood) fields using these; factorizations:. .. math::; \mathrm{P}(x = (AA, AA, AB) \mid d) = \left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
Modifiability,inherit,inheritance,"t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Me",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
Performance,cache,cache,"nt_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative *de novo* events from trio data. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Call de novo events:. >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
Safety,avoid,avoids,"aset = dataset.filter_rows(dataset.auto_or_x_par | dataset.locus.in_x_nonpar()). hom_ref = 0; het = 1; hom_var = 2. auto = 2; hemi_x = 1. # kid, dad, mom, copy, t, u; config_counts = [; (hom_ref, het, het, auto, 0, 2),; (hom_ref, hom_ref, het, auto, 0, 1),; (hom_ref, het, hom_ref, auto, 0, 1),; (het, het, het, auto, 1, 1),; (het, hom_ref, het, auto, 1, 0),; (het, het, hom_ref, auto, 1, 0),; (het, hom_var, het, auto, 0, 1),; (het, het, hom_var, auto, 0, 1),; (hom_var, het, het, auto, 2, 0),; (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: Matrix",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
Security,validat,validation,"}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (A",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
Testability,test,test,"tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> | int64 | int64 | float64 | float64 |; +---------------+------------+-------+-------+----------+----------+; | 1:246714629 | [""C"",""A""] | 0 | 4 | 4.00e+00 | 4.55e-02 |; | 2:167262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+---------",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
Usability,simpl,simplifying,"; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Father column fields from `mt`.; - `mother` (``struct``) -- Mother column fields from `mt`.; - `proband_entry` (``struct``) -- Proband entry fields from `mt`.; - `father_entry` (``struct``) -- Father entry fields from `mt`.; - `proband_entry` (``struct``) -- Mother entry fields from `mt`.; - `is_female` (``bool``) -- ``True`` if proband is female.; - `p_de_novo` (``float64``) -- Unfiltered posterior probability; that the event is *de novo* rather than a missed heterozygous; event in a parent.; - `confidence` (``str``) Validation confidence. One of: ``'HIGH'``,; ``'MEDIUM'``, ``'LOW'``. The key of the table is ``['locus', 'alleles', 'id']``. The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/family_methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html
Availability,error,errors,"call}. fam_exprs = {; 'fam_id': expr_or_else(fam_id, '0'),; 'ind_id': hl.or_else(ind_id, '0'),; 'pat_id': expr_or_else(pat_id, '0'),; 'mat_id': expr_or_else(mat_id, '0'),; 'is_female': expr_or_else(is_female, '0', lambda x: hl.if_else(x, '2', '1')),; 'pheno': expr_or_else(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
Deployability,pipeline,pipeline,"will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
Integrability,interface,interface,"a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface and defaults; may change in future versions.; """"""; hl.current_backend().validate_file(output). _, ext = os.path.splitext(output); if ext == '.gz':; warning(; 'VCF export with standard gzip compression requested. This is almost *never* desired and will '; 'cause issues with other tools that consume VCF files. The compression format used for VCF '; 'files is traditionally *block* gzip compression. To use block gzip compression with hail VCF '; 'export, use a path ending in `.bgz`.'; ). if isinstance(dataset, Table):; mt = MatrixTable.from_rows_table(dataset); dataset = mt.key_cols_by(sample=""""). require_col_key_str(dataset, 'export_vcf'); require_row_key_variant(dataset, 'export_vcf'). if 'filters' in dataset.row and dataset.filters.dtype != hl.tset(hl.tstr):; raise ValueError(; f""'export_vcf': expect the 'filters' field to be set<str>, found {dataset.filters.dtype}""; f""\n Either transform this field to set<str> to export as VCF FILTERS field, or drop it from the dataset.""; ). in",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
Modifiability,parameteriz,parameterized,"vals(; path, reference_genome='default', skip_invalid_intervals=False, contig_recoding=None, **kwargs; ) -> Table:; """"""Import a locus interval list as a :class:`.Table`. Examples; --------. Add the row field `capture_region` indicating inclusion in; at least one locus interval from `capture_intervals.txt`:. >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_defined(intervals[dataset.locus])). Notes; -----. Hail expects an interval file to contain either one, three or five fields; per line in the following formats:. - ``contig:start-end``; - ``contig start end`` (tab-separated); - ``contig start end direction target`` (tab-separated). A file in either of the first two formats produces a table with one; field:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :obj:`.tstr` and `position` with type :py:data:`.tint32`. A file in the third format (with a ""target"" column) produces a table with two; fields:. - **interval** (:class:`.tinterval`) - Row key. Same schema as above.; - **target** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to fi",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
Performance,load,loaded,"arget** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in file to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid; arguments here except: `no_header`, `comment`, `impute`, and; `types`, as these are used by :func:`import_locus_intervals`. Returns; -------; :class:`.Table`; Interval-keyed table.; """""". if contig_recoding is not None:; contig_recoding = hl.literal(contig_recoding). def recode_contig(x):; if contig_recoding is None:; return x; return contig_recoding.get(x, x). t = import_table(; path,; comment=""@"",; impute=False,; no_header=True,; types={'f0': tstr, 'f1': tint32, 'f2': tint32, 'f3': tstr, 'f4': tstr},; **kwargs,; ). if t.row.dtype == tstruct(f0=tstr):; if reference_genome:; t = t.select(interval=hl.parse_locus_interval(t['f0'], reference_genome)); else:; interval_regex = r""([^:]*):(\d+)\-(\d+)"". def checked_match",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
Safety,avoid,avoid,"first_row_ht = ht.head(1). if find_replace is not None:; ht = ht.annotate(text=ht['text'].replace(*find_replace)). first_rows = first_row_ht.annotate(; header=first_row_ht.text._split_line(; delimiter, missing=hl.empty_array(hl.tstr), quote=quote, regex=len(delimiter) > 1; ); ).collect(); except FatalError as err:; if '_filter_partitions: no partition with index 0' in err.args[0]:; first_rows = []; else:; raise. if len(first_rows) == 0:; raise ValueError(f""Invalid file: no lines remaining after filters\n Files provided: {', '.join(paths)}""); first_row = first_rows[0]. if no_header:; fields = [f'f{index}' for index in range(0, len(first_row.header))]; else:; maybe_duplicated_fields = first_row.header; renamings, fields = deduplicate(maybe_duplicated_fields); ht = ht.filter(; ht.text == first_row.text, keep=False; ) # FIXME: seems wrong. Could easily fix with partition index and row_within_partition_index.; if renamings:; hl.utils.warning(; f'import_table: renamed the following {plural(""field"", len(renamings))} to avoid name conflicts:'; + ''.join(f'\n {k!r} -> {v!r}' for k, v in renamings); ). ht = ht.annotate(; split_text=(; hl.case(); .when(hl.len(ht.text) > 0, split_lines(ht, fields, delimiter=delimiter, missing=missing, quote=quote)); .or_error(hl.str(""Blank line found in file "") + ht.file); ); ); ht = ht.drop('text'). fields_to_value = {}; strs = []; if impute:; fields_to_impute_idx = []; fields_to_guess = []; for idx, field in enumerate(fields):; if types.get(field) is None:; fields_to_impute_idx.append(idx); fields_to_guess.append(field). hl.utils.info('Reading table to impute column types'); guessed = ht.aggregate(; hl.agg.array_agg(lambda x: hl.agg._impute_type(x), [ht.split_text[i] for i in fields_to_impute_idx]); ). reasons = {f: 'user-supplied type' for f in types}; imputed_types = dict(); for field, s in zip(fields_to_guess, guessed):; if not s['anyNonMissing']:; imputed_types[field] = hl.tstr; reasons[field] = 'no non-missing observations'; else:; if s[",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
Testability,sandbox,sandbox,"na(type_and_data['data']), typ, key=['id']). [docs]@typecheck(regex=str, path=oneof(str, sequenceof(str)), max_count=int, show=bool, force=bool, force_bgz=bool); def grep(regex, path, max_count=100, *, show: bool = True, force: bool = False, force_bgz: bool = False):; r""""""Searches given paths for all lines containing regex matches. Examples; --------. Print all lines containing the string ``hello`` in *file.txt*:. >>> hl.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hl.grep('\\d', ['data/file1.txt','data/file2.txt']). Notes; -----; :func:`.grep` mimics the basic functionality of Unix ``grep`` in; parallel, printing results to the screen. This command is provided as a; convenience to those in the statistical genetics community who often; search enormous text files like VCFs. Hail uses `Java regular expression; patterns; <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html>`__.; The `RegExr sandbox <http://regexr.com/>`__ may be helpful. Parameters; ----------; regex : :class:`str`; The regular expression to match.; path : :class:`str` or :obj:`list` of :obj:`str`; The files to search.; max_count : :obj:`int`; The maximum number of matches to return; show : :obj:`bool`; When `True`, show the values on stdout. When `False`, return a; dictionary mapping file names to lines.; force_bgz : :obj:`bool`; If ``True``, read files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, read gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; ---; :obj:`dict` of :class:`str` to :obj:`list` of :obj:`str`; """"""; from hail.backend.spark_backend import S",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
Usability,guid,guide,"titions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/impex.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html
Availability,checkpoint,checkpoint,"rce; if not isinstance(source, Table):; raise ValueError(; ""'maximal_independent_set' expects an expression of 'Table'. Found {}"".format(; ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ). if i._indices.source != j._indices.source:; raise ValueError(; ""'maximal_independent_set' expects arguments `i` and `j` to be expressions of the same Table. ""; ""Found\n{}\n{}"".format(i, j); ). node_t = i.dtype. if tie_breaker:; wrapped_node_t = ttuple(node_t); left_id = Env.get_uid(); right_id = Env.get_uid(); left = construct_variable(left_id, wrapped_node_t); right = construct_variable(right_id, wrapped_node_t); tie_breaker_expr = hl.float64(tie_breaker(left[0], right[0])); tie_breaker_ir = tie_breaker_expr._ir; t, _ = source._process_joins(i, j, tie_breaker_expr); else:; left_id, right_id, tie_breaker_ir = None, None, None; t, _ = source._process_joins(i, j). edges = t.select(__i=i, __j=j).key_by().select('__i', '__j'); edges = edges.checkpoint(new_temp_file()). mis_nodes = hl.set(; construct_expr(; ir.ArrayMaximalIndependentSet(edges.collect(_localize=False)._ir, left_id, right_id, tie_breaker_ir),; hl.tarray(node_t),; ); ). nodes = edges.select(node=[edges.__i, edges.__j]); nodes = nodes.explode(nodes.node); nodes = nodes.annotate_globals(mis_nodes=mis_nodes); nodes = nodes.filter(nodes.mis_nodes.contains(nodes.node), keep); nodes = nodes.select_globals(); if keyed:; return nodes.key_by('node').distinct(); return nodes. def require_col_key_str(dataset: MatrixTable, method: str):; if not len(dataset.col_key) == 1 or dataset[next(iter(dataset.col_key))].dtype != hl.tstr:; raise ValueError(; f""Method '{method}' requires column key to be one field of type 'str', found ""; f""{list(str(x.dtype) for x in dataset.col_key.values())}""; ). def require_table_key_variant(ht, method):; if (; list(ht.key) != ['locus', 'alleles']; or not isinstance(ht['locus'].dtype, tlocus); or not ht['alleles'].dtype == tarray(tstr); ):; raise ValueError(; """,MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
Deployability,update,updated,"pects a table with interval keys""); point_type = ht.key[0].dtype.point_type; if isinstance(points, Table):; if len(points.key) != 1 or points.key[0].dtype != point_type:; raise ValueError(; ""'segment_intervals' expects points to be a table with a single""; "" key of the same type as the intervals in 'ht', or an array of those points:""; f""\n expect {point_type}, found {list(points.key.dtype.values())}""; ); points = hl.array(hl.set(points.collect(_localize=False))); if points.dtype.element_type != point_type:; raise ValueError(; f""'segment_intervals' expects points to be a table with a single""; f"" key of the same type as the intervals in 'ht', or an array of those points:""; f""\n expect {point_type}, found {points.dtype.element_type}""; ). points = hl._sort_by(points, lambda l, r: hl._compare(l, r) < 0). ht = ht.annotate_globals(__points=points). interval = ht.key[0]; points = ht.__points; lower = hl.expr.functions._lower_bound(points, interval.start); higher = hl.expr.functions._lower_bound(points, interval.end); n_points = hl.len(points); lower = hl.if_else((lower < n_points) & (points[lower] == interval.start), lower + 1, lower); higher = hl.if_else((higher < n_points) & (points[higher] == interval.end), higher - 1, higher); interval_results = hl.rbind(; lower,; higher,; lambda lower, higher: hl.if_else(; lower >= higher,; [interval],; hl.flatten([; [; hl.interval(; interval.start, points[lower], includes_start=interval.includes_start, includes_end=False; ); ],; hl.range(lower, higher - 1).map(; lambda x: hl.interval(points[x], points[x + 1], includes_start=True, includes_end=False); ),; [; hl.interval(; points[higher - 1], interval.end, includes_start=True, includes_end=interval.includes_end; ); ],; ]),; ),; ); ht = ht.annotate(__new_intervals=interval_results, lower=lower, higher=higher).explode('__new_intervals'); return ht.key_by(**{next(iter(ht.key)): ht.__new_intervals}).drop('__new_intervals'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
Modifiability,parameteriz,parameterized,"(dataset, method); return dataset._select_rows(; method,; hl.case(); .when(dataset.alleles.length() == 2, dataset._rvrow); .or_error(; f""'{method}' expects biallelic variants ('alleles' field of length 2), found ""; + hl.str(dataset.locus); + "", ""; + hl.str(dataset.alleles); ),; ). [docs]@typecheck(dataset=MatrixTable, name=str); def rename_duplicates(dataset, name='unique_id') -> MatrixTable:; """"""Rename duplicate column keys. .. include:: ../_templates/req_tstring.rst. Examples; --------. >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; -----. This method produces a new column field from the string column key by; appending a unique suffix ``_N`` as necessary. For example, if the column; key ""NA12878"" appears three times in the dataset, the first will produce; ""NA12878"", the second will produce ""NA12878_1"", and the third will produce; ""NA12878_2"". The name of this new field is parameterized by `name`. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name of new field. Returns; -------; :class:`.MatrixTable`; """""". require_col_key_str(dataset, 'rename_duplicates'); ids = dataset.col_key[0].collect(). mapping, new_ids = deduplicate(ids). if mapping:; info(; f'Renamed {len(mapping)} duplicate {plural(""sample ID"", len(mapping))}. Mangled IDs as follows:'; + ''.join(f'\n ""{pre}"" => ""{post}""' for pre, post in mapping); ); else:; info('No duplicate sample IDs found.'); return dataset.annotate_cols(**{name: hl.literal(new_ids)[hl.int(hl.scan.count())]}). [docs]@typecheck(ds=oneof(Table, MatrixTable), intervals=expr_array(expr_interval(expr_any)), keep=bool); def filter_intervals(ds, intervals, keep=True) -> Union[Table, MatrixTable]:; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
Performance,load,loaded," + ''.join(f'\n ""{pre}"" => ""{post}""' for pre, post in mapping); ); else:; info('No duplicate sample IDs found.'); return dataset.annotate_cols(**{name: hl.literal(new_ids)[hl.int(hl.scan.count())]}). [docs]@typecheck(ds=oneof(Table, MatrixTable), intervals=expr_array(expr_interval(expr_any)), keep=bool); def filter_intervals(ds, intervals, keep=True) -> Union[Table, MatrixTable]:; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """""". if isinstance(ds, MatrixTable):; k_type = ds.row_key.dtype; else:; assert isinstance(ds, Table); k_type = ds.key.dtype. point_type = intervals.dtype.element_type.point_type. def is_struct_prefix(partial, full):; if list(partial) != list(full)[: len(partial)]:; return False; for k, v i",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
Testability,assert,assert,"lter(nodes.mis_nodes.contains(nodes.node), keep); nodes = nodes.select_globals(); if keyed:; return nodes.key_by('node').distinct(); return nodes. def require_col_key_str(dataset: MatrixTable, method: str):; if not len(dataset.col_key) == 1 or dataset[next(iter(dataset.col_key))].dtype != hl.tstr:; raise ValueError(; f""Method '{method}' requires column key to be one field of type 'str', found ""; f""{list(str(x.dtype) for x in dataset.col_key.values())}""; ). def require_table_key_variant(ht, method):; if (; list(ht.key) != ['locus', 'alleles']; or not isinstance(ht['locus'].dtype, tlocus); or not ht['alleles'].dtype == tarray(tstr); ):; raise ValueError(; ""Method '{}' requires key to be two fields 'locus' (type 'locus<any>') and ""; ""'alleles' (type 'array<str>')\n""; "" Found:{}"".format(method, ''.join(""\n '{}': {}"".format(k, str(ht[k].dtype)) for k in ht.key)); ). def require_row_key_variant(dataset, method):; if isinstance(dataset, Table):; key = dataset.key; else:; assert isinstance(dataset, MatrixTable); key = dataset.row_key; if (; list(key) != ['locus', 'alleles']; or not isinstance(dataset['locus'].dtype, tlocus); or not dataset['alleles'].dtype == tarray(tstr); ):; raise ValueError(; ""Method '{}' requires row key to be two fields 'locus' (type 'locus<any>') and ""; ""'alleles' (type 'array<str>')\n""; "" Found:{}"".format(method, ''.join(""\n '{}': {}"".format(k, str(dataset[k].dtype)) for k in key)); ). def require_alleles_field(dataset, method):; if 'alleles' not in dataset.row:; raise ValueError(f""Method '{method}' requires a field 'alleles' (type 'array<str>')\n""); if dataset.alleles.dtype != tarray(tstr):; raise ValueError(; f""Method '{method}' requires a field 'alleles' (type 'array<str>')\n""; f"" Found:\n""; f"" 'alleles': {dataset.alleles.dtype}""; ). def require_row_key_variant_w_struct_locus(dataset, method):; if (; list(dataset.row_key) != ['locus', 'alleles']; or not dataset['alleles'].dtype == tarray(tstr); or (; not isinstance(dataset['locus'].dtype, tlocus); ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html
Availability,error,error,"om hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _hwe_normalized_blanczos(call_expr, k, compute_loadings). return pca(hwe_normalize(call_expr), k, compute_loadings). [docs]@typecheck(entry_expr=expr_float64, k=int, compute_loadings=bool); def pca(entry_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on numeric columns derived from a; matrix table. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; This method does **not** automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in `entry_expr`. Hail will return an error if `entry_expr` evaluates to missing, nan, or; infinity on any entry. Notes; -----. PCA is run on the columns of the numeric matrix obtained by evaluating; `entry_expr` on each entry of the matrix table, or equivalently on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
Deployability,update,updated,"; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_iterations=q_iterations,; oversampling_param=oversampling_param,; block_size=block_size,; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
Energy Efficiency,power,power,"The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
Performance,load,loadings,"annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix. Examples; --------. >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; -----; This method specializes :func:`.pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`.pca` for more details. Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; :math:`ij` entry of the GRM :math:`MM^T` is simply the dot product of rows; :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}`. In; PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
Testability,assert,assert,"j in range(0, p):; info(f""krylov_factorization: Beginning iteration {j+1}/{p}""); G_i = t.aggregate(hl.agg.ndarray_sum(A_expr.T @ (A_expr @ G_i)), _localize=False); G_i = hl.nd.qr(G_i)[0]._persist(); g_list.append(G_i). info(""krylov_factorization: Iterations complete. Computing local QR""); V0 = hl.nd.hstack(g_list). if compute_V:; V = hl.nd.qr(V0)[0]._persist(); t = t.annotate(AV=A_expr @ V); else:; V = hl.nd.qr(V0)[0]; t = t.annotate(AV=A_expr @ V); V = None. if compute_U:; temp_file_name = hl.utils.new_temp_file(""_krylov_factorization_intermediate"", ""ht""); t = t.checkpoint(temp_file_name); AV_local = t.aggregate(hl.nd.vstack(hl.agg.collect(t.AV)), _localize=False); U, R = hl.nd.qr(AV_local)._persist(); else:; Rs = t.aggregate(hl.nd.vstack(hl.agg.collect(hl.nd.qr(t.AV)[1])), _localize=False); R = hl.nd.qr(Rs)[1]._persist(); U = None. return KrylovFactorization(U, R, V, k). def _reduced_svd(A: TallSkinnyMatrix, k=10, compute_U=False, iterations=2, iteration_size=None):; # Set Parameters; q = iterations; if iteration_size is None:; L = k + 2; else:; L = iteration_size; assert (q + 1) * L >= k; n = A.ncols. # Generate random matrix G; G = hl.rand_norm(0, 1, size=(n, L)); G = hl.nd.qr(G)[0]._persist(). fact = _krylov_factorization(A, G, q, compute_U); info(""_reduced_svd: Computing local SVD""); return fact.reduced_svd(k). @typecheck(; A=oneof(expr_float64, TallSkinnyMatrix), num_moments=int, p=nullable(int), moment_samples=int, block_size=int; ); def _spectral_moments(A, num_moments, p=None, moment_samples=500, block_size=128):; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_spectral_moments/entry_expr', A); A = _make_tsm(A, block_size). n = A.ncols. if p is None:; p = min(num_moments // 2, 10). # TODO: When moment_samples > n, we should just do a TSQR on A, and compute; # the spectrum of R.; assert moment_samples < n, '_spectral_moments: moment_samples must be smaller than num cols of A'; G = hl.rand_unif(-1, 1, size=(n, moment_samples)).map(lambd",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
Usability,simpl,simply,"ormalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix. Examples; --------. >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; -----; This method specializes :func:`.pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`.pca` for more details. Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; :math:`ij` entry of the GRM :math:`MM^T` is simply the dot product of rows; :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}`. In; PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance e",MatchSource.WIKI,docs/0.2/_modules/hail/methods/pca.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html
Availability,error,error,"-------. Compute sample QC metrics and remove low-quality samples:. >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; -----. This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; `name` parameter. If `mt` contains an entry field `DP` of type :py:data:`.tint32`, then the; field `dp_stats` is computed. If `mt` contains an entry field `GQ` of type; :py:data:`.tint32`, then the field `gq_stats` is computed. Both `dp_stats`; and `gq_stats` are structs with with four fields:. - `mean` (``float64``) -- Mean value.; - `stdev` (``float64``) -- Standard deviation (zero degrees of freedom).; - `min` (``int32``) -- Minimum value.; - `max` (``int32``) -- Maximum value. If the dataset does not contain an entry field `GT` of type; :py:data:`.tcall`, then an error is raised. The following fields are always; computed from `GT`:. - `call_rate` (``float64``) -- Fraction of calls not missing or filtered.; Equivalent to `n_called` divided by :meth:`.count_rows`.; - `n_called` (``int64``) -- Number of non-missing calls.; - `n_not_called` (``int64``) -- Number of missing calls.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_hom_ref` (``int64``) -- Number of homozygous reference calls.; - `n_het` (``int64``) -- Number of heterozygous calls.; - `n_hom_var` (``int64``) -- Number of homozygous alternate calls.; - `n_non_ref` (``int64``) -- Sum of `n_het` and `n_hom_var`.; - `n_snp` (``int64``) -- Number of SNP alternate alleles.; - `n_insertion` (``int64``) -- Number of insertion alternate alleles.; - `n_deletion` (``int64``) -- Number of deletion alternate alleles.; - `n_singleton` (``int64``) -- Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; - ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
Deployability,configurat,configuration,"=tint32,; transcript_consequences=tarray(; tstruct(; allele_num=tint32,; amino_acids=tstr,; biotype=tstr,; canonical=tint32,; ccds=tstr,; cdna_start=tint32,; cdna_end=tint32,; cds_end=tint32,; cds_start=tint32,; codons=tstr,; consequence_terms=tarray(tstr),; distance=tint32,; domains=tarray(tstruct(db=tstr, name=tstr)),; exon=tstr,; gene_id=tstr,; gene_pheno=tint32,; gene_symbol=tstr,; gene_symbol_source=tstr,; hgnc_id=tstr,; hgvsc=tstr,; hgvsp=tstr,; hgvs_offset=tint32,; impact=tstr,; intron=tstr,; lof=tstr,; lof_flags=tstr,; lof_filter=tstr,; lof_info=tstr,; minimised=tint32,; polyphen_prediction=tstr,; polyphen_score=tfloat,; protein_end=tint32,; protein_start=tint32,; protein_id=tstr,; sift_prediction=tstr,; sift_score=tfloat,; strand=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bu",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
Integrability,message,message,"=[(local_output_file, f'{vep_output_path}/annotations/{part_name}.tsv.gz')],; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, cancelling the batch...""); b.cancel(); backend._batch = None; raise. if status['n_succeeded'] != status['n_jobs']:; failing_job = next(iter(b.jobs('!success'))); failing_job = b.get_job(failing_job['job_id']); message = {'batch_status': status, 'job_status': failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; )",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
Modifiability,config,config,"e for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; return hl.bind(; lambda at: hl.if_else(; at == AlleleType.SNP,; hl.if_else(hl.is_transition(ref, alt), AlleleType.TRANSITION, AlleleType.TRANSVERSION),; at,; ),; numeric_allele_type(ref, alt),; ). [docs]@typecheck(mt=MatrixTable, name=str); def sample_qc(mt, name='sample_qc') -> MatrixTable:; """"""Compute per-sample metrics useful for quality control. .. include:: ../_templates/req_tvariant.rst. Examples; --------. Compute sample QC metrics and rem",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
Performance,perform,performs,"0].p_value,; 'p_value_excess_het': hwe[1].p_value,; }),; ),; ). return mt.annotate_rows(**{name: result}). [docs]@typecheck(left=MatrixTable, right=MatrixTable, _localize_global_statistics=bool); def concordance(left, right, *, _localize_global_statistics=True) -> Tuple[List[List[int]], Table, Table]:; """"""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
Testability,log,logging,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html
Availability,error,error,"s a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root and `y` a single expression, the following row-indexed; fields are added. - **<row key fields>** (Any) -- Row key fields.; - **<pass_through fields>** (Any) -- Row fields in `pass_through`.; - **n** (:py:data:`.tint32`) -- Number of columns used.; - **sum_x** (:py:data:`.tfloat64`) -- Sum of input values `x`.; - **y_transpose_x** (:py:data:`.tfloat64`) -- Dot product of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (:py:data:`.tfloat64`) -- :math:`p`-value. If `y` is a list of expressions, then the last five fields instead have type; :class:`.tarray` of :py:data:`.tfloat64`, with corresponding indexing of; the list and each array. If `y` is a list of lists of expressions, then `n` and `sum_x` are of type; ``array<float64>``, and the last five fields are of type; ``array<array<float64>>``. Index into these arrays with; ``a[index_in_outer_list, index_in_inner_list]``. For example, if; ``y=[[a], [b, c]]`` then the p-value for ``b`` is ``p_value[1][0]``. In the statistical genetics example above, the input variable `x` encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. .. math:",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
Deployability,integrat,integration,"-----+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pape",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
Energy Efficiency,reduce,reduces,"=== ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statisti",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
Integrability,depend,depends,"l supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio te",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
Modifiability,variab,variable,"s; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root a",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
Performance,perform,perform,"ssion model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_indexed('linear_regression_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows': found no values for 'y'""); is_chain",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
Safety,predict,predicting,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
Testability,assert,assert,"parse_locus_interval(x_contig, rg), rg.x_contigs), keep=True; ); if not include_par:; interval_type = hl.tarray(hl.tinterval(hl.tlocus(rg))); mt = hl.filter_intervals(mt, hl.literal(rg.par, interval_type), keep=False). mt = mt.filter_rows((mt[aaf] > aaf_threshold) & (mt[aaf] < (1 - aaf_threshold))); mt = mt.annotate_cols(ib=agg.inbreeding(mt.call, mt[aaf])); kt = mt.select_cols(; is_female=hl.if_else(; mt.ib.f_stat < female_threshold, True, hl.if_else(mt.ib.f_stat > male_threshold, False, hl.missing(tbool)); ),; **mt.ib,; ).cols(). return kt. def _get_regression_row_fields(mt, pass_through, method) -> Dict[str, str]:; row_fields = dict(zip(mt.row_key.keys(), mt.row_key.keys())); for f in pass_through:; if isinstance(f, str):; if f not in mt.row:; raise ValueError(f""'{method}/pass_through': MatrixTable has no row field {f!r}""); if f in row_fields:; # allow silent pass through of key fields; if f in mt.row_key:; pass; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y,",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
Usability,simpl,simplify,"etric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of :math:`Q` is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call :math:`Z Z^T`:. .. math::. \begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}. The eigenvalues of :math:`Z Z^T` and :math:`Z^T Z` are the squared singular values of :math:`Z`;; therefore, we instead focus on :math:`Z^T Z`. In the expressions below, we elide transpositions; of symmetric matrices:. .. math::. \begin{align*}; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2} \\; Z &= P_0^{1/2} G W^{1/2} \\; Z^T Z &= W^{1/2} G^T P_0 G W^{1/2}; \end{align*}. Before substituting the definition of :math:`P_0`, simplify it using the reduced QR; decomposition:. .. math::. \begin{align*}; Q R &= V^{1/2} X \\; R^T Q^T &= X^T V^{1/2} \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; &= V - V X (R^T Q^T Q R)^{-1} X^T V \\; &= V - V X (R^T R)^{-1} X^T V \\; &= V - V X R^{-1} (R^T)^{-1} X^T V \\; &= V - V^{1/2} Q (R^T)^{-1} X^T V^{1/2} \\; &= V - V^{1/2} Q Q^T V^{1/2} \\; &= V^{1/2} (I - Q Q^T) V^{1/2} \\; \end{align*}. Substitute this simplified expression into :math:`Z`:. .. math::. \begin{align*}; Z^T Z &= W^{1/2} G^T V^{1/2} (I - Q Q^T) V^{1/2} G W^{1/2} \\; \end{align*}. Split this symmetric matrix by observing that :math:`I - Q Q^T` is idempotent:. .. math::. \begin{align*}; I - Q Q^T &= (I - Q Q^T)(I - Q Q^T)^T \\; \\; Z &= (I - Q Q^T) V^{1/2} G W^{1/2} \\; Z &= (G - Q Q^T G) V^{1/2} W^{1/2}; \end{align*}. Finally, the squared singular values of :math:`Z` are the eigenvalues of :math:`Z^T Z`, so; :math:`Q` should be distributed as follows:. .. math::. \begin{align*}; U S V^T &= Z \quad\quad \t",MatchSource.WIKI,docs/0.2/_modules/hail/methods/statgen.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html
Deployability,update,updated,"2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, the resulting; array will be of that shape. """""". if (nd1.dtype.element_type or nd2.dtype.element_type) == (tfloat64 or tfloat32):; return nd1.map2(; nd2, lambda a, b: hl.if_else(hl.is_nan(a) | hl.is_nan(b), hl.float64(float(""NaN"")), hl.if_else(a > b, a, b)); ); return nd1.map2(nd2, lambda a, b: hl.if_else(a > b, a, b)). [docs]@typecheck(nd1=expr_ndarray(), nd2=oneof(expr_ndarray(), list)); def minimum(nd1, nd2):; """"""Compares elements at corresponding indexes in arrays; and returns an array of the minimum element found; at each compared index. If an array element being compared has the value NaN,; the minimum for that index will be NaN. Examples; --------; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.minimum(a, b)); array([1, 3, 3], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.minimum(a, b)); array([nan, 3., nan]). Parameters; ----------; nd1 : :class:`.NDArrayExpression`; nd2 : class:`.NDArrayExpression`, `.ArrayExpression`, numpy ndarray, or nested python lists/tuples.; nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns; -------; min_array : :class:`.NDArrayExpression`; Element-wise minimums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, resulting array; will be of that shape. """""". if (nd1.dtype.element_type or nd2.dtype.element_type) == (tfloat64 or tfloat32):; return nd1.map2(; nd2, lambda a, b: hl.if_else(hl.is_nan(a) | hl.is_nan(b), hl.float64(float(""NaN"")), hl.if_else(a < b, a, b)); ); return nd1.map2(nd2, lambda a, b: hl.if_else(a < b, a, b)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
Energy Efficiency,reduce,reduce,"﻿. Hail | ; hail.nd.nd. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.nd.nd. Source code for hail.nd.nd; from functools import reduce. import hail as hl; from hail.expr.expressions import (; Int64Expression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_tuple,; unify_all,; ); from hail.expr.expressions.typed_expressions import NDArrayNumericExpression; from hail.expr.functions import _ndarray; from hail.expr.functions import array as aarray; from hail.expr.types import HailType, tfloat32, tfloat64, tndarray, ttuple; from hail.ir import Apply, NDArrayConcat, NDArrayEigh, NDArrayInv, NDArrayQR, NDArraySVD; from hail.typecheck import nullable, oneof, sequenceof, tupleof, typecheck. tsequenceof_nd = oneof(sequenceof(expr_ndarray()), expr_array(expr_ndarray())); shape_type = oneof(expr_int64, tupleof(expr_int64), expr_tuple()). [docs]def array(input_array, dtype=None):; """"""Construct an :class:`.NDArrayExpression`. Examples; --------. >>> hl.eval(hl.nd.array([1, 2, 3, 4])); array([1, 2, 3, 4], dtype=int32). >>> hl.eval(hl.nd.array([[1, 2, 3], [4, 5, 6]])); array([[1, 2, 3],; [4, 5, 6]], dtype=int32). >>> hl.eval(hl.nd.array(np.identity(3))); array([[1., 0., 0.],; [0., 1., 0.],; [0., 0., 1.]]). >>> hl.eval(hl.nd.array(hl.range(10, 20))); array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19], dtype=int32). Parameters; ----------; input_array : :class:`.ArrayExpression`, numpy ndarray, or nested python lists/tuples; The array to convert to a Hail ndarray.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. Returns; -------; :class:`.NDArray",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
Modifiability,variab,variables,"rrayNumericExpression`; ndarray of the specified size full of ones.; """"""; return full(shape, 1, dtype). [docs]@typecheck(nd=expr_ndarray()); def diagonal(nd):; """"""Gets the diagonal of a 2 dimensional NDArray. Examples; --------. >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional NDArray, shape(M, N). Returns; -------; :class:`.NDArrayExpression`; A 1 dimension NDArray of length min(M, N), containing the diagonal of `nd`.; """"""; assert nd.ndim == 2, ""diagonal requires 2 dimensional ndarray""; shape_min = hl.min(nd.shape[0], nd.shape[1]); return hl.nd.array(hl.range(hl.int32(shape_min)).map(lambda i: nd[i, i])). [docs]@typecheck(a=expr_ndarray(), b=expr_ndarray(), no_crash=bool); def solve(a, b, no_crash=False):; """"""Solve a linear system. Parameters; ----------; a : :class:`.NDArrayNumericExpression`, (N, N); Coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""; b_ndim_orig = b.ndim; a, b = solve_helper(a, b, b_ndim_orig); if no_crash:; name = ""linear_solve_no_crash""; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); else:; name = ""linear_solve""; return_type = hl.tndarray(hl.tfloat64, 2). indices, aggregations = unify_all(a, b); ir = Apply(name, return_type, a._ir, b._ir); result = construct_expr(ir, return_type, indices, aggregations). if b_ndim_orig == 1:; if no_crash:; result = hl.struct(solution=result.solution.reshape((-1)), failed=result.failed); else:; result = result.reshape((-1)); return result. [docs]@typecheck(A=expr_ndarray(), b=expr_ndarray(), lower=expr_bool, no_crash=bool); def solve_triangular(A, b, lower=False, no_crash=False):; """"""Solve a triangular linear system Ax = b for x. Parameters; ----------; A : :class:`.NDArrayNumericExpr",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
Testability,assert,assert,"pr_ndarray()), expr_array(expr_ndarray())); shape_type = oneof(expr_int64, tupleof(expr_int64), expr_tuple()). [docs]def array(input_array, dtype=None):; """"""Construct an :class:`.NDArrayExpression`. Examples; --------. >>> hl.eval(hl.nd.array([1, 2, 3, 4])); array([1, 2, 3, 4], dtype=int32). >>> hl.eval(hl.nd.array([[1, 2, 3], [4, 5, 6]])); array([[1, 2, 3],; [4, 5, 6]], dtype=int32). >>> hl.eval(hl.nd.array(np.identity(3))); array([[1., 0., 0.],; [0., 1., 0.],; [0., 0., 1.]]). >>> hl.eval(hl.nd.array(hl.range(10, 20))); array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19], dtype=int32). Parameters; ----------; input_array : :class:`.ArrayExpression`, numpy ndarray, or nested python lists/tuples; The array to convert to a Hail ndarray.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. Returns; -------; :class:`.NDArrayExpression`; An ndarray based on the input array.; """"""; return _ndarray(input_array, dtype=dtype). @typecheck(a=expr_array(), shape=shape_type); def from_column_major(a, shape):; assert len(shape) == 2; return array(a).reshape(tuple(reversed(shape))).T. [docs]@typecheck(start=expr_int32, stop=nullable(expr_int32), step=expr_int32); def arange(start, stop=None, step=1) -> NDArrayNumericExpression:; """"""Returns a 1-dimensions ndarray of integers from `start` to `stop` by `step`. Examples; --------. >>> hl.eval(hl.nd.arange(10)); array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32). >>> hl.eval(hl.nd.arange(3, 10)); array([3, 4, 5, 6, 7, 8, 9], dtype=int32). >>> hl.eval(hl.nd.arange(0, 10, step=3)); array([0, 3, 6, 9], dtype=int32). Notes; -----; The range includes `start`, but excludes `stop`. If provided exactly one argument, the argument is interpreted as `stop` and; `start` is set to zero. This matches the behavior of Python's ``range``. Parameters; ----------; start : int or :class:`.Expression` of type :py:data:`.tint32`; Start of range.; stop : int or :class:`.Expression` of type :py:data:`.tint32`; End of range.; step : int or :class:`.Exp",MatchSource.WIKI,docs/0.2/_modules/hail/nd/nd.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html
Availability,down,down,"es), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j = 1; while ui < len(x) and li < len(x):; if j == len(x):; ub = 1; lb = 1; xj = max_x; else:; ub = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] - e; new_y[li] = fy; keep[li] = True; j = li + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; elif compare(udx, udy, dx, jldy) > 0:; # line must bend up at j; fx = x[ui]; fy = y[ui] + e; new_y[ui] = fy; keep[ui] = True; j = ui + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; if j >= len(x):; break; if compare(udx, udy, dx, judy) < 0:; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; if compare(ldx, ldy, dx, jldy) > 0:; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; j += 1; return new_y, keep. [docs]def smoothed_pdf(; data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; tit",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
Deployability,update,update," is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; fig = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j ",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
Energy Efficiency,power,power,"g.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the res",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
Modifiability,variab,variable,"s.major_label_text_font_size = font_size; if hasattr(p.title, 'text_font_size'):; p.title.text_font_size = font_size; if hasattr(p.xaxis, 'group_text_font_size'):; p.xaxis.group_text_font_size = font_size; return p. [docs]@typecheck(; x=expr_numeric,; y=expr_numeric,; bins=oneof(int, sequenceof(int)),; range=nullable(sized_tupleof(nullable(sized_tupleof(numeric, numeric)), nullable(sized_tupleof(numeric, numeric)))),; title=nullable(str),; width=int,; height=int,; colors=sequenceof(str),; log=bool,; ); def histogram2d(; x: NumericExpression,; y: NumericExpression,; bins: int = 40,; range: Optional[Tuple[int, int]] = None,; title: Optional[str] = None,; width: int = 600,; height: int = 600,; colors: Sequence[str] = bokeh.palettes.all_palettes['Blues'][7][::-1],; log: bool = False,; ) -> figure:; """"""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be consid",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
Testability,log,log,"able import MatrixTable; from hail.table import Table; from hail.typecheck import dictof, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils.java import warning; from hail.utils.struct import Struct. palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']. [docs]def output_notebook():; """"""Configure the Bokeh output state to generate output in notebook; cells when :func:`bokeh.io.show` is called. Calls; :func:`bokeh.io.output_notebook`. """"""; bokeh.io.output_notebook(). def show(obj, interact=None):; """"""Immediately display a Bokeh object or application. Calls; :func:`bokeh.io.show`. Parameters; ----------; obj; A Bokeh object to display.; interact; A handle returned by a plotting method with `interactive=True`.; """"""; if interact is None:; bokeh.io.show(obj); else:; handle = bokeh.io.show(obj, notebook_handle=True); interact(handle). [docs]def cdf(data, k=350, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". if normalize:; y_axis_label = 'Quantile'; else:; y_axis_label = 'Rank'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; background_fill_colo",MatchSource.WIKI,docs/0.2/_modules/hail/plot/plots.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html
Deployability,update,updated,"﻿. Hail | ; hail.stats.linear_mixed_model. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.stats.linear_mixed_model. Source code for hail.stats.linear_mixed_model; [docs]class LinearMixedModel(object):; r""""""Class representing a linear mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94. """""". def __init__(self, py, px, s, y=None, x=None, p_path=None):; raise NotImplementedError(""LinearMixedModel is no longer implemented/supported as of Hail 0.2.94""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/stats/linear_mixed_model.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/stats/linear_mixed_model.html
Availability,error,error,"://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mod",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
Deployability,update,updated,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
Security,access,access,"---; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mode, buffer_size); _, ext = os.path.splitext(path); if ext in ('.gz', '.bgz'):; binary_mode = 'wb' if mode[0] == 'w' else 'rb'; file = fs.open(path, binary_mode, buffer_size); file = gzip.GzipFile(fileobj=file, mode=mode); if 'b' not in mode:; file = io.TextIOWrapper(file, encoding='utf-8'); else:; file = fs.open(path, mode, buffer_size); return file. [docs]@typecheck(src=str, dest=str); def hadoop_copy(src, dest):; """"""Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----.",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
Testability,log,log,"-- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
Usability,simpl,simpler,"e, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mode, buffer_size); _, ext = os.path.splitext(path); if ext in ('.gz', '.bgz'):; binary_mode = 'wb' if mode[0] == 'w' else 'rb'; file = fs.open(path, binary_mode, buffer_size); file = gzip.GzipFile(fileobj=file, mode=mode); if 'b' not in mode:; file = io.TextIOWrapper(file, encoding='utf-8'); else:; file = fs.open(path, mode, buffer_size); return file. [docs]@typecheck(src=str, dest=str); def hadoop_copy(src, dest):; """"""Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. Try using :func:`.hadoop_open` first, it's simpler, but not great; for large data! For example:. >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; return Env.fs().copy(src, dest). [docs]def hadoop_exists(path: str) -> bool:; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().exists(path). [docs]def hadoop_is_file(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_file(path). [docs]def hadoop_is_dir(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_di",MatchSource.WIKI,docs/0.2/_modules/hail/utils/hadoop_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html
Deployability,update,updated,"es_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Type`; """""". return self._point_type. def contains(self, value):; """"""True if `value` is contained within the interval. Examples; --------. >>> interval2.contains(5); True. >>> interval2.contains(6); False. Parameters; ----------; value :; Object with type :meth:`.point_type`. Returns; -------; :obj:`bool`; """""". return hl.eval(hl.literal(self, hl.tinterval(self._point_type)).contains(value)). @typecheck_method(interval=interval_type); def overlaps(self, interval):; """"""True if the the supplied interval contains any value in common with this one. Parameters; ----------; interval : :class:`.Interval`; Interval object with the same point type. Returns; -------; :obj:`bool`; """""". return hl.eval(hl.literal(self, hl.tinterval(self._point_type)).overlaps(interval)). interval_type.set(Interval). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html
Security,hash,hash,"e incompatible types: '{}', '{}'."".format(start_type, end_type)). self._point_type = point_type; self._start = start; self._end = end; self._includes_start = includes_start; self._includes_end = includes_end. def __str__(self):; if isinstance(self._start, hl.genetics.Locus) and self._start.contig == self._end.contig:; bounds = f'{self._start}-{self._end.position}'; else:; bounds = f'{self._start}-{self._end}'; open = '[' if self._includes_start else '('; close = ']' if self._includes_end else ')'; return f'{open}{bounds}{close}'. def __repr__(self):; return 'Interval(start={}, end={}, includes_start={}, includes_end={})'.format(; repr(self.start), repr(self.end), repr(self.includes_start), repr(self._includes_end); ). def __eq__(self, other):; return (; (; self._start == other._start; and self._end == other._end; and self._includes_start == other._includes_start; and self._includes_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/interval.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html
Availability,error,error,"﻿. Hail | ; hail.utils.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.misc. Source code for hail.utils.misc; import atexit; import datetime; import difflib; import json; import os; import re; import secrets; import shutil; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', '",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
Deployability,update,updated,"ape_str(s, backticked=True)). def parsable_strings(strs):; strs = ' '.join(f'""{escape_str(s)}""' for s in strs); return f""({strs})"". def _dumps_partitions(partitions, row_key_type):; parts_type = partitions.dtype; if not (isinstance(parts_type, hl.tarray) and isinstance(parts_type.element_type, hl.tinterval)):; raise ValueError(f'partitions type invalid: {parts_type} must be array of intervals'). point_type = parts_type.element_type.point_type. f1, t1 = next(iter(row_key_type.items())); if point_type == t1:; partitions = hl.map(; lambda x: hl.interval(; start=hl.struct(**{f1: x.start}),; end=hl.struct(**{f1: x.end}),; includes_start=x.includes_start,; includes_end=x.includes_end,; ),; partitions,; ); else:; if not isinstance(point_type, hl.tstruct):; raise ValueError(f'partitions has wrong type: {point_type} must be struct or type of first row key field'); if not point_type._is_prefix_of(row_key_type):; raise ValueError(f'partitions type invalid: {point_type} must be prefix of {row_key_type}'). s = json.dumps(partitions.dtype._convert_to_json(hl.eval(partitions))); return s, partitions.dtype. def default_handler():; try:; from IPython.display import display. return display; except ImportError:; return print. def guess_cloud_spark_provider() -> Optional[Literal['dataproc', 'hdinsight']]:; if 'HAIL_DATAPROC' in os.environ:; return 'dataproc'; if 'AZURE_SPARK' in os.environ or 'hdinsight' in os.getenv('CLASSPATH', ''):; return 'hdinsight'; return None. def no_service_backend(unsupported_feature):; from hail import current_backend; from hail.backend.service_backend import ServiceBackend. if isinstance(current_backend(), ServiceBackend):; raise NotImplementedError(; f'{unsupported_feature!r} is not yet supported on the service backend.'; f'\n If this is a pressing need, please alert the team on the discussion'; f'\n forum to aid in prioritization: https://discuss.hail.is'; ). ANY_REGION = ['any_region']. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
Modifiability,inherit,inherited,"):; return 'GroupedTable', GroupedTable, table_error(obj), False; elif isinstance(obj, Struct):; return 'Struct', Struct, struct_error(obj), False; elif isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
Performance,optimiz,optimized,"til; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
Safety,redund,redundant," v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e in exprs:; if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.select('x')\n""; f"" Correct: ht = ht.select(ht.x)\n""; f"" Correct: ht = ht.select(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.select(ht.x.replace(' ', '_'))""; ); analyze(caller, e, indices, broadcast=False). name = e._ir.name; check_keys(caller, name, protected_key); final_fields.append(name); if is_top_level_field(e):; select_fields.append(name); else:; insertions[name] = e; for k, e in named_exprs.items():; check_keys(caller, k, protected_key); final_fields.append(k); insertions[k] = e. check_collisions(caller, final_fields, indices). if final_fields == select_fields + list(insertions):; # don't clog the IR with redundant field names; s = base_struct.select(*select_fields).annotate(**insertions); else:; s = base_struct.select(*select_fields)._annotate_ordered(insertions, final_fields). assert list(s) == final_fields; return s. def check_annotate_exprs(caller, named_exprs, indices, agg_axes):; from hail.expr.expressions import analyze. protected_key = set(indices.protected_key); for k, v in named_exprs.items():; analyze(f'{caller}: field {k!r}', v, indices, agg_axes, broadcast=True); check_keys(caller, k, protected_key); check_collisions(caller, list(named_exprs), indices); return named_exprs. def process_joins(obj, exprs):; all_uids = []; left = obj; used_joins = set(). for e in exprs:; joins = e._ir.search(lambda a: isinstance(a, hail.ir.Join)); for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; if j.idx not in used_joins:; left = j.join_func(left); all_uids.extend(j.temp_vars); used_joins.add(j.idx). def cleanup(table):; remaining_uids = [uid ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
Security,access,access,"; return 'MatrixTable', MatrixTable, table_error(obj), True; elif isinstance(obj, GroupedMatrixTable):; return 'GroupedMatrixTable', GroupedMatrixTable, table_error(obj._parent), True; elif isinstance(obj, Table):; return 'Table', Table, table_error(obj), True; elif isinstance(obj, GroupedTable):; return 'GroupedTable', GroupedTable, table_error(obj), False; elif isinstance(obj, Struct):; return 'Struct', Struct, struct_error(obj), False; elif isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
Testability,test,testing,"til; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
Usability,learn,learning,"til; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >",MatchSource.WIKI,docs/0.2/_modules/hail/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html
Deployability,update,updated,"lf):; return str(self). def __str__(self):; if all(k.isidentifier() for k in self._fields):; return 'Struct(' + ', '.join(f'{k}={v!r}' for k, v in self._fields.items()) + ')'; return 'Struct(**{' + ', '.join(f'{k!r}: {v!r}' for k, v in self._fields.items()) + '})'. def __eq__(self, other):; return self._fields == other._fields if isinstance(other, Struct) else NotImplemented. def __hash__(self):; return 37 + hash(tuple(sorted(self._fields.items()))). def __iter__(self):; return iter(self._fields). def __dir__(self):; super_dir = super().__dir__(); return super_dir + list(self._fields.keys()). def annotate(self, **kwargs):; """"""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""; d = OrderedDict(self.items()); for k, v in kwargs.items():; d[k] = v; return Struct(**d). @typecheck_method(fields=str, kwargs=anytype); def select(self, *fields, **kwargs):; """"""Select existing fields and compute new ones. Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `kwargs` arguments are new fields to add. Parameters; ----------; fields : varargs of :class:`str`; Field names to keep.; named_exprs : keyword args; New field. Returns; -------; :class:`.Struct`; Struct containing specified",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
Safety,avoid,avoid,"t_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = state. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). @typecheck_method(item=str); def __getitem__(self, item):; return self._get_field(item). def __setattr__(self, key, value):; if key in self._fields:; raise ValueError(""Structs are immutable, cannot overwrite a field.""); else:; super().__setattr__(key, value). def __getattr__(self, item):; if item in self.__dict__:; return self.__dict__[item]; elif item in self._fields:; return self._fields[item]; else:; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __repr__(self):; return str(self). def __str__(self):; if a",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
Security,access,accessing,"﻿. Hail | ; hail.utils.struct. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.struct. Source code for hail.utils.struct; import pprint; from collections import OrderedDict; from collections.abc import Mapping; from typing import Any, Dict. from hail.typecheck import anytype, typecheck, typecheck_method; from hail.utils.misc import get_nice_attr_error, get_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = st",MatchSource.WIKI,docs/0.2/_modules/hail/utils/struct.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html
Availability,down,download,"al/ensembl_gene_annotations.txt',; 'HGDP_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_pop_and_sex_annotations.tsv',; 'HGDP_matrix_table': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_subset.vcf.bgz',; 'HGDP_ensembl_gene_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_gene_annotations.tsv',; 'movie_lens_100k': 'https://files.grouplens.org/datasets/movielens/ml-100k.zip',; }. tmp_dir: str = None. def init_temp_dir():; global tmp_dir; if tmp_dir is None:; tmp_dir = new_local_temp_dir(). def _dir_exists(fs, path):; return fs.exists(path) and fs.is_dir(path). def _file_exists(fs, path):; return fs.exists(path) and fs.is_file(path). def _copy_to_tmp(fs, src, extension=None):; dst = new_temp_file(extension=extension); fs.copy(src, dst); return dst. [docs]def get_1kg(output_dir, overwrite: bool = False):; """"""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sample annotations. Notes; -----; The download is about 15M. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, '1kg.mt'); vcf_path = os.path.join(output_dir, '1kg.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, '1kg_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['1kg_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
Deployability,release,release,"; movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'importing users table and writing to {users_path} ...'). users = rename_columns(; hl.import_table(user_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'),; ['id', 'age', 'sex', 'occupation', 'zipcode'],; ); users.write(users_path, overwrite=True). info(f'importing movies table and writing to {movies_path} ...'). movies = hl.import_table(movie_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'); movies = rename_columns(; movies, ['id', 'title', 'release date', 'video release date', 'IMDb URL', 'unknown', *genres]; ); movies = movies.drop('release date', 'video release date', 'unknown', 'IMDb URL'); movies = movies.transmute(genres=fields_to_array(movies, genres)); movies.write(movies_path, overwrite=True). info(f'importing ratings table and writing to {ratings_path} ...'). ratings = hl.import_table(ratings_cluster_readable, no_header=True, impute=True); ratings = rename_columns(ratings, ['user_id', 'movie_id', 'rating', 'timestamp']); ratings = ratings.drop('timestamp'); ratings.write(ratings_path, overwrite=True). else:; info('Movie Lens files found!'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
Testability,assert,assert,"t 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not _dir_exists(fs, f) for f in paths):; init_temp_dir(); source = resources['movie_lens_100k']; tmp_path = os.path.join(tmp_dir, 'ml-100k.zip'); info(f'downloading MovieLens-100k data ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_path); with zipfile.ZipFile(tmp_path, 'r') as z:; z.extractall(tmp_dir). user_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.user'); movie_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.item'); ratings_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.data'); assert os.path.exists(user_table_path); assert os.path.exists(movie_table_path); assert os.path.exists(ratings_table_path). user_cluster_readable = _copy_to_tmp(fs, local_path_uri(user_table_path), extension='txt'); movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip",MatchSource.WIKI,docs/0.2/_modules/hail/utils/tutorial.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html
Deployability,update,updated,"[9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; -----; The `number` parameter matches the `VCF specification <https://samtools.github.io/hts-specs/VCFv4.3.pdf>`__; number definitions:. - ``A`` indicates one value per allele, excluding the reference.; - ``R`` indicates one value per allele, including the reference.; - ``G`` indicates one value per unique diploid genotype. Warning; -------; Using this function can lead to an enormous explosion in data size, without increasing; information capacity. Its appropriate use is to conform to antiquated and badly-scaling; representations (e.g. pVCF), but even so, caution should be exercised. Reindexing local; PLs (or any G-numbered field) at a site with 1000 alleles will produce an array with; more than 5,000 values per sample -- with 100,000 samples, nearly 50GB per variant!. See Also; --------; :func:`.lgt_to_gt`. Parameters; ----------; array : :class:`.ArrayExpression`; Array to reindex.; local_alleles : :class:`.ArrayExpression`; Local alleles array.; n_alleles : :class:`.Int32Expression`; Total number of alleles to reindex to.; fill_value; Value to fill in at global indices with no data.; number : :class:`str`; One of 'A', 'R', 'G'. Returns; -------; :class:`.ArrayExpression`; """"""; try:; fill_value = hl.coercer_from_dtype(array.dtype.element_type).coerce(fill_value); except Exception as e:; raise ValueError(f'fill_value type {fill_value.dtype} is incompatible with array type {array.dtype}') from e. if number == 'G':; return _func(""local_to_global_g"", array.dtype, array, local_alleles, n_alleles, fill_value); elif number == 'R':; omit_first = False; elif number == 'A':; omit_first = True; else:; raise ValueError(f'unrecognized number {number}'). return _func(""local_to_global_a_r"", array.dtype, array, local_alleles, n_alleles, fill_value, hl.bool(omit_first)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/functions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/functions.html
Availability,checkpoint,checkpointing,"comparison. Returns; -------; :class:`.Table`; """""". rg = mt.interval.start.dtype.reference_genome. if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_x = rg.x_contigs[0]; if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_y = rg.y_contigs[0]. mt = mt.annotate_rows(contig=mt.interval.start.contig); mt = mt.annotate_cols(__mean_dp=hl.agg.group_by(mt.contig, hl.agg.sum(mt.sum_dp) / hl.agg.sum(mt.interval_size))). mean_dp_dict = mt.__mean_dp; auto_dp = mean_dp_dict.get(normalization_contig, 0.0); x_dp = mean_dp_dict.get(chr_x, 0.0); y_dp = mean_dp_dict.get(chr_y, 0.0); per_sample = mt.transmute_cols(; autosomal_mean_dp=auto_dp,; x_mean_dp=x_dp,; x_ploidy=2 * x_dp / auto_dp,; y_mean_dp=y_dp,; y_ploidy=2 * y_dp / auto_dp,; ); info(""'impute_sex_chromosome_ploidy': computing and checkpointing coverage and karyotype metrics""); return per_sample.cols().checkpoint(new_temp_file('impute_sex_karyotype', extension='ht')). [docs]@typecheck(; vds=VariantDataset,; calling_intervals=oneof(Table, expr_array(expr_interval(expr_locus()))),; normalization_contig=str,; use_variant_dataset=bool,; ); def impute_sex_chromosome_ploidy(; vds: VariantDataset, calling_intervals, normalization_contig: str, use_variant_dataset: bool = False; ) -> Table:; """"""Impute sex chromosome ploidy from depth of reference or variant data within calling intervals. Returns a :class:`.Table` with sample ID keys, with the following fields:. - ``autosomal_mean_dp`` (*float64*): Mean depth on calling intervals on normalization contig.; - ``x_mean_dp`` (*float64*): Mean depth on calling intervals on X chromosome.; - ``x_ploidy`` (*float64*): Estimated ploidy on X chromosome. Equal to ``2 * x_mean_dp / autosomal_mean_dp``.; - ``y_mean_dp`` (",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
Deployability,patch,patch,"reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None):; """"""Cap reference blocks at a maximum length in order to permit faster interval filtering. Examples; --------; Truncate reference blocks to 5 kilobases:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) # doctest: +SKIP. Truncate the longest 1% of reference blocks to the length of the 99th percentile block:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) # doctest: +SKIP. Notes; -----; After this function has been run, the reference blocks have a known maximum length `ref_block_max_length`,; stored in the global fields, which permits :func:`.vds.filter_intervals` to filter to intervals of the reference; data by reading `ref_block_max_length` bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work. It is also possible to patch an existing VDS to store the max reference block length with :func:`.vds.store_ref_block_max_length`. See Also; --------; :func:`.vds.store_ref_block_max_length`. Parameters; ----------; vds : :class:`.VariantDataset` or :class:`.MatrixTable`; max_ref_block_base_pairs; Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction; Fraction of reference block length distribution to truncate / winsorize. Returns; -------; :class:`.VariantDataset` or :class:`.MatrixTable`; """"""; if isinstance(ds, VariantDataset):; rd = ds.reference_data; else:; rd = ds. fd_name = hl.vds.VariantDataset.ref_block_max_length_field; if fd_name in rd.globals:; rd = rd.drop(fd_name). if int(ref_block_winsorize_fraction is None) + int(max_ref_block_base_pairs is None) != 1:; raise ValueError(; 'truncate_reference_blocks: require exactly one of ""max_ref_block_base_pairs"", ""ref_block_winsorize_fraction""'; ). if ref_block_winsorize_fraction is not None:; assert (; ref_block_winsorize_fraction > 0 and ref_block_winsoriz",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
Energy Efficiency,efficient,efficient,"t_type),; key='interval',; ); else:; key_dtype = calling_intervals.key.dtype; if (; len(key_dtype) != 1; or not isinstance(calling_intervals.key[0].dtype, hl.tinterval); or calling_intervals.key[0].dtype.point_type != vds.reference_data.locus.dtype; ):; raise ValueError(; f""'impute_sex_chromosome_ploidy': expect calling_intervals to be list of intervals or""; f"" table with single key of type interval<locus>, found table with key: {key_dtype}""; ). rg = vds.reference_data.locus.dtype.reference_genome. par_boundaries = []; for par_interval in rg.par:; par_boundaries.append(par_interval.start); par_boundaries.append(par_interval.end). # segment on PAR interval boundaries; calling_intervals = hl.segment_intervals(calling_intervals, par_boundaries). # remove intervals overlapping PAR; calling_intervals = calling_intervals.filter(; hl.all(lambda x: ~x.overlaps(calling_intervals.interval), hl.literal(rg.par)); ). # checkpoint for efficient multiple downstream usages; info(""'impute_sex_chromosome_ploidy': checkpointing calling intervals""); calling_intervals = calling_intervals.checkpoint(new_temp_file(extension='ht')). interval = calling_intervals.key[0]; (any_bad_intervals, chrs_represented) = calling_intervals.aggregate((; hl.agg.any(interval.start.contig != interval.end.contig),; hl.agg.collect_as_set(interval.start.contig),; )); if any_bad_intervals:; raise ValueError(; ""'impute_sex_chromosome_ploidy' does not support calling intervals that span chromosome boundaries""; ). if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ); if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ). kept_contig_filter = hl.array(chrs_represented).map(lambda x: hl.parse_locus_interval(x, reference_genome=rg)); vds = VariantDataset(; hl.filter_intervals(vds.referen",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
Modifiability,extend,extend,"antDataset` with those chromosomes; removed.; - ``keep_autosomes``: This argument expects the value ``True``, and returns a dataset without; sex and mitochondrial chromosomes. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset.; keep; Keep a specified list of contigs.; remove; Remove a specified list of contigs; keep_autosomes; If true, keep only autosomal chromosomes. Returns; -------; :class:`.VariantDataset`.; """""". n_args_passed = (keep is not None) + (remove is not None) + keep_autosomes; if n_args_passed == 0:; raise ValueError(""filter_chromosomes: expect one of 'keep', 'remove', or 'keep_autosomes' arguments""); if n_args_passed > 1:; raise ValueError(; ""filter_chromosomes: expect ONLY one of 'keep', 'remove', or 'keep_autosomes' arguments""; ""\n In order use 'keep_autosomes' with 'keep' or 'remove', call the function twice""; ). rg = vds.reference_genome. to_keep = []. if keep is not None:; keep = wrap_to_list(keep); to_keep.extend(keep); elif remove is not None:; remove = set(wrap_to_list(remove)); for c in rg.contigs:; if c not in remove:; to_keep.append(c); elif keep_autosomes:; to_remove = set(rg.x_contigs + rg.y_contigs + rg.mt_contigs); for c in rg.contigs:; if c not in to_remove:; to_keep.append(c). parsed_intervals = hl.literal(to_keep, hl.tarray(hl.tstr)).map(; lambda c: hl.parse_locus_interval(c, reference_genome=rg); ); return _parameterized_filter_intervals(vds, intervals=parsed_intervals, keep=True, mode='unchecked_filter_both'). [docs]@typecheck(; vds=VariantDataset,; intervals=oneof(Table, expr_array(expr_interval(expr_any))),; split_reference_blocks=bool,; keep=bool,; ); def filter_intervals(; vds: 'VariantDataset', intervals, *, split_reference_blocks: bool = False, keep: bool = True; ) -> 'VariantDataset':; """"""Filter intervals in a :class:`.VariantDataset`. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation.; intervals : :class:`.Table` or :class:`.ArrayExpression` of type :class:`.tint",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
Testability,assert,assert,"IXME(chrisvittal) consider changing END semantics on VDS to make this better; # see https://github.com/hail-is/hail/issues/13183 for why this is here and more discussion; # we assume that END <= contig.length; ref = ref.annotate_rows(_locus_global_pos=ref.locus.global_position(), _locus_pos=ref.locus.position); ref = ref.transmute_entries(_END_GLOBAL=ref._locus_global_pos + (ref.END - ref._locus_pos)). to_drop = 'alleles', 'rsid', 'ref_allele', '_locus_global_pos', '_locus_pos'; ref = ref.drop(*(x for x in to_drop if x in ref.row)); var = vds.variant_data; refl = ref.localize_entries('_ref_entries'); varl = var.localize_entries('_var_entries', '_var_cols'); varl = varl.annotate(_variant_defined=True); joined = varl.key_by('locus').join(refl, how='outer'); dr = joined.annotate(; dense_ref=hl.or_missing(; joined._variant_defined, hl.scan._densify(hl.len(joined._var_cols), joined._ref_entries); ); ); dr = dr.filter(dr._variant_defined). def coalesce_join(ref, var):; call_field = 'GT' if 'GT' in var else 'LGT'; assert call_field in var, var.dtype. if call_field not in ref:; ref_call_field = 'GT' if 'GT' in ref else 'LGT'; if ref_call_field not in ref:; ref = ref.annotate(**{call_field: hl.call(0, 0)}); else:; ref = ref.annotate(**{call_field: ref[ref_call_field]}). # call_field is now in both ref and var; ref_set, var_set = set(ref.dtype), set(var.dtype); shared_fields, var_fields = var_set & ref_set, var_set - ref_set. return hl.if_else(; hl.is_defined(var),; var.select(*shared_fields, *var_fields),; ref.select(*shared_fields, **{f: hl.missing(var[f].dtype) for f in var_fields}),; ). dr = dr.annotate(; _dense=hl.rbind(; dr._ref_entries,; lambda refs_at_this_row: hl.enumerate(hl.zip(dr._var_entries, dr.dense_ref)).map(; lambda tup: coalesce_join(; hl.coalesce(; refs_at_this_row[tup[0]],; hl.or_missing(tup[1][1]._END_GLOBAL >= dr.locus.global_position(), tup[1][1]),; ),; tup[1][0],; ); ),; ),; ). dr = dr._key_by_assert_sorted('locus', 'alleles'); fields_to_drop = ['_var_",MatchSource.WIKI,docs/0.2/_modules/hail/vds/methods.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html
Deployability,update,updated,"(GQ) scores.; dp_bins : :class:`tuple` of :obj:`int`; Tuple containing cutoffs for depth (DP) scores.; dp_field : :obj:`str`; Name of depth field. If not supplied, DP or MIN_DP will be used, in that order. Returns; -------; :class:`.Table`; Hail Table of results, keyed by sample.; """""". require_first_key_field_locus(vds.reference_data, 'sample_qc'); require_first_key_field_locus(vds.variant_data, 'sample_qc'). if dp_field is not None:; ref_dp_field_to_use = dp_field; elif 'DP' in vds.reference_data.entry:; ref_dp_field_to_use = 'DP'; elif 'MIN_DP' in vds.reference_data.entry:; ref_dp_field_to_use = 'MIN_DP'; else:; ref_dp_field_to_use = None. vmt = vds.variant_data; if 'GT' not in vmt.entry:; vmt = vmt.annotate_entries(GT=hl.vds.lgt_to_gt(vmt.LGT, vmt.LA)); allele_count, atypes = vmt_sample_qc_variant_annotations(global_gt=vmt.GT, alleles=vmt.alleles); variant_ac = Env.get_uid(); variant_atypes = Env.get_uid(); vmt = vmt.annotate_rows(**{variant_ac: allele_count, variant_atypes: atypes}); vmt_dp = vmt['DP'] if ref_dp_field_to_use is not None and 'DP' in vmt.entry else None; variant_results = vmt.select_cols(; **vmt_sample_qc(; global_gt=vmt.GT,; gq=vmt.GQ,; variant_ac=vmt[variant_ac],; variant_atypes=vmt[variant_atypes],; dp=vmt_dp,; gq_bins=gq_bins,; dp_bins=dp_bins,; ); ).cols(). rmt = vds.reference_data; rmt_dp = rmt[ref_dp_field_to_use] if ref_dp_field_to_use is not None else None; reference_results = rmt.select_cols(; **rmt_sample_qc(; locus=rmt.locus,; gq=rmt.GQ,; end=rmt.END,; dp=rmt_dp,; gq_bins=gq_bins,; dp_bins=dp_bins,; ); ).cols(). joined = reference_results[variant_results.key]; dp_bins_field = {}; if ref_dp_field_to_use is not None:; dp_bins_field['dp_bins'] = hl.tuple(dp_bins); joined_results = variant_results.transmute(**combine_sample_qc(joined, variant_results.row)); joined_results = joined_results.annotate_globals(gq_bins=hl.tuple(gq_bins), **dp_bins_field); return joined_results. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/vds/sample_qc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html
Availability,down,downstream,".path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_block_max_length=False). if VariantDataset.ref_block_max_length_field in vds.reference_data.globals:; warning(f""VDS at {vds_path} already contains a global annotation with the max reference block length""); return; rd = vds.reference_data; rd = rd.annotate_rows(__start_pos=rd.locus.position); fs = hl.current_backend().fs; ref_block_max_len = rd.aggregate_entries(hl.agg.max(rd.END - rd.__start_pos + 1)); with fs.open(os.path.join(vds_path, extra_ref_globals_file), 'w') as f:; json.dump({VariantDataset.",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
Deployability,patch,patch,"ntDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
Performance,load,load,"eturns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make inter",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
Security,validat,validate,"(f""\n {k!r}"" for k in mt.entry if k in used_ref_block_fields); ). rmt = mt.filter_entries(; hl.case(); .when(hl.is_missing(mt.END), False); .when(hl.is_defined(mt.END) & mt[gt_field].is_hom_ref(), True); .or_error(; hl.str(; 'cannot create VDS from merged representation -' ' found END field with non-reference genotype at '; ); + hl.str(mt.locus); + hl.str(' / '); + hl.str(mt.col_key[0]); ); ); rmt = rmt.select_entries(*(x for x in rmt.entry if x in used_ref_block_fields)); rmt = rmt.filter_rows(hl.agg.count() > 0). rmt = rmt.key_rows_by('locus').select_rows().select_cols(). if is_split:; rmt = rmt.distinct_by_row(). vmt = mt.filter_entries(hl.is_missing(mt.END)).drop('END')._key_rows_by_assert_sorted('locus', 'alleles'); vmt = vmt.filter_rows(hl.agg.count() > 0). return VariantDataset(rmt, vmt). def __init__(self, reference_data: MatrixTable, variant_data: MatrixTable):; self.reference_data: MatrixTable = reference_data; self.variant_data: MatrixTable = variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
Testability,assert,assert,"ces; Change Log And Version Policy. menu; Hail. Module code; hail.vds.variant_dataset. Source code for hail.vds.variant_dataset; import json; import os. import hail as hl; from hail.genetics import ReferenceGenome; from hail.matrixtable import MatrixTable; from hail.typecheck import typecheck_method; from hail.utils.java import info, warning. extra_ref_globals_file = 'extra_reference_globals.json'. [docs]def read_vds(; path,; *,; intervals=None,; n_partitions=None,; _assert_reference_type=None,; _assert_variant_type=None,; _warn_no_ref_block_max_length=True,; ) -> 'VariantDataset':; """"""Read in a :class:`.VariantDataset` written with :meth:`.VariantDataset.write`. Parameters; ----------; path: :obj:`str`. Returns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds",MatchSource.WIKI,docs/0.2/_modules/hail/vds/variant_dataset.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html
Availability,down,downstream,"aining two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y ≤ values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""; raw_res = _agg_func(; 'ApproxCDF',; [hl.float64(expr)],; tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)),; init_op_args=[k],; ); conv = {; tint32: lambda x: x.map(hl.int),; tint64: lambda x: x.map(hl.int64),; tfloat32: lambda x: x.map(hl.float32),; tfloat64: identity,; }; if _raw:; return raw_res; else:; raw_res = raw_res.annotate(items=conv[expr.dtype](raw_res['items'])); return _result_from_raw_cdf(raw_res). [docs]@typecheck(expr=expr_numeric, qs=expr_oneof(expr_numeric, expr_array(expr_numeric)), k=int); def approx_quantiles(expr, qs, k=100) -> Expression:; """,MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
Deployability,update,updated,"ons(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum` (for non-missing values) as:. >>> ht.aggregate(hl.agg.fold(0, lambda accum: accum + ht.idx, lambda comb_left, comb_right: comb_left + comb_right)); 4950. Parameters; ----------; initial_value : :class:`.Expression`; The initial value to start the aggregator with. This is a value of type `A`.; seq_op : function ( (:class:`.Expression`) -> :class:`.Expression`); The function used to combine the current aggregator state with the next element you're aggregating over. Type is; `A => A`; comb_op : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); The function used to combine two aggregator states together and produce final result. Type is `(A, A) => A`.; """""". return _agg_func._fold(initial_value, seq_op, comb_op). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
Integrability,wrap,wraps,"﻿. Hail | ; hail.expr.aggregators.aggregators. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.aggregators.aggregators. Source code for hail.expr.aggregators.aggregators; import difflib; from functools import update_wrapper, wraps. import hail as hl; from hail import ir; from hail.expr import (; Aggregation,; ArrayExpression,; BooleanExpression,; DictExpression,; Expression,; ExpressionException,; Float64Expression,; Indices,; Int64Expression,; NDArrayNumericExpression,; NumericExpression,; SetExpression,; StringExpression,; StructExpression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; to_expr,; unify_all,; unify_types,; ); from hail.expr.expressions.typed_expressions import construct_variable; from hail.expr.functions import _quantile_from_cdf, _result_from_raw_cdf, float32, rbind; from hail.expr.types import (; hail_type,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tset,; tstr,; tstruct,; ttuple,; ); from hail.typecheck import TypeChecker, func_spec, identity, nullable, oneof, sequenceof, typecheck, typecheck_method; from hail.utils import wrap_to_list; from hail.utils.java import Env. class AggregableChecker(TypeChecker):; def __init__(self, coercer):; self.coercer = coercer; super(AggregableChecker, self).__init__(). def expects(self):; return self.coercer.expects(). def format(self, arg):; return self.coercer.format(arg). def check(self, x, caller, param):; x = self.coercer.check(x, caller, param); if len(x._ir.search(lambda node: isinstance(nod",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
Modifiability,variab,variables,"nt_type. var = Env.get_uid(base='agg'); ref = construct_expr(ir.Ref(var, elt), elt, array._indices); aggregated = f(ref). if not aggregated._aggregations:; raise ExpressionException(; ""'hl.aggregate_local_array' "" ""must take a mapping that contains aggregation expression.""; ). indices, _ = unify_all(array, aggregated); if isinstance(array.dtype, tarray):; stream = ir.toStream(array._ir); else:; stream = array._ir; return construct_expr(; ir.StreamAgg(stream, var, aggregated._ir),; aggregated.dtype,; Indices(indices.source, indices.axes),; array._aggregations,; ). _agg_func = AggFunc(). def _check_agg_bindings(expr, bindings):; bound_references = {; ref.name; for ref in expr._ir.search(; lambda x: (; isinstance(x, ir.Ref); and not isinstance(x, ir.TopLevelReference); and not x.name.startswith('__uid_scan'); and not x.name.startswith('__uid_agg'); and not x.name == '__rng_state'; ); ); }; free_variables = bound_references - expr._ir.bound_variables - bindings; if free_variables:; raise ExpressionException(; ""dynamic variables created by 'hl.bind' or lambda methods like 'hl.map' may not be aggregated""; ). [docs]@typecheck(expr=expr_numeric, k=int, _raw=bool); def approx_cdf(expr, k=100, *, _raw=False):; """"""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y ≤ values(i)`. An alternative intuition is that the summary encodes a compressed",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
Performance,perform,performs,"egate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; -----; Missing values for `predicate` are treated as ``False``. Parameters; ----------; predicate : :class:`.BooleanExpression`; Boolean predicate. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Fraction of records where `predicate` is ``True``.; """"""; return hl.bind(; lambda n: hl.if_else(n == 0, hl.missing(hl.tfloat64), hl.float64(filter(predicate, count())) / n), count(); ). [docs]@typecheck(expr=expr_call, one_sided=expr_bool); def hardy_weinberg_test(expr, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are define",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
Security,access,accessing,"type; var = Env.get_uid(); ref = construct_expr(ir.Ref(var, elt), elt, array._indices); self._agg_bindings.add(var); aggregated = f(ref); _check_agg_bindings(aggregated, self._agg_bindings); self._agg_bindings.remove(var). if not self._as_scan and not aggregated._aggregations:; raise ExpressionException(; f""'hl.{self.correct_prefix()}.array_agg' "" f""must take mapping that contains aggregation expression.""; ). indices, _ = unify_all(array, aggregated); aggregations = hl.utils.LinkedList(Aggregation); if not self._as_scan:; aggregations = aggregations.push(Aggregation(array, aggregated)); return construct_expr(; ir.AggArrayPerElement(array._ir, var, 'unused', aggregated._ir, self._as_scan),; tarray(aggregated.dtype),; Indices(indices.source, aggregated._indices.axes),; aggregations,; ). @property; def context(self):; if self._as_scan:; return 'scan'; else:; return 'agg'. def _aggregate_local_array(array, f):; """"""Compute a summary of an array using aggregators. Useful for accessing; functionality that exists in `hl.agg` but not elsewhere, like `hl.agg.call_stats`. Parameters; ----------; array; f. Returns; -------; Aggregation result.; """"""; elt = array.dtype.element_type. var = Env.get_uid(base='agg'); ref = construct_expr(ir.Ref(var, elt), elt, array._indices); aggregated = f(ref). if not aggregated._aggregations:; raise ExpressionException(; ""'hl.aggregate_local_array' "" ""must take a mapping that contains aggregation expression.""; ). indices, _ = unify_all(array, aggregated); if isinstance(array.dtype, tarray):; stream = ir.toStream(array._ir); else:; stream = array._ir; return construct_expr(; ir.StreamAgg(stream, var, aggregated._ir),; aggregated.dtype,; Indices(indices.source, indices.axes),; array._aggregations,; ). _agg_func = AggFunc(). def _check_agg_bindings(expr, bindings):; bound_references = {; ref.name; for ref in expr._ir.search(; lambda x: (; isinstance(x, ir.Ref); and not isinstance(x, ir.TopLevelReference); and not x.name.startswith('__uid_scan'); and",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
Testability,test,test,"table1.aggregate(hl.agg.count_where(table1.HT > 68)); 2. Parameters; ----------; condition : :class:`.BooleanExpression`; Criteria for inclusion. Returns; -------; :class:`.Expression` of type :py:data:`.tint64`; Total number of records where `condition` is ``True``.; """""". return _agg_func('Sum', [hl.int64(condition)], tint64). [docs]@typecheck(condition=expr_bool); def any(condition) -> BooleanExpression:; """"""Returns ``True`` if `condition` is ``True`` for any record. Examples; --------. >>> (table1.group_by(table1.SEX); ... .aggregate(any_over_70 = hl.agg.any(table1.HT > 70)); ... .show()); +-----+-------------+; | SEX | any_over_70 |; +-----+-------------+; | str | bool |; +-----+-------------+; | ""F"" | False |; | ""M"" | True |; +-----+-------------+. Notes; -----; If there are no records to aggregate, the result is ``False``. Missing records are not considered. If every record is missing,; the result is also ``False``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""; return count_where(condition) > 0. [docs]@typecheck(condition=expr_bool); def all(condition) -> BooleanExpression:; """"""Returns ``True`` if `condition` is ``True`` for every record. Examples; --------. >>> (table1.group_by(table1.SEX); ... .aggregate(all_under_70 = hl.agg.all(table1.HT < 70)); ... .show()); +-----+--------------+; | SEX | all_under_70 |; +-----+--------------+; | str | bool |; +-----+--------------+; | ""F"" | False |; | ""M"" | False |; +-----+--------------+. Notes; -----; If there are no records to aggregate, the result is ``True``. Missing records are not considered. If every record is missing,; the result is also ``True``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""; return count_where(~condition) == 0. [docs]@typecheck(expr=expr_any, weight=nullable(expr_numeric)); def counter(expr, *, weight=None) -> DictEx",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
Usability,intuit,intuition,"s:; raise ExpressionException(; ""dynamic variables created by 'hl.bind' or lambda methods like 'hl.map' may not be aggregated""; ). [docs]@typecheck(expr=expr_numeric, k=int, _raw=bool); def approx_cdf(expr, k=100, *, _raw=False):; """"""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y ≤ values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""; raw_res = _agg_func(; 'ApproxCDF',; [hl.float64(expr)],; tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)),; init_op_args=[k],; ); conv = {; tint32: lambda x: x.map(hl.int),; tint64: lambda x: x.map(hl.int64),; tfloat32: lambda x: x.map(hl.",MatchSource.WIKI,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html
Availability,error,error,", aggregations). @property; def dtype(self) -> HailType:; """"""The data type of the expression. Returns; -------; :class:`.HailType`. """"""; return self._type. def __bool__(self):; raise TypeError(; ""'Expression' objects cannot be converted to a 'bool'. Use 'hl.if_else' instead of Python if statements.""; ). def __len__(self):; raise TypeError(""'Expression' objects have no static length: use 'hl.len' for the length of collections""). def __contains__(self, item):; class_name = type(self).__name__; raise TypeError(f""`{class_name}` objects don't support the `in` operator.""). def __hash__(self):; return super(Expression, self).__hash__(). def __repr__(self):; return f'<{self.__class__.__name__} of type {self.dtype}>'. [docs] def __eq__(self, other):; """"""Returns ``True`` if the two expressions are equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for equality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are equal.; """"""; return self._compare_op(""=="", other). [docs] def __ne__(self, other):; """"""Returns ``True`` if the two expressions are not equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for inequality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are not equal.; """"""; return self._compare_op(""!="", other). def _to_table(self, name):; name, ds = self._to_relational(name); if isinstance(ds, hail.MatrixTable):; entries = ds.key_cols_by().entries(); entries = entr",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
Deployability,update,updated,"lf), hl.agg.count()),; self._summary_aggs(),; )). def _summarize(self, agg_res=None, *, name=None, header=None, top=False):; src = self._indices.source; summary_header = None; if src is None or len(self._indices.axes) == 0:; raise ValueError(""Cannot summarize a scalar expression""); if agg_res is None:; count, agg_res = self._aggregation_method()(hl.tuple((hl.agg.count(), self._all_summary_aggs()))); summary_header = f'{count} records.'; sum_fields, nested = self._summary_fields(agg_res, top); summary = Summary(self._type, agg_res[0], sum_fields, nested, header=summary_header); if name is None and header is None:; return summary; else:; return NamedSummary(summary, name, header). [docs] def summarize(self, handler=None):; """"""Compute and print summary information about the expression. .. include:: _templates/experimental.rst; """""". src = self._indices.source; if self in src._fields:; field_name = src._fields_inverse[self]; prefix = field_name; elif self._ir.is_nested_field:; prefix = self._ir.name; else:; prefix = '<expr>'. if handler is None:; handler = hl.utils.default_handler(); handler(self._summarize(name=prefix)). def _selector_and_agg_method(self):; src = self._indices.source; assert src is not None; assert len(self._indices.axes) > 0; if isinstance(src, hl.MatrixTable):; if self._indices == src._row_indices:; return src.select_rows, lambda t: t.aggregate_rows; elif self._indices == src._col_indices:; return src.select_cols, lambda t: t.aggregate_cols; else:; return src.select_entries, lambda t: t.aggregate_entries; else:; return src.select, lambda t: t.aggregate. def _aggregation_method(self):; return self._selector_and_agg_method()[1](self._indices.source). def _persist(self):; src = self._indices.source; if src is not None:; raise ValueError(""Can only persist a scalar (no Table/MatrixTable source)""); expr = Env.backend().persist_expression(self); assert expr.dtype == self.dtype; return expr. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
Integrability,depend,dependencies," ts):; return t0. return None. def unify_exprs(*exprs: 'Expression') -> Tuple:; assert len(exprs) > 0; types = {e.dtype for e in exprs}. # all types are the same; if len(types) == 1:; return (*exprs, True). for t in types:; c = expressions.coercer_from_dtype(t); if all(c.can_coerce(e.dtype) for e in exprs):; return (*tuple([c.coerce(e) for e in exprs]), True). # cannot coerce all types to the same type; return (*exprs, False). [docs]class Expression(object):; """"""Base class for Hail expressions."""""". __array_ufunc__ = None # disable NumPy coercions, so Hail coercions take priority. @typecheck_method(x=ir.IR, type=nullable(HailType), indices=Indices, aggregations=linked_list(Aggregation)); def __init__(; self, x: ir.IR, type: HailType, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation); ):; self._ir: ir.IR = x; self._type = type; self._indices = indices; self._aggregations = aggregations; self._summary = None. [docs] def describe(self, handler=print):; """"""Print information about type, index, and dependencies.""""""; if self._aggregations:; agg_indices = set(); for a in self._aggregations:; agg_indices = agg_indices.union(a.indices.axes); agg_tag = ' (aggregated)'; agg_str = (; f'Includes aggregation with index {list(agg_indices)}\n'; f' (Aggregation index may be promoted based on context)'; ); else:; agg_tag = ''; agg_str = ''. bar = '--------------------------------------------------------'; s = (; '{bar}\n'; 'Type:\n'; ' {t}\n'; '{bar}\n'; 'Source:\n'; ' {src}\n'; 'Index:\n'; ' {inds}{agg_tag}{maybe_bar}{agg}\n'; '{bar}'.format(; bar=bar,; t=self.dtype.pretty(indent=4),; src=self._indices.source,; inds=list(self._indices.axes),; maybe_bar='\n' + bar + '\n' if agg_str else '',; agg_tag=agg_tag,; agg=agg_str,; ); ); handler(s). [docs] def __lt__(self, other):; return self._compare_op(""<"", other). [docs] def __le__(self, other):; return self._compare_op(""<="", other). [docs] def __gt__(self, other):; return self._compare_op("">"", other). [docs] d",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
Testability,assert,assert,"nstance(t, (tlocus, tinterval)):; return; if isinstance(t, tstruct):; for k, vt in t.items():; try:; raise_for_holes(vt); except ExpressionException as exc:; raise ExpressionException(f'cannot impute field {k}') from exc; return; if isinstance(t, ttuple):; for k, vt in enumerate(t):; try:; raise_for_holes(vt); except ExpressionException as exc:; raise ExpressionException(f'cannot impute {k}th element') from exc; return; if isinstance(t, (tarray, tset)):; try:; raise_for_holes(t.element_type); except ExpressionException as exc:; raise ExpressionException('cannot impute array elements') from exc; return; if isinstance(t, tdict):; try:; raise_for_holes(t.key_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict keys') from exc; try:; raise_for_holes(t.value_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict values') from exc; return. def to_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if isinstance(e, Expression):; if dtype and not dtype == e.dtype:; raise TypeError(""expected expression of type '{}', found expression of type '{}'"".format(dtype, e.dtype)); return e; return cast_expr(e, dtype, partial_type). def cast_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if not dtype:; dtype = impute_type(e, partial_type); x = _to_expr(e, dtype); if isinstance(x, Expression):; return x; else:; return hl.literal(x, dtype). def _to_expr(e, dtype):; if e is None:; return None; elif isinstance(e, Expression):; if e.dtype != dtype:; assert is_numeric(dtype), 'expected {}, got {}'.format(dtype, e.dtype); if dtype == tfloat64:; return hl.float64(e); elif dtype == tfloat32:; return hl.float32(e); elif dtype == tint64:; return hl.int64(e); else:; assert dtype == tint32; return hl.int32(e); return e; elif not is_compound(dtype):; # these are not container types and cannot contain expressions if we got her",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/base_expression.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html
Availability,error,error,"il.expr.expressions.expression_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.expression_utils. Source code for hail.expr.expressions.expression_utils; from typing import Dict, Set. from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpec",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
Deployability,update,updated,"le_source(caller, expr):; from hail import MatrixTable. source = expr._indices.source; if not isinstance(source, MatrixTable):; raise ValueError(; ""{}: Expect an expression of 'MatrixTable', found {}"".format(; caller, ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ); return source. @typecheck(caller=str, expr=Expression); def table_source(caller, expr):; from hail import Table. source = expr._indices.source; if not isinstance(source, Table):; raise ValueError(; ""{}: Expect an expression of 'Table', found {}"".format(; caller, ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ); return source. @typecheck(caller=str, expr=Expression); def raise_unless_entry_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be entry-indexed"" f"", found no indices (no source)""); if expr._indices != expr._indices.source._entry_indices:; raise ExpressionException(; f""{caller}: expression must be entry-indexed"" f"", found indices {list(expr._indices.axes)}.""; ). @typecheck(caller=str, expr=Expression); def raise_unless_row_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be row-indexed"" f"", found no indices (no source).""); if expr._indices != expr._indices.source._row_indices:; raise ExpressionException(; f""{caller}: expression must be row-indexed"" f"", found indices {list(expr._indices.axes)}.""; ). @typecheck(caller=str, expr=Expression); def raise_unless_column_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be column-indexed"" f"", found no indices (no source).""); if expr._indices != expr._indices.source._col_indices:; raise ExpressionException(; f""{caller}: expression must be column-indexed"" f"", found indices ({list(expr._indices.axes)}).""; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
Testability,assert,assert,"urce mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'strictly '. if unexpected_axes:; # one or more out-of-scope fields; refs = get_refs(expr); bad_refs = []; for name, inds in refs.items():; if broadcast:; bad_axes = inds.axes.intersection(unexpected_axes); if bad_axes:; bad_refs.append((name, inds)); elif inds.axes != expected_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0; errors.append(; ExpressionException(; ""scope violation: '{caller}' expects an expression {strictness}indexed by {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}{agg}"".format(; caller=caller,; strictness=strictness,; expected=list(expected_axes),; axes=list(indices.axes),; stray=list(unexpected_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)) for name, inds in bad_refs; ),; agg=''; if (unexpected_axes - aggregation_axes); else ""\n '{}' supports aggregation over axes {}, ""; ""so these fields may appear inside an aggregator function."".format(caller, list(aggregation_axes)),; ); ); ). if aggregations:; if aggregation_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
Usability,learn,learning," stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); ir = expression._ir; else:; uid = Env.get_uid(); ir = expression._indices.source.select_globals(**{uid: expression}).index_globals()[uid]._ir. return Env.backend().execute(MakeTuple([ir]), timed=True)[0]. [docs]@typecheck(expression=expr_any); def eval(expression):; """"""Evaluate a Hail expression, returning the result. This method is extremely useful for learning about Hail expressions and; understanding how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.Table` or :class:`.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; Any; """"""; return eval_timed(expression)[0]. @typecheck(expression=expr_any); def eval_typed(expression):; """"""Evaluate a Hail expression, returning the result and the type of the result. This method is extremely useful for learning about Hail expressions and understanding; how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.hail.Table` or :class:`.hail.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval_typed(hl.if_else(x % 2 == 0, 'Even', 'Odd')); ('Even', dtype('str",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html
Availability,error,error,"== {1, 2, 3}; True; >>> hl.eval(hl.flatten(a.b.inner)) == {1, 2, 3}; True. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.SetExpression`; A set formed by getting the given field for each struct in; this set; """""". return self.map(lambda x: x[item]). [docs]class DictExpression(Expression):; """"""Expression of type :class:`.tdict`. >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}); """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(DictExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tdict); self._kc = coercer_from_dtype(type.key_type); self._vc = coercer_from_dtype(type.value_type). [docs] @typecheck_method(item=expr_any); def __getitem__(self, item):; """"""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""dict encountered an invalid key type\n"" "" dict key type: '{}'\n"" "" type of 'item': '{}'"".format(; self.dtype.key_type, item.dtype; ); ); return self._index(self.dtype.value_type, self._kc.coerce(item)). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns whether a given key is present in the dictionary. Examples; --------. >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters; ----------; item : :class:`.Expression`; Key to test for inclusion. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is a key of the dictionary, ``False`` otherwise.; """"""; if not self._kc.can_coerce(item.dtype):; raise Ty",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
Deployability,pipeline,pipeline,"rn construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @deprecated(version=""0.2.58"", reason=""Replaced by first""); def head(self):; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; return self.first(). [docs] def first(self):; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.eval(na",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
Energy Efficiency,efficient,efficient,"rn construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @deprecated(version=""0.2.58"", reason=""Replaced by first""); def head(self):; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; return self.first(). [docs] def first(self):; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.eval(na",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
Integrability,message,message,"kedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type, indices, aggregations); self._fields: Dict[str, Expression] = {}; self._warn_on_shadowed_name = set(). for i, (f, t) in enumerate(self.dtype.items()):; if isinstance(self._ir, ir.MakeStruct):; expr = construct_expr(self._ir.fields[i][1], t, self._indices, self._aggregations); elif isinstance(self._ir, ir.SelectedTopLevelReference):; expr = construct_expr(; ir.ProjectedTopLevelReference(self._ir.ref.name, f, t), t, self._indices, self._aggregations; ); elif isinstance(self._ir, ir.SelectFields):; expr = construct_expr(ir.GetField(self._ir.old, f), t, self._indices, self._aggregations); else:; expr = construct_expr(ir.GetField(self._ir, f), t, self._indices, self._aggregations); self._set_field(f, expr). def _set_field(self, key, value):; if key not in self._fields:; # Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.; if key in self.__dict__ or hasattr(super(), key):; self._warn_on_shadowed_name.add(key); else:; self.__dict__[key] = value; self._fields[key] = value. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). def __getattribute__(self, item):; if item in super().__getattribute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); ",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
Modifiability,extend,extend,"eturn x(elt). else:. def f(elt, x):; return elt == x. return hl.bind(lambda a: hl.range(0, a.length()).filter(lambda i: f(a[i], x)).first(), self). [docs] @typecheck_method(item=expr_any); def append(self, item):; """"""Append an element to the array and return the result. Examples; --------. >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; ----; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding `item`. Parameters; ----------; item : :class:`.Expression`; Element to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; if item._type != self._type.element_type:; raise TypeError(; ""'ArrayExpression.append' expects 'item' to be the same type as its elements\n""; "" array element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self._type._element_type, item._type); ); return self._method(""append"", self._type, item). [docs] @typecheck_method(a=expr_array()); def extend(self, a):; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; if not a._type == self._type:; raise TypeError(; ""'ArrayExpression.extend' expects 'a' to be the same type as the caller\n""; "" caller type: '{}'\n""; "" type of 'a': '{}'"".format(self._type, a._type); ); return self._method(""extend"", self._type, a). [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def scan(self, f, zero):; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the c",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
Performance,load,load," hl.eval(locus.in_autosome_or_par()); True. Returns; -------; :class:`.BooleanExpression`; """"""; return self._method(""isAutosomalOrPseudoAutosomal"", tbool). [docs] def in_mito(self):; """"""Returns ``True`` if the locus is on mitochondrial DNA. Examples; --------. >>> hl.eval(locus.in_mito()); False. Returns; -------; :class:`.BooleanExpression`; """"""; return self._method(""isMitochondrial"", tbool). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def sequence_context(self, before=0, after=0):; """"""Return the reference genome sequence at the locus. Examples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
Safety,safe,safer,"top):; k = construct_variable(Env.get_uid(), self.dtype.key_type, indices=self._indices); v = construct_variable(Env.get_uid(), self.dtype.value_type, indices=self._indices); return {; '[<keys>]': k._summarize(agg_result[3][0]),; '[<values>]': v._summarize(agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = {}; for k, v in fields.items():; s._set_field(k, v); return s. @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
Security,access,accessing,"se TypeError(; ""array expects key to be type 'slice' or expression of type 'int32', ""; ""found expression of type '{}'"".format(item._type); ); else:; return self._method(""indexArray"", self.dtype.element_type, item). @typecheck_method(start=nullable(expr_int32), stop=nullable(expr_int32), step=nullable(expr_int32)); def _slice(self, start=None, stop=None, step=None):; indices, aggregations = unify_all(self, *(x for x in (start, stop, step) if x is not None)); if step is None:; step = hl.int(1); if start is None:; start = hl.if_else(step >= 0, 0, -1); if stop is not None:; slice_ir = ir.ArraySlice(self._ir, start._ir, stop._ir, step._ir); else:; slice_ir = ir.ArraySlice(self._ir, start._ir, stop, step._ir). return construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @dep",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
Testability,assert,assert,"octest: +SKIP_OUTPUT_CHECK; {'Bob'}. Notes; -----; Returns a same-type expression; evaluated on a :class:`.SetExpression`, returns a; :class:`.SetExpression`. Evaluated on an :class:`.ArrayExpression`,; returns an :class:`.ArrayExpression`. Parameters; ----------; f : function ( (arg) -> :class:`.BooleanExpression`); Function to evaluate for each element of the collection. Must return a; :class:`.BooleanExpression`. Returns; -------; :class:`.CollectionExpression`; Expression of the same type as the callee.; """"""; # FIXME: enable doctest. def unify_ret(t):; if t != tbool:; raise TypeError(""'filter' expects 'f' to return an expression of type 'bool', found '{}'"".format(t)); return hl.tarray(self._type.element_type). def transform_ir(array, name, body):; return ir.toArray(ir.StreamFilter(ir.toStream(array), name, body)). array_filter = hl.array(self)._ir_lambda_method(transform_ir, f, self.dtype.element_type, unify_ret). if isinstance(self.dtype, tset):; return hl.set(array_filter); else:; assert isinstance(self.dtype, tarray), self.dtype; return array_filter. [docs] @typecheck_method(f=func_spec(1, expr_bool)); def find(self, f):; """"""Returns the first element where `f` returns ``True``. Examples; --------. >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Notes; -----; If `f` returns ``False`` for every element, then the result is missing. Parameters; ----------; f : function ( (arg) -> :class:`.BooleanExpression`); Function to evaluate for each element of the collection. Must return a; :class:`.BooleanExpression`. Returns; -------; :class:`.Expression`; Expression whose type is the element type of the collection.; """""". # FIXME this should short-circuit; return self.fold(; lambda accum, x: hl.if_else(hl.is_missing(accum) & f(x), x, accum), hl.missing(self._type.element_type); ). [docs] @typecheck_method(f=func_spec(1, expr_any)); def flatmap(self, f):; """"""Map each element of the collection to a new collection, and flatten",MatchSource.WIKI,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html
Deployability,update,updated,"otate_fields[coords] = coord_expr; else:; coords = src._fields_inverse[coord_expr]. if isinstance(src, hl.MatrixTable):; new_src = src.annotate_rows(**annotate_fields); else:; new_src = src.annotate(**annotate_fields). locus_expr = new_src[locus]; if coord_expr is not None:; coord_expr = new_src[coords]. if coord_expr is None:; coord_expr = locus_expr.position. rg = locus_expr.dtype.reference_genome; contig_group_expr = hl.agg.group_by(hl.locus(locus_expr.contig, 1, reference_genome=rg), hl.agg.collect(coord_expr)). # check loci are in sorted order; last_pos = hl.fold(; lambda a, elt: (; hl.case(); .when(a <= elt, elt); .or_error(; hl.str(""locus_windows: 'locus_expr' global position must be in ascending order. ""); + hl.str(a); + hl.str("" was not less then or equal to ""); + hl.str(elt); ); ),; -1,; hl.agg.collect(; hl.case(); .when(hl.is_defined(locus_expr), locus_expr.global_position()); .or_error(""locus_windows: missing value for 'locus_expr'.""); ),; ); checked_contig_groups = (; hl.case().when(last_pos >= 0, contig_group_expr).or_error(""locus_windows: 'locus_expr' has length 0""); ). contig_groups = locus_expr._aggregation_method()(checked_contig_groups, _localize=False). coords = hl.sorted(hl.array(contig_groups)).map(lambda t: t[1]); starts_and_stops = hl._locus_windows_per_contig(coords, radius). if not _localize:; return starts_and_stops. starts, stops = hl.eval(starts_and_stops); return np.array(starts), np.array(stops). def _check_dims(a, name, ndim, min_size=1):; if len(a.shape) != ndim:; raise ValueError(f'{name} must be {ndim}-dimensional, ' f'found {a.ndim}'); for i in range(ndim):; if a.shape[i] < min_size:; raise ValueError(f'{name}.shape[{i}] must be at least ' f'{min_size}, found {a.shape[i]}'). def _ndarray_matmul_ndim(left, right):; if left == 1 and right == 1:; return 0; elif left == 1:; return right - 1; elif right == 1:; return left - 1; else:; assert left == right; return left. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html
Testability,assert,assert,"otate_fields[coords] = coord_expr; else:; coords = src._fields_inverse[coord_expr]. if isinstance(src, hl.MatrixTable):; new_src = src.annotate_rows(**annotate_fields); else:; new_src = src.annotate(**annotate_fields). locus_expr = new_src[locus]; if coord_expr is not None:; coord_expr = new_src[coords]. if coord_expr is None:; coord_expr = locus_expr.position. rg = locus_expr.dtype.reference_genome; contig_group_expr = hl.agg.group_by(hl.locus(locus_expr.contig, 1, reference_genome=rg), hl.agg.collect(coord_expr)). # check loci are in sorted order; last_pos = hl.fold(; lambda a, elt: (; hl.case(); .when(a <= elt, elt); .or_error(; hl.str(""locus_windows: 'locus_expr' global position must be in ascending order. ""); + hl.str(a); + hl.str("" was not less then or equal to ""); + hl.str(elt); ); ),; -1,; hl.agg.collect(; hl.case(); .when(hl.is_defined(locus_expr), locus_expr.global_position()); .or_error(""locus_windows: missing value for 'locus_expr'.""); ),; ); checked_contig_groups = (; hl.case().when(last_pos >= 0, contig_group_expr).or_error(""locus_windows: 'locus_expr' has length 0""); ). contig_groups = locus_expr._aggregation_method()(checked_contig_groups, _localize=False). coords = hl.sorted(hl.array(contig_groups)).map(lambda t: t[1]); starts_and_stops = hl._locus_windows_per_contig(coords, radius). if not _localize:; return starts_and_stops. starts, stops = hl.eval(starts_and_stops); return np.array(starts), np.array(stops). def _check_dims(a, name, ndim, min_size=1):; if len(a.shape) != ndim:; raise ValueError(f'{name} must be {ndim}-dimensional, ' f'found {a.ndim}'); for i in range(ndim):; if a.shape[i] < min_size:; raise ValueError(f'{name}.shape[{i}] must be at least ' f'{min_size}, found {a.shape[i]}'). def _ndarray_matmul_ndim(left, right):; if left == 1 and right == 1:; return 0; elif left == 1:; return right - 1; elif right == 1:; return left - 1; else:; assert left == right; return left. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/linalg/utils/misc.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html
Availability,checkpoint,checkpoint," 3))),; _e10=(; 4 * (p**3) * q * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + 4 * p * (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e20=(; (p**4) * ((X - 1) / X) * ((X - 2) / X) * ((X - 3) / X) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + (q**4) * ((Y - 1) / Y) * ((Y - 2) / Y) * ((Y - 3) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + 4 * (p**2) * (q**2) * ((X - 1) / X) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e11=(; 2 * (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + 2 * p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
Deployability,update,updated,"s.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); result = result.drop('Z0', 'Z1', 'Z2'); if bounded:; result = result.annotate(ibd=bound_result(result.ibd)); result = result.annotate(ibd=result.ibd.annotate(PI_HAT=result.ibd.Z1 / 2 + result.ibd.Z2)); result = result.filter((result.i < result.j) & (min <= result.ibd.PI_HAT) & (result.ibd.PI_HAT <= max)). samples = hl.literal(dataset.s.collect()); result = result.key_by(i=samples[hl.int32(result.i)], j=samples[hl.int32(result.j)]). return result.persist(). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
Performance,perform,perform,"by_descent(dataset, maf=None, bounded=True, min=None, max=None) -> Table:; """"""Compute matrix of identity-by-descent estimates. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:. >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in :math:`[0.2, 0.9]`, using minor allele frequencies stored in; the row field `panel_maf`:. >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; -----. The dataset must have a column field named `s` which is a :class:`.StringExpression`; and which uniquely identifies a column. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :func:`.identity_by_descent` requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first. The resulting :class:`.Table` entries have the type: *{ i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }*. The key list is: `*i: String, j:; String*`. Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form. .. code-block:: text. i		j	ibd.Z0	ibd.Z1	ibd.Z2	ibd.PI_HAT ibs0	ibs1	ibs2; sample1	sample2	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample3	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample4	0.6807	0.0000	0.3193	0.3193 ...; sample1	sample5	0.1966	0.0000	0.8034	0.8034 ... Parameters; ----------; dataset : :class:`.MatrixTable`; Variant-keyed and sample-keyed :class:`.MatrixTable` containing genotype information.; maf : :class:`.Float64Expression`, optional; Row-indexed expression for the minor allele frequency.; bounded : :obj:`bool`; Fo",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html
Availability,checkpoint,checkpoint,"able` whose rows and columns are keys are taken from; `call-expr`'s column keys. It has one entry field, `phi`.; """"""; mt = matrix_table_source('king/call_expr', call_expr); call = Env.get_uid(); mt = mt.annotate_entries(**{call: call_expr}). is_hom_ref = Env.get_uid(); is_het = Env.get_uid(); is_hom_var = Env.get_uid(); is_defined = Env.get_uid(); mt = mt.unfilter_entries(); mt = mt.select_entries(**{; is_hom_ref: hl.float(hl.or_else(mt[call].is_hom_ref(), 0)),; is_het: hl.float(hl.or_else(mt[call].is_het(), 0)),; is_hom_var: hl.float(hl.or_else(mt[call].is_hom_var(), 0)),; is_defined: hl.float(hl.is_defined(mt[call])),; }); ref = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_ref], block_size=block_size); het = hl.linalg.BlockMatrix.from_entry_expr(mt[is_het], block_size=block_size); var = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_var], block_size=block_size); defined = hl.linalg.BlockMatrix.from_entry_expr(mt[is_defined], block_size=block_size); ref_var = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[ki",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
Deployability,configurat,configurations,"non-missing genotype. - :math:`X_{i,s}` be the genotype score matrix. Each entry corresponds to; the genotype of individual :math:`i` at variant; :math:`s`. Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. :math:`X_{i,s}` is calculated by invoking; :meth:`~.CallExpression.n_alt_alleles` on the `call_expr`. The three counts above, :math:`N^{Aa}`, :math:`N^{Aa,Aa}`, and; :math:`N^{AA,aa}`, exclude variants where one or both individuals have; missing genotypes. In terms of the symbols above, we can define :math:`d`, the genetic distance; between two samples. We can interpret :math:`d` as an unnormalized; measurement of the genetic material not shared identically-by-descent:. .. math::. d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2. In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. +-------------------------------+----------+----------+----------+; |:math:`(X_{i,s} - X_{j,s})^2` |homref |het |homalt |; +-------------------------------+----------+----------+----------+; |homref |0 |1 |4 |; +-------------------------------+----------+----------+----------+; |het |1 |0 |1 |; +-------------------------------+----------+----------+----------+; |homalt |4 |1 |0 |; +-------------------------------+----------+----------+----------+. which leads to this expression for genetic distance:. .. math::. d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}. The first term, :math:`4 N^{AA,aa}_{i,j}`, accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
Modifiability,config,configurations,"non-missing genotype. - :math:`X_{i,s}` be the genotype score matrix. Each entry corresponds to; the genotype of individual :math:`i` at variant; :math:`s`. Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. :math:`X_{i,s}` is calculated by invoking; :meth:`~.CallExpression.n_alt_alleles` on the `call_expr`. The three counts above, :math:`N^{Aa}`, :math:`N^{Aa,Aa}`, and; :math:`N^{AA,aa}`, exclude variants where one or both individuals have; missing genotypes. In terms of the symbols above, we can define :math:`d`, the genetic distance; between two samples. We can interpret :math:`d` as an unnormalized; measurement of the genetic material not shared identically-by-descent:. .. math::. d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2. In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. +-------------------------------+----------+----------+----------+; |:math:`(X_{i,s} - X_{j,s})^2` |homref |het |homalt |; +-------------------------------+----------+----------+----------+; |homref |0 |1 |4 |; +-------------------------------+----------+----------+----------+; |het |1 |0 |1 |; +-------------------------------+----------+----------+----------+; |homalt |4 |1 |0 |; +-------------------------------+----------+----------+----------+. which leads to this expression for genetic distance:. .. math::. d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}. The first term, :math:`4 N^{AA,aa}_{i,j}`, accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
Testability,assert,assert,"ar = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[kinship_between.row_key, kinship_between.col_key].element,; ). col_index_field = Env.get_uid(); col_key = mt.col_key; cols = mt.add_col_index(col_index_field).key_cols_by(col_index_field).cols(). kinship_between = kinship_between.key_cols_by(**cols[kinship_between.col_idx].select(*col_key)). renaming, _ = deduplicate(list(col_key), already_used=set(col_key)); assert len(renaming) == len(col_key). kinship_between = kinship_between.key_rows_by(; **cols[kinship_between.row_idx].select(*col_key).rename(dict(renaming)); ). kinship_between = kinship_between.annotate_entries(; min_n_hets=hl.min(kinship_between.n_hets_row, kinship_between.n_hets_col); ); return (; kinship_between.select_entries(; phi=(0.5); + (; (2 * kinship_between.het_hom_balance + -kinship_between.n_hets_row - kinship_between.n_hets_col); / (4 * kinship_between.min_n_hets); ); ); .select_rows(); .select_cols(); .select_globals(); ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/king.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html
Deployability,update,updated," ); if n_rounds < 1:; raise ValueError(f""simulate_random_mating: 'n_rounds' must be positive: got {n_rounds}""). ck = next(iter(mt.col_key)). mt = mt.select_entries('GT'). ht = mt.localize_entries('__entries', '__cols'). ht = ht.annotate_globals(; generation_0=hl.range(hl.len(ht.__cols)).map(; lambda i: hl.struct(; s=hl.str('generation_0_idx_') + hl.str(i),; original=hl.str(ht.__cols[i][ck]),; mother=hl.missing('int32'),; father=hl.missing('int32'),; ); ); ). def make_new_generation(prev_generation_tup, idx):; prev_size = prev_generation_tup[1]; n_new = hl.int32(hl.floor(prev_size * generation_size_multiplier)); new_generation = hl.range(n_new).map(; lambda i: hl.struct(; s=hl.str('generation_') + hl.str(idx + 1) + hl.str('_idx_') + hl.str(i),; original=hl.missing('str'),; mother=hl.rand_int32(0, prev_size),; father=hl.rand_int32(0, prev_size),; ); ); return (new_generation, (prev_size + n_new) if keep_founders else n_new). ht = ht.annotate_globals(; generations=hl.range(n_rounds).scan(; lambda prev, idx: make_new_generation(prev, idx), (ht.generation_0, hl.len(ht.generation_0)); ); ). def simulate_mating_calls(prev_generation_calls, new_generation):; new_samples = new_generation.map(; lambda samp: hl.call(; prev_generation_calls[samp.mother][hl.rand_int32(0, 2)],; prev_generation_calls[samp.father][hl.rand_int32(0, 2)],; ); ); if keep_founders:; return prev_generation_calls.extend(new_samples); else:; return new_samples. ht = ht.annotate(; __new_entries=hl.fold(; lambda prev_calls, generation_metadata: simulate_mating_calls(prev_calls, generation_metadata[0]),; ht.__entries.GT,; ht.generations[1:],; ).map(lambda gt: hl.struct(GT=gt)); ); ht = ht.annotate_globals(; __new_cols=ht.generations.flatmap(lambda x: x[0]) if keep_founders else ht.generations[-1][0]; ); ht = ht.drop('__entries', '__cols', 'generation_0', 'generations'); return ht._unlocalize_entries('__new_entries', '__new_cols', list('s')). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html
Modifiability,extend,extend," ); if n_rounds < 1:; raise ValueError(f""simulate_random_mating: 'n_rounds' must be positive: got {n_rounds}""). ck = next(iter(mt.col_key)). mt = mt.select_entries('GT'). ht = mt.localize_entries('__entries', '__cols'). ht = ht.annotate_globals(; generation_0=hl.range(hl.len(ht.__cols)).map(; lambda i: hl.struct(; s=hl.str('generation_0_idx_') + hl.str(i),; original=hl.str(ht.__cols[i][ck]),; mother=hl.missing('int32'),; father=hl.missing('int32'),; ); ); ). def make_new_generation(prev_generation_tup, idx):; prev_size = prev_generation_tup[1]; n_new = hl.int32(hl.floor(prev_size * generation_size_multiplier)); new_generation = hl.range(n_new).map(; lambda i: hl.struct(; s=hl.str('generation_') + hl.str(idx + 1) + hl.str('_idx_') + hl.str(i),; original=hl.missing('str'),; mother=hl.rand_int32(0, prev_size),; father=hl.rand_int32(0, prev_size),; ); ); return (new_generation, (prev_size + n_new) if keep_founders else n_new). ht = ht.annotate_globals(; generations=hl.range(n_rounds).scan(; lambda prev, idx: make_new_generation(prev, idx), (ht.generation_0, hl.len(ht.generation_0)); ); ). def simulate_mating_calls(prev_generation_calls, new_generation):; new_samples = new_generation.map(; lambda samp: hl.call(; prev_generation_calls[samp.mother][hl.rand_int32(0, 2)],; prev_generation_calls[samp.father][hl.rand_int32(0, 2)],; ); ); if keep_founders:; return prev_generation_calls.extend(new_samples); else:; return new_samples. ht = ht.annotate(; __new_entries=hl.fold(; lambda prev_calls, generation_metadata: simulate_mating_calls(prev_calls, generation_metadata[0]),; ht.__entries.GT,; ht.generations[1:],; ).map(lambda gt: hl.struct(GT=gt)); ); ht = ht.annotate_globals(; __new_cols=ht.generations.flatmap(lambda x: x[0]) if keep_founders else ht.generations[-1][0]; ); ht = ht.drop('__entries', '__cols', 'generation_0', 'generations'); return ht._unlocalize_entries('__new_entries', '__new_cols', list('s')). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html
Availability,down,down,"s=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of a sample's first ``k``; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. - an individual's first ``k`` principal component coordinates fully; describe their allele-frequency-relevant ancestry, and. - the relationship between ancestry ",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
Deployability,release,release,"s \in S_{ij}}; \widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \widehat{k^{(0)}_{ij}} \coloneqq; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs b",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
Energy Efficiency,efficient,efficient,"float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def pc_relate(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = 'all',; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; r""""""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using a minimum minor; allele frequency filter of 0.01 and 10 principal components to control; for population structure. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10) # doctest: +SKIP. Only compute the kinship statistic. This is more efficient than; computing all statistics. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, statistics='kin') # doctest: +SKIP. Compute all statistics, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full table and; then filtering using :meth:`.Table.filter`. >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) # doctest: +SKIP. One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
Integrability,depend,depending,"netic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; :math:`2^{-3} = 12.5 %` of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that :math:`g_{is}` is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first. The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the `statistics` parameter:. - `i` (``col_key.dtype``) -- First sample. (key field); - `j` (``col_key.dtype``) -- Second sample. (key field); - `kin` (:py:data:`.tfloat64`) -- Kinship estimate, :math:`\widehat{\phi_{ij}}`.; - `ibd2` (:py:data:`.tfloat64`) -- IBD2 estimate, :math:`\widehat{k^{(2)}_{ij}}`.; - `ibd0` (:py:data:`.tfloat64`) -- IBD0 estimate, :math:`\widehat{k^{(0)}_{ij}}`.; - `ibd1` (:py:data:`.tfloat64`) -- IBD1 estimate, :math:`\widehat{k^{(1)}_{ij}}`. Here ``col_key`` refers to the column key of the source matrix table,; and ``col_key.dtype`` is a struct containing the column key fields. There is one row for each pair of distinct samples (columns), where `i`; corresponds to the column of smaller column index. In particular, if the; same column key value exists for :math:`n` columns, then the resulting; table will have :math:`\binom{n-1}{2}` rows with both key fields equal to; that column key value. This may result in unexpected behavior in downst",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
Modifiability,inherit,inherited,"puted principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of a sample's first ``k``; principal component coordinates. As such, the efficacy of this method; rests o",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
Performance,load,loadings,"{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
Testability,assert,assert,"B_plus_BtA(A: BlockMatrix, B: BlockMatrix) -> BlockMatrix:; """"""Compute `(A.T @ B) + (B.T @ A)`, used in estimating IBD0 (k0). Parameters; ----------; A : :class:`.BlockMatrix`; B : :class:`.BlockMatrix`. Returns; -------; :class:`.BlockMatrix`; `(A.T @ B) + (B.T @ A)`; """"""; temp = (A.T @ B).checkpoint(new_temp_file()); return temp + temp.T. def _replace_nan(M: BlockMatrix, value: float) -> BlockMatrix:; """"""Replace NaN entries in a dense :class:`.BlockMatrix` with provided value. Parameters; ----------; M: :class:`.BlockMatrix`; value: :obj:`float`; Value to replace NaN entries with. Returns; -------; :class:`.BlockMatrix`; """"""; return M._map_dense(lambda x: hl.if_else(hl.is_nan(x), value, x)). @typecheck(; call_expr=expr_call,; min_individual_maf=numeric,; k=nullable(int),; scores_expr=nullable(expr_array(expr_float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def _pc_relate_bm(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = ""all"",; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; assert 0.0 <= min_individual_maf <= 1.0, (; f'invalid argument: min_individual_maf={min_individual_maf}. '; f'Must have min_individual_maf on interval [0.0, 1.0].'; ); mt = matrix_table_source('pc_relate_bm/call_expr', call_expr); if k and scores_expr is None:; eigens, scores, _ = _hwe_normalized_blanczos(call_expr, k, compute_loadings=False, q_iterations=10); scores_table = scores.select(__scores=scores.scores).key_by().select('__scores'); compute_S0 = False; elif not k and scores_expr is not None:; analyze('pc_relate_bm/scores_expr', scores_expr, mt._col_indices); eigens = None; scores_table = mt.select_cols(__scores=scores_expr).key_cols_by().select_cols('__scores').cols(); compute_S0 = True; elif k",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
Usability,simpl,simply,"itional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of a sample's first ``k``; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. - an individual's first ``k`` principal component coordinates fully; describe their allele-frequency-relevant ancestry, and. - the relationship between ancestry (as described by principal; component coordinates) and population allele frequency is linear. The estimators for kinship, and identity-by-descent zero, one, and two; follow.",MatchSource.WIKI,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html
Availability,failure,failure-tolerant,"iantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instance-attributes; """"""A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. Examples; --------. A Variant Dataset comprises one or more sequences. A new Variant Dataset is constructed from; GVCF files and/or extant Variant Datasets. For example, the following produces a new Variant; Dataset from four GVCF files containing whole genome sequences ::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
Deployability,update,update,"call_fields:; warning(; ""Mismatch between 'call_fields' and VDS call fields. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encod",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
Integrability,depend,depends,"10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :cl",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
Performance,load,load,"ath}'); print('An old version of this state may be there.'); print(; 'Dumping current state as json to standard output, you may wish '; 'to save this output in order to resume the combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; interval",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
Security,hash,hashlib,"﻿. Hail | ; hail.vds.combiner.variant_dataset_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.combiner.variant_dataset_combiner. Source code for hail.vds.combiner.variant_dataset_combiner; import collections; import hashlib; import json; import os; import sys; import uuid; from itertools import chain; from math import floor, log; from typing import ClassVar, Collection, Dict, List, NamedTuple, Optional, Union. import hail as hl; from hail.expr import HailType, tmatrix; from hail.genetics.reference_genome import ReferenceGenome; from hail.utils import FatalError, Interval; from hail.utils.java import info, warning. from ..variant_dataset import VariantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instanc",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
Testability,log,log,"﻿. Hail | ; hail.vds.combiner.variant_dataset_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.combiner.variant_dataset_combiner. Source code for hail.vds.combiner.variant_dataset_combiner; import collections; import hashlib; import json; import os; import sys; import uuid; from itertools import chain; from math import floor, log; from typing import ClassVar, Collection, Dict, List, NamedTuple, Optional, Union. import hail as hl; from hail.expr import HailType, tmatrix; from hail.genetics.reference_genome import ReferenceGenome; from hail.utils import FatalError, Interval; from hail.utils.java import info, warning. from ..variant_dataset import VariantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instanc",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
Usability,resume,resume,"; return False; return True. @property; def finished(self) -> bool:; """"""Have all GVCFs and input Variant Datasets been combined?""""""; return not self._gvcfs and not self._vdses. [docs] def save(self):; """"""Save a :class:`.VariantDatasetCombiner` to its `save_path`.""""""; fs = hl.current_backend().fs; try:; backup_path = self._save_path + '.bak'; if fs.exists(self._save_path):; fs.copy(self._save_path, backup_path); with fs.open(self._save_path, 'w') as out:; json.dump(self, out, indent=2, cls=Encoder); if fs.exists(backup_path):; fs.remove(backup_path); except OSError as e:; # these messages get printed, because there is absolutely no guarantee; # that the hail context is in a sane state if any of the above operations; # fail; print(f'Failed saving {self.__class__.__name__} state at {self._save_path}'); print(f'An attempt was made to copy {self._save_path} to {backup_path}'); print('An old version of this state may be there.'); print(; 'Dumping current state as json to standard output, you may wish '; 'to save this output in order to resume the combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.",MatchSource.WIKI,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html
Availability,error,error,"le Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
Deployability,update,updated,"ot exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return _fses[requester_pays_config].ls(path). [docs]def mkdir(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Ensure files can be created whose dirname is `path`. Warning; -------. On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does nothing. """"""; _fses[requester_pays_config].mkdir(path). [docs]def remove(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Removes the file at `path`. If the file does not exist, this function does; nothing. `path` must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters; ----------; path : :class:`str`; """"""; _fses[requester_pays_config].remove(path). [docs]def rmtree(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Recursively remove all files under the given `path`. On a local filesystem,; this removes the directory tree at `path`. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with `path`. As such,; `path` must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters; ----------; path : :class:`str`; """"""; _fses[requester_pays_config].rmtree(path). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
Performance,load,load,"ter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you can use; :func:`.open` instead. For example:. >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; _fses[requester_pays_config].copy(src, dest). [docs]def exists(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].exists(path). [docs]def is_file(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[r",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
Security,access,access,"CSRequesterPaysFSCache(fs_constructor=RouterFS). [docs]def open(; path: str,; mode: str = 'r',; buffer_size: int = 8192,; *,; requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None,; ) -> io.IOBase:; """"""Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS. Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; ",MatchSource.WIKI,docs/0.2/_modules/hailtop/fs/fs_utils.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html
Availability,avail,available,"w page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that take",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
Deployability,install,installed,"s 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the com",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
Energy Efficiency,consumption,consumption,", and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; ",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
Integrability,depend,dependencies,"ant_qc.AF[1] > 0.01). eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). mt = mt.annotate_cols(scores=pcs[mt.s].scores). gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The followin",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
Modifiability,flexible,flexible,"Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. Th",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
Performance,perform,performing,"﻿. Clumping GWAS Results — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Introduction; Hail GWAS Script; Docker Image; Batch Script; Functions; Control Code. Synopsis. Random Forest. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
Safety,detect,detection,"ob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine’s memory. PLINK’s memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
Security,access,access,"ed=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
Testability,test,test,"oogle Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. The image is the image created in the previous step. We copied the run_gwas.py; script into the root directory /. Therefore, to execute the run_gwas.py script, we; call /run_gwas.py.; The run_gwas.py script takes an output-file parameter and then creates files ending with; the extensions .bed, .bim, .fam, and .assoc. In order for Batch to know the script is; creating files as a group with a common file root, we need to use the BashJob.declare_resource_group(); method. We then pass g.ofile as the output file root to ru",MatchSource.WIKI,docs/batch/cookbook/clumping.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html
Availability,checkpoint,checkpointing,"rsion Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
Deployability,install,installed,"r=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.Se",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
Modifiability,variab,variables,"ity as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio.; We write a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == w",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
Performance,perform,perform,"ckpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
Safety,predict,predict,". Random Forest Model — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Random Forest; Introduction; Batch Code; Imports; Random Forest Function; Format Result Function; Build Python Image; Control Code. Add Checkpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Rand",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
Testability,test,testing,"om Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio.; We write a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your compute",MatchSource.WIKI,docs/batch/cookbook/random_forest.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html
Modifiability,variab,variable,". LocalBackend — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; LocalBackend; LocalBackend. ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; LocalBackend. View page source. LocalBackend. class hailtop.batch.backend.LocalBackend(tmp_dir='/tmp/', gsa_key_file=None, extra_docker_run_flags=None); Bases: Backend[None]; Backend that executes batches on a local computer.; .. rubric:: Examples; >>> local_backend = LocalBackend(tmp_dir='/tmp/user/'); >>> b = Batch(backend=local_backend). Parameters:. tmp_dir (str) – Temporary directory to use.; gsa_key_file (Optional[str]) – Mount a file with a gsa key to /gsa-key/key.json. Only used if a; job specifies a docker image. This option will override the value set by; the environment variable HAIL_BATCH_GSA_KEY_FILE.; extra_docker_run_flags (Optional[str]) – Additional flags to pass to docker run. Only used if a job specifies; a docker image. This option will override the value set by the environment; variable HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS. Methods. _async_run; Execute a batch. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(). Parameters:. batch (Batch) – Batch to execute.; dry_run (bool) – If True, don’t execute code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit (bool) – If True, delete temporary directories with intermediate files. Return type:; None. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html
Energy Efficiency,monitor,monitor,". RunningBatchType — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; RunningBatchType. Backend; LocalBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; RunningBatchType. View page source. RunningBatchType. class hailtop.batch.backend.RunningBatchType; The type of value returned by Backend._run(). The value returned by some backends; enables the user to monitor the asynchronous execution of a Batch.; alias of TypeVar(‘RunningBatchType’). Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html
Availability,echo,echo,"nd; LocalBackend; ServiceBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
Deployability,configurat,configuration,"nced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-acc",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
Modifiability,config,configuration,"nced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-acc",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
Security,authoriz,authorization,"h/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str]) – The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]]) – Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
Usability,progress bar,progress bar,"r None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word arguments. Parameters:. batch (Batch) – Batch to execute.; dry_run (bool) – If True, don’t execute code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit (bool) – If True, delete temporary directories with intermediate files.; wait (bool) – If True, wait for the batch to finish executing before returning.; open (bool) – If True, open the UI page for the batch.; disable_progress_bar (bool) – If True, disable the progress bar.; callback (Optional[str]) – If not None, a URL that will receive at most one POST request; after the entire batch completes.; token (Optional[str]) – If not None, a string used for idempotency of batch submission. Return type:; Optional[Batch]. static supported_regions(); Get the supported cloud regions; Examples; >>> regions = ServiceBackend.supported_regions(). Returns:; A list of the supported cloud regions. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html
Availability,echo,echo,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Batch. Job; BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Batch. View page source. Batch. class hailtop.batch.batch.Batch(name=None, backend=None, attributes=None, requester_pays_project=None, default_image=None, default_memory=None, default_cpu=None, default_storage=None, default_regions=None, default_timeout=None, default_shell=None, default_python_image=None, default_spot=None, project=None, cancel_after_n_failures=None); Bases: object; Object representing the distributed acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints “hello”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respective",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
Deployability,configurat,configurations,"llo”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the Local",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
Energy Efficiency,charge,charges,"ult_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str) – File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and it’s index (file extensions matter!):; >>> fasta =",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
Modifiability,variab,variable," acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints “hello”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
Safety,timeout,timeout," any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None]) – Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]]) – Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See ServiceBackend for details.; default_timeout (Union[int, float, None]) – Maximum time in seconds for a job to run before being killed. Only; applicable for the ServiceBackend. If None, there is no; timeout.; default_python_image (Optional[str]) – Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool]) – If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str]) – DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Opt",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
Security,access,accessing,"t already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None]) – Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]]) – Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See S",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
Testability,assert,assert,"eFile.add_extension(); to add an extension to a resource file. Parameters:; kwargs (Union[str, PathLike]) – Key word arguments where the name/key is the identifier and the value; is the file path. Return type:; ResourceGroup. run(dry_run=False, verbose=False, delete_scratch_on_exit=True, **backend_kwargs); Execute a batch.; Examples; Create a simple batch with one job and execute it:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. dry_run (bool) – If True, don’t execute code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit (bool) – If True, delete temporary directories with intermediate files.; backend_kwargs (Any) – See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str) – Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in w",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
Usability,simpl,simple,"foo and rg.bar will not have the .txt file extension and; instead will be {root}.foo and {root}.bar where {root} is a random; identifier.; Notes; The identifier is used to refer to a specific resource file. For example,; given the resource group rg, you can use the attribute notation; rg.identifier or the get item notation rg[identifier].; The file extensions for each file are derived from the identifier. This; is equivalent to “{root}.identifier” from; BashJob.declare_resource_group(). We are planning on adding; flexibility to incorporate more complicated extensions in the future; such as .vcf.bgz. For now, use JobResourceFile.add_extension(); to add an extension to a resource file. Parameters:; kwargs (Union[str, PathLike]) – Key word arguments where the name/key is the identifier and the value; is the file path. Return type:; ResourceGroup. run(dry_run=False, verbose=False, delete_scratch_on_exit=True, **backend_kwargs); Execute a batch.; Examples; Create a simple batch with one job and execute it:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. dry_run (bool) – If True, don’t execute code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit (bool) – If True, delete temporary directories with intermediate files.; backend_kwargs (Any) – See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str) – Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_outp",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.batch.Batch.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html
Availability,echo,echo,". BashJob — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; BashJob. PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BashJob. View page source. BashJob. class hailtop.batch.job.BashJob(batch, token, *, name=None, attributes=None, shell=None); Bases: Job; Object representing a single bash job to execute.; Examples; Create a batch object:; >>> b = Batch(). Create a new bash job that prints hello to a temporary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'e",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
Testability,log,log,"he get item syntax of; job[‘identifier’]. If an object for that identifier doesn’t exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; command (str) – A bash command. Return type:; BashJob. Returns:; Same job object with command appended. declare_resource_group(**mappings); Declare a resource group for a job.; Examples; Declare a resource group:; >>> b = Batch(); >>> input = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'); >>> j = b.new_job(); >>> j.declare_resource_group(tmp1={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam',; ... 'log': '{root}.log'}); >>> j.command(f'plink --bfile {input} --make-bed --out {j.tmp1}'); >>> b.run() . Warning; Be careful when specifying the expressions for each file as this is Python; code that is executed with eval!. Parameters:; mappings (Dict[str, Any]) – Keywords (in the above example tmp1) are the name(s) of the; resource group(s). File names may contain arbitrary Python; expressions, which will be evaluated by Python eval. To use the; keyword as the file name, use {root} (in the above example {root}; will be replaced with tmp1). Return type:; BashJob. Returns:; Same job object with resource groups set. image(image); Set the job’s docker image.; Examples; Set the job’s docker image to ubuntu:22.04:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.image('ubuntu:22.04'); ... .command(f'echo ""hello""')); >>> b.run() . Parameters:; image (str) – Docker image to use. Return type:; BashJob. Returns:; Same job object with docker image set. Previous; Next . © Copyright 2024, Hail Team. Built w",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.BashJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html
Availability,echo,echo,"hon_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Clou",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
Energy Efficiency,power,power,"est')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str) – Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str) – The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool) – If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be exp",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
Integrability,depend,dependencies,". Job — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; Job. BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
Modifiability,variab,variable,"u('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the job’s memory requirements.; Examples; Set the job’s memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
Performance,perform,performance," = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str) – Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str) – The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool) – If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where th",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
Safety,timeout,timeout,"ompatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always ",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
Testability,test,test,"tly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performanc",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.Job.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html
Availability,down,downstream,"lds[1]),; 'add': int(fields[2]),; 'mult': int(fields[3])}; data.append(d); return json.dumps(data). # Get all the multiplication and addition table results. b = Batch(name='add-mult-table'). formatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; Unlike the BashJob, a PythonJob returns a new; PythonResult for every invocation of PythonJob.call(). A; PythonResult can be used as an argument in subsequent invocations of; PythonJob.call(), as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
Deployability,install,installed,"her bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that is compatible; with Python jobs.; Here are some tips to make sure your function can be used with Batch:. Only reference top-level modules in your functions: like numpy or pandas.; If you get a serialization error, try moving your imports into your function.; Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters:. unapplied (Callable) – A reference to a Python function to execute.; args (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, R",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
Integrability,depend,dependencies,"rmatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; Unlike the BashJob, a PythonJob returns a new; PythonResult for every invocation of PythonJob.call(). A; PythonResult can be used as an argument in subsequent invocations of; PythonJob.call(), as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that i",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
Safety,detect,detects,"rmatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; Unlike the BashJob, a PythonJob returns a new; PythonResult for every invocation of PythonJob.call(). A; PythonResult can be used as an argument in subsequent invocations of; PythonJob.call(), as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that i",MatchSource.WIKI,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html
Availability,avail,available,". BatchPoolExecutor — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
Deployability,install,installed,"ior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
Energy Efficiency,allocate,allocate,"achine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to b",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
Modifiability,variab,variables,". map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to be cleaned up. submit; Call fn on a cloud machine with all remaining arguments and keyword arguments. async async_map(fn, iterables, timeout=None, chunksize=1); Aysncio compati",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
Performance,concurren,concurrent,". BatchPoolExecutor — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
Safety,safe,safely,"xecutes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
Security,access,access,"parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool) – If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the underlying Docker image. For more; details see the default_image argument to BatchPoolExecutor; This function does not return the function’s output, it returns a; BatchPoolFuture whose BatchPoolFuture.result() method; can be used to access the value.; Examples; Do nothing, but on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... future = bpe.submit(lambda x: x, 4); ... future.result(); 4. Call a function with two arguments and one keyword argument, on the; cloud:; >>> with BatchPoolExecutor() as bpe: ; ... future = bpe.submit(lambda x, y, z: x + y + z,; ... ""poly"", ""ethyl"", z=""ene""); ... future.result(); ""polyethylene"". Generate a product of two random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... future = bpe.submit(random_product, 1); ... future.result(); [23.325755364428026]. Parameters:. fn (Callable) – The function to execute.; args (Any) – Arguments for the funciton.; kwargs (Any) – Keyword arguments for the function. Return type:; BatchPoolFuture. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; prov",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html
Availability,error,error,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
Performance,concurren,concurrent," Policy; Change Log. Batch. Python API; BatchPoolFuture. View page source. BatchPoolFuture. class hailtop.batch.batch_pool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This fu",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
Safety,timeout,timeout,"iguration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolFuture. View page source. BatchPoolFuture. class hailtop.batch.batch_pool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long ",MatchSource.WIKI,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html
Availability,echo,echo,". JobResourceFile — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; JobResourceFile. ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; JobResourceFile. View page source. JobResourceFile. class hailtop.batch.resource.JobResourceFile(value, source); Bases: ResourceFile; Class representing an intermediate file from a job.; Examples; j.ofile is a JobResourceFile on the job`j`:; >>> b = Batch(); >>> j = b.new_job(name='hello-tmp'); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.run(). Notes; All JobResourceFile are temporary files and must be written; to a permanent location using Batch.write_output() if the output needs; to be saved.; Methods. add_extension; Specify the file extension to use. source. rtype:; Job. add_extension(extension); Specify the file extension to use.; Examples; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> j.ofile.add_extension('.txt'); >>> b.run(). Notes; The default file name for a JobResourceFile is the name; of the identifier. Parameters:; extension (str) – File extension to use. Return type:; JobResourceFile. Returns:; JobResourceFile – Same resource file with the extension specified. source(). Return type:; Job. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html
Deployability,install,installed,". hailtop.batch.docker.build_python_image — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; build_python_image(). hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.docker.build_python_image. View page source. hailtop.batch.docker.build_python_image. hailtop.batch.docker.build_python_image(fullname, requirements=None, python_version=None, _tmp_dir='/tmp', *, show_docker_output=False); Build a new Python image with dill and the specified pip packages installed.; Notes; This function is used to build Python images for PythonJob.; Examples; >>> image = build_python_image('us-docker.pkg.dev/<MY_GCP_PROJECT>/hail/batch-python',; ... requirements=['pandas']) . Parameters:. fullname (str) – Full name of where to build the image including any repository prefix and tags; if desired (default tag is latest).; requirements (Optional[List[str]]) – List of pip packages to install.; python_version (Optional[str]) – String in the format of major_version.minor_version (ex: 3.9). Defaults to; current version of Python that is running.; _tmp_dir (str) – Location to place local temporary files used while building the image.; show_docker_output (bool) – Print the output from Docker when building / pushing the image. Return type:; str. Returns:; Full name where built image is located. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,docs/batch/api/utils/hailtop.batch.docker.build_python_image.html,hail-is,hail,0.2.133,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html
