id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/pull/7056:305,Performance,load,load,305,"- I sneaked in another change where I pass in a single file containing a list of input_vcfs instead of an array of input_vcfs. I made this because Terra couldn't save my inputs when I passed in 700 samples.; - Most of the logic was moved into `CreateTables`, including the determination for what files to load. It would have been cleaner to move all of the file loading logic into `LoadTable` but the current approach cuts down the on the number of `gsutil ls` calls made and more importantly, only spins up a shard if there are files to load.; - I pushed the logic into a separate workflow because I wanted to refactor it as two tasks and I couldn't find a way to get a Task to call another Task without wrapping it in a workflow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7056
https://github.com/broadinstitute/gatk/pull/7056:362,Performance,load,loading,362,"- I sneaked in another change where I pass in a single file containing a list of input_vcfs instead of an array of input_vcfs. I made this because Terra couldn't save my inputs when I passed in 700 samples.; - Most of the logic was moved into `CreateTables`, including the determination for what files to load. It would have been cleaner to move all of the file loading logic into `LoadTable` but the current approach cuts down the on the number of `gsutil ls` calls made and more importantly, only spins up a shard if there are files to load.; - I pushed the logic into a separate workflow because I wanted to refactor it as two tasks and I couldn't find a way to get a Task to call another Task without wrapping it in a workflow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7056
https://github.com/broadinstitute/gatk/pull/7056:382,Performance,Load,LoadTable,382,"- I sneaked in another change where I pass in a single file containing a list of input_vcfs instead of an array of input_vcfs. I made this because Terra couldn't save my inputs when I passed in 700 samples.; - Most of the logic was moved into `CreateTables`, including the determination for what files to load. It would have been cleaner to move all of the file loading logic into `LoadTable` but the current approach cuts down the on the number of `gsutil ls` calls made and more importantly, only spins up a shard if there are files to load.; - I pushed the logic into a separate workflow because I wanted to refactor it as two tasks and I couldn't find a way to get a Task to call another Task without wrapping it in a workflow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7056
https://github.com/broadinstitute/gatk/pull/7056:538,Performance,load,load,538,"- I sneaked in another change where I pass in a single file containing a list of input_vcfs instead of an array of input_vcfs. I made this because Terra couldn't save my inputs when I passed in 700 samples.; - Most of the logic was moved into `CreateTables`, including the determination for what files to load. It would have been cleaner to move all of the file loading logic into `LoadTable` but the current approach cuts down the on the number of `gsutil ls` calls made and more importantly, only spins up a shard if there are files to load.; - I pushed the logic into a separate workflow because I wanted to refactor it as two tasks and I couldn't find a way to get a Task to call another Task without wrapping it in a workflow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7056
https://github.com/broadinstitute/gatk/pull/7056:222,Testability,log,logic,222,"- I sneaked in another change where I pass in a single file containing a list of input_vcfs instead of an array of input_vcfs. I made this because Terra couldn't save my inputs when I passed in 700 samples.; - Most of the logic was moved into `CreateTables`, including the determination for what files to load. It would have been cleaner to move all of the file loading logic into `LoadTable` but the current approach cuts down the on the number of `gsutil ls` calls made and more importantly, only spins up a shard if there are files to load.; - I pushed the logic into a separate workflow because I wanted to refactor it as two tasks and I couldn't find a way to get a Task to call another Task without wrapping it in a workflow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7056
https://github.com/broadinstitute/gatk/pull/7056:370,Testability,log,logic,370,"- I sneaked in another change where I pass in a single file containing a list of input_vcfs instead of an array of input_vcfs. I made this because Terra couldn't save my inputs when I passed in 700 samples.; - Most of the logic was moved into `CreateTables`, including the determination for what files to load. It would have been cleaner to move all of the file loading logic into `LoadTable` but the current approach cuts down the on the number of `gsutil ls` calls made and more importantly, only spins up a shard if there are files to load.; - I pushed the logic into a separate workflow because I wanted to refactor it as two tasks and I couldn't find a way to get a Task to call another Task without wrapping it in a workflow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7056
https://github.com/broadinstitute/gatk/pull/7056:560,Testability,log,logic,560,"- I sneaked in another change where I pass in a single file containing a list of input_vcfs instead of an array of input_vcfs. I made this because Terra couldn't save my inputs when I passed in 700 samples.; - Most of the logic was moved into `CreateTables`, including the determination for what files to load. It would have been cleaner to move all of the file loading logic into `LoadTable` but the current approach cuts down the on the number of `gsutil ls` calls made and more importantly, only spins up a shard if there are files to load.; - I pushed the logic into a separate workflow because I wanted to refactor it as two tasks and I couldn't find a way to get a Task to call another Task without wrapping it in a workflow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7056
https://github.com/broadinstitute/gatk/issues/7057:103,Availability,down,downstream,103,"Because of the initial use cases for Funcotator everything is reported on the + strand (eg. upstream / downstream bases, sequences, etc.). We should add a flag that would change the output to be native to the strand on which the gene is located. For + strand genes the output would not change, but any bases reported for genes on the - strand would be reverse-complemented.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7057
https://github.com/broadinstitute/gatk/issues/7059:161,Availability,error,error,161,"Upon running Mutect2 using the germline and pon provided by gatk inside a pipeline while running in parallel. If I use an intervals BED file I get the following error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:383); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:335); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.FeatureManager.addToFeatureSources(FeatureManager.java:246); 	at org.broadinstitute.hellbender.engine.FeatureManager.initializeFeatureSources(FeatureManager.java:209); 	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:156); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeFeatures(GATKTool.java:488); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy), for input source: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at htsjdk.tribble.TabixFea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059
https://github.com/broadinstitute/gatk/issues/7059:230,Availability,Error,Error,230,"Upon running Mutect2 using the germline and pon provided by gatk inside a pipeline while running in parallel. If I use an intervals BED file I get the following error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:383); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:335); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.FeatureManager.addToFeatureSources(FeatureManager.java:246); 	at org.broadinstitute.hellbender.engine.FeatureManager.initializeFeatureSources(FeatureManager.java:209); 	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:156); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeFeatures(GATKTool.java:488); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy), for input source: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at htsjdk.tribble.TabixFea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059
https://github.com/broadinstitute/gatk/issues/7059:1821,Availability,error,error,1821,"tureSources(FeatureManager.java:209); 	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:156); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeFeatures(GATKTool.java:488); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy), for input source: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:380); 	... 14 more; Caused by: java.io.FileNotFoundException: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy); 	at java.io.RandomAccessFile.open0(Native Method); 	at java.io.RandomAccessFile.open(RandomAccessFile.java:316); 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:243); 	at htsjdk.samtools.seekablestream.SeekableFileStream.<init>(SeekableFileStream.java:47); 	at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059
https://github.com/broadinstitute/gatk/issues/7059:3062,Availability,avail,available,3062,"p(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy), for input source: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:380); 	... 14 more; Caused by: java.io.FileNotFoundException: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy); 	at java.io.RandomAccessFile.open0(Native Method); 	at java.io.RandomAccessFile.open(RandomAccessFile.java:316); 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:243); 	at htsjdk.samtools.seekablestream.SeekableFileStream.<init>(SeekableFileStream.java:47); 	at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:111); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:94); 	... 17 more; ```. Which doesn't happen when running it without the intervals file. I am using the latest version of GATK4 available in conda.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059
https://github.com/broadinstitute/gatk/issues/7059:74,Deployability,pipeline,pipeline,74,"Upon running Mutect2 using the germline and pon provided by gatk inside a pipeline while running in parallel. If I use an intervals BED file I get the following error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:383); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:335); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.FeatureManager.addToFeatureSources(FeatureManager.java:246); 	at org.broadinstitute.hellbender.engine.FeatureManager.initializeFeatureSources(FeatureManager.java:209); 	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:156); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeFeatures(GATKTool.java:488); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy), for input source: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at htsjdk.tribble.TabixFea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059
https://github.com/broadinstitute/gatk/issues/7060:123,Availability,error,error,123,"I ran CalibrateDragstrModel on one of the NYGC 1000G crams (which should be Functionally Equivalent with ours) and got the error: `A reference must be supplied that includes the reference sequence for chr12` I did pass a reference to the tool, but couldn't get it to run until I set the samjdk.reference_fasta black magic (at @droazen 's suggestion) in the java invocation. Huge stack trace:; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005); 	at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:473); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:152); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1057); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caus",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:2759,Energy Efficiency,Reduce,ReduceOps,2759,mandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:2769,Energy Efficiency,Reduce,ReduceOp,2769,mandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:2795,Energy Efficiency,Reduce,ReduceOps,2795,1); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:2930,Energy Efficiency,reduce,reduce,2930,rg.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Contai,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:3142,Energy Efficiency,Adapt,AdaptedCallable,3142,ce for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at htsjdk.samtools.CRAMFi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:5929,Energy Efficiency,Reduce,ReduceOps,5929,ase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:553); 	at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:425); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:835); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$10(CalibrateDragstrModel.java:478); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721); 	at java.util.stream.AbstractTask.compute(AbstractTask.java:327); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	... 4 more; Using GATK jar /gatk/gatk-package-4.1.9.0-15-g8f07c46-SNAPSHOT-local.jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:5939,Energy Efficiency,Reduce,ReduceTask,5939,ase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:553); 	at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:425); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:835); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$10(CalibrateDragstrModel.java:478); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721); 	at java.util.stream.AbstractTask.compute(AbstractTask.java:327); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	... 4 more; Using GATK jar /gatk/gatk-package-4.1.9.0-15-g8f07c46-SNAPSHOT-local.jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:5957,Energy Efficiency,Reduce,ReduceOps,5957,ase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:553); 	at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:425); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:835); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$10(CalibrateDragstrModel.java:478); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721); 	at java.util.stream.AbstractTask.compute(AbstractTask.java:327); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	... 4 more; Using GATK jar /gatk/gatk-package-4.1.9.0-15-g8f07c46-SNAPSHOT-local.jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:5999,Energy Efficiency,Reduce,ReduceOps,5999,ase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:553); 	at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:425); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:835); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$10(CalibrateDragstrModel.java:478); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721); 	at java.util.stream.AbstractTask.compute(AbstractTask.java:327); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	... 4 more; Using GATK jar /gatk/gatk-package-4.1.9.0-15-g8f07c46-SNAPSHOT-local.jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:6009,Energy Efficiency,Reduce,ReduceTask,6009,ase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:553); 	at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:425); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:835); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$10(CalibrateDragstrModel.java:478); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721); 	at java.util.stream.AbstractTask.compute(AbstractTask.java:327); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	... 4 more; Using GATK jar /gatk/gatk-package-4.1.9.0-15-g8f07c46-SNAPSHOT-local.jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:6027,Energy Efficiency,Reduce,ReduceOps,6027,ase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:553); 	at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:425); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:835); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$10(CalibrateDragstrModel.java:478); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721); 	at java.util.stream.AbstractTask.compute(AbstractTask.java:327); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	... 4 more; Using GATK jar /gatk/gatk-package-4.1.9.0-15-g8f07c46-SNAPSHOT-local.jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:5864,Integrability,wrap,wrapAndCopyInto,5864,ase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:553); 	at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:425); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:835); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$10(CalibrateDragstrModel.java:478); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721); 	at java.util.stream.AbstractTask.compute(AbstractTask.java:327); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	... 4 more; Using GATK jar /gatk/gatk-package-4.1.9.0-15-g8f07c46-SNAPSHOT-local.jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:3142,Modifiability,Adapt,AdaptedCallable,3142,ce for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at htsjdk.samtools.CRAMFi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:942,Performance,concurren,concurrent,942,"I ran CalibrateDragstrModel on one of the NYGC 1000G crams (which should be Functionally Equivalent with ours) and got the error: `A reference must be supplied that includes the reference sequence for chr12` I did pass a reference to the tool, but couldn't get it to run until I set the samjdk.reference_fasta black magic (at @droazen 's suggestion) in the java invocation. Huge stack trace:; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005); 	at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:473); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:152); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1057); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caus",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:1026,Performance,concurren,concurrent,1026,"one of the NYGC 1000G crams (which should be Functionally Equivalent with ours) and got the error: `A reference must be supplied that includes the reference sequence for chr12` I did pass a reference to the tool, but couldn't get it to run until I set the samjdk.reference_fasta black magic (at @droazen 's suggestion) in the java invocation. Huge stack trace:; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005); 	at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:473); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:152); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1057); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:2521,Performance,concurren,concurrent,2521,neProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgume,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:2605,Performance,concurren,concurrent,2605,dline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:2683,Performance,concurren,concurrent,2683,); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:3118,Performance,concurren,concurrent,3118,the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at ht,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:3202,Performance,concurren,concurrent,3202,wInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:3271,Performance,concurren,concurrent,3271,mpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:527); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:3352,Performance,concurren,concurrent,3352,gConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:527); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:3425,Performance,concurren,concurrent,3425,va:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:527); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:4843,Performance,load,loadNextIterator,4843,normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:527); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:553); 	at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:425); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:835); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$10(CalibrateDragstrModel.java:478); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.w,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/issues/7060:6128,Performance,concurren,concurrent,6128,ase.initializeIterator(CRAMFileReader.java:500); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:558); 	at htsjdk.samtools.CRAMFileReader$CRAMIntervalIterator.<init>(CRAMFileReader.java:553); 	at htsjdk.samtools.CRAMFileReader.query(CRAMFileReader.java:425); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:389); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.query(ReadsPathDataSource.java:352); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.readStream(CalibrateDragstrModel.java:835); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$null$10(CalibrateDragstrModel.java:478); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747); 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721); 	at java.util.stream.AbstractTask.compute(AbstractTask.java:327); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	... 4 more; Using GATK jar /gatk/gatk-package-4.1.9.0-15-g8f07c46-SNAPSHOT-local.jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060
https://github.com/broadinstitute/gatk/pull/7062:76,Deployability,pipeline,pipeline,76,* improvements leading to tieout of cohort extract; * WDLs for running WARP pipeline for tieout; * tweaks to WDLs (memory size) for running extract,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7062
https://github.com/broadinstitute/gatk/pull/7063:85,Testability,test,tests,85,The `ReadsDataSourcePool` class and the `CalibrateDragstrModel` tool both don't have tests... @ldgauthier can you test if this branch fixes your cram inputs?. Fixes #7060,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7063
https://github.com/broadinstitute/gatk/pull/7063:114,Testability,test,test,114,The `ReadsDataSourcePool` class and the `CalibrateDragstrModel` tool both don't have tests... @ldgauthier can you test if this branch fixes your cram inputs?. Fixes #7060,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7063
https://github.com/broadinstitute/gatk/issues/7064:1150,Availability,avail,available,1150,"ription. **(Background)**; I've spent a lot of time working with Mutect2 in the past year (I've built a whole workflow centered around this tool). But, while I recognize that the internal reassembly feature leads to ""best-in-class"" results in terms of calling variants, for my purposes it generally just creates headaches since it makes interpreting (certain) calls and verifying (certain) base-level behaviors/expectations very difficult (even when looking at the bamout and assembly logs). Moreover, while we know our alignment process isn't perfect, we think it's appropriate for our purposes, and we would gladly accept the loss of a few calls to be able to have more control over the expected behaviors. With that, I purpose a ""--skip-assembly"" flag that would cause the Mutect2/HaplotypeCaller engine to use the original alignment information to determine the haplotypes. . All that said, I imagine this could be a niche feature request, so I've spent some time digging through the source code trying to see if there could be a quick fix that could be made available to whatever group of developers would want this. It seemed like there could be another conditional branched added here (https://github.com/broadinstitute/gatk/blob/9ff3f8b180c063a3fa67dae129b0cbd04012448e/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/ReadThreadingAssembler.java#L159) to build a `resultSet` based on a non-assembly based approach. However, I'm not certain how using the original alignment information would affect the statistics employed for genotyping the candidate haplotypes, so I'm starting to back off implementing a custom fix and hoping the experts can help (or at least explain to me why this feature is not currently possible OR if there is a way that I can access this behavior that I'm missing). Thank you for the consideration. **(TL;DR)**; Introduce a `--skip-assembly` option that would cause the Mutect2/HaplotypeCaller engine to use the original alignm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7064
https://github.com/broadinstitute/gatk/issues/7064:1886,Security,access,access,1886,"of time working with Mutect2 in the past year (I've built a whole workflow centered around this tool). But, while I recognize that the internal reassembly feature leads to ""best-in-class"" results in terms of calling variants, for my purposes it generally just creates headaches since it makes interpreting (certain) calls and verifying (certain) base-level behaviors/expectations very difficult (even when looking at the bamout and assembly logs). Moreover, while we know our alignment process isn't perfect, we think it's appropriate for our purposes, and we would gladly accept the loss of a few calls to be able to have more control over the expected behaviors. With that, I purpose a ""--skip-assembly"" flag that would cause the Mutect2/HaplotypeCaller engine to use the original alignment information to determine the haplotypes. . All that said, I imagine this could be a niche feature request, so I've spent some time digging through the source code trying to see if there could be a quick fix that could be made available to whatever group of developers would want this. It seemed like there could be another conditional branched added here (https://github.com/broadinstitute/gatk/blob/9ff3f8b180c063a3fa67dae129b0cbd04012448e/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/ReadThreadingAssembler.java#L159) to build a `resultSet` based on a non-assembly based approach. However, I'm not certain how using the original alignment information would affect the statistics employed for genotyping the candidate haplotypes, so I'm starting to back off implementing a custom fix and hoping the experts can help (or at least explain to me why this feature is not currently possible OR if there is a way that I can access this behavior that I'm missing). Thank you for the consideration. **(TL;DR)**; Introduce a `--skip-assembly` option that would cause the Mutect2/HaplotypeCaller engine to use the original alignment information to determine the haplotypes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7064
https://github.com/broadinstitute/gatk/issues/7064:572,Testability,log,logs,572,"## Feature request. ### Tool(s) or class(es) involved; Mutect/HaplotypeCaller. ### Description. **(Background)**; I've spent a lot of time working with Mutect2 in the past year (I've built a whole workflow centered around this tool). But, while I recognize that the internal reassembly feature leads to ""best-in-class"" results in terms of calling variants, for my purposes it generally just creates headaches since it makes interpreting (certain) calls and verifying (certain) base-level behaviors/expectations very difficult (even when looking at the bamout and assembly logs). Moreover, while we know our alignment process isn't perfect, we think it's appropriate for our purposes, and we would gladly accept the loss of a few calls to be able to have more control over the expected behaviors. With that, I purpose a ""--skip-assembly"" flag that would cause the Mutect2/HaplotypeCaller engine to use the original alignment information to determine the haplotypes. . All that said, I imagine this could be a niche feature request, so I've spent some time digging through the source code trying to see if there could be a quick fix that could be made available to whatever group of developers would want this. It seemed like there could be another conditional branched added here (https://github.com/broadinstitute/gatk/blob/9ff3f8b180c063a3fa67dae129b0cbd04012448e/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/ReadThreadingAssembler.java#L159) to build a `resultSet` based on a non-assembly based approach. However, I'm not certain how using the original alignment information would affect the statistics employed for genotyping the candidate haplotypes, so I'm starting to back off implementing a custom fix and hoping the experts can help (or at least explain to me why this feature is not currently possible OR if there is a way that I can access this behavior that I'm missing). Thank you for the consideration. **(TL;DR)**; Introduce a `--skip-assembly`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7064
https://github.com/broadinstitute/gatk/pull/7066:31,Deployability,update,updated,31,Test will fail until htsjdk is updated. Fixes https://github.com/broadinstitute/gatk/issues/6475.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7066
https://github.com/broadinstitute/gatk/pull/7066:0,Testability,Test,Test,0,Test will fail until htsjdk is updated. Fixes https://github.com/broadinstitute/gatk/issues/6475.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7066
https://github.com/broadinstitute/gatk/pull/7067:92,Energy Efficiency,reduce,reduces,92,For AoU/BQ solution we want to be able to produce VCFs w/o PLs. In a 1k sample WGS VCF this reduces the .vcf.gz total size from 120GB to 73GB (almost 40%),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7067
https://github.com/broadinstitute/gatk/pull/7069:370,Availability,down,downstream,370,"Fixes #7068 . - When adding AC, AF, AN, DP header lines, SelectVariants now checks if these lines are in the original header already and if so, overwrites these lines with the respective standard lines; - Without this check, an issue in htsjdk causes duplicate header lines with the same ID if the description differs. This should be fixed there but this fix provides a downstream workaround; - Modified the integration test validation files, which have been invalid VCF files with duplicate header lines; - Removed addition of AC, AF, AN if `--set-filtered-gt-to-nocall` is set, because these lines will be added later anyway",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7069
https://github.com/broadinstitute/gatk/pull/7069:408,Deployability,integrat,integration,408,"Fixes #7068 . - When adding AC, AF, AN, DP header lines, SelectVariants now checks if these lines are in the original header already and if so, overwrites these lines with the respective standard lines; - Without this check, an issue in htsjdk causes duplicate header lines with the same ID if the description differs. This should be fixed there but this fix provides a downstream workaround; - Modified the integration test validation files, which have been invalid VCF files with duplicate header lines; - Removed addition of AC, AF, AN if `--set-filtered-gt-to-nocall` is set, because these lines will be added later anyway",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7069
https://github.com/broadinstitute/gatk/pull/7069:408,Integrability,integrat,integration,408,"Fixes #7068 . - When adding AC, AF, AN, DP header lines, SelectVariants now checks if these lines are in the original header already and if so, overwrites these lines with the respective standard lines; - Without this check, an issue in htsjdk causes duplicate header lines with the same ID if the description differs. This should be fixed there but this fix provides a downstream workaround; - Modified the integration test validation files, which have been invalid VCF files with duplicate header lines; - Removed addition of AC, AF, AN if `--set-filtered-gt-to-nocall` is set, because these lines will be added later anyway",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7069
https://github.com/broadinstitute/gatk/pull/7069:425,Security,validat,validation,425,"Fixes #7068 . - When adding AC, AF, AN, DP header lines, SelectVariants now checks if these lines are in the original header already and if so, overwrites these lines with the respective standard lines; - Without this check, an issue in htsjdk causes duplicate header lines with the same ID if the description differs. This should be fixed there but this fix provides a downstream workaround; - Modified the integration test validation files, which have been invalid VCF files with duplicate header lines; - Removed addition of AC, AF, AN if `--set-filtered-gt-to-nocall` is set, because these lines will be added later anyway",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7069
https://github.com/broadinstitute/gatk/pull/7069:420,Testability,test,test,420,"Fixes #7068 . - When adding AC, AF, AN, DP header lines, SelectVariants now checks if these lines are in the original header already and if so, overwrites these lines with the respective standard lines; - Without this check, an issue in htsjdk causes duplicate header lines with the same ID if the description differs. This should be fixed there but this fix provides a downstream workaround; - Modified the integration test validation files, which have been invalid VCF files with duplicate header lines; - Removed addition of AC, AF, AN if `--set-filtered-gt-to-nocall` is set, because these lines will be added later anyway",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7069
https://github.com/broadinstitute/gatk/issues/7070:81,Performance,perform,performance,81,"### Summary; This user was able to access the GenomicsDB workspace but is having performance issues with SelectVariants. They tried the same command locally and it took less than a minute. Are there any changes with how the user is running SelectVariants to improve the performance?. ### GATK Info; GATK 4.1.9.0; . This request was created from a contribution made by Lucas Taniguti on February 01, 2021 22:41 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community\_comment\_360014183291](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community_comment_360014183291). \--. Thank you, it has started to work with gendb.gs://. But now I think it does not run. I have only one sample stored into the database and I'm selecting only chr20:1-1000000 and it is running for more than 30 minutes. Is it expected?. I'm using a VM from GCE, in the same region as the GCS bucket. Using GATK jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ; ; ```; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx10g -Xms5g - ; ; jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R Homo\_sapiens\_assembly38.fasta -V gendb.gs://mybucket/genomicsdb -L chr20:1-1000000 -O teste. ; ; vcf.gz ; ; 23:01:23.595 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compres ; ; sion.so ; ; 23:01:23.914 INFO SelectVariants - ------------------------------------------------------------ ; ; 23:01:23.915 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 23:01:23.915 INFO SelectVariants - For support and documentation go to [https://software.bro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070
https://github.com/broadinstitute/gatk/issues/7070:270,Performance,perform,performance,270,"### Summary; This user was able to access the GenomicsDB workspace but is having performance issues with SelectVariants. They tried the same command locally and it took less than a minute. Are there any changes with how the user is running SelectVariants to improve the performance?. ### GATK Info; GATK 4.1.9.0; . This request was created from a contribution made by Lucas Taniguti on February 01, 2021 22:41 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community\_comment\_360014183291](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community_comment_360014183291). \--. Thank you, it has started to work with gendb.gs://. But now I think it does not run. I have only one sample stored into the database and I'm selecting only chr20:1-1000000 and it is running for more than 30 minutes. Is it expected?. I'm using a VM from GCE, in the same region as the GCS bucket. Using GATK jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ; ; ```; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx10g -Xms5g - ; ; jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R Homo\_sapiens\_assembly38.fasta -V gendb.gs://mybucket/genomicsdb -L chr20:1-1000000 -O teste. ; ; vcf.gz ; ; 23:01:23.595 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compres ; ; sion.so ; ; 23:01:23.914 INFO SelectVariants - ------------------------------------------------------------ ; ; 23:01:23.915 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 23:01:23.915 INFO SelectVariants - For support and documentation go to [https://software.bro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070
https://github.com/broadinstitute/gatk/issues/7070:1565,Performance,Load,Loading,1565,"nt\_360014183291](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community_comment_360014183291). \--. Thank you, it has started to work with gendb.gs://. But now I think it does not run. I have only one sample stored into the database and I'm selecting only chr20:1-1000000 and it is running for more than 30 minutes. Is it expected?. I'm using a VM from GCE, in the same region as the GCS bucket. Using GATK jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ; ; ```; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx10g -Xms5g - ; ; jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R Homo\_sapiens\_assembly38.fasta -V gendb.gs://mybucket/genomicsdb -L chr20:1-1000000 -O teste. ; ; vcf.gz ; ; 23:01:23.595 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compres ; ; sion.so ; ; 23:01:23.914 INFO SelectVariants - ------------------------------------------------------------ ; ; 23:01:23.915 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 23:01:23.915 INFO SelectVariants - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 23:01:23.918 INFO SelectVariants - Executing as taniguti@phasing-shapeit4-taniguti on Linux v5.4.0-1036-gcp amd64 ; ; 23:01:23.918 INFO SelectVariants - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1+1-Ubuntu-0ubuntu1.20.04 ; ; 23:01:23.919 INFO SelectVariants - Start Date/Time: February 1, 2021 at 11:01:23 PM UTC ; ; 23:01:23.919 INFO SelectVariants - ------------------------------------------------------------ ; ; 23:01:23.919 INFO SelectVariants - ------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070
https://github.com/broadinstitute/gatk/issues/7070:35,Security,access,access,35,"### Summary; This user was able to access the GenomicsDB workspace but is having performance issues with SelectVariants. They tried the same command locally and it took less than a minute. Are there any changes with how the user is running SelectVariants to improve the performance?. ### GATK Info; GATK 4.1.9.0; . This request was created from a contribution made by Lucas Taniguti on February 01, 2021 22:41 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community\_comment\_360014183291](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community_comment_360014183291). \--. Thank you, it has started to work with gendb.gs://. But now I think it does not run. I have only one sample stored into the database and I'm selecting only chr20:1-1000000 and it is running for more than 30 minutes. Is it expected?. I'm using a VM from GCE, in the same region as the GCS bucket. Using GATK jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ; ; ```; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx10g -Xms5g - ; ; jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R Homo\_sapiens\_assembly38.fasta -V gendb.gs://mybucket/genomicsdb -L chr20:1-1000000 -O teste. ; ; vcf.gz ; ; 23:01:23.595 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compres ; ; sion.so ; ; 23:01:23.914 INFO SelectVariants - ------------------------------------------------------------ ; ; 23:01:23.915 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 23:01:23.915 INFO SelectVariants - For support and documentation go to [https://software.bro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070
https://github.com/broadinstitute/gatk/issues/7070:1502,Testability,test,teste,1502,"0076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community\_comment\_360014183291](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community_comment_360014183291). \--. Thank you, it has started to work with gendb.gs://. But now I think it does not run. I have only one sample stored into the database and I'm selecting only chr20:1-1000000 and it is running for more than 30 minutes. Is it expected?. I'm using a VM from GCE, in the same region as the GCS bucket. Using GATK jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ; ; ```; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx10g -Xms5g - ; ; jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R Homo\_sapiens\_assembly38.fasta -V gendb.gs://mybucket/genomicsdb -L chr20:1-1000000 -O teste. ; ; vcf.gz ; ; 23:01:23.595 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compres ; ; sion.so ; ; 23:01:23.914 INFO SelectVariants - ------------------------------------------------------------ ; ; 23:01:23.915 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 23:01:23.915 INFO SelectVariants - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 23:01:23.918 INFO SelectVariants - Executing as taniguti@phasing-shapeit4-taniguti on Linux v5.4.0-1036-gcp amd64 ; ; 23:01:23.918 INFO SelectVariants - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1+1-Ubuntu-0ubuntu1.20.04 ; ; 23:01:23.919 INFO SelectVariants - Start Date/Time: February 1, 2021 at 11:01:23 PM UTC ; ; 23:01:23.919 INFO SelectVariants - --------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070
https://github.com/broadinstitute/gatk/issues/7070:3562,Testability,log,logger,3562,------------------------------------------------------ ; ; 23:01:23.928 INFO SelectVariants - HTSJDK Version: 2.23.0 ; ; 23:01:23.929 INFO SelectVariants - Picard Version: 2.23.3 ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 23:01:23.930 INFO SelectVariants - Deflater: IntelDeflater ; ; 23:01:23.930 INFO SelectVariants - Inflater: IntelInflater ; ; 23:01:23.930 INFO SelectVariants - GCS max retries/reopens: 20 ; ; 23:01:23.930 INFO SelectVariants - Requester pays: disabled ; ; 23:01:23.930 INFO SelectVariants - Initializing engine ; ; 23:01:25.939 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63 ; ; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory). ; ; log4j:WARN Please initialize the log4j system properly. ; ; log4j:WARN See [http://logging.apache.org/log4j/1.2/faq.html#noconfig](http://logging.apache.org/log4j/1.2/faq.html#noconfig) for more info. ; ; 23:01:39.847 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field AS\_InbreedingCoeff - the field will NOT be part of INFO fields in the g ; ; enerated VCF records ; ; 23:01:39.847 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field AS\_QD - the field will NOT be part of INFO fields in the generated VCF ; ; records ; ; 23:01:39.848 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field DS - the field will NOT be part of INFO fields in the generated VCF rec ; ; ords ; ; 23:01:39.848 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO fi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070
https://github.com/broadinstitute/gatk/issues/7070:3712,Testability,log,logging,3712,d Version: 2.23.3 ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 23:01:23.930 INFO SelectVariants - Deflater: IntelDeflater ; ; 23:01:23.930 INFO SelectVariants - Inflater: IntelInflater ; ; 23:01:23.930 INFO SelectVariants - GCS max retries/reopens: 20 ; ; 23:01:23.930 INFO SelectVariants - Requester pays: disabled ; ; 23:01:23.930 INFO SelectVariants - Initializing engine ; ; 23:01:25.939 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63 ; ; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory). ; ; log4j:WARN Please initialize the log4j system properly. ; ; log4j:WARN See [http://logging.apache.org/log4j/1.2/faq.html#noconfig](http://logging.apache.org/log4j/1.2/faq.html#noconfig) for more info. ; ; 23:01:39.847 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field AS\_InbreedingCoeff - the field will NOT be part of INFO fields in the g ; ; enerated VCF records ; ; 23:01:39.847 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field AS\_QD - the field will NOT be part of INFO fields in the generated VCF ; ; records ; ; 23:01:39.848 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field DS - the field will NOT be part of INFO fields in the generated VCF rec ; ; ords ; ; 23:01:39.848 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the gene ; ; rated VCF records ; ; 23:01:39.848 info NativeGenomicsDB - pid=4376 tid=4377 No ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070
https://github.com/broadinstitute/gatk/issues/7070:3767,Testability,log,logging,3767,HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 23:01:23.929 INFO SelectVariants - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 23:01:23.930 INFO SelectVariants - Deflater: IntelDeflater ; ; 23:01:23.930 INFO SelectVariants - Inflater: IntelInflater ; ; 23:01:23.930 INFO SelectVariants - GCS max retries/reopens: 20 ; ; 23:01:23.930 INFO SelectVariants - Requester pays: disabled ; ; 23:01:23.930 INFO SelectVariants - Initializing engine ; ; 23:01:25.939 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63 ; ; log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory). ; ; log4j:WARN Please initialize the log4j system properly. ; ; log4j:WARN See [http://logging.apache.org/log4j/1.2/faq.html#noconfig](http://logging.apache.org/log4j/1.2/faq.html#noconfig) for more info. ; ; 23:01:39.847 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field AS\_InbreedingCoeff - the field will NOT be part of INFO fields in the g ; ; enerated VCF records ; ; 23:01:39.847 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field AS\_QD - the field will NOT be part of INFO fields in the generated VCF ; ; records ; ; 23:01:39.848 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field DS - the field will NOT be part of INFO fields in the generated VCF rec ; ; ords ; ; 23:01:39.848 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the gene ; ; rated VCF records ; ; 23:01:39.848 info NativeGenomicsDB - pid=4376 tid=4377 No valid combination operation found for INFO field MLEAC - ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070
https://github.com/broadinstitute/gatk/pull/7072:115,Deployability,pipeline,pipeline,115,"There's a small set of tools that only outputs their results to stdout, making it difficult to use the output in a pipeline/script. This PR adds a way to output simple results from such tools to an (optional) output file. I Added this option to the following tools:; - CountBases; - CountBasesInReference; - CountReads; - CountVariants; - FlagStat. Other tools that might benefit from this (but it will require an API change, so I didn't do it):; - CompareIntervalLists; - ValidateVariants",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7072
https://github.com/broadinstitute/gatk/pull/7072:473,Security,Validat,ValidateVariants,473,"There's a small set of tools that only outputs their results to stdout, making it difficult to use the output in a pipeline/script. This PR adds a way to output simple results from such tools to an (optional) output file. I Added this option to the following tools:; - CountBases; - CountBasesInReference; - CountReads; - CountVariants; - FlagStat. Other tools that might benefit from this (but it will require an API change, so I didn't do it):; - CompareIntervalLists; - ValidateVariants",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7072
https://github.com/broadinstitute/gatk/pull/7072:161,Usability,simpl,simple,161,"There's a small set of tools that only outputs their results to stdout, making it difficult to use the output in a pipeline/script. This PR adds a way to output simple results from such tools to an (optional) output file. I Added this option to the following tools:; - CountBases; - CountBasesInReference; - CountReads; - CountVariants; - FlagStat. Other tools that might benefit from this (but it will require an API change, so I didn't do it):; - CompareIntervalLists; - ValidateVariants",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7072
https://github.com/broadinstitute/gatk/pull/7075:2,Deployability,Update,Update,2,* Update Picard 2.23.0 -> 2.25.0; * Add serialVersionUID to classes now marked as Serializable in picard.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7075
https://github.com/broadinstitute/gatk/issues/7076:223,Availability,error,error,223,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar. ### Description . When run on the 30x 1000 genomes samples, I am getting this error. Not an issue on other crams we have. ```; /restricted/projectnb/genpro/github/gatk/gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:39:56.283 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:39:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:39:56.484 INFO HaplotypeCaller - ------------------------------------------------------------; 14:39:56.484 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.9.0-33-g31df35b-SNAPSHOT; 14:39:56.484 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:39:56.485 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:39:56.485 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:39:56.485 INFO HaplotypeCaller - Start Date/Time: February 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:3042,Availability,down,down,3042,"eCaller - ------------------------------------------------------------; 14:39:56.485 INFO HaplotypeCaller - ------------------------------------------------------------; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; 14:39:56.486 INFO HaplotypeCaller - Picard Version: 2.23.3; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:56.486 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:39:56.486 INFO HaplotypeCaller - Inflater: IntelInflater; 14:39:56.486 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:39:56.487 INFO HaplotypeCaller - Requester pays: disabled; 14:39:56.487 INFO HaplotypeCaller - Initializing engine; 14:39:57.467 INFO HaplotypeCaller - Shutting down engine; [February 10, 2021 2:39:57 PM EST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=1681391616; htsjdk.samtools.cram.CRAMException: Attempt to unmapped with non zero alignment start (0) or span (150); at htsjdk.samtools.cram.BAIEntry.<init>(BAIEntry.java:60); at htsjdk.samtools.cram.BAIEntry.<init>(BAIEntry.java:83); at htsjdk.samtools.cram.CRAIIndex.openCraiFileAsBaiStream(CRAIIndex.java:89); at htsjdk.samtools.SamIndexes.asBaiSeekableStreamOrNull(SamIndexes.java:91); at htsjdk.samtools.CRAMFileReader.initWithStreams(CRAMFileReader.java:202); at htsjdk.samtools.CRAMFileReader.<init>(CRAMFileReader.java:193); at htsjdk.samtools.SamReaderFactory$SamReaderFactoryImpl.open(SamReaderFactory.java:422); at htsjdk.samtools.SamReaderFactory.open(SamReaderFactory.java:105); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.<init>(ReadsPathDataSource.java:245); at org.broadinstitute.hellbender.engi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:5279,Availability,error,error,5279,"5); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; #### Steps to reproduce; `gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:8663,Availability,Avail,Available,8663,"E_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 14:40:58.312 INFO ProgressMeter - chr22:10659064 0.2 35790 208384.3; 14:41:09.992 INFO ProgressMeter - chr22:10687910 0.4 35990 98217.0. ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:8781,Availability,avail,available,8781,"E_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 14:40:58.312 INFO ProgressMeter - chr22:10659064 0.2 35790 208384.3; 14:41:09.992 INFO ProgressMeter - chr22:10687910 0.4 35990 98217.0. ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:5487,Deployability,install,install,5487,"llbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; #### Steps to reproduce; `gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:5732,Deployability,install,install,5732,"neProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; #### Steps to reproduce; `gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:45.788 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:40:45.788 INFO Haplotyp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:6032,Deployability,install,install,6032,"96.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:45.788 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:40:45.788 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:45.789 INFO HaplotypeCaller - Start Date/Time: February 10, 2021 2:40:45 PM EST; 14:40:45.789 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.789 INFO HaplotypeCaller - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:8264,Deployability,install,install,8264,"MPRESSION_LEVEL : 2; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 14:40:58.312 INFO ProgressMeter - chr22:10659064 0.2 35790 208384.3; 14:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:8456,Deployability,install,install,8456,"E_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 14:40:58.312 INFO ProgressMeter - chr22:10659064 0.2 35790 208384.3; 14:41:09.992 INFO ProgressMeter - chr22:10687910 0.4 35990 98217.0. ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:1033,Performance,Load,Loading,1033,"l(s) or class(es); HaplotypeCaller. ### Affected version(s); gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar. ### Description . When run on the 30x 1000 genomes samples, I am getting this error. Not an issue on other crams we have. ```; /restricted/projectnb/genpro/github/gatk/gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:39:56.283 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:39:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:39:56.484 INFO HaplotypeCaller - ------------------------------------------------------------; 14:39:56.484 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.9.0-33-g31df35b-SNAPSHOT; 14:39:56.484 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:39:56.485 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:39:56.485 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:39:56.485 INFO HaplotypeCaller - Start Date/Time: February 10, 2021 2:39:56 PM EST; 14:39:5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:5962,Performance,Load,Loading,5962,"stricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:45.788 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:40:45.788 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:45.789 INFO HaplotypeCaller - Start Date/Time: February 10, 2021 2:40:45 PM EST; 14:40:45.789 INFO HaplotypeCaller - ------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:8200,Performance,Load,Loading,8200," - Picard Version: 2.21.9; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:8386,Performance,Load,Loading,8386,"E_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 14:40:58.312 INFO ProgressMeter - chr22:10659064 0.2 35790 208384.3; 14:41:09.992 INFO ProgressMeter - chr22:10687910 0.4 35990 98217.0. ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:8867,Performance,multi-thread,multi-threaded,8867,"E_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 14:40:58.312 INFO ProgressMeter - chr22:10659064 0.2 35790 208384.3; 14:41:09.992 INFO ProgressMeter - chr22:10687910 0.4 35990 98217.0. ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:1346,Safety,detect,detect,1346,"2 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:39:56.283 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:39:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:39:56.484 INFO HaplotypeCaller - ------------------------------------------------------------; 14:39:56.484 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.9.0-33-g31df35b-SNAPSHOT; 14:39:56.484 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:39:56.485 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:39:56.485 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:39:56.485 INFO HaplotypeCaller - Start Date/Time: February 10, 2021 2:39:56 PM EST; 14:39:56.485 INFO HaplotypeCaller - ------------------------------------------------------------; 14:39:56.485 INFO HaplotypeCaller - ------------------------------------------------------------; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; 14:39:56.486 INFO HaplotypeCaller - Picard Version: 2.23.3; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:6249,Safety,detect,detect,6249,"rsion v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:45.788 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:40:45.788 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:45.789 INFO HaplotypeCaller - Start Date/Time: February 10, 2021 2:40:45 PM EST; 14:40:45.789 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.789 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Version: 2.21.2; 14:40:45.791 INFO HaplotypeCaller - Picard Version: 2.21.9; 14:40:45.791 INFO Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:450,Testability,test,test,450,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar. ### Description . When run on the 30x 1000 genomes samples, I am getting this error. Not an issue on other crams we have. ```; /restricted/projectnb/genpro/github/gatk/gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:39:56.283 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:39:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:39:56.484 INFO HaplotypeCaller - ------------------------------------------------------------; 14:39:56.484 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.9.0-33-g31df35b-SNAPSHOT; 14:39:56.484 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:39:56.485 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:39:56.485 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:39:56.485 INFO HaplotypeCaller - Start Date/Time: February 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:978,Testability,test,test,978,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar. ### Description . When run on the 30x 1000 genomes samples, I am getting this error. Not an issue on other crams we have. ```; /restricted/projectnb/genpro/github/gatk/gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:39:56.283 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rprojectnb2/genpro/github/gatk/build/libs/gatk-package-4.1.9.0-33-g31df35b-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:39:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:39:56.484 INFO HaplotypeCaller - ------------------------------------------------------------; 14:39:56.484 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.9.0-33-g31df35b-SNAPSHOT; 14:39:56.484 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:39:56.485 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:39:56.485 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:39:56.485 INFO HaplotypeCaller - Start Date/Time: February 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:5061,Testability,test,test,5061,"athDataSource.<init>(ReadsPathDataSource.java:181); at org.broadinstitute.hellbender.engine.GATKTool.initializeReads(GATKTool.java:456); at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:705); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; #### Steps to reproduce; `gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:5431,Testability,test,test,5431,"Program.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; #### Steps to reproduce; `gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO Haplotyp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7076:5907,Testability,test,test,5907,"eps to reproduce; `gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:45.788 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:40:45.788 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:45.789 INFO HaplotypeCaller - Start Date/Time: February 10, 2021 2:40:45 PM EST; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076
https://github.com/broadinstitute/gatk/issues/7077:169,Availability,error,errors,169,"If I clone GATK with the ssh URL (`git@github.com:broadinstitute/gatk.git`), and then run a `docker build` command from the root of that clone, I get ssh authentication errors at the `git lfs pull` step:. ```; Step 9/36 : RUN git lfs pull; ---> Running in 1f415556efd2; Git LFS: (0 of 104 files) 0 B / 1.28 GB ; batch request: Host key verification failed.: exit status 255; batch request: Host key verification failed.: exit status 255; error: failed to fetch some objects from 'https://github.com/broadinstitute/gatk.git/info/lfs'; The command '/bin/sh -c git lfs pull' returned a non-zero code: 2; ```. If I do the same thing from a GATK clone created using the https URL (`https://github.com/broadinstitute/gatk.git`), I get no lfs error. This also raises the larger question of whether we are authenticating with github before doing `git lfs pull` during the docker build, as I believe that the quotas for unauthenticated `git lfs` operations are much smaller than for authenticated operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7077
https://github.com/broadinstitute/gatk/issues/7077:438,Availability,error,error,438,"If I clone GATK with the ssh URL (`git@github.com:broadinstitute/gatk.git`), and then run a `docker build` command from the root of that clone, I get ssh authentication errors at the `git lfs pull` step:. ```; Step 9/36 : RUN git lfs pull; ---> Running in 1f415556efd2; Git LFS: (0 of 104 files) 0 B / 1.28 GB ; batch request: Host key verification failed.: exit status 255; batch request: Host key verification failed.: exit status 255; error: failed to fetch some objects from 'https://github.com/broadinstitute/gatk.git/info/lfs'; The command '/bin/sh -c git lfs pull' returned a non-zero code: 2; ```. If I do the same thing from a GATK clone created using the https URL (`https://github.com/broadinstitute/gatk.git`), I get no lfs error. This also raises the larger question of whether we are authenticating with github before doing `git lfs pull` during the docker build, as I believe that the quotas for unauthenticated `git lfs` operations are much smaller than for authenticated operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7077
https://github.com/broadinstitute/gatk/issues/7077:736,Availability,error,error,736,"If I clone GATK with the ssh URL (`git@github.com:broadinstitute/gatk.git`), and then run a `docker build` command from the root of that clone, I get ssh authentication errors at the `git lfs pull` step:. ```; Step 9/36 : RUN git lfs pull; ---> Running in 1f415556efd2; Git LFS: (0 of 104 files) 0 B / 1.28 GB ; batch request: Host key verification failed.: exit status 255; batch request: Host key verification failed.: exit status 255; error: failed to fetch some objects from 'https://github.com/broadinstitute/gatk.git/info/lfs'; The command '/bin/sh -c git lfs pull' returned a non-zero code: 2; ```. If I do the same thing from a GATK clone created using the https URL (`https://github.com/broadinstitute/gatk.git`), I get no lfs error. This also raises the larger question of whether we are authenticating with github before doing `git lfs pull` during the docker build, as I believe that the quotas for unauthenticated `git lfs` operations are much smaller than for authenticated operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7077
https://github.com/broadinstitute/gatk/issues/7077:154,Security,authenticat,authentication,154,"If I clone GATK with the ssh URL (`git@github.com:broadinstitute/gatk.git`), and then run a `docker build` command from the root of that clone, I get ssh authentication errors at the `git lfs pull` step:. ```; Step 9/36 : RUN git lfs pull; ---> Running in 1f415556efd2; Git LFS: (0 of 104 files) 0 B / 1.28 GB ; batch request: Host key verification failed.: exit status 255; batch request: Host key verification failed.: exit status 255; error: failed to fetch some objects from 'https://github.com/broadinstitute/gatk.git/info/lfs'; The command '/bin/sh -c git lfs pull' returned a non-zero code: 2; ```. If I do the same thing from a GATK clone created using the https URL (`https://github.com/broadinstitute/gatk.git`), I get no lfs error. This also raises the larger question of whether we are authenticating with github before doing `git lfs pull` during the docker build, as I believe that the quotas for unauthenticated `git lfs` operations are much smaller than for authenticated operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7077
https://github.com/broadinstitute/gatk/issues/7077:798,Security,authenticat,authenticating,798,"If I clone GATK with the ssh URL (`git@github.com:broadinstitute/gatk.git`), and then run a `docker build` command from the root of that clone, I get ssh authentication errors at the `git lfs pull` step:. ```; Step 9/36 : RUN git lfs pull; ---> Running in 1f415556efd2; Git LFS: (0 of 104 files) 0 B / 1.28 GB ; batch request: Host key verification failed.: exit status 255; batch request: Host key verification failed.: exit status 255; error: failed to fetch some objects from 'https://github.com/broadinstitute/gatk.git/info/lfs'; The command '/bin/sh -c git lfs pull' returned a non-zero code: 2; ```. If I do the same thing from a GATK clone created using the https URL (`https://github.com/broadinstitute/gatk.git`), I get no lfs error. This also raises the larger question of whether we are authenticating with github before doing `git lfs pull` during the docker build, as I believe that the quotas for unauthenticated `git lfs` operations are much smaller than for authenticated operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7077
https://github.com/broadinstitute/gatk/issues/7077:974,Security,authenticat,authenticated,974,"If I clone GATK with the ssh URL (`git@github.com:broadinstitute/gatk.git`), and then run a `docker build` command from the root of that clone, I get ssh authentication errors at the `git lfs pull` step:. ```; Step 9/36 : RUN git lfs pull; ---> Running in 1f415556efd2; Git LFS: (0 of 104 files) 0 B / 1.28 GB ; batch request: Host key verification failed.: exit status 255; batch request: Host key verification failed.: exit status 255; error: failed to fetch some objects from 'https://github.com/broadinstitute/gatk.git/info/lfs'; The command '/bin/sh -c git lfs pull' returned a non-zero code: 2; ```. If I do the same thing from a GATK clone created using the https URL (`https://github.com/broadinstitute/gatk.git`), I get no lfs error. This also raises the larger question of whether we are authenticating with github before doing `git lfs pull` during the docker build, as I believe that the quotas for unauthenticated `git lfs` operations are much smaller than for authenticated operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7077
https://github.com/broadinstitute/gatk/issues/7080:454,Availability,error,error,454,"### Summary ; A user wrote in to the forum regarding running FilterSamReads through GATK and the output bam file has formatting issues. After they sent in a bug report, I found that the exit code is getting written to the bam file, causing this issue. . This request was created from a contribution made by rcorbett on February 03, 2021 23:47 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076905711-filterSamReads-stdout-format-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076905711-filterSamReads-stdout-format-error). #### GATK Info; FilterSamReads 4.1.9.0 and 4.0.10.0; Command to stdout:; `gatk FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET > test_stdout.bam`; Log:; ```; Using GATK jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE??r#U???/Qm'dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080
https://github.com/broadinstitute/gatk/issues/7080:560,Availability,error,error,560,"### Summary ; A user wrote in to the forum regarding running FilterSamReads through GATK and the output bam file has formatting issues. After they sent in a bug report, I found that the exit code is getting written to the bam file, causing this issue. . This request was created from a contribution made by rcorbett on February 03, 2021 23:47 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076905711-filterSamReads-stdout-format-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076905711-filterSamReads-stdout-format-error). #### GATK Info; FilterSamReads 4.1.9.0 and 4.0.10.0; Command to stdout:; `gatk FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET > test_stdout.bam`; Log:; ```; Using GATK jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE??r#U???/Qm'dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080
https://github.com/broadinstitute/gatk/issues/7080:2102,Availability,down,downloads,2102,"ar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE??r#U???/Qm'dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```. It looks like this issue has been discussed at https://github.com/broadinstitute/gatk/issues/4433 and https://github.com/broadinstitute/gatk/issues/4329 but this issue seems to still exist in GATK 4.1.9.0. Please let me know if you want any of these test files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080
https://github.com/broadinstitute/gatk/issues/7080:2249,Availability,down,downloads,2249,"ar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE??r#U???/Qm'dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```. It looks like this issue has been discussed at https://github.com/broadinstitute/gatk/issues/4433 and https://github.com/broadinstitute/gatk/issues/4329 but this issue seems to still exist in GATK 4.1.9.0. Please let me know if you want any of these test files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080
https://github.com/broadinstitute/gatk/issues/7080:2396,Availability,down,downloads,2396,"ar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE??r#U???/Qm'dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```. It looks like this issue has been discussed at https://github.com/broadinstitute/gatk/issues/4433 and https://github.com/broadinstitute/gatk/issues/4329 but this issue seems to still exist in GATK 4.1.9.0. Please let me know if you want any of these test files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080
https://github.com/broadinstitute/gatk/issues/7080:2543,Availability,down,downloads,2543,"ar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE??r#U???/Qm'dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```. It looks like this issue has been discussed at https://github.com/broadinstitute/gatk/issues/4433 and https://github.com/broadinstitute/gatk/issues/4329 but this issue seems to still exist in GATK 4.1.9.0. Please let me know if you want any of these test files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080
https://github.com/broadinstitute/gatk/issues/7080:1290,Performance,Load,Loading,1290,"ntribution made by rcorbett on February 03, 2021 23:47 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076905711-filterSamReads-stdout-format-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076905711-filterSamReads-stdout-format-error). #### GATK Info; FilterSamReads 4.1.9.0 and 4.0.10.0; Command to stdout:; `gatk FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET > test_stdout.bam`; Log:; ```; Using GATK jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE??r#U???/Qm'dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080
https://github.com/broadinstitute/gatk/issues/7080:811,Testability,Log,Log,811,"### Summary ; A user wrote in to the forum regarding running FilterSamReads through GATK and the output bam file has formatting issues. After they sent in a bug report, I found that the exit code is getting written to the bam file, causing this issue. . This request was created from a contribution made by rcorbett on February 03, 2021 23:47 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076905711-filterSamReads-stdout-format-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076905711-filterSamReads-stdout-format-error). #### GATK Info; FilterSamReads 4.1.9.0 and 4.0.10.0; Command to stdout:; `gatk FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET > test_stdout.bam`; Log:; ```; Using GATK jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE??r#U???/Qm'dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080
https://github.com/broadinstitute/gatk/issues/7080:2869,Testability,test,test,2869,"ar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE??r#U???/Qm'dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```. It looks like this issue has been discussed at https://github.com/broadinstitute/gatk/issues/4433 and https://github.com/broadinstitute/gatk/issues/4329 but this issue seems to still exist in GATK 4.1.9.0. Please let me know if you want any of these test files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080
https://github.com/broadinstitute/gatk/issues/7081:848,Availability,error,error,848,"## Bug Report. ### Affected tool(s) or class(es); gatk GenotypeGVCFs. ### Affected version(s); - [X] Latest public release version [GATK 4.1.9.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Files generated by 'gatk GenotypeGVCFs' with french locale in February (Fvrier in french) August (Aot) or December (Dcembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081
https://github.com/broadinstitute/gatk/issues/7081:115,Deployability,release,release,115,"## Bug Report. ### Affected tool(s) or class(es); gatk GenotypeGVCFs. ### Affected version(s); - [X] Latest public release version [GATK 4.1.9.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Files generated by 'gatk GenotypeGVCFs' with french locale in February (Fvrier in french) August (Aot) or December (Dcembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081
https://github.com/broadinstitute/gatk/issues/7081:1483,Deployability,update,updates,1483," (Fvrier in french) August (Aot) or December (Dcembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --disable-tool-default-annotations false --enable-all-annotations false --allow-old-rms-mapping-quality-annotation-data false"",Version=""4.1.9.0"",Date=""5 f<E9>vrier 2021 10:42:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081
https://github.com/broadinstitute/gatk/issues/7081:1246,Performance,optimiz,optimizations,1246," (Fvrier in french) August (Aot) or December (Dcembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --disable-tool-default-annotations false --enable-all-annotations false --allow-old-rms-mapping-quality-annotation-data false"",Version=""4.1.9.0"",Date=""5 f<E9>vrier 2021 10:42:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081
https://github.com/broadinstitute/gatk/issues/7081:1427,Security,validat,validation-stringency,1427," (Fvrier in french) August (Aot) or December (Dcembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --disable-tool-default-annotations false --enable-all-annotations false --allow-old-rms-mapping-quality-annotation-data false"",Version=""4.1.9.0"",Date=""5 f<E9>vrier 2021 10:42:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081
https://github.com/broadinstitute/gatk/issues/7081:1526,Security,validat,validation,1526,"vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --disable-tool-default-annotations false --enable-all-annotations false --allow-old-rms-mapping-quality-annotation-data false"",Version=""4.1.9.0"",Date=""5 f<E9>vrier 2021 10:42:27 CET""`. Please note the `f<E9>vrier` at the end when the file is read with `less` on an UTF-8 system. #### Steps to reproduce. Run `gatk GenotypeGVCFs` with a french locale when the `date` command produces a line with an accent (in February, August or December). #### Expected behavior; The output file's encoding should be UTF-8. #### Actual behavior; The output file's encoding is ISO-8859-1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081
https://github.com/broadinstitute/gatk/issues/7081:189,Testability,test,test,189,"## Bug Report. ### Affected tool(s) or class(es); gatk GenotypeGVCFs. ### Affected version(s); - [X] Latest public release version [GATK 4.1.9.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Files generated by 'gatk GenotypeGVCFs' with french locale in February (Fvrier in french) August (Aot) or December (Dcembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081
https://github.com/broadinstitute/gatk/pull/7082:105,Deployability,Update,Updated,105,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7082
https://github.com/broadinstitute/gatk/pull/7082:217,Deployability,Update,Updated,217,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7082
https://github.com/broadinstitute/gatk/pull/7082:205,Testability,Test,Tested,205,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7082
https://github.com/broadinstitute/gatk/pull/7082:257,Testability,test,testQueryWithEmptyDatasetStorageAPI,257,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7082
https://github.com/broadinstitute/gatk/pull/7082:307,Testability,test,test,307,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7082
https://github.com/broadinstitute/gatk/pull/7083:105,Deployability,Update,Updated,105,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7083
https://github.com/broadinstitute/gatk/pull/7083:217,Deployability,Update,Updated,217,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7083
https://github.com/broadinstitute/gatk/pull/7083:205,Testability,Test,Tested,205,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7083
https://github.com/broadinstitute/gatk/pull/7083:257,Testability,test,testQueryWithEmptyDatasetStorageAPI,257,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7083
https://github.com/broadinstitute/gatk/pull/7083:307,Testability,test,test,307,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7083
https://github.com/broadinstitute/gatk/pull/7084:105,Deployability,Update,Updated,105,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7084
https://github.com/broadinstitute/gatk/pull/7084:217,Deployability,Update,Updated,217,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7084
https://github.com/broadinstitute/gatk/pull/7084:205,Testability,Test,Tested,205,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7084
https://github.com/broadinstitute/gatk/pull/7084:257,Testability,test,testQueryWithEmptyDatasetStorageAPI,257,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7084
https://github.com/broadinstitute/gatk/pull/7084:307,Testability,test,test,307,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7084
https://github.com/broadinstitute/gatk/issues/7085:278,Availability,recover,recover,278,"## Bug Report. ### Affected tool(s) or class(es); Mutect2, HaplotypeCaller; ./gatk Mutect2 -I scripts/microbial/mtb/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:927,Availability,recover,recoverDanglingHead,927,"## Bug Report. ### Affected tool(s) or class(es); Mutect2, HaplotypeCaller; ./gatk Mutect2 -I scripts/microbial/mtb/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:1090,Availability,recover,recoverDanglingHeads,1090,b/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:3139,Availability,error,error,3139,"bender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:233); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). #### Steps to reproduce; Get gs bucket with files from @bhanugandham ; To get error, do not use interval list (i.e. run the command as shown above), but we were able to avoid the error by using this interval list:; @HD VN:1.6; @SQ SN:gi|395136682|gb|CP003248.1| LN:4411708 M5:26f1f5c8a8a8c6e33c79d3fc6d40373e UR:file:/Users/bgandham/variant/mtb/Mycobacterium_tuberculosis_H37Rv.fasta; gi|395136682|gb|CP003248.1| 2074300 2074800 + target_1. #### Expected behavior; The tool should produce a vcf. #### Actual behavior; Fails with an ArrayIndexOutOfBounds Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:3240,Availability,error,error,3240,"bender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:233); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). #### Steps to reproduce; Get gs bucket with files from @bhanugandham ; To get error, do not use interval list (i.e. run the command as shown above), but we were able to avoid the error by using this interval list:; @HD VN:1.6; @SQ SN:gi|395136682|gb|CP003248.1| LN:4411708 M5:26f1f5c8a8a8c6e33c79d3fc6d40373e UR:file:/Users/bgandham/variant/mtb/Mycobacterium_tuberculosis_H37Rv.fasta; gi|395136682|gb|CP003248.1| 2074300 2074800 + target_1. #### Expected behavior; The tool should produce a vcf. #### Actual behavior; Fails with an ArrayIndexOutOfBounds Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:3615,Availability,Error,Error,3615,"bender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:233); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). #### Steps to reproduce; Get gs bucket with files from @bhanugandham ; To get error, do not use interval list (i.e. run the command as shown above), but we were able to avoid the error by using this interval list:; @HD VN:1.6; @SQ SN:gi|395136682|gb|CP003248.1| LN:4411708 M5:26f1f5c8a8a8c6e33c79d3fc6d40373e UR:file:/Users/bgandham/variant/mtb/Mycobacterium_tuberculosis_H37Rv.fasta; gi|395136682|gb|CP003248.1| 2074300 2074800 + target_1. #### Expected behavior; The tool should produce a vcf. #### Actual behavior; Fails with an ArrayIndexOutOfBounds Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:588,Modifiability,extend,extendDanglingPathAgainstReference,588,"## Bug Report. ### Affected tool(s) or class(es); Mutect2, HaplotypeCaller; ./gatk Mutect2 -I scripts/microbial/mtb/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:278,Safety,recover,recover,278,"## Bug Report. ### Affected tool(s) or class(es); Mutect2, HaplotypeCaller; ./gatk Mutect2 -I scripts/microbial/mtb/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:927,Safety,recover,recoverDanglingHead,927,"## Bug Report. ### Affected tool(s) or class(es); Mutect2, HaplotypeCaller; ./gatk Mutect2 -I scripts/microbial/mtb/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:1090,Safety,recover,recoverDanglingHeads,1090,b/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:3230,Safety,avoid,avoid,3230,"bender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:233); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). #### Steps to reproduce; Get gs bucket with files from @bhanugandham ; To get error, do not use interval list (i.e. run the command as shown above), but we were able to avoid the error by using this interval list:; @HD VN:1.6; @SQ SN:gi|395136682|gb|CP003248.1| LN:4411708 M5:26f1f5c8a8a8c6e33c79d3fc6d40373e UR:file:/Users/bgandham/variant/mtb/Mycobacterium_tuberculosis_H37Rv.fasta; gi|395136682|gb|CP003248.1| 2074300 2074800 + target_1. #### Expected behavior; The tool should produce a vcf. #### Actual behavior; Fails with an ArrayIndexOutOfBounds Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/issues/7085:229,Testability,test,test,229,"## Bug Report. ### Affected tool(s) or class(es); Mutect2, HaplotypeCaller; ./gatk Mutect2 -I scripts/microbial/mtb/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085
https://github.com/broadinstitute/gatk/pull/7086:71,Testability,test,tests,71,It looks like the last rebase in the previous PR broke some things and tests were insufficient to catch them. I have added checks to the tests as to the actual contents of the paths as well as some more extreme tests. . Fixes #7085,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7086
https://github.com/broadinstitute/gatk/pull/7086:137,Testability,test,tests,137,It looks like the last rebase in the previous PR broke some things and tests were insufficient to catch them. I have added checks to the tests as to the actual contents of the paths as well as some more extreme tests. . Fixes #7085,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7086
https://github.com/broadinstitute/gatk/pull/7086:211,Testability,test,tests,211,It looks like the last rebase in the previous PR broke some things and tests were insufficient to catch them. I have added checks to the tests as to the actual contents of the paths as well as some more extreme tests. . Fixes #7085,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7086
https://github.com/broadinstitute/gatk/issues/7089:1235,Availability,down,downloadable,1235,"@nalinigans @droazen @mlathara I posted a while ago about an odd difference between data aggregated using CombineGVCFs and data aggregated using GenomicsDBImport. We created a minimal repro case to illustrate it. The basic idea is this:. - Run either CombineGVCFs or GenomicsDBImport on a set of gVCFs; - Run SelectVariants on the output. The data processed using GenomicsDBImport will contain a bunch of lines like these, with ambiguous REF alleles:. 19	75166	.	N	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. the data processed through CombineGVCFs shows the proper REF:. 19	75166	.	G	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. I dont fully understand the data format in a GenomicsDB Workspace, but it would suggest either a) it's storing the wrong REF, or b) the codec is somehow not properly re-adding the REF at read time. Here is a public, downloadable set of repro data. It's large mostly b/c it has our reference genome. The included bash script takes just a few minutes to run, should you want to repro the whole thing. I bundled our actual GATK JAR as well. You can view/download through the browser here:. https://prime-seq.ohsu.edu/project/Labs/Bimber/Collaborations/GATK/begin.view?. or download on the command line:. wget https://prime-seq.ohsu.edu/_webdav/Labs/Bimber/Collaborations/GATK/%40files/ambigRefRepro.tar.gz. Please let me know if there's anything we can check locally. We have a test case set up in intellij if there's something useful we could try through there. . These incorrect REF alleles become a problem downstream when genotyping. Thanks for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7089
https://github.com/broadinstitute/gatk/issues/7089:1470,Availability,down,download,1470,"@nalinigans @droazen @mlathara I posted a while ago about an odd difference between data aggregated using CombineGVCFs and data aggregated using GenomicsDBImport. We created a minimal repro case to illustrate it. The basic idea is this:. - Run either CombineGVCFs or GenomicsDBImport on a set of gVCFs; - Run SelectVariants on the output. The data processed using GenomicsDBImport will contain a bunch of lines like these, with ambiguous REF alleles:. 19	75166	.	N	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. the data processed through CombineGVCFs shows the proper REF:. 19	75166	.	G	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. I dont fully understand the data format in a GenomicsDB Workspace, but it would suggest either a) it's storing the wrong REF, or b) the codec is somehow not properly re-adding the REF at read time. Here is a public, downloadable set of repro data. It's large mostly b/c it has our reference genome. The included bash script takes just a few minutes to run, should you want to repro the whole thing. I bundled our actual GATK JAR as well. You can view/download through the browser here:. https://prime-seq.ohsu.edu/project/Labs/Bimber/Collaborations/GATK/begin.view?. or download on the command line:. wget https://prime-seq.ohsu.edu/_webdav/Labs/Bimber/Collaborations/GATK/%40files/ambigRefRepro.tar.gz. Please let me know if there's anything we can check locally. We have a test case set up in intellij if there's something useful we could try through there. . These incorrect REF alleles become a problem downstream when genotyping. Thanks for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7089
https://github.com/broadinstitute/gatk/issues/7089:1589,Availability,down,download,1589,"@nalinigans @droazen @mlathara I posted a while ago about an odd difference between data aggregated using CombineGVCFs and data aggregated using GenomicsDBImport. We created a minimal repro case to illustrate it. The basic idea is this:. - Run either CombineGVCFs or GenomicsDBImport on a set of gVCFs; - Run SelectVariants on the output. The data processed using GenomicsDBImport will contain a bunch of lines like these, with ambiguous REF alleles:. 19	75166	.	N	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. the data processed through CombineGVCFs shows the proper REF:. 19	75166	.	G	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. I dont fully understand the data format in a GenomicsDB Workspace, but it would suggest either a) it's storing the wrong REF, or b) the codec is somehow not properly re-adding the REF at read time. Here is a public, downloadable set of repro data. It's large mostly b/c it has our reference genome. The included bash script takes just a few minutes to run, should you want to repro the whole thing. I bundled our actual GATK JAR as well. You can view/download through the browser here:. https://prime-seq.ohsu.edu/project/Labs/Bimber/Collaborations/GATK/begin.view?. or download on the command line:. wget https://prime-seq.ohsu.edu/_webdav/Labs/Bimber/Collaborations/GATK/%40files/ambigRefRepro.tar.gz. Please let me know if there's anything we can check locally. We have a test case set up in intellij if there's something useful we could try through there. . These incorrect REF alleles become a problem downstream when genotyping. Thanks for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7089
https://github.com/broadinstitute/gatk/issues/7089:1926,Availability,down,downstream,1926,"@nalinigans @droazen @mlathara I posted a while ago about an odd difference between data aggregated using CombineGVCFs and data aggregated using GenomicsDBImport. We created a minimal repro case to illustrate it. The basic idea is this:. - Run either CombineGVCFs or GenomicsDBImport on a set of gVCFs; - Run SelectVariants on the output. The data processed using GenomicsDBImport will contain a bunch of lines like these, with ambiguous REF alleles:. 19	75166	.	N	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. the data processed through CombineGVCFs shows the proper REF:. 19	75166	.	G	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. I dont fully understand the data format in a GenomicsDB Workspace, but it would suggest either a) it's storing the wrong REF, or b) the codec is somehow not properly re-adding the REF at read time. Here is a public, downloadable set of repro data. It's large mostly b/c it has our reference genome. The included bash script takes just a few minutes to run, should you want to repro the whole thing. I bundled our actual GATK JAR as well. You can view/download through the browser here:. https://prime-seq.ohsu.edu/project/Labs/Bimber/Collaborations/GATK/begin.view?. or download on the command line:. wget https://prime-seq.ohsu.edu/_webdav/Labs/Bimber/Collaborations/GATK/%40files/ambigRefRepro.tar.gz. Please let me know if there's anything we can check locally. We have a test case set up in intellij if there's something useful we could try through there. . These incorrect REF alleles become a problem downstream when genotyping. Thanks for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7089
https://github.com/broadinstitute/gatk/issues/7089:1794,Testability,test,test,1794,"@nalinigans @droazen @mlathara I posted a while ago about an odd difference between data aggregated using CombineGVCFs and data aggregated using GenomicsDBImport. We created a minimal repro case to illustrate it. The basic idea is this:. - Run either CombineGVCFs or GenomicsDBImport on a set of gVCFs; - Run SelectVariants on the output. The data processed using GenomicsDBImport will contain a bunch of lines like these, with ambiguous REF alleles:. 19	75166	.	N	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. the data processed through CombineGVCFs shows the proper REF:. 19	75166	.	G	*,<NON_REF>	.	.	DP=3	GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:PS:SB	./.:.:2:6:2:.:.:0,6,57,6,57,57	.|.:0,1,0:1:3:.:0|1:75165_AGCGGAGGGGAGCGGCGCGGAGGGGCGCGGAGGGGC_A:45,3,0,45,3,45:75165:0,0,1,0	./.:.:0:0:0:.:.:0,0,0,0,0,0	./.:.:0:0:0:.:.:0,0,0,0,0,0. I dont fully understand the data format in a GenomicsDB Workspace, but it would suggest either a) it's storing the wrong REF, or b) the codec is somehow not properly re-adding the REF at read time. Here is a public, downloadable set of repro data. It's large mostly b/c it has our reference genome. The included bash script takes just a few minutes to run, should you want to repro the whole thing. I bundled our actual GATK JAR as well. You can view/download through the browser here:. https://prime-seq.ohsu.edu/project/Labs/Bimber/Collaborations/GATK/begin.view?. or download on the command line:. wget https://prime-seq.ohsu.edu/_webdav/Labs/Bimber/Collaborations/GATK/%40files/ambigRefRepro.tar.gz. Please let me know if there's anything we can check locally. We have a test case set up in intellij if there's something useful we could try through there. . These incorrect REF alleles become a problem downstream when genotyping. Thanks for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7089
https://github.com/broadinstitute/gatk/issues/7090:6683,Availability,error,errors,6683,"DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.652 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.663 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.annotation.REORDERED.gtf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.663 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; 06:42:41.665 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.665 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.666 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.691 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.pc_transcripts.fa -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.pc_transcripts.fa; 06:42:46.805 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:7150,Availability,error,errors,7150,"riants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.663 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.annotation.REORDERED.gtf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.663 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; 06:42:41.665 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.665 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.666 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.691 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.pc_transcripts.fa -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.pc_transcripts.fa; 06:42:46.805 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.805 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 06:42:46.807 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.951 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///dat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:9362,Availability,down,down,9362,ce file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:47.107 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_rec/hg38/acmg59_test_cleaned.txt; 06:42:47.109 INFO Funcotator - Initializing Funcotator Engine...; 06:42:47.139 INFO Funcotator - Creating a MAF file for output: file:/data/nws/WES/GenomicsDBImport/200923_A00268_0517_AHKL37DSXY/Set20-5_L2_159A59.somatic.filterMutectCalls.funcotator.maf; 06:42:47.186 INFO ProgressMeter - Starting traversal; 06:42:47.187 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 06:42:47.233 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; 06:42:47.233 INFO VcfFuncotationFactory - LMMKnown 20180618 cache hits/total: 0/0; 06:42:47.287 INFO Funcotator - Shutting down engine; [2021221 064247] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.62 minutes.; Runtime.totalMemory()=1988100096; java.lang.IllegalArgumentException: Unexpected value: lncRNA; 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1016); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature.<init>(GencodeGtfFeature.java:144); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:729); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature.create(GencodeGtfFeature.java:299); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfCodec.decod,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:12347,Energy Efficiency,Reduce,ReduceOps,12347,:304); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.getFeaturesFromFeatureContext(DataSourceFuncotationFactory.java:219); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:197); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:172); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForVariant$0(FuncotatorEngine.java:147); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:157); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:903); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:857); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:12357,Energy Efficiency,Reduce,ReduceOp,12357,:304); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.getFeaturesFromFeatureContext(DataSourceFuncotationFactory.java:219); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:197); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:172); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForVariant$0(FuncotatorEngine.java:147); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:157); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:903); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:857); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:12385,Energy Efficiency,Reduce,ReduceOps,12385,itute.hellbender.tools.funcotator.DataSourceFuncotationFactory.getFeaturesFromFeatureContext(DataSourceFuncotationFactory.java:219); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:197); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:172); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForVariant$0(FuncotatorEngine.java:147); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:157); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:903); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:857); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:12282,Integrability,wrap,wrapAndCopyInto,12282,aturesFromFeatureContext(DataSourceFuncotationFactory.java:304); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.getFeaturesFromFeatureContext(DataSourceFuncotationFactory.java:219); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:197); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:172); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForVariant$0(FuncotatorEngine.java:147); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:157); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:903); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:857); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:13556,Integrability,wrap,wrapAndCopyInto,13556,stitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:157); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:903); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:857); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); 	at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:774,Performance,Load,Loading,774,Using GATK jar /usr/local/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/local/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar Funcotator -R /data/nws/WES/reference/Homo_sapiens_assembly38.fasta -V /data/nws/WES/GenomicsDBImport/200923_A00268_0517_AHKL37DSXY/Set20-5_L2_159A59.somatic.filterMutectCalls.vcf.gz -O /data/nws/WES/GenomicsDBImport/200923_A00268_0517_AHKL37DSXY/Set20-5_L2_159A59.somatic.filterMutectCalls.funcotator.maf --data-sources-path /data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g --output-file-format MAF --ref-version hg38; 06:42:10.395 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:42:25.662 INFO Funcotator - ------------------------------------------------------------; 06:42:25.662 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.3.0; 06:42:25.663 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:42:40.672 INFO Funcotator - Executing as nws@cuckoolab on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 06:42:40.672 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_262-b10; 06:42:40.672 INFO Funcotator - Start Date/Time: 2021221 064210; 06:42:40.672 INFO Funcotator - ------------------------------------------------------------; 06:42:40.672 INFO Funcotator - ------------------------------------------------------------; 06:42:40.673 INFO Funcotator - HTSJDK Version: 2.20.1; 06:42:40.673 INFO Funcotator - Picard Version: 2.20.5; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 06:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:5288,Performance,cache,cache,5288,/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:41.615 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:41.617 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_rec/hg38/acmg59_test_cleaned.txt; 06:42:41.617 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; 06:42:41.618 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.618 INFO DataSourceUtils - Setting lookahead cache for data source: LMMKnown : 100000; 06:42:41.622 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.641 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.652 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.663 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.annotation.REORDERED.gtf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencod,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:6390,Performance,cache,cache,6390,"d file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.641 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.652 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.663 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.annotation.REORDERED.gtf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.663 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; 06:42:41.665 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.665 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.666 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.691 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.pc_transcripts.fa -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.pc_trans",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:7685,Performance,cache,cache,7685," may occur.; 06:42:41.665 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.666 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.691 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.pc_transcripts.fa -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.pc_transcripts.fa; 06:42:46.805 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.805 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 06:42:46.807 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.951 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:47.023 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:47.098 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:47.107 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:9216,Performance,cache,cache,9216,e file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:47.098 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:47.107 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_rec/hg38/acmg59_test_cleaned.txt; 06:42:47.109 INFO Funcotator - Initializing Funcotator Engine...; 06:42:47.139 INFO Funcotator - Creating a MAF file for output: file:/data/nws/WES/GenomicsDBImport/200923_A00268_0517_AHKL37DSXY/Set20-5_L2_159A59.somatic.filterMutectCalls.funcotator.maf; 06:42:47.186 INFO ProgressMeter - Starting traversal; 06:42:47.187 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 06:42:47.233 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; 06:42:47.233 INFO VcfFuncotationFactory - LMMKnown 20180618 cache hits/total: 0/0; 06:42:47.287 INFO Funcotator - Shutting down engine; [2021221 064247] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.62 minutes.; Runtime.totalMemory()=1988100096; java.lang.IllegalArgumentException: Unexpected value: lncRNA; 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1016); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature.<init>(GencodeGtfFeature.java:144); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:729); 	at org.broadinstitu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:9299,Performance,cache,cache,9299,nvar_20180429_hg38.vcf; 06:42:47.098 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:47.107 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_rec/hg38/acmg59_test_cleaned.txt; 06:42:47.109 INFO Funcotator - Initializing Funcotator Engine...; 06:42:47.139 INFO Funcotator - Creating a MAF file for output: file:/data/nws/WES/GenomicsDBImport/200923_A00268_0517_AHKL37DSXY/Set20-5_L2_159A59.somatic.filterMutectCalls.funcotator.maf; 06:42:47.186 INFO ProgressMeter - Starting traversal; 06:42:47.187 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 06:42:47.233 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; 06:42:47.233 INFO VcfFuncotationFactory - LMMKnown 20180618 cache hits/total: 0/0; 06:42:47.287 INFO Funcotator - Shutting down engine; [2021221 064247] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.62 minutes.; Runtime.totalMemory()=1988100096; java.lang.IllegalArgumentException: Unexpected value: lncRNA; 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1016); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature.<init>(GencodeGtfFeature.java:144); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:729); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature.create(GencodeGtfFeature.java:299),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:2643,Security,Validat,Validating,2643,2:40.673 INFO Funcotator - HTSJDK Version: 2.20.1; 06:42:40.673 INFO Funcotator - Picard Version: 2.20.5; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 06:42:40.673 INFO Funcotator - Deflater: IntelDeflater; 06:42:40.674 INFO Funcotator - Inflater: IntelInflater; 06:42:40.674 INFO Funcotator - GCS max retries/reopens: 20; 06:42:40.674 INFO Funcotator - Requester pays: disabled; 06:42:40.674 INFO Funcotator - Initializing engine; 06:42:41.406 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/GenomicsDBImport/200923_A00268_0517_AHKL37DSXY/Set20-5_L2_159A59.somatic.filterMutectCalls.vcf.gz; 06:42:41.561 INFO Funcotator - Done initializing engine; 06:42:41.561 INFO Funcotator - Validating Sequence Dictionaries...; 06:42:41.589 INFO Funcotator - Processing user transcripts/defaults/overrides...; 06:42:41.590 INFO Funcotator - Initializing data sources...; 06:42:41.594 INFO DataSourceUtils - Initializing data sources from directory: /data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g; 06:42:41.596 INFO DataSourceUtils - Data sources version: 1.7.2020521g; 06:42:41.597 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200521g.tar.gz; 06:42:41.597 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521.tar.gz; 06:42:41.609 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:6536,Testability,test,tested,6536,"DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.652 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.663 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.annotation.REORDERED.gtf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.663 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; 06:42:41.665 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.665 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.666 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.691 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.pc_transcripts.fa -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.pc_transcripts.fa; 06:42:46.805 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7090:7003,Testability,test,tested,7003,"riants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.663 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.annotation.REORDERED.gtf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.663 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; 06:42:41.665 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.665 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.666 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.691 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.pc_transcripts.fa -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.pc_transcripts.fa; 06:42:46.805 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.805 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 06:42:46.807 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.951 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///dat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090
https://github.com/broadinstitute/gatk/issues/7091:0,Availability,Error,Error,0,"Error type 1: . gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMD00025146_mkdup.bam -R Gallus_gallus.GRCg6a.dna.toplevel.fa -L chicken_chr.list -O 11_cigar/SAMD00025146_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.347 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:01:27.611 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.612 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.9.0; 00:01:27.612 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:01:27.612 INFO SplitNCigarReads - Executing as dguan@bigmem8 on Linux v4.15.0-118-generic amd64; 00:01:27.613 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:01:27.613 INFO SplitNCigarReads - Start Date/Time: February 21, 2021 at 12:01:27 AM PST; 00:01:27.613 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.613 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.614 INFO SplitNCigarReads - HTSJDK Version: 2.23.0; 00:01:27.614 INFO SplitNCigarReads - Picard Version: 2.23.3; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:01:27.615 INFO Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:58691,Availability,down,down,58691,":189445 131.2 141330000 1077550.0; 02:12:47.714 INFO ProgressMeter - 33:3337674 131.3 141488000 1077385.4; 02:12:57.758 INFO ProgressMeter - 33:4894812 131.5 141812000 1078478.0; 02:13:07.778 INFO ProgressMeter - 33:5541459 131.7 141930000 1078006.1; 02:13:17.801 INFO ProgressMeter - 33:6552319 131.8 142090000 1077853.8; 02:13:29.312 INFO ProgressMeter - 33:6897003 132.0 142231000 1077355.5; 02:13:39.368 INFO ProgressMeter - 33:7093963 132.2 142374000 1077071.3; 02:13:49.368 INFO ProgressMeter - 33:7125494 132.4 142594000 1077377.2; 02:13:59.416 INFO ProgressMeter - 33:7127107 132.5 142752000 1077208.0; 02:14:09.424 INFO ProgressMeter - 33:7726380 132.7 142902000 1076984.3; 02:14:11.026 INFO SplitNCigarReads - 0 read(s) filtered by: AllowAllReadsReadFilter. 02:14:11.026 INFO ProgressMeter - 33:7774932 132.7 142924170 1076934.7; 02:14:11.026 INFO ProgressMeter - Traversal complete. Processed 142924170 total reads in 132.7 minutes.; 02:14:55.471 INFO SplitNCigarReads - Shutting down engine; [February 21, 2021 at 2:14:55 AM PST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 133.47 minutes.; Runtime.totalMemory()=1283457024; htsjdk.samtools.SAMException: Exception when processing alignment for BAM index DRR029822.14231344 1/2 94b aligned to 1:68375-68468.; at htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:141); at htsjdk.samtools.SAMFileWriterImpl.close(SAMFileWriterImpl.java:212); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyClose(AsyncSAMFileWriter.java:38); at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:89); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.close(SAMFileGATKReadWriter.java:26); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:193); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1053); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:60190,Availability,error,error,60190,syncSAMFileWriter.java:38); at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:89); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.close(SAMFileGATKReadWriter.java:26); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:193); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1053); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.samtools.util.RuntimeIOException: Write error; BinaryCodec in writemode; streamed file (filename not available); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:222); at htsjdk.samtools.util.BlockCompressedOutputStream.writeGzipBlock(BlockCompressedOutputStream.java:451); at htsjdk.samtools.util.BlockCompressedOutputStream.deflateBlock(BlockCompressedOutputStream.java:415); at htsjdk.samtools.util.BlockCompressedOutputStream.write(BlockCompressedOutputStream.java:305); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:212); at htsjdk.samtools.BAMRecordCodec.encode(BAMRecordCodec.java:168); at htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:134); ... 12 more; Caused by: java.io.IOException: Stale file handle; at java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method); at java.base/sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:62); at java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:115); at java.base/sun.nio,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:60251,Availability,avail,available,60251,syncSAMFileWriter.java:38); at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:89); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.close(SAMFileGATKReadWriter.java:26); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:193); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1053); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.samtools.util.RuntimeIOException: Write error; BinaryCodec in writemode; streamed file (filename not available); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:222); at htsjdk.samtools.util.BlockCompressedOutputStream.writeGzipBlock(BlockCompressedOutputStream.java:451); at htsjdk.samtools.util.BlockCompressedOutputStream.deflateBlock(BlockCompressedOutputStream.java:415); at htsjdk.samtools.util.BlockCompressedOutputStream.write(BlockCompressedOutputStream.java:305); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:212); at htsjdk.samtools.BAMRecordCodec.encode(BAMRecordCodec.java:168); at htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:134); ... 12 more; Caused by: java.io.IOException: Stale file handle; at java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method); at java.base/sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:62); at java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:115); at java.base/sun.nio,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:61843,Availability,Error,Error,61843," htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:134); ... 12 more; Caused by: java.io.IOException: Stale file handle; at java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method); at java.base/sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:62); at java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:115); at java.base/sun.nio.ch.IOUtil.write(IOUtil.java:80); at java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:280); at java.base/java.nio.channels.Channels.writeFullyImpl(Channels.java:74); at java.base/java.nio.channels.Channels.writeFully(Channels.java:97); at java.base/java.nio.channels.Channels.access$000(Channels.java:62); at java.base/java.nio.channels.Channels$1.write(Channels.java:172); at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81); at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220). Error type2:. gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/group/zhougrp2/dguan/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMN05828173_mkdup.bam -R /group/zhougrp2/dguan/00_ref/Gallus_gallus.GRCg6a.dna.toplevel.fa -L /group/zhougrp2/dguan/00_ref/chicken_chr.list -O 11_cigar/SAMN05828173_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.003 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:01:27.275 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.276 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.9.0; 00:01:27.276 INFO Sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:114171,Availability,down,down,114171,"gressMeter - Z:9213604 123.3 49233000 399397.5; 02:04:54.256 INFO ProgressMeter - Z:9213609 123.4 49270000 399145.4; 02:05:04.520 INFO ProgressMeter - Z:9213615 123.6 49305000 398876.1; 02:05:14.836 INFO ProgressMeter - Z:9213617 123.8 49333000 398548.3; 02:05:24.952 INFO ProgressMeter - Z:9213623 124.0 49360000 398224.1; 02:05:35.007 INFO ProgressMeter - Z:9213627 124.1 49387000 397903.9; 02:05:45.376 INFO ProgressMeter - Z:9213630 124.3 49413000 397559.8; 02:05:55.709 INFO ProgressMeter - Z:9213632 124.5 49439000 397218.6; 02:06:05.774 INFO ProgressMeter - Z:9213633 124.6 49464000 396884.6; 02:06:15.808 INFO ProgressMeter - Z:9213636 124.8 49489000 396553.0; 02:06:25.926 INFO ProgressMeter - Z:9213644 125.0 49515000 396226.0; 02:06:36.016 INFO ProgressMeter - Z:9213645 125.1 49540000 395893.3; 02:06:46.370 INFO ProgressMeter - Z:9213647 125.3 49565000 395547.6; 02:06:56.432 INFO ProgressMeter - Z:9213649 125.5 49589000 395210.2; 02:07:03.641 INFO SplitNCigarReads - Shutting down engine; [February 21, 2021 at 2:07:03 AM PST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 125.62 minutes.; Runtime.totalMemory()=5721030656; htsjdk.samtools.util.RuntimeIOException: Stale file handle; at htsjdk.samtools.util.BinaryCodec.close(BinaryCodec.java:628); at htsjdk.samtools.util.BlockCompressedOutputStream.close(BlockCompressedOutputStream.java:344); at htsjdk.samtools.util.BlockCompressedOutputStream.close(BlockCompressedOutputStream.java:331); at htsjdk.samtools.util.BinaryCodec.close(BinaryCodec.java:624); at htsjdk.samtools.BAMFileWriter.finish(BAMFileWriter.java:155); at htsjdk.samtools.SAMFileWriterImpl.close(SAMFileWriterImpl.java:220); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyClose(AsyncSAMFileWriter.java:38); at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:89); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.close(SAMFileGATKReadWriter.java:26); at org.broadinstitute.hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:359,Performance,Load,Loading,359,"Error type 1: . gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMD00025146_mkdup.bam -R Gallus_gallus.GRCg6a.dna.toplevel.fa -L chicken_chr.list -O 11_cigar/SAMD00025146_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.347 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:01:27.611 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.612 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.9.0; 00:01:27.612 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:01:27.612 INFO SplitNCigarReads - Executing as dguan@bigmem8 on Linux v4.15.0-118-generic amd64; 00:01:27.613 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:01:27.613 INFO SplitNCigarReads - Start Date/Time: February 21, 2021 at 12:01:27 AM PST; 00:01:27.613 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.613 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.614 INFO SplitNCigarReads - HTSJDK Version: 2.23.0; 00:01:27.614 INFO SplitNCigarReads - Picard Version: 2.23.3; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:01:27.615 INFO Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:62279,Performance,Load,Loading,62279,"FileChannelImpl.write(FileChannelImpl.java:280); at java.base/java.nio.channels.Channels.writeFullyImpl(Channels.java:74); at java.base/java.nio.channels.Channels.writeFully(Channels.java:97); at java.base/java.nio.channels.Channels.access$000(Channels.java:62); at java.base/java.nio.channels.Channels$1.write(Channels.java:172); at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81); at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220). Error type2:. gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/group/zhougrp2/dguan/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMN05828173_mkdup.bam -R /group/zhougrp2/dguan/00_ref/Gallus_gallus.GRCg6a.dna.toplevel.fa -L /group/zhougrp2/dguan/00_ref/chicken_chr.list -O 11_cigar/SAMN05828173_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.003 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:01:27.275 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.276 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.9.0; 00:01:27.276 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:01:27.276 INFO SplitNCigarReads - Executing as dguan@bigmem6 on Linux v4.15.0-122-generic amd64; 00:01:27.276 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:01:27.277 INFO SplitNCigarReads - Start Date/Time: February 21, 2021 at 12:01:26 AM PST; 00:01:27.277 INFO SplitNCigarReads - ---------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:116241,Performance,perform,performCleanup,116241,ls.BAMFileWriter.finish(BAMFileWriter.java:155); at htsjdk.samtools.SAMFileWriterImpl.close(SAMFileWriterImpl.java:220); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyClose(AsyncSAMFileWriter.java:38); at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:89); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.close(SAMFileGATKReadWriter.java:26); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:193); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1053); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.io.IOException: Stale file handle; at java.base/java.io.FileDescriptor.close0(Native Method); at java.base/java.io.FileDescriptor.close(FileDescriptor.java:239); at java.base/java.io.FileDescriptor$1.close(FileDescriptor.java:87); at java.base/sun.nio.ch.FileChannelImpl$Closer.run(FileChannelImpl.java:114); at java.base/jdk.internal.ref.CleanerImpl$PhantomCleanableRef.performCleanup(CleanerImpl.java:186); at java.base/jdk.internal.ref.PhantomCleanable.clean(PhantomCleanable.java:133); at java.base/sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:198); at java.base/java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:112); at java.base/java.nio.channels.Channels$1.close(Channels.java:177); at java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188); at htsjdk.samtools.util.BinaryCodec.close(BinaryCodec.java:624),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:671,Safety,detect,detect,671,"Error type 1: . gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMD00025146_mkdup.bam -R Gallus_gallus.GRCg6a.dna.toplevel.fa -L chicken_chr.list -O 11_cigar/SAMD00025146_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.347 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:01:27.611 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.612 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.9.0; 00:01:27.612 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:01:27.612 INFO SplitNCigarReads - Executing as dguan@bigmem8 on Linux v4.15.0-118-generic amd64; 00:01:27.613 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:01:27.613 INFO SplitNCigarReads - Start Date/Time: February 21, 2021 at 12:01:27 AM PST; 00:01:27.613 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.613 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.614 INFO SplitNCigarReads - HTSJDK Version: 2.23.0; 00:01:27.614 INFO SplitNCigarReads - Picard Version: 2.23.3; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:01:27.615 INFO Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:62591,Safety,detect,detect,62591,"e(Channels.java:172); at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81); at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220). Error type2:. gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/group/zhougrp2/dguan/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMN05828173_mkdup.bam -R /group/zhougrp2/dguan/00_ref/Gallus_gallus.GRCg6a.dna.toplevel.fa -L /group/zhougrp2/dguan/00_ref/chicken_chr.list -O 11_cigar/SAMN05828173_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.003 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:01:27.275 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.276 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.9.0; 00:01:27.276 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:01:27.276 INFO SplitNCigarReads - Executing as dguan@bigmem6 on Linux v4.15.0-122-generic amd64; 00:01:27.276 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:01:27.277 INFO SplitNCigarReads - Start Date/Time: February 21, 2021 at 12:01:26 AM PST; 00:01:27.277 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.277 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.279 INFO SplitNCigarReads - HTSJDK Version: 2.23.0; 00:01:27.279 INFO SplitNCigarReads - Picard Version: 2.23.3; 00:01:27.279 INFO SplitNCigarReads - HTSJ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7091:61510,Security,access,access,61510,"utputStream.java:415); at htsjdk.samtools.util.BlockCompressedOutputStream.write(BlockCompressedOutputStream.java:305); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:212); at htsjdk.samtools.BAMRecordCodec.encode(BAMRecordCodec.java:168); at htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:134); ... 12 more; Caused by: java.io.IOException: Stale file handle; at java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method); at java.base/sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:62); at java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:115); at java.base/sun.nio.ch.IOUtil.write(IOUtil.java:80); at java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:280); at java.base/java.nio.channels.Channels.writeFullyImpl(Channels.java:74); at java.base/java.nio.channels.Channels.writeFully(Channels.java:97); at java.base/java.nio.channels.Channels.access$000(Channels.java:62); at java.base/java.nio.channels.Channels$1.write(Channels.java:172); at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81); at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220). Error type2:. gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/group/zhougrp2/dguan/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMN05828173_mkdup.bam -R /group/zhougrp2/dguan/00_ref/Gallus_gallus.GRCg6a.dna.toplevel.fa -L /group/zhougrp2/dguan/00_ref/chicken_chr.list -O 11_cigar/SAMN05828173_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.003 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091
https://github.com/broadinstitute/gatk/issues/7092:8487,Availability,down,down,8487," 4:61122642 10.0 19223000 1919197.3; 01:10:04.009 INFO ProgressMeter - 4:77404115 10.2 19546000 1919442.2; 01:10:14.027 INFO ProgressMeter - 4:90982301 10.4 19859000 1918719.2; 01:10:24.037 INFO ProgressMeter - 5:5319910 10.5 20215000 1922132.2; 01:10:34.043 INFO ProgressMeter - 5:13428416 10.7 20557000 1924140.1; 01:10:44.052 INFO ProgressMeter - 5:18554429 10.9 20887000 1924971.5; 01:10:54.053 INFO ProgressMeter - 5:23247594 11.0 21241000 1927979.5; 01:11:04.057 INFO ProgressMeter - 5:25901452 11.2 21588000 1930263.3; 01:11:14.089 INFO ProgressMeter - 5:32482380 11.4 21916000 1930729.5; 01:11:24.106 INFO ProgressMeter - 5:38674297 11.5 22249000 1931652.6; 01:11:34.133 INFO ProgressMeter - 5:49679881 11.7 22573000 1931754.3; 01:11:44.145 INFO ProgressMeter - 5:53234595 11.9 22925000 1934259.1; 04:10:15.659 INFO ProgressMeter - 6:1726401 190.4 23183000 121774.0; 04:10:25.671 INFO ProgressMeter - 6:10206926 190.5 23517000 123420.2; 04:10:31.341 INFO BaseRecalibrator - Shutting down engine; [February 22, 2021 at 4:10:31 AM PST] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 190.65 minutes.; Runtime.totalMemory()=1268776960; java.lang.IllegalStateException: cigar is completely soft-clipped; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:129); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:143); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.consolidateCigar(BaseRecalibrationEngine.java:293); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transfo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7092
https://github.com/broadinstitute/gatk/issues/7092:10535,Integrability,wrap,wrapAndCopyInto,10535,nsformer.java:20); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:118); at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:189); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:96); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7092
https://github.com/broadinstitute/gatk/issues/7092:464,Performance,Load,Loading,464,"gatk --java-options ""-Xmx8G -XX:ParallelGCThreads=16 -Djava.io.tmpdir=/group/zhougrp2/dguan/tmp"" BaseRecalibrator --spark-runner LOCAL -I 11_cigar/SAMN06242676_cigar.bam --known-sites /group/zhougrp2/dguan/00_ref/gallus_gallus.vcf.gz -L /group/zhougrp2/dguan/00_ref/chicken_chr.list -O 12_bqsr/SAMN06242676_bqsr.table -R /group/zhougrp2/dguan/00_ref/Gallus_gallus.GRCg6a.dna.toplevel.fa --tmp-dir /group/zhougrp2/dguan/tmp. 00:59:52.106 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 22, 2021 12:59:52 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:59:52.360 INFO BaseRecalibrator - ------------------------------------------------------------; 00:59:52.361 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:59:52.361 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:59:52.361 INFO BaseRecalibrator - Executing as dguan@c11-95 on Linux v4.15.0-122-generic amd64; 00:59:52.361 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:59:52.362 INFO BaseRecalibrator - Start Date/Time: February 22, 2021 at 12:59:52 AM PST; 00:59:52.362 INFO BaseRecalibrator - ------------------------------------------------------------; 00:59:52.362 INFO BaseRecalibrator - ------------------------------------------------------------; 00:59:52.363 INFO BaseRecalibrator - HTSJDK Version: 2.24.0; 00:59:52.363 INFO BaseRecalibrator - Picard Version: 2.25.0; 00:59:52.363 INFO BaseRecalibrator - Built for Spark Version: 2.4.5; 00:59:52.363 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:59:52.363 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:59:52.363 INFO BaseRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7092
https://github.com/broadinstitute/gatk/issues/7092:776,Safety,detect,detect,776,"gatk --java-options ""-Xmx8G -XX:ParallelGCThreads=16 -Djava.io.tmpdir=/group/zhougrp2/dguan/tmp"" BaseRecalibrator --spark-runner LOCAL -I 11_cigar/SAMN06242676_cigar.bam --known-sites /group/zhougrp2/dguan/00_ref/gallus_gallus.vcf.gz -L /group/zhougrp2/dguan/00_ref/chicken_chr.list -O 12_bqsr/SAMN06242676_bqsr.table -R /group/zhougrp2/dguan/00_ref/Gallus_gallus.GRCg6a.dna.toplevel.fa --tmp-dir /group/zhougrp2/dguan/tmp. 00:59:52.106 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 22, 2021 12:59:52 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:59:52.360 INFO BaseRecalibrator - ------------------------------------------------------------; 00:59:52.361 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:59:52.361 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:59:52.361 INFO BaseRecalibrator - Executing as dguan@c11-95 on Linux v4.15.0-122-generic amd64; 00:59:52.361 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:59:52.362 INFO BaseRecalibrator - Start Date/Time: February 22, 2021 at 12:59:52 AM PST; 00:59:52.362 INFO BaseRecalibrator - ------------------------------------------------------------; 00:59:52.362 INFO BaseRecalibrator - ------------------------------------------------------------; 00:59:52.363 INFO BaseRecalibrator - HTSJDK Version: 2.24.0; 00:59:52.363 INFO BaseRecalibrator - Picard Version: 2.25.0; 00:59:52.363 INFO BaseRecalibrator - Built for Spark Version: 2.4.5; 00:59:52.363 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:59:52.363 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:59:52.363 INFO BaseRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7092
https://github.com/broadinstitute/gatk/issues/7092:8787,Security,validat,validate,8787,"557000 1924140.1; 01:10:44.052 INFO ProgressMeter - 5:18554429 10.9 20887000 1924971.5; 01:10:54.053 INFO ProgressMeter - 5:23247594 11.0 21241000 1927979.5; 01:11:04.057 INFO ProgressMeter - 5:25901452 11.2 21588000 1930263.3; 01:11:14.089 INFO ProgressMeter - 5:32482380 11.4 21916000 1930729.5; 01:11:24.106 INFO ProgressMeter - 5:38674297 11.5 22249000 1931652.6; 01:11:34.133 INFO ProgressMeter - 5:49679881 11.7 22573000 1931754.3; 01:11:44.145 INFO ProgressMeter - 5:53234595 11.9 22925000 1934259.1; 04:10:15.659 INFO ProgressMeter - 6:1726401 190.4 23183000 121774.0; 04:10:25.671 INFO ProgressMeter - 6:10206926 190.5 23517000 123420.2; 04:10:31.341 INFO BaseRecalibrator - Shutting down engine; [February 22, 2021 at 4:10:31 AM PST] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 190.65 minutes.; Runtime.totalMemory()=1268776960; java.lang.IllegalStateException: cigar is completely soft-clipped; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:129); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:143); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.consolidateCigar(BaseRecalibrationEngine.java:293); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:118); at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:189)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7092
https://github.com/broadinstitute/gatk/issues/7094:174,Security,access,access,174,"The tool-specific WDLs we generate in https://github.com/broadinstitute/gatk-tool-wdls don't have the `localization_optional` parameter turned on for args that support cloud access (ie., arguments of type GATKPath), and so always localize.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7094
https://github.com/broadinstitute/gatk/issues/7095:120,Availability,error,error,120,"### Summary; A user running Mutect2 in GATK 4.1.9.0 was inputting an interval list using the -XL option and received an error message that did not illuminate the problem. Their intervals file was made using BedToIntervalList and was a picard-style interval list. The final solution they found was the [correct extension](https://gatk.broadinstitute.org/hc/en-us/articles/360035531852-Intervals-and-interval-lists), .interval_list. ### Example 1; Using .intervallist extension, the error message said the interval was not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""combinedblacklist.intervallist"" is not valid for this input`. ### Example 2; Using .intervals extension, the error message said the interval list was badly formed and not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""@HD VN:1.6 SO:coordinate"" is not valid for this input.`. <br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113538'>Zendesk ticket #113538</a>)<br>gz#113538</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7095
https://github.com/broadinstitute/gatk/issues/7095:481,Availability,error,error,481,"### Summary; A user running Mutect2 in GATK 4.1.9.0 was inputting an interval list using the -XL option and received an error message that did not illuminate the problem. Their intervals file was made using BedToIntervalList and was a picard-style interval list. The final solution they found was the [correct extension](https://gatk.broadinstitute.org/hc/en-us/articles/360035531852-Intervals-and-interval-lists), .interval_list. ### Example 1; Using .intervallist extension, the error message said the interval was not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""combinedblacklist.intervallist"" is not valid for this input`. ### Example 2; Using .intervals extension, the error message said the interval list was badly formed and not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""@HD VN:1.6 SO:coordinate"" is not valid for this input.`. <br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113538'>Zendesk ticket #113538</a>)<br>gz#113538</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7095
https://github.com/broadinstitute/gatk/issues/7095:536,Availability,ERROR,ERROR,536,"### Summary; A user running Mutect2 in GATK 4.1.9.0 was inputting an interval list using the -XL option and received an error message that did not illuminate the problem. Their intervals file was made using BedToIntervalList and was a picard-style interval list. The final solution they found was the [correct extension](https://gatk.broadinstitute.org/hc/en-us/articles/360035531852-Intervals-and-interval-lists), .interval_list. ### Example 1; Using .intervallist extension, the error message said the interval was not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""combinedblacklist.intervallist"" is not valid for this input`. ### Example 2; Using .intervals extension, the error message said the interval list was badly formed and not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""@HD VN:1.6 SO:coordinate"" is not valid for this input.`. <br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113538'>Zendesk ticket #113538</a>)<br>gz#113538</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7095
https://github.com/broadinstitute/gatk/issues/7095:715,Availability,error,error,715,"### Summary; A user running Mutect2 in GATK 4.1.9.0 was inputting an interval list using the -XL option and received an error message that did not illuminate the problem. Their intervals file was made using BedToIntervalList and was a picard-style interval list. The final solution they found was the [correct extension](https://gatk.broadinstitute.org/hc/en-us/articles/360035531852-Intervals-and-interval-lists), .interval_list. ### Example 1; Using .intervallist extension, the error message said the interval was not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""combinedblacklist.intervallist"" is not valid for this input`. ### Example 2; Using .intervals extension, the error message said the interval list was badly formed and not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""@HD VN:1.6 SO:coordinate"" is not valid for this input.`. <br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113538'>Zendesk ticket #113538</a>)<br>gz#113538</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7095
https://github.com/broadinstitute/gatk/issues/7095:792,Availability,ERROR,ERROR,792,"### Summary; A user running Mutect2 in GATK 4.1.9.0 was inputting an interval list using the -XL option and received an error message that did not illuminate the problem. Their intervals file was made using BedToIntervalList and was a picard-style interval list. The final solution they found was the [correct extension](https://gatk.broadinstitute.org/hc/en-us/articles/360035531852-Intervals-and-interval-lists), .interval_list. ### Example 1; Using .intervallist extension, the error message said the interval was not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""combinedblacklist.intervallist"" is not valid for this input`. ### Example 2; Using .intervals extension, the error message said the interval list was badly formed and not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""@HD VN:1.6 SO:coordinate"" is not valid for this input.`. <br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113538'>Zendesk ticket #113538</a>)<br>gz#113538</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7095
https://github.com/broadinstitute/gatk/issues/7095:126,Integrability,message,message,126,"### Summary; A user running Mutect2 in GATK 4.1.9.0 was inputting an interval list using the -XL option and received an error message that did not illuminate the problem. Their intervals file was made using BedToIntervalList and was a picard-style interval list. The final solution they found was the [correct extension](https://gatk.broadinstitute.org/hc/en-us/articles/360035531852-Intervals-and-interval-lists), .interval_list. ### Example 1; Using .intervallist extension, the error message said the interval was not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""combinedblacklist.intervallist"" is not valid for this input`. ### Example 2; Using .intervals extension, the error message said the interval list was badly formed and not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""@HD VN:1.6 SO:coordinate"" is not valid for this input.`. <br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113538'>Zendesk ticket #113538</a>)<br>gz#113538</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7095
https://github.com/broadinstitute/gatk/issues/7095:487,Integrability,message,message,487,"### Summary; A user running Mutect2 in GATK 4.1.9.0 was inputting an interval list using the -XL option and received an error message that did not illuminate the problem. Their intervals file was made using BedToIntervalList and was a picard-style interval list. The final solution they found was the [correct extension](https://gatk.broadinstitute.org/hc/en-us/articles/360035531852-Intervals-and-interval-lists), .interval_list. ### Example 1; Using .intervallist extension, the error message said the interval was not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""combinedblacklist.intervallist"" is not valid for this input`. ### Example 2; Using .intervals extension, the error message said the interval list was badly formed and not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""@HD VN:1.6 SO:coordinate"" is not valid for this input.`. <br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113538'>Zendesk ticket #113538</a>)<br>gz#113538</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7095
https://github.com/broadinstitute/gatk/issues/7095:721,Integrability,message,message,721,"### Summary; A user running Mutect2 in GATK 4.1.9.0 was inputting an interval list using the -XL option and received an error message that did not illuminate the problem. Their intervals file was made using BedToIntervalList and was a picard-style interval list. The final solution they found was the [correct extension](https://gatk.broadinstitute.org/hc/en-us/articles/360035531852-Intervals-and-interval-lists), .interval_list. ### Example 1; Using .intervallist extension, the error message said the interval was not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""combinedblacklist.intervallist"" is not valid for this input`. ### Example 2; Using .intervals extension, the error message said the interval list was badly formed and not valid; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""@HD VN:1.6 SO:coordinate"" is not valid for this input.`. <br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113538'>Zendesk ticket #113538</a>)<br>gz#113538</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7095
https://github.com/broadinstitute/gatk/issues/7096:437,Availability,Error,Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates,437,"### Summary; In the AnalyzeCovariates documentation, there is mention of the previous GATK3 method to obtain recalibration tables before and after BQSR. There does not seem to be that functionality in GATK4. If there is, it is not well documented. ### User Request. This request was created from a contribution made by ISmolicz on February 17, 2021 12:03 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates). \--. Dear GATK Team,. In previous GATK4 versions, I understand there was a -bqsr option to use with BaseRecalibrator so that a second recalibration table could be generated and then submitted to AnalyzeCovariates. However, it appears this option is longer available as I receive the following error with GATK version 4.1.9.0:. A USER ERROR has occurred: -bqsr is not a recognized option. The command used:. gatk BaseRecalibrator \\ ; \--input input.bam \\ ; \-R ucsc.hg19.fasta \\ ; \--known-sites dbsnp\_138.hg19.vcf \\ ; \--known-sites Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf \\ ; \--known-sites 1000G\_phase1.indels.hg19.sites.vcf \\ ; \-bqsr sample.recal\_data.grp \\ ; \--output sample.recal\_data\_2.grp \\ ; \--tmp-dir $TMPDIR. Please could you confirm the current method used to generate the second recalibration table with GATK 4.1.9.0?. Unfortunately I could not identify the current method, including in the \[Base Quality Score Recalibration (BQSR)\](/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR- ""Base Quality Score Recalibration (BQSR)"")documentation. Thank you for your time and help.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113534'>Zendesk ticket #113534</a>)<br>gz#113534</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7096
https://github.com/broadinstitute/gatk/issues/7096:591,Availability,Error,Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates,591,"### Summary; In the AnalyzeCovariates documentation, there is mention of the previous GATK3 method to obtain recalibration tables before and after BQSR. There does not seem to be that functionality in GATK4. If there is, it is not well documented. ### User Request. This request was created from a contribution made by ISmolicz on February 17, 2021 12:03 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates). \--. Dear GATK Team,. In previous GATK4 versions, I understand there was a -bqsr option to use with BaseRecalibrator so that a second recalibration table could be generated and then submitted to AnalyzeCovariates. However, it appears this option is longer available as I receive the following error with GATK version 4.1.9.0:. A USER ERROR has occurred: -bqsr is not a recognized option. The command used:. gatk BaseRecalibrator \\ ; \--input input.bam \\ ; \-R ucsc.hg19.fasta \\ ; \--known-sites dbsnp\_138.hg19.vcf \\ ; \--known-sites Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf \\ ; \--known-sites 1000G\_phase1.indels.hg19.sites.vcf \\ ; \-bqsr sample.recal\_data.grp \\ ; \--output sample.recal\_data\_2.grp \\ ; \--tmp-dir $TMPDIR. Please could you confirm the current method used to generate the second recalibration table with GATK 4.1.9.0?. Unfortunately I could not identify the current method, including in the \[Base Quality Score Recalibration (BQSR)\](/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR- ""Base Quality Score Recalibration (BQSR)"")documentation. Thank you for your time and help.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113534'>Zendesk ticket #113534</a>)<br>gz#113534</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7096
https://github.com/broadinstitute/gatk/issues/7096:932,Availability,avail,available,932,"### Summary; In the AnalyzeCovariates documentation, there is mention of the previous GATK3 method to obtain recalibration tables before and after BQSR. There does not seem to be that functionality in GATK4. If there is, it is not well documented. ### User Request. This request was created from a contribution made by ISmolicz on February 17, 2021 12:03 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates). \--. Dear GATK Team,. In previous GATK4 versions, I understand there was a -bqsr option to use with BaseRecalibrator so that a second recalibration table could be generated and then submitted to AnalyzeCovariates. However, it appears this option is longer available as I receive the following error with GATK version 4.1.9.0:. A USER ERROR has occurred: -bqsr is not a recognized option. The command used:. gatk BaseRecalibrator \\ ; \--input input.bam \\ ; \-R ucsc.hg19.fasta \\ ; \--known-sites dbsnp\_138.hg19.vcf \\ ; \--known-sites Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf \\ ; \--known-sites 1000G\_phase1.indels.hg19.sites.vcf \\ ; \-bqsr sample.recal\_data.grp \\ ; \--output sample.recal\_data\_2.grp \\ ; \--tmp-dir $TMPDIR. Please could you confirm the current method used to generate the second recalibration table with GATK 4.1.9.0?. Unfortunately I could not identify the current method, including in the \[Base Quality Score Recalibration (BQSR)\](/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR- ""Base Quality Score Recalibration (BQSR)"")documentation. Thank you for your time and help.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113534'>Zendesk ticket #113534</a>)<br>gz#113534</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7096
https://github.com/broadinstitute/gatk/issues/7096:969,Availability,error,error,969,"### Summary; In the AnalyzeCovariates documentation, there is mention of the previous GATK3 method to obtain recalibration tables before and after BQSR. There does not seem to be that functionality in GATK4. If there is, it is not well documented. ### User Request. This request was created from a contribution made by ISmolicz on February 17, 2021 12:03 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates). \--. Dear GATK Team,. In previous GATK4 versions, I understand there was a -bqsr option to use with BaseRecalibrator so that a second recalibration table could be generated and then submitted to AnalyzeCovariates. However, it appears this option is longer available as I receive the following error with GATK version 4.1.9.0:. A USER ERROR has occurred: -bqsr is not a recognized option. The command used:. gatk BaseRecalibrator \\ ; \--input input.bam \\ ; \-R ucsc.hg19.fasta \\ ; \--known-sites dbsnp\_138.hg19.vcf \\ ; \--known-sites Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf \\ ; \--known-sites 1000G\_phase1.indels.hg19.sites.vcf \\ ; \-bqsr sample.recal\_data.grp \\ ; \--output sample.recal\_data\_2.grp \\ ; \--tmp-dir $TMPDIR. Please could you confirm the current method used to generate the second recalibration table with GATK 4.1.9.0?. Unfortunately I could not identify the current method, including in the \[Base Quality Score Recalibration (BQSR)\](/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR- ""Base Quality Score Recalibration (BQSR)"")documentation. Thank you for your time and help.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113534'>Zendesk ticket #113534</a>)<br>gz#113534</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7096
https://github.com/broadinstitute/gatk/issues/7096:1010,Availability,ERROR,ERROR,1010,"### Summary; In the AnalyzeCovariates documentation, there is mention of the previous GATK3 method to obtain recalibration tables before and after BQSR. There does not seem to be that functionality in GATK4. If there is, it is not well documented. ### User Request. This request was created from a contribution made by ISmolicz on February 17, 2021 12:03 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077098031-Error-when-trying-to-generate-the-second-recalibration-table-for-AnalyzeCovariates). \--. Dear GATK Team,. In previous GATK4 versions, I understand there was a -bqsr option to use with BaseRecalibrator so that a second recalibration table could be generated and then submitted to AnalyzeCovariates. However, it appears this option is longer available as I receive the following error with GATK version 4.1.9.0:. A USER ERROR has occurred: -bqsr is not a recognized option. The command used:. gatk BaseRecalibrator \\ ; \--input input.bam \\ ; \-R ucsc.hg19.fasta \\ ; \--known-sites dbsnp\_138.hg19.vcf \\ ; \--known-sites Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf \\ ; \--known-sites 1000G\_phase1.indels.hg19.sites.vcf \\ ; \-bqsr sample.recal\_data.grp \\ ; \--output sample.recal\_data\_2.grp \\ ; \--tmp-dir $TMPDIR. Please could you confirm the current method used to generate the second recalibration table with GATK 4.1.9.0?. Unfortunately I could not identify the current method, including in the \[Base Quality Score Recalibration (BQSR)\](/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR- ""Base Quality Score Recalibration (BQSR)"")documentation. Thank you for your time and help.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/113534'>Zendesk ticket #113534</a>)<br>gz#113534</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7096
https://github.com/broadinstitute/gatk/issues/7097:564,Availability,error,error,564,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; When running Funcotator on a large VCF, it can take several hours to complete, but each variant row is handled separately, row 1 and row 5 million are equally likely to have problems. Currently, a sufficiently malformed variant row causes it to crash and leave partial output. . It would be much friendlier if the true crash problems (e.g. something like ""invalid interval"") were saved and the crash-causing variant lines reported in bulk at completion, sending an OS exit code/error then. . It might even be a configurable option to exit immediately or save until end. . I do think it is appropriate to crash if a problem is encountered while parsing the header lines.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7097
https://github.com/broadinstitute/gatk/issues/7097:597,Modifiability,config,configurable,597,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; When running Funcotator on a large VCF, it can take several hours to complete, but each variant row is handled separately, row 1 and row 5 million are equally likely to have problems. Currently, a sufficiently malformed variant row causes it to crash and leave partial output. . It would be much friendlier if the true crash problems (e.g. something like ""invalid interval"") were saved and the crash-causing variant lines reported in bulk at completion, sending an OS exit code/error then. . It might even be a configurable option to exit immediately or save until end. . I do think it is appropriate to crash if a problem is encountered while parsing the header lines.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7097
https://github.com/broadinstitute/gatk/pull/7098:379,Safety,avoid,avoiding,379,"related to issue #211 and #233 - in CreateVariantIngestFiles, when writing missing positions to the pet tsv, we were looping through blocks of missing intervals twice, once to construct the pet rows, holding them all in memory, and again to write the pet rows. in this PR, we merge those steps into one: for each missing block, we write the pet lines to file as we loop through, avoiding the need to hold large blocks in memory. note that this does not resolve the memory issues that originally prompted issues #211 and #233, but it's nonetheless a minor improvement that helps immensely when there are large numbers of missing locations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7098
https://github.com/broadinstitute/gatk/issues/7099:1306,Deployability,release,release,1306,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7099
https://github.com/broadinstitute/gatk/issues/7099:1376,Testability,test,test,1376,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7099
https://github.com/broadinstitute/gatk/issues/7099:1476,Testability,log,logs,1476,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7099
https://github.com/broadinstitute/gatk/pull/7101:77,Performance,Load,Loading,77,Part of resolving https://github.com/broadinstitute/dsp-spec-ops/issues/244. Loading site-level QualApprox into VET... usage will be in a different PR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7101
https://github.com/broadinstitute/gatk/issues/7102:35,Availability,down,down,35,To avoid quota issues when pulling down the base image during tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7102
https://github.com/broadinstitute/gatk/issues/7102:3,Safety,avoid,avoid,3,To avoid quota issues when pulling down the base image during tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7102
https://github.com/broadinstitute/gatk/issues/7102:62,Testability,test,tests,62,To avoid quota issues when pulling down the base image during tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7102
https://github.com/broadinstitute/gatk/pull/7104:753,Availability,error,error,753,"This PR addresses spec-ops issue #235 - Use -m flag in final gsutil mv of files in ImportGenomes. . Additionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```;  cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt;  cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Ope",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104
https://github.com/broadinstitute/gatk/pull/7104:793,Availability,failure,failures,793,"This PR addresses spec-ops issue #235 - Use -m flag in final gsutil mv of files in ImportGenomes. . Additionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```;  cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt;  cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Ope",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104
https://github.com/broadinstitute/gatk/pull/7104:1588,Availability,avail,available,1588,"tionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```;  cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt;  cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104
https://github.com/broadinstitute/gatk/pull/7104:1458,Modifiability,config,config,1458,"tionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```;  cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt;  cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104
https://github.com/broadinstitute/gatk/pull/7104:410,Safety,timeout,timeouts,410,"This PR addresses spec-ops issue #235 - Use -m flag in final gsutil mv of files in ImportGenomes. . Additionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```;  cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt;  cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Ope",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104
https://github.com/broadinstitute/gatk/pull/7104:643,Testability,test,tested,643,"This PR addresses spec-ops issue #235 - Use -m flag in final gsutil mv of files in ImportGenomes. . Additionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```;  cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt;  cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Ope",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104
https://github.com/broadinstitute/gatk/pull/7106:166,Availability,down,down,166,"Resolves https://github.com/broadinstitute/dsp-spec-ops/issues/239. See README.md in this PR for full details. ----; To make this easier to review, the changes break down into a few sections. 1. Docs -- the README.md. Does it make sense? Could you follow it?. 2. Comparison Script (compare_data.py)-- is it clear? Obvs any bugs would be great. The Github Issue for this PR describes _what_ is compared. 3. WDL changes -- should be straightforward to review, just minor changes; ; 4. Code changes (java) -- we can walk through this together if that's more effective",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7106
https://github.com/broadinstitute/gatk/pull/7106:307,Usability,clear,clear,307,"Resolves https://github.com/broadinstitute/dsp-spec-ops/issues/239. See README.md in this PR for full details. ----; To make this easier to review, the changes break down into a few sections. 1. Docs -- the README.md. Does it make sense? Could you follow it?. 2. Comparison Script (compare_data.py)-- is it clear? Obvs any bugs would be great. The Github Issue for this PR describes _what_ is compared. 3. WDL changes -- should be straightforward to review, just minor changes; ; 4. Code changes (java) -- we can walk through this together if that's more effective",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7106
https://github.com/broadinstitute/gatk/pull/7112:193,Availability,error,error,193,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:125,Modifiability,refactor,refactor,125,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:432,Modifiability,refactor,refactoring,432,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:134,Performance,Load,LoadBigQueryData,134,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:210,Performance,load,load,210,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:393,Performance,race condition,race condition,393,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:608,Performance,Load,LoadTables,608,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:679,Performance,load,loading,679,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:378,Safety,safe,safe,378,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:762,Testability,test,testing,762,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/pull/7112:795,Testability,test,tested,795,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112
https://github.com/broadinstitute/gatk/issues/7114:2721,Availability,down,down,2721,ilterMutectCalls - HTSJDK Version: 2.20.1; 22:09:03.376 INFO FilterMutectCalls - Picard Version: 2.20.5; 22:09:03.376 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:09:03.376 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:09:03.376 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:09:03.377 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:09:03.377 INFO FilterMutectCalls - Deflater: IntelDeflater; 22:09:03.377 INFO FilterMutectCalls - Inflater: IntelInflater; 22:09:03.377 INFO FilterMutectCalls - GCS max retries/reopens: 20; 22:09:03.377 INFO FilterMutectCalls - Requester pays: disabled; 22:09:03.378 INFO FilterMutectCalls - Initializing engine; 22:09:04.031 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.vcf.gz; 22:09:04.071 INFO FilterMutectCalls - Shutting down engine; [2021228 100904] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.52 minutes.; Runtime.totalMemory()=1552416768; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.vcf.gz; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:375); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:327); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:277); 	at org.broadinstitute.hellbender.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:58); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:706); 	at org.broadinstit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114
https://github.com/broadinstitute/gatk/issues/7114:2963,Availability,Error,Error,2963, FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:09:03.377 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:09:03.377 INFO FilterMutectCalls - Deflater: IntelDeflater; 22:09:03.377 INFO FilterMutectCalls - Inflater: IntelInflater; 22:09:03.377 INFO FilterMutectCalls - GCS max retries/reopens: 20; 22:09:03.377 INFO FilterMutectCalls - Requester pays: disabled; 22:09:03.378 INFO FilterMutectCalls - Initializing engine; 22:09:04.031 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.vcf.gz; 22:09:04.071 INFO FilterMutectCalls - Shutting down engine; [2021228 100904] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.52 minutes.; Runtime.totalMemory()=1552416768; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.vcf.gz; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:375); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:327); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:277); 	at org.broadinstitute.hellbender.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:58); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:706); 	at org.broadinstitute.hellbender.engine.VariantWalker.onStartup(VariantWalker.java:45); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114
https://github.com/broadinstitute/gatk/issues/7114:4408,Availability,error,error,4408,"der.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:58); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:706); 	at org.broadinstitute.hellbender.engine.VariantWalker.onStartup(VariantWalker.java:45); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Did not inflate expected amount, for input source: /data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:372); 	... 12 more; Caused by: htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114
https://github.com/broadinstitute/gatk/issues/7114:5625,Availability,avail,available,5625,htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:372); 	... 12 more; Caused by: htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132); 	at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:84); 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); 	at java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:300); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:356); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114
https://github.com/broadinstitute/gatk/issues/7114:778,Performance,Load,Loading,778,Using GATK jar /usr/local/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/local/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar FilterMutectCalls -R /data/nws/WES/reference/Homo_sapiens_assembly38.fasta -V /data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.vcf.gz --contamination-table /data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.getpileupsummaries.calculatecontamination.table -O /data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.filterMutectCalls.vcf.gz; 22:08:32.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:08:48.358 INFO FilterMutectCalls - ------------------------------------------------------------; 22:08:48.359 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.3.0; 22:08:48.359 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:09:03.373 INFO FilterMutectCalls - Executing as nws@cuckoolab on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 22:09:03.374 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_262-b10; 22:09:03.374 INFO FilterMutectCalls - Start Date/Time: 2021228 100832; 22:09:03.374 INFO FilterMutectCalls - ------------------------------------------------------------; 22:09:03.374 INFO FilterMutectCalls - ------------------------------------------------------------; 22:09:03.376 INFO FilterMutectCalls - HTSJDK Version: 2.20.1; 22:09:03.376 INFO FilterMutectCalls - Picard Version: 2.20.5; 22:09:03.376 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:09:03.376 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114
https://github.com/broadinstitute/gatk/pull/7115:191,Testability,Test,Testing,191,"### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/199. ### **Commit Summary**. -Add labels to Sample list, Extract Cohort, and Extract features; - Edit Bigquery Utils Testing ; - Add labels as an input to a Bigquery functions ; - Created UID class for a unique identifier . ### **Output**; For Sample list, Extract Cohort and Extract features; ```; ""query"", ""Run_SampleTable_<UID>""; ""query"", ""extract_cohort_<UID>""; ""query"", ""extract_features_<UID>""; ```; Testing:; ```; ""test_query"", ""get_all_records_<UID>""; ""test_query"", ""test_where_clause_<UID>""; ""test_query"", ""test_batch_mode_<UID>""; ""test_query"", ""test_specified_execute_query_<UID>""; ""test_query"", ""test_storage_api_<UID>""; ""test_query"", ""test_storage_api_with_empty_dataset_<UID>"". ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7115
https://github.com/broadinstitute/gatk/pull/7115:480,Testability,Test,Testing,480,"### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/199. ### **Commit Summary**. -Add labels to Sample list, Extract Cohort, and Extract features; - Edit Bigquery Utils Testing ; - Add labels as an input to a Bigquery functions ; - Created UID class for a unique identifier . ### **Output**; For Sample list, Extract Cohort and Extract features; ```; ""query"", ""Run_SampleTable_<UID>""; ""query"", ""extract_cohort_<UID>""; ""query"", ""extract_features_<UID>""; ```; Testing:; ```; ""test_query"", ""get_all_records_<UID>""; ""test_query"", ""test_where_clause_<UID>""; ""test_query"", ""test_batch_mode_<UID>""; ""test_query"", ""test_specified_execute_query_<UID>""; ""test_query"", ""test_storage_api_<UID>""; ""test_query"", ""test_storage_api_with_empty_dataset_<UID>"". ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7115
https://github.com/broadinstitute/gatk/issues/7116:87,Safety,detect,detection,87,"Hello,. I was wondering if there is a generic WGS cohort model to be used for the gCNV detection already built.; This is because I do not have a cohort myself, only my WGS case samples. I believe the data from the [tutorial](https://gatk.broadinstitute.org/hc/en-us/articles/360035531152) provided was based of the 1000 Genome Project [files](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/integrated_sv_map) but I am not aware of the existence of the files to be used directly in DetermineGermlineContigPloidy and GermlineCNVCaller as model. Thank you",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7116
https://github.com/broadinstitute/gatk/pull/7121:194,Availability,error,errored,194,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/pull/7121:436,Availability,error,error,436,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/pull/7121:18,Deployability,update,updates,18,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/pull/7121:55,Energy Efficiency,reduce,reduce,55,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/pull/7121:279,Performance,load,loading,279,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/pull/7121:303,Performance,load,load,303,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/pull/7121:366,Performance,load,load,366,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/pull/7121:583,Performance,load,load,583,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/pull/7121:164,Testability,test,tested,164,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/pull/7121:614,Testability,test,tested,614,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121
https://github.com/broadinstitute/gatk/issues/7123:345,Performance,optimiz,optimizations,345,I am getting the following exception when I set `--minimum-mapping-quality` to 60 (but not 50). . ```console; $ gatk --version; ...; The Genome Analysis Toolkit (GATK) v4.2.0.0; HTSJDK Version: 2.24.0; Picard Version: 2.25.0; ```. ```console; $ gatk HaplotypeCaller \; -I in.bam \; -L chr7:145945238-145945238 \; -stand-call-conf 0 \; --disable-optimizations \; --force-active -O out.vcf \; --reference /path/to/ucsc.hg19.fasta \; --minimum-mapping-quality 60;; ...; java.lang.IllegalStateException: There is no variation present.; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer$Result.getVariantRegion(AssemblyRegionTrimmer.java:108); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:595); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:273); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. <details>; <summary>test.sam</summary>. ```; @HD	VN:1.6	SO:coordinate; @SQ	SN:chr1	LN:249250621; @SQ	SN:chr2	LN:243199373; @SQ	SN:chr3	LN:198022430; @SQ	SN:chr4	LN:191154276; @SQ	SN:chr5	LN:18,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7123
https://github.com/broadinstitute/gatk/issues/7123:578,Security,validat,validate,578,I am getting the following exception when I set `--minimum-mapping-quality` to 60 (but not 50). . ```console; $ gatk --version; ...; The Genome Analysis Toolkit (GATK) v4.2.0.0; HTSJDK Version: 2.24.0; Picard Version: 2.25.0; ```. ```console; $ gatk HaplotypeCaller \; -I in.bam \; -L chr7:145945238-145945238 \; -stand-call-conf 0 \; --disable-optimizations \; --force-active -O out.vcf \; --reference /path/to/ucsc.hg19.fasta \; --minimum-mapping-quality 60;; ...; java.lang.IllegalStateException: There is no variation present.; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer$Result.getVariantRegion(AssemblyRegionTrimmer.java:108); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:595); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:273); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. <details>; <summary>test.sam</summary>. ```; @HD	VN:1.6	SO:coordinate; @SQ	SN:chr1	LN:249250621; @SQ	SN:chr2	LN:243199373; @SQ	SN:chr3	LN:198022430; @SQ	SN:chr4	LN:191154276; @SQ	SN:chr5	LN:18,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7123
https://github.com/broadinstitute/gatk/issues/7123:1829,Testability,test,test,1829,"allerEngine.callRegion(HaplotypeCallerEngine.java:595); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:273); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. <details>; <summary>test.sam</summary>. ```; @HD	VN:1.6	SO:coordinate; @SQ	SN:chr1	LN:249250621; @SQ	SN:chr2	LN:243199373; @SQ	SN:chr3	LN:198022430; @SQ	SN:chr4	LN:191154276; @SQ	SN:chr5	LN:180915260; @SQ	SN:chr6	LN:171115067; @SQ	SN:chr7	LN:159138663; @SQ	SN:chr8	LN:146364022; @SQ	SN:chr9	LN:141213431; @SQ	SN:chr10	LN:135534747; @SQ	SN:chr11	LN:135006516; @SQ	SN:chr12	LN:133851895; @SQ	SN:chr13	LN:115169878; @SQ	SN:chr14	LN:107349540; @SQ	SN:chr15	LN:102531392; @SQ	SN:chr16	LN:90354753; @SQ	SN:chr17	LN:81195210; @SQ	SN:chr18	LN:78077248; @SQ	SN:chr19	LN:59128983; @SQ	SN:chr20	LN:63025520; @SQ	SN:chr21	LN:48129895; @SQ	SN:chr22	LN:51304566; @SQ	SN:chrX	LN:155270560; @SQ	SN:chrY	LN:59373566; @SQ	SN:chrM	LN:16571; @RG	ID:1	SM:Sample	LB:Sample	PU:na	PL:Illumina; @PG	PN:MarkDuplicates	ID:MarkDuplicates	CL:<redacted>; q0	32	chr7	145945113	3	50M	=	145945249	0	AGAGATATAAGAGGTTGGGGCACGGAAATAAGGGATCGGGGCACAGAGAT	GGAGAGGGIIIIIGIIIIIIGIIIIGGIIIIIIIIIIIIIIIGGIGIGGG	XA:Z:chr7,+145945069,146M5S,3;chr7,+14594504",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7123
https://github.com/broadinstitute/gatk/issues/7124:682,Performance,optimiz,optimizations,682,"I was investigating an issue where the depth is reported lower than expected at a given site. The default value of `--minimum-mapping-quality` is `20`, so I tried `1`, `20,` and `60`. Both values `1` and `60` give _higher_ depth (`INFO.DP`) than `20`, which is very counter-intuitive. The read pairs are overlapping, so I tried the `--do-not-correct-overlapping-quality` option, which caused this bias to go away. I'd still don't understand why increasing and decreasing the minimum mapping quality makes a difference, but it is likely to do with overlapping read pairs. ```bash; $ gatk HaplotypeCaller \; -I in.bam \; -L chr7:145945238-145945238 \; -stand-call-conf 0 \; --disable-optimizations \; --force-active -O out.vcf \; --reference /path/to/ucsc.hg19.fasta \; --minimum-mapping-quality <value>;; $ gatk --version; ...; The Genome Analysis Toolkit (GATK) v4.2.0.0; HTSJDK Version: 2.24.0; Picard Version: 2.25.0; ```. (I tried this `4.1.4.0`). `--minimum-mapping-quality 1`:; ```bash; chr7	145945238	.	A	G	7534.06	.	AC=2;AF=1.00;AN=2;DP=247;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=58.06;QD=31.52;SOR=1.050	GT:AD:DP:GQ:PL	1/1:0,239:239:99:7548,716,0; ```. `--minimum-mapping-quality 20`:; ```bash; chr7	145945238	.	A	G	267.64	.	AC=1;AF=0.500;AN=2;BaseQRankSum=2.838;DP=14;ExcessHet=3.0103;FS=6.264;MLEAC=1;MLEAF=0.500;MQ=59.06;MQRankSum=0.000;QD=22.30;ReadPosRankSum=2.208;SOR=2.022	GT:AD:DP:GQ:PL	0/1:3,9:12:28:275,0,28; ```. `--minimum-mapping-quality 60`:; ```bash; chr7	145945238	.	A	G	7150.06	.	AC=2;AF=1.00;AN=2;DP=224;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=32.06;SOR=1.008	GT:AD:DP:GQ:PL	1/1:0,223:223:99:7164,668,0. ```. <details>; <summary>test.bam</summary>. ```; @HD	VN:1.6	SO:coordinate; @SQ	SN:chr1	LN:249250621; @SQ	SN:chr2	LN:243199373; @SQ	SN:chr3	LN:198022430; @SQ	SN:chr4	LN:191154276; @SQ	SN:chr5	LN:180915260; @SQ	SN:chr6	LN:171115067; @SQ	SN:chr7	LN:159138663; @SQ	SN:chr8	LN:146364022; @SQ	SN:chr9	LN:141213431; @SQ	SN:chr10	LN:135534747; @SQ	SN:c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7124
https://github.com/broadinstitute/gatk/issues/7124:1681,Testability,test,test,1681,"-disable-optimizations \; --force-active -O out.vcf \; --reference /path/to/ucsc.hg19.fasta \; --minimum-mapping-quality <value>;; $ gatk --version; ...; The Genome Analysis Toolkit (GATK) v4.2.0.0; HTSJDK Version: 2.24.0; Picard Version: 2.25.0; ```. (I tried this `4.1.4.0`). `--minimum-mapping-quality 1`:; ```bash; chr7	145945238	.	A	G	7534.06	.	AC=2;AF=1.00;AN=2;DP=247;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=58.06;QD=31.52;SOR=1.050	GT:AD:DP:GQ:PL	1/1:0,239:239:99:7548,716,0; ```. `--minimum-mapping-quality 20`:; ```bash; chr7	145945238	.	A	G	267.64	.	AC=1;AF=0.500;AN=2;BaseQRankSum=2.838;DP=14;ExcessHet=3.0103;FS=6.264;MLEAC=1;MLEAF=0.500;MQ=59.06;MQRankSum=0.000;QD=22.30;ReadPosRankSum=2.208;SOR=2.022	GT:AD:DP:GQ:PL	0/1:3,9:12:28:275,0,28; ```. `--minimum-mapping-quality 60`:; ```bash; chr7	145945238	.	A	G	7150.06	.	AC=2;AF=1.00;AN=2;DP=224;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=32.06;SOR=1.008	GT:AD:DP:GQ:PL	1/1:0,223:223:99:7164,668,0. ```. <details>; <summary>test.bam</summary>. ```; @HD	VN:1.6	SO:coordinate; @SQ	SN:chr1	LN:249250621; @SQ	SN:chr2	LN:243199373; @SQ	SN:chr3	LN:198022430; @SQ	SN:chr4	LN:191154276; @SQ	SN:chr5	LN:180915260; @SQ	SN:chr6	LN:171115067; @SQ	SN:chr7	LN:159138663; @SQ	SN:chr8	LN:146364022; @SQ	SN:chr9	LN:141213431; @SQ	SN:chr10	LN:135534747; @SQ	SN:chr11	LN:135006516; @SQ	SN:chr12	LN:133851895; @SQ	SN:chr13	LN:115169878; @SQ	SN:chr14	LN:107349540; @SQ	SN:chr15	LN:102531392; @SQ	SN:chr16	LN:90354753; @SQ	SN:chr17	LN:81195210; @SQ	SN:chr18	LN:78077248; @SQ	SN:chr19	LN:59128983; @SQ	SN:chr20	LN:63025520; @SQ	SN:chr21	LN:48129895; @SQ	SN:chr22	LN:51304566; @SQ	SN:chrX	LN:155270560; @SQ	SN:chrY	LN:59373566; @SQ	SN:chrM	LN:16571; @RG	ID:1	SM:Sample	LB:Sample	PU:na	PL:Illumina; @PG	PN:bwa	ID:bwa	CL:<redacted>; @PG	PN:MarkDuplicates	ID:MarkDuplicates	CL:<redacted>; q0	99	chr7	145945113	3	151M	=	145945249	287	AGAGATATAAGAGGTTGGGGCACGGAAATAAGGGATCGGGGCACAGAGATATAAGAGGCTGGGGCACGGAAATAAGGGATCGGGGCACAGAGATATAAGAGGCTGGGGCA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7124
https://github.com/broadinstitute/gatk/issues/7124:274,Usability,intuit,intuitive,274,"I was investigating an issue where the depth is reported lower than expected at a given site. The default value of `--minimum-mapping-quality` is `20`, so I tried `1`, `20,` and `60`. Both values `1` and `60` give _higher_ depth (`INFO.DP`) than `20`, which is very counter-intuitive. The read pairs are overlapping, so I tried the `--do-not-correct-overlapping-quality` option, which caused this bias to go away. I'd still don't understand why increasing and decreasing the minimum mapping quality makes a difference, but it is likely to do with overlapping read pairs. ```bash; $ gatk HaplotypeCaller \; -I in.bam \; -L chr7:145945238-145945238 \; -stand-call-conf 0 \; --disable-optimizations \; --force-active -O out.vcf \; --reference /path/to/ucsc.hg19.fasta \; --minimum-mapping-quality <value>;; $ gatk --version; ...; The Genome Analysis Toolkit (GATK) v4.2.0.0; HTSJDK Version: 2.24.0; Picard Version: 2.25.0; ```. (I tried this `4.1.4.0`). `--minimum-mapping-quality 1`:; ```bash; chr7	145945238	.	A	G	7534.06	.	AC=2;AF=1.00;AN=2;DP=247;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=58.06;QD=31.52;SOR=1.050	GT:AD:DP:GQ:PL	1/1:0,239:239:99:7548,716,0; ```. `--minimum-mapping-quality 20`:; ```bash; chr7	145945238	.	A	G	267.64	.	AC=1;AF=0.500;AN=2;BaseQRankSum=2.838;DP=14;ExcessHet=3.0103;FS=6.264;MLEAC=1;MLEAF=0.500;MQ=59.06;MQRankSum=0.000;QD=22.30;ReadPosRankSum=2.208;SOR=2.022	GT:AD:DP:GQ:PL	0/1:3,9:12:28:275,0,28; ```. `--minimum-mapping-quality 60`:; ```bash; chr7	145945238	.	A	G	7150.06	.	AC=2;AF=1.00;AN=2;DP=224;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=32.06;SOR=1.008	GT:AD:DP:GQ:PL	1/1:0,223:223:99:7164,668,0. ```. <details>; <summary>test.bam</summary>. ```; @HD	VN:1.6	SO:coordinate; @SQ	SN:chr1	LN:249250621; @SQ	SN:chr2	LN:243199373; @SQ	SN:chr3	LN:198022430; @SQ	SN:chr4	LN:191154276; @SQ	SN:chr5	LN:180915260; @SQ	SN:chr6	LN:171115067; @SQ	SN:chr7	LN:159138663; @SQ	SN:chr8	LN:146364022; @SQ	SN:chr9	LN:141213431; @SQ	SN:chr10	LN:135534747; @SQ	SN:c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7124
https://github.com/broadinstitute/gatk/pull/7127:121,Deployability,Update,Update,121,### **Addresses** ; https://github.com/broadinstitute/gatk/pull/7115. ### **Commit Summary**; -Created AvroFileReader; - Update ExtractCohort and ExtractCohortEngine to accept a AvroFile. Testing:; Created a test for AvroFileReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7127
https://github.com/broadinstitute/gatk/pull/7127:188,Testability,Test,Testing,188,### **Addresses** ; https://github.com/broadinstitute/gatk/pull/7115. ### **Commit Summary**; -Created AvroFileReader; - Update ExtractCohort and ExtractCohortEngine to accept a AvroFile. Testing:; Created a test for AvroFileReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7127
https://github.com/broadinstitute/gatk/pull/7127:208,Testability,test,test,208,### **Addresses** ; https://github.com/broadinstitute/gatk/pull/7115. ### **Commit Summary**; -Created AvroFileReader; - Update ExtractCohort and ExtractCohortEngine to accept a AvroFile. Testing:; Created a test for AvroFileReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7127
https://github.com/broadinstitute/gatk/pull/7129:191,Availability,error,error,191,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129
https://github.com/broadinstitute/gatk/pull/7129:227,Availability,error,error,227,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129
https://github.com/broadinstitute/gatk/pull/7129:621,Availability,avail,available,621,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129
https://github.com/broadinstitute/gatk/pull/7129:491,Modifiability,config,config,491,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129
https://github.com/broadinstitute/gatk/pull/7129:84,Performance,load,load,84,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129
https://github.com/broadinstitute/gatk/pull/7129:109,Testability,test,tested,109,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129
https://github.com/broadinstitute/gatk/pull/7131:408,Security,Hash,HashMap,408,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:625,Security,hash,hashmap,625,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:676,Security,hash,hashcode,676,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:737,Security,hash,hashcode,737,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:910,Security,hash,hash,910,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:969,Security,Hash,HashMap,969,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:1075,Security,Hash,HashMap,1075,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:791,Testability,test,tests,791,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:1130,Testability,test,test,1130,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:1157,Testability,test,test,1157,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:1250,Testability,test,test,1250,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7131:949,Usability,learn,learned,949,"We found this trying to tie out the BQ version of the VQSR annotations with the Evoquer+Gnarly outputs, specifically in the RankSumTest annotations. This never got merged into master. The short story is... the implementation of median in the Histogram class is broken. Precisely, in this line it iterates through the bins of the histogram to find the midpoint index... however the keySet() it gets is from a HashMap and therefore not ordered. That code hasn't been touched in 2 years, so it's been this way for a while. It's tough to catch because of course there is a non-guaranteed ordering to the keys, and in the case of hashmap today when there are < 16 keys will be the hashcode of the keys modulo 16 until there is collision. The hashcode of integers is just the integer... so in our tests we have today it just happens to work because we have a small number of keys and they are close to each other so hash in the order we happen to want. I learned more about [HashMap internals](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/HashMap.java) than I wanted to know in order to make a test case for it :/ . I. A test on data like this breaks:. 1,2,16. as it returns 1 instead of 2 as the median. I have a test for this, and have a fix (basically sort the keys, running through a TreeSet). Thanks to @mmorgantaylor and @schaluva for helping with this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7131
https://github.com/broadinstitute/gatk/pull/7133:48,Performance,load,load,48,- use aou service account to access gvcf and to load to BQ; - do manual localization with aou service account in create ingest tsv task to minimize vm spin up,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7133
https://github.com/broadinstitute/gatk/pull/7133:29,Security,access,access,29,- use aou service account to access gvcf and to load to BQ; - do manual localization with aou service account in create ingest tsv task to minimize vm spin up,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7133
https://github.com/broadinstitute/gatk/issues/7134:1998,Availability,error,errors,1998,"encode.v37lift37.annotation.REORDERED.gtf; 18:53:59.113 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 08, 2021 6:53:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.419 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.422 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.433 INFO ProgressMeter - Starting traversal; 18:53:59.433 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 18:54:01.952 INFO IndexFeatureFile - Shutting down engine; [March 8, 2021 at 6:54:01 PM CET] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 min",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134
https://github.com/broadinstitute/gatk/issues/7134:2308,Availability,error,errors,2308,"ngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.419 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.422 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.433 INFO ProgressMeter - Starting traversal; 18:53:59.433 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 18:54:01.952 INFO IndexFeatureFile - Shutting down engine; [March 8, 2021 at 6:54:01 PM CET] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=473956352; java.lang.IllegalArgumentException: Unexpected value: MANE_Plus_Clinical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1388); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(Genc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134
https://github.com/broadinstitute/gatk/issues/7134:2738,Availability,down,down,2738,"NCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.419 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.422 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.433 INFO ProgressMeter - Starting traversal; 18:53:59.433 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 18:54:01.952 INFO IndexFeatureFile - Shutting down engine; [March 8, 2021 at 6:54:01 PM CET] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=473956352; java.lang.IllegalArgumentException: Unexpected value: MANE_Plus_Clinical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1388); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:197); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfTranscriptFeature.<init>(GencodeGtfTranscriptFeature.java:19); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfTranscriptFeature.create(GencodeGtfTranscriptFeature.java:23); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$2.create(GencodeGtfFeature.java:768); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.create(GencodeGtfFeature.java:327); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(Ab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134
https://github.com/broadinstitute/gatk/issues/7134:44,Deployability,update,update,44,"## Bug Report. Dear developers,. I tried to update the GENCODE database and used the getGencode.sh scripts to get the data. However, I was not able to index the feature-file: Do you have any idea why that happens and how to get it done?. Code:; /home/robby/Tools/NGS/gatk-4.2.0.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.113 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 08, 2021 6:53:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but err",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134
https://github.com/broadinstitute/gatk/issues/7134:948,Performance,Load,Loading,948,"## Bug Report. Dear developers,. I tried to update the GENCODE database and used the getGencode.sh scripts to get the data. However, I was not able to index the feature-file: Do you have any idea why that happens and how to get it done?. Code:; /home/robby/Tools/NGS/gatk-4.2.0.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.113 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 08, 2021 6:53:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but err",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134
https://github.com/broadinstitute/gatk/issues/7134:1232,Safety,detect,detect,1232," it done?. Code:; /home/robby/Tools/NGS/gatk-4.2.0.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.113 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 08, 2021 6:53:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.419 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38),",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134
https://github.com/broadinstitute/gatk/issues/7134:1812,Testability,test,tested,1812,"encode.v37lift37.annotation.REORDERED.gtf; 18:53:59.113 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 08, 2021 6:53:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.419 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.422 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.433 INFO ProgressMeter - Starting traversal; 18:53:59.433 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 18:54:01.952 INFO IndexFeatureFile - Shutting down engine; [March 8, 2021 at 6:54:01 PM CET] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 min",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134
https://github.com/broadinstitute/gatk/issues/7134:2122,Testability,test,tested,2122,"ngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.419 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.422 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.433 INFO ProgressMeter - Starting traversal; 18:53:59.433 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 18:54:01.952 INFO IndexFeatureFile - Shutting down engine; [March 8, 2021 at 6:54:01 PM CET] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=473956352; java.lang.IllegalArgumentException: Unexpected value: MANE_Plus_Clinical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1388); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(Genc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134
https://github.com/broadinstitute/gatk/issues/7135:122,Availability,error,errors,122,"Hi,; I tried to use Funcotator to annotate a vcf file. The vcf file had just 10 records. The program kept running without errors but no results got. . **The command I used:**. /technology/software_tools/gatk-4.2.0.0/gatk Funcotator \; --variant P01.mutect2.somatic.filterMutectCalls.indels.vcf.gz \; --reference /technology/dependent_resource/genome/hsa/ensembl/GRCh37.p13_GATK/genome.fa \; --ref-version hg19 \; --data-sources-path /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s \; --output P01.mutect2.somatic.filterMutectCalls.indels.funcotator.vcf \; --output-file-format VCF. **Screen output:**. Using GATK jar /technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar Funcotator --variant P01.mutect2.somatic.filterMutectCalls.indels.vcf.gz --reference /technology/dependent_resource/genome/hsa/ensembl/GRCh37.p13_GATK/genome.fa --ref-version hg19 --data-sources-path /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s --output P01.mutect2.somatic.filterMutectCalls.indels.funcotator.vcf --output-file-format VCF; 10:25:49.484 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 09, 2021 10:25:49 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:25:49.665 INFO Funcotator - ------------------------------------------------------------; 10:25:49.665 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:25:49.665 INFO Funcotator - For support and documentation go to https://software.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135
https://github.com/broadinstitute/gatk/issues/7135:5833,Availability,error,errors,5833,v1.7.20200521s; 10:25:49.785 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 10:25:49.785 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 10:25:49.785 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 10:25:49.788 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/gencode_xrefseq_v75_37.tsv -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg19/gencode_xrefseq_v75_37.tsv; 10:25:49.789 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/achilles_lineage_results.import.txt -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/achilles/hg19/achilles_lineage_results.import.txt; 10:25:49.789 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/clinvar_20180401.vcf -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 10:25:49.790 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/simple_uniprot_Dec012014.tsv -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/simple_uniprot/hg19/simple_uniprot_Dec012014.tsv; 10:25:49.791 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/clinvar_hgmd.tsv -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv. **The input vcf file was from gatk FilterMutectCalls and just keep indel record**. what's the problem with the program? It have run a long time but without any results and errors. Thank you.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135
https://github.com/broadinstitute/gatk/issues/7135:1390,Performance,Load,Loading,1390,"\; --ref-version hg19 \; --data-sources-path /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s \; --output P01.mutect2.somatic.filterMutectCalls.indels.funcotator.vcf \; --output-file-format VCF. **Screen output:**. Using GATK jar /technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar Funcotator --variant P01.mutect2.somatic.filterMutectCalls.indels.vcf.gz --reference /technology/dependent_resource/genome/hsa/ensembl/GRCh37.p13_GATK/genome.fa --ref-version hg19 --data-sources-path /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s --output P01.mutect2.somatic.filterMutectCalls.indels.funcotator.vcf --output-file-format VCF; 10:25:49.484 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 09, 2021 10:25:49 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:25:49.665 INFO Funcotator - ------------------------------------------------------------; 10:25:49.665 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:25:49.665 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:25:49.665 INFO Funcotator - Executing as qin_dan@server on Linux v5.4.0-65-generic amd64; 10:25:49.665 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v11.0.10+9-Ubuntu-0ubuntu1.20.04; 10:25:49.666 INFO Funcotator - Start Date/Time: March 9, 2021 at 10:25:49 AM UTC; 10:25:49.666 INFO Funcotator - ------------------------------------------------------------; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135
https://github.com/broadinstitute/gatk/issues/7135:1680,Safety,detect,detect,1680,"ools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar Funcotator --variant P01.mutect2.somatic.filterMutectCalls.indels.vcf.gz --reference /technology/dependent_resource/genome/hsa/ensembl/GRCh37.p13_GATK/genome.fa --ref-version hg19 --data-sources-path /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s --output P01.mutect2.somatic.filterMutectCalls.indels.funcotator.vcf --output-file-format VCF; 10:25:49.484 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 09, 2021 10:25:49 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:25:49.665 INFO Funcotator - ------------------------------------------------------------; 10:25:49.665 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:25:49.665 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:25:49.665 INFO Funcotator - Executing as qin_dan@server on Linux v5.4.0-65-generic amd64; 10:25:49.665 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v11.0.10+9-Ubuntu-0ubuntu1.20.04; 10:25:49.666 INFO Funcotator - Start Date/Time: March 9, 2021 at 10:25:49 AM UTC; 10:25:49.666 INFO Funcotator - ------------------------------------------------------------; 10:25:49.666 INFO Funcotator - ------------------------------------------------------------; 10:25:49.666 INFO Funcotator - HTSJDK Version: 2.24.0; 10:25:49.666 INFO Funcotator - Picard Version: 2.25.0; 10:25:49.666 INFO Funcotator - Built for Spark Version: 2.4.5; 10:25:49.666 INFO Fun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135
https://github.com/broadinstitute/gatk/issues/7135:3524,Security,Validat,Validating,3524,.24.0; 10:25:49.666 INFO Funcotator - Picard Version: 2.25.0; 10:25:49.666 INFO Funcotator - Built for Spark Version: 2.4.5; 10:25:49.666 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:25:49.667 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:25:49.667 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:25:49.667 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:25:49.667 INFO Funcotator - Deflater: IntelDeflater; 10:25:49.667 INFO Funcotator - Inflater: IntelInflater; 10:25:49.667 INFO Funcotator - GCS max retries/reopens: 20; 10:25:49.667 INFO Funcotator - Requester pays: disabled; 10:25:49.667 INFO Funcotator - Initializing engine; 10:25:49.761 INFO FeatureManager - Using codec VCFCodec to read file file:///technology/research_development/WES/vcf/P01.mutect2.somatic.filterMutectCalls.indels.vcf.gz; 10:25:49.781 INFO Funcotator - Done initializing engine; 10:25:49.781 INFO Funcotator - Validating sequence dictionaries...; 10:25:49.782 INFO Funcotator - Processing user transcripts/defaults/overrides...; 10:25:49.783 INFO Funcotator - Initializing data sources...; 10:25:49.784 INFO DataSourceUtils - Initializing data sources from directory: /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s; 10:25:49.785 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 10:25:49.785 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 10:25:49.785 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 10:25:49.788 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/gencode_xrefseq_v75_37.tsv -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg19/gencode_xref,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135
https://github.com/broadinstitute/gatk/pull/7136:210,Modifiability,refactor,refactoring,210,"Use argument for execution project id for queries, specifically in the sample query. This was a gap since the project id was already used for Storage API usage. I think the SampleList class could also use some refactoring, but wanted to keep this PR small",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7136
https://github.com/broadinstitute/gatk/issues/7137:1612,Availability,down,down,1612,"{VariantContext@5509} ""[VC HC0 @ chr#:###551-###560 Q. of type=INDEL alleles=[CTTTTTTTTT*, C] attr={} GT=[] filters="" ; ```; And by chance that last variant (which happens to be supported by all haplotypes present) falls outside of our active region in the padding then we try to draw the variant span based on the first 4 haplotypes by the rules of haplotype expansion we end up making our trimming span `chr#:###326-###555` (note ###555 falls inside the span the 5th haplotype). When we go to trim all of our variant haplotypes (which happen to all have variant #5) they run into this code inside `Haplotype.trim()`:; ```; // note: the following returns null if the bases covering the ref interval start or end in a deletion.; final byte[] newBases = AlignmentUtils.getBasesCoveringRefInterval(newStart, newStop, getBases(), 0, getCigar());. if ( newBases == null || newBases.length == 0 ) { // we cannot meaningfully chop down the haplotype, so return null; return null;; }; ```; For all of our variant haplotypes at this site we find deletions at the end base and throw the whole haplotypes away when we try to trim it. In this particular case it meant we lost real variants in the previous 4 haplotypes as a result. I propose remedying this in one of two ways:; 1) Allow `AlignmentUtils.getBasesCoveringRefInterval()` to return partially spanning haplotypes when there are potentially 'shorter' than the reference haplotype span (this could easily cause all sorts of errors as the later code might not account for those mismatches. ; 2) Make `AlignmentUtils.getBasesCoveringRefInterval()` cheat and paste reference bases at the front or back of the haplotype to make it square with the reference offsets (we should never call or worry about deletions at the ends of haplotypes anyway) ; 3) Try to catch this edge case at the `AssemblyRegionTrimmer.trim()` stage, try to make the trimmer aware that there might be deletions overlapping its boundaries and expand them until there are no more overla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7137
https://github.com/broadinstitute/gatk/issues/7137:2159,Availability,error,errors,2159,"t (which happens to be supported by all haplotypes present) falls outside of our active region in the padding then we try to draw the variant span based on the first 4 haplotypes by the rules of haplotype expansion we end up making our trimming span `chr#:###326-###555` (note ###555 falls inside the span the 5th haplotype). When we go to trim all of our variant haplotypes (which happen to all have variant #5) they run into this code inside `Haplotype.trim()`:; ```; // note: the following returns null if the bases covering the ref interval start or end in a deletion.; final byte[] newBases = AlignmentUtils.getBasesCoveringRefInterval(newStart, newStop, getBases(), 0, getCigar());. if ( newBases == null || newBases.length == 0 ) { // we cannot meaningfully chop down the haplotype, so return null; return null;; }; ```; For all of our variant haplotypes at this site we find deletions at the end base and throw the whole haplotypes away when we try to trim it. In this particular case it meant we lost real variants in the previous 4 haplotypes as a result. I propose remedying this in one of two ways:; 1) Allow `AlignmentUtils.getBasesCoveringRefInterval()` to return partially spanning haplotypes when there are potentially 'shorter' than the reference haplotype span (this could easily cause all sorts of errors as the later code might not account for those mismatches. ; 2) Make `AlignmentUtils.getBasesCoveringRefInterval()` cheat and paste reference bases at the front or back of the haplotype to make it square with the reference offsets (we should never call or worry about deletions at the ends of haplotypes anyway) ; 3) Try to catch this edge case at the `AssemblyRegionTrimmer.trim()` stage, try to make the trimmer aware that there might be deletions overlapping its boundaries and expand them until there are no more overlaps. . This is a very unlikely case I suspect but it could cost us some sensitivity in noisy low complexity regions. @davidbenjamin what are your thoughts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7137
https://github.com/broadinstitute/gatk/pull/7138:536,Deployability,Release,ReleaseLoadLock,536,"spec ops issue #248. process implemented here:; - new task `SetLoadLock` is called at the beginning of `ImportGenomes` - it generates a UUID for the submission, writes that run_uuid to a lock file, and uploads that lock file to the output_directory (where the tsvs will be generated). ; - CreateImportTsvs and LoadTables take the run_uuid as an input, compare it against the contents of the lock file in the bucket, and only proceed if the uuids match. otherwise they exit out.; - after all LoadTables tasks have completed, a new task `ReleaseLoadLock` is called that removes the lock file from the bucket (again only if the uuid in the lockfile matches this run). tested and confirmed that:; - the `loadlock` file is created and removed: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/b0b9c7a1-70fd-4d44-a76e-b5604a5068f0; - the task fails if the lock file is present: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/293687f9-e7b9-474b-bfe8-e50f4c555199",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7138
https://github.com/broadinstitute/gatk/pull/7138:310,Performance,Load,LoadTables,310,"spec ops issue #248. process implemented here:; - new task `SetLoadLock` is called at the beginning of `ImportGenomes` - it generates a UUID for the submission, writes that run_uuid to a lock file, and uploads that lock file to the output_directory (where the tsvs will be generated). ; - CreateImportTsvs and LoadTables take the run_uuid as an input, compare it against the contents of the lock file in the bucket, and only proceed if the uuids match. otherwise they exit out.; - after all LoadTables tasks have completed, a new task `ReleaseLoadLock` is called that removes the lock file from the bucket (again only if the uuid in the lockfile matches this run). tested and confirmed that:; - the `loadlock` file is created and removed: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/b0b9c7a1-70fd-4d44-a76e-b5604a5068f0; - the task fails if the lock file is present: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/293687f9-e7b9-474b-bfe8-e50f4c555199",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7138
https://github.com/broadinstitute/gatk/pull/7138:491,Performance,Load,LoadTables,491,"spec ops issue #248. process implemented here:; - new task `SetLoadLock` is called at the beginning of `ImportGenomes` - it generates a UUID for the submission, writes that run_uuid to a lock file, and uploads that lock file to the output_directory (where the tsvs will be generated). ; - CreateImportTsvs and LoadTables take the run_uuid as an input, compare it against the contents of the lock file in the bucket, and only proceed if the uuids match. otherwise they exit out.; - after all LoadTables tasks have completed, a new task `ReleaseLoadLock` is called that removes the lock file from the bucket (again only if the uuid in the lockfile matches this run). tested and confirmed that:; - the `loadlock` file is created and removed: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/b0b9c7a1-70fd-4d44-a76e-b5604a5068f0; - the task fails if the lock file is present: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/293687f9-e7b9-474b-bfe8-e50f4c555199",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7138
https://github.com/broadinstitute/gatk/pull/7138:700,Performance,load,loadlock,700,"spec ops issue #248. process implemented here:; - new task `SetLoadLock` is called at the beginning of `ImportGenomes` - it generates a UUID for the submission, writes that run_uuid to a lock file, and uploads that lock file to the output_directory (where the tsvs will be generated). ; - CreateImportTsvs and LoadTables take the run_uuid as an input, compare it against the contents of the lock file in the bucket, and only proceed if the uuids match. otherwise they exit out.; - after all LoadTables tasks have completed, a new task `ReleaseLoadLock` is called that removes the lock file from the bucket (again only if the uuid in the lockfile matches this run). tested and confirmed that:; - the `loadlock` file is created and removed: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/b0b9c7a1-70fd-4d44-a76e-b5604a5068f0; - the task fails if the lock file is present: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/293687f9-e7b9-474b-bfe8-e50f4c555199",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7138
https://github.com/broadinstitute/gatk/pull/7138:665,Testability,test,tested,665,"spec ops issue #248. process implemented here:; - new task `SetLoadLock` is called at the beginning of `ImportGenomes` - it generates a UUID for the submission, writes that run_uuid to a lock file, and uploads that lock file to the output_directory (where the tsvs will be generated). ; - CreateImportTsvs and LoadTables take the run_uuid as an input, compare it against the contents of the lock file in the bucket, and only proceed if the uuids match. otherwise they exit out.; - after all LoadTables tasks have completed, a new task `ReleaseLoadLock` is called that removes the lock file from the bucket (again only if the uuid in the lockfile matches this run). tested and confirmed that:; - the `loadlock` file is created and removed: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/b0b9c7a1-70fd-4d44-a76e-b5604a5068f0; - the task fails if the lock file is present: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/293687f9-e7b9-474b-bfe8-e50f4c555199",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7138
https://github.com/broadinstitute/gatk/issues/7139:195,Availability,error,error,195,"## Bug Report. ### Affected tool(s) or class(es); ApplyBQSRSpark. ### Affected version(s); gatk-4.1.9.0. ### Description ; After roughly 1h of running ApplyBQSRSpark on my WGS BAM, it throws the error `java.io.IOException: Bad file descriptor`.; I then used `samtools quickcheck` to validate the integrity of my BAM file and everything seems fine. The BAM file is coordinate sorted and the duplicates are marked. The size of the BAM file is 142GB. #### Steps to reproduce; This is the command I used:. ```bash; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-package-4.1.9.0-local.jar ApplyBQSRSpark -R ref-genome.fa -I buffy_coat.sorted.markdup.bam --spark-master local[45] --tmp-dirtmp --bqsr-recal-file buffy_coat_recal_bqsr.table -O buffy_coat.recal.bam; ```. Any help is much appreciated!. Cheers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7139
https://github.com/broadinstitute/gatk/issues/7139:283,Security,validat,validate,283,"## Bug Report. ### Affected tool(s) or class(es); ApplyBQSRSpark. ### Affected version(s); gatk-4.1.9.0. ### Description ; After roughly 1h of running ApplyBQSRSpark on my WGS BAM, it throws the error `java.io.IOException: Bad file descriptor`.; I then used `samtools quickcheck` to validate the integrity of my BAM file and everything seems fine. The BAM file is coordinate sorted and the duplicates are marked. The size of the BAM file is 142GB. #### Steps to reproduce; This is the command I used:. ```bash; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-package-4.1.9.0-local.jar ApplyBQSRSpark -R ref-genome.fa -I buffy_coat.sorted.markdup.bam --spark-master local[45] --tmp-dirtmp --bqsr-recal-file buffy_coat_recal_bqsr.table -O buffy_coat.recal.bam; ```. Any help is much appreciated!. Cheers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7139
https://github.com/broadinstitute/gatk/issues/7139:296,Security,integrity,integrity,296,"## Bug Report. ### Affected tool(s) or class(es); ApplyBQSRSpark. ### Affected version(s); gatk-4.1.9.0. ### Description ; After roughly 1h of running ApplyBQSRSpark on my WGS BAM, it throws the error `java.io.IOException: Bad file descriptor`.; I then used `samtools quickcheck` to validate the integrity of my BAM file and everything seems fine. The BAM file is coordinate sorted and the duplicates are marked. The size of the BAM file is 142GB. #### Steps to reproduce; This is the command I used:. ```bash; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-package-4.1.9.0-local.jar ApplyBQSRSpark -R ref-genome.fa -I buffy_coat.sorted.markdup.bam --spark-master local[45] --tmp-dirtmp --bqsr-recal-file buffy_coat_recal_bqsr.table -O buffy_coat.recal.bam; ```. Any help is much appreciated!. Cheers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7139
https://github.com/broadinstitute/gatk/pull/7140:79,Deployability,update,update,79,add in service account auth for aou; localize with service account in same vm; update disk size; fix input_vcf to work with manual localization and streaming,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7140
https://github.com/broadinstitute/gatk/pull/7141:261,Testability,test,tests,261,In #6991 I introduced an option for HC to not print artificial haplotypes in the bamout. I overlooked the fact that when this option is enabled we should not include the read group for the artificial haplotypes in the header. The present PR fixes this and adds tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7141
https://github.com/broadinstitute/gatk/issues/7144:301,Availability,down,down,301,"HaplotypeCaller discard reads in active regions that do not pass a small set QC criteria. These reads are then not used for assembly nor to calculate the likelihoods that we use for GT and QUAL and other call quality annotations. However these reads are incorporated back into the data that is passed down to vcf record annotators. . This might be ok under some circumstances but arguably most user would like to see consistency between ; PL and AD or DP,. . Moreover there is an argument that seem that should be included in order to add those filtered reads:; ```---use-filtered-reads-for-annotations```; However its doc line indicates that it means those reads excluded to correct for contamination. The question here is whether should we revise this behavior and make it default to NOT include them instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7144
https://github.com/broadinstitute/gatk/issues/7147:325,Availability,ERROR,ERROR,325,"I have already get "".g.vcf "" through ""gatk Haplotype Caller"" but when I used the code ""./gatk GenotypeGVCFs -R /Users/lubo/sorgum/GCF_000003195.3_Sorghum_bicolor_NCBIv3_genomic.fna. ; -V /Users/lubo/sorgum/propinquum_variation.g.vcf; -O /Users/lubo/sorgum/propinquum.vcf"" to generate the output file ""propinquum.vcf"" ,A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 11733; please use the Haplotype Caller with gVCF output to generate appropriate records ; I don't know what's wrong with my code ,is that mean my input file have something wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7147
https://github.com/broadinstitute/gatk/pull/7148:131,Deployability,integrat,integration,131,It seems fairly rare that the PL array is truly uninformative and consequently would be removed based on the fact that none of the integration tests seem to have failed as a result of this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7148
https://github.com/broadinstitute/gatk/pull/7148:131,Integrability,integrat,integration,131,It seems fairly rare that the PL array is truly uninformative and consequently would be removed based on the fact that none of the integration tests seem to have failed as a result of this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7148
https://github.com/broadinstitute/gatk/pull/7148:143,Testability,test,tests,143,It seems fairly rare that the PL array is truly uninformative and consequently would be removed based on the fact that none of the integration tests seem to have failed as a result of this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7148
https://github.com/broadinstitute/gatk/pull/7149:0,Deployability,Update,Update,0,Update to htsjdk 2.24.1. This fixes a gross issue where we accidentally included Junit as a runtime dependency.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7149
https://github.com/broadinstitute/gatk/pull/7149:100,Integrability,depend,dependency,100,Update to htsjdk 2.24.1. This fixes a gross issue where we accidentally included Junit as a runtime dependency.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7149
https://github.com/broadinstitute/gatk/issues/7152:395,Testability,test,testing,395,## Bug Report. ### Affected tool(s) or class(es); CalibrateDragstrModel. ### Affected version(s); Custom build: us.gcr.io/broad-dsde-methods/broad-gatk-snapshots/dragen_final_test_v2. ### Description ; An exception occurs on some AoU crams when running CalibrateDragstrModel; We have tried reindexing the cram.; This data is not public but @ahaessly can be a resource to help with debugging and testing. htsjdk.samtools.cram.CRAMException: Attempt to unmapped with non zero alignment start (0) or span (-2147483647); 	at htsjdk.samtools.cram.BAIEntry.<init>(BAIEntry.java:60); 	at htsjdk.samtools.cram.BAIEntry.<init>(BAIEntry.java:83); 	at htsjdk.samtools.cram.CRAIIndex.openCraiFileAsBaiStream(CRAIIndex.java:89); 	at htsjdk.samtools.SamIndexes.asBaiSeekableStreamOrNull(SamIndexes.java:91); 	at htsjdk.samtools.CRAMFileReader.initWithStreams(CRAMFileReader.java:202); 	at htsjdk.samtools.CRAMFileReader.<init>(CRAMFileReader.java:193); 	at htsjdk.samtools.SamReaderFactory$SamReaderFactoryImpl.open(SamReaderFactory.java:422); 	at htsjdk.samtools.SamReaderFactory.open(SamReaderFactory.java:105); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.<init>(ReadsPathDataSource.java:245); 	at org.broadinstitute.hellbender.engine.ReadsPathDataSource.<init>(ReadsPathDataSource.java:181); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeReads(GATKTool.java:459); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:708); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.onStartup(CalibrateDragstrModel.java:108); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hell,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7152
https://github.com/broadinstitute/gatk/issues/7153:417,Availability,down,down,417,"### Instructions. ## Bug Report; ### Affected tool(s) or class(es); - tools: HaplotypeCaller perhaps Mutec. ; - classes: AlleleLikelihoods. ### Affected version(s); - [ X] Latest public release version [version?]; - [ X] Latest master branch as of [date of test?]. ### Description ; Right before calling annotators HC engine adds filtered reads as additional evidence in the AlleleLikelihoods instance that is passed down to the annotators. The code requests the new evidence to have 0.0 likelihoods so label them as uninformative. However due to an error in how the lk arrays are ""extended"" inside the AlleleLikelihoods these reads inherit past reads (removed) zombie likelihoods instead. Fix is easy. as simple as remove this enclosing ```if``` in AlleleLikelihoods, and simply executed its body; always:. ```; line 793:; if (initialLikelihood != 0.0) // the default array new value.; {; for (int a = 0; a < alleleCount; a++) {; Arrays.fill(sampleValues[a], sampleEvidenceCount, newSampleEvidenceCount, initialLikelihood);; }; }; ```. #### Steps to reproduce. Debug and active region with filtered reads. . #### Expected behavior. Those reads won't contribute to AD or DP. #### Actual behavior. They do contribute, at random, to those count annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7153
https://github.com/broadinstitute/gatk/issues/7153:550,Availability,error,error,550,"### Instructions. ## Bug Report; ### Affected tool(s) or class(es); - tools: HaplotypeCaller perhaps Mutec. ; - classes: AlleleLikelihoods. ### Affected version(s); - [ X] Latest public release version [version?]; - [ X] Latest master branch as of [date of test?]. ### Description ; Right before calling annotators HC engine adds filtered reads as additional evidence in the AlleleLikelihoods instance that is passed down to the annotators. The code requests the new evidence to have 0.0 likelihoods so label them as uninformative. However due to an error in how the lk arrays are ""extended"" inside the AlleleLikelihoods these reads inherit past reads (removed) zombie likelihoods instead. Fix is easy. as simple as remove this enclosing ```if``` in AlleleLikelihoods, and simply executed its body; always:. ```; line 793:; if (initialLikelihood != 0.0) // the default array new value.; {; for (int a = 0; a < alleleCount; a++) {; Arrays.fill(sampleValues[a], sampleEvidenceCount, newSampleEvidenceCount, initialLikelihood);; }; }; ```. #### Steps to reproduce. Debug and active region with filtered reads. . #### Expected behavior. Those reads won't contribute to AD or DP. #### Actual behavior. They do contribute, at random, to those count annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7153
https://github.com/broadinstitute/gatk/issues/7153:186,Deployability,release,release,186,"### Instructions. ## Bug Report; ### Affected tool(s) or class(es); - tools: HaplotypeCaller perhaps Mutec. ; - classes: AlleleLikelihoods. ### Affected version(s); - [ X] Latest public release version [version?]; - [ X] Latest master branch as of [date of test?]. ### Description ; Right before calling annotators HC engine adds filtered reads as additional evidence in the AlleleLikelihoods instance that is passed down to the annotators. The code requests the new evidence to have 0.0 likelihoods so label them as uninformative. However due to an error in how the lk arrays are ""extended"" inside the AlleleLikelihoods these reads inherit past reads (removed) zombie likelihoods instead. Fix is easy. as simple as remove this enclosing ```if``` in AlleleLikelihoods, and simply executed its body; always:. ```; line 793:; if (initialLikelihood != 0.0) // the default array new value.; {; for (int a = 0; a < alleleCount; a++) {; Arrays.fill(sampleValues[a], sampleEvidenceCount, newSampleEvidenceCount, initialLikelihood);; }; }; ```. #### Steps to reproduce. Debug and active region with filtered reads. . #### Expected behavior. Those reads won't contribute to AD or DP. #### Actual behavior. They do contribute, at random, to those count annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7153
https://github.com/broadinstitute/gatk/issues/7153:582,Modifiability,extend,extended,582,"### Instructions. ## Bug Report; ### Affected tool(s) or class(es); - tools: HaplotypeCaller perhaps Mutec. ; - classes: AlleleLikelihoods. ### Affected version(s); - [ X] Latest public release version [version?]; - [ X] Latest master branch as of [date of test?]. ### Description ; Right before calling annotators HC engine adds filtered reads as additional evidence in the AlleleLikelihoods instance that is passed down to the annotators. The code requests the new evidence to have 0.0 likelihoods so label them as uninformative. However due to an error in how the lk arrays are ""extended"" inside the AlleleLikelihoods these reads inherit past reads (removed) zombie likelihoods instead. Fix is easy. as simple as remove this enclosing ```if``` in AlleleLikelihoods, and simply executed its body; always:. ```; line 793:; if (initialLikelihood != 0.0) // the default array new value.; {; for (int a = 0; a < alleleCount; a++) {; Arrays.fill(sampleValues[a], sampleEvidenceCount, newSampleEvidenceCount, initialLikelihood);; }; }; ```. #### Steps to reproduce. Debug and active region with filtered reads. . #### Expected behavior. Those reads won't contribute to AD or DP. #### Actual behavior. They do contribute, at random, to those count annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7153
https://github.com/broadinstitute/gatk/issues/7153:633,Modifiability,inherit,inherit,633,"### Instructions. ## Bug Report; ### Affected tool(s) or class(es); - tools: HaplotypeCaller perhaps Mutec. ; - classes: AlleleLikelihoods. ### Affected version(s); - [ X] Latest public release version [version?]; - [ X] Latest master branch as of [date of test?]. ### Description ; Right before calling annotators HC engine adds filtered reads as additional evidence in the AlleleLikelihoods instance that is passed down to the annotators. The code requests the new evidence to have 0.0 likelihoods so label them as uninformative. However due to an error in how the lk arrays are ""extended"" inside the AlleleLikelihoods these reads inherit past reads (removed) zombie likelihoods instead. Fix is easy. as simple as remove this enclosing ```if``` in AlleleLikelihoods, and simply executed its body; always:. ```; line 793:; if (initialLikelihood != 0.0) // the default array new value.; {; for (int a = 0; a < alleleCount; a++) {; Arrays.fill(sampleValues[a], sampleEvidenceCount, newSampleEvidenceCount, initialLikelihood);; }; }; ```. #### Steps to reproduce. Debug and active region with filtered reads. . #### Expected behavior. Those reads won't contribute to AD or DP. #### Actual behavior. They do contribute, at random, to those count annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7153
https://github.com/broadinstitute/gatk/issues/7153:257,Testability,test,test,257,"### Instructions. ## Bug Report; ### Affected tool(s) or class(es); - tools: HaplotypeCaller perhaps Mutec. ; - classes: AlleleLikelihoods. ### Affected version(s); - [ X] Latest public release version [version?]; - [ X] Latest master branch as of [date of test?]. ### Description ; Right before calling annotators HC engine adds filtered reads as additional evidence in the AlleleLikelihoods instance that is passed down to the annotators. The code requests the new evidence to have 0.0 likelihoods so label them as uninformative. However due to an error in how the lk arrays are ""extended"" inside the AlleleLikelihoods these reads inherit past reads (removed) zombie likelihoods instead. Fix is easy. as simple as remove this enclosing ```if``` in AlleleLikelihoods, and simply executed its body; always:. ```; line 793:; if (initialLikelihood != 0.0) // the default array new value.; {; for (int a = 0; a < alleleCount; a++) {; Arrays.fill(sampleValues[a], sampleEvidenceCount, newSampleEvidenceCount, initialLikelihood);; }; }; ```. #### Steps to reproduce. Debug and active region with filtered reads. . #### Expected behavior. Those reads won't contribute to AD or DP. #### Actual behavior. They do contribute, at random, to those count annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7153
https://github.com/broadinstitute/gatk/issues/7153:706,Usability,simpl,simple,706,"### Instructions. ## Bug Report; ### Affected tool(s) or class(es); - tools: HaplotypeCaller perhaps Mutec. ; - classes: AlleleLikelihoods. ### Affected version(s); - [ X] Latest public release version [version?]; - [ X] Latest master branch as of [date of test?]. ### Description ; Right before calling annotators HC engine adds filtered reads as additional evidence in the AlleleLikelihoods instance that is passed down to the annotators. The code requests the new evidence to have 0.0 likelihoods so label them as uninformative. However due to an error in how the lk arrays are ""extended"" inside the AlleleLikelihoods these reads inherit past reads (removed) zombie likelihoods instead. Fix is easy. as simple as remove this enclosing ```if``` in AlleleLikelihoods, and simply executed its body; always:. ```; line 793:; if (initialLikelihood != 0.0) // the default array new value.; {; for (int a = 0; a < alleleCount; a++) {; Arrays.fill(sampleValues[a], sampleEvidenceCount, newSampleEvidenceCount, initialLikelihood);; }; }; ```. #### Steps to reproduce. Debug and active region with filtered reads. . #### Expected behavior. Those reads won't contribute to AD or DP. #### Actual behavior. They do contribute, at random, to those count annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7153
https://github.com/broadinstitute/gatk/issues/7153:773,Usability,simpl,simply,773,"### Instructions. ## Bug Report; ### Affected tool(s) or class(es); - tools: HaplotypeCaller perhaps Mutec. ; - classes: AlleleLikelihoods. ### Affected version(s); - [ X] Latest public release version [version?]; - [ X] Latest master branch as of [date of test?]. ### Description ; Right before calling annotators HC engine adds filtered reads as additional evidence in the AlleleLikelihoods instance that is passed down to the annotators. The code requests the new evidence to have 0.0 likelihoods so label them as uninformative. However due to an error in how the lk arrays are ""extended"" inside the AlleleLikelihoods these reads inherit past reads (removed) zombie likelihoods instead. Fix is easy. as simple as remove this enclosing ```if``` in AlleleLikelihoods, and simply executed its body; always:. ```; line 793:; if (initialLikelihood != 0.0) // the default array new value.; {; for (int a = 0; a < alleleCount; a++) {; Arrays.fill(sampleValues[a], sampleEvidenceCount, newSampleEvidenceCount, initialLikelihood);; }; }; ```. #### Steps to reproduce. Debug and active region with filtered reads. . #### Expected behavior. Those reads won't contribute to AD or DP. #### Actual behavior. They do contribute, at random, to those count annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7153
https://github.com/broadinstitute/gatk/pull/7154:122,Modifiability,Refactor,Refactor,122,"involving ""zombie"" likelihoods from past removed evidences being assigned to new appended evidences in AlleleLikelihoods. Refactor and clean some code as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7154
https://github.com/broadinstitute/gatk/issues/7155:463,Availability,error,errors,463,"## Feature request. ### Tool(s) or class(es) involved; DepthOfCoverage --intervals parameter. ### Description; Please supply an example of how intervals should be used. I just want to run DepthOfCoverage over the entire reference sequence (imho this should be the default and make --include optional), but cant work out how to do this. I tried. DepthOfCoverage other_parameters --intervals 1-30000; DepthOfCoverage other_parameters --intervals 1:30000. Both gave errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7155
https://github.com/broadinstitute/gatk/issues/7156:579,Energy Efficiency,efficient,efficient,579,"Our H.P.C. administrator noted that. > Each of the tasks is launching a large number of threads  89, to be precise  even though the task as a whole is bound to a single core (because that's what you're asking for). Moreover, I know you said that GATK is single threaded, however those extra threads are definitely not completely idle, and so it is fairly busy constantly context switching between these threads  the one I was watching registered well over 100000 context switches in the one minute that I was observing it. Can MuTect2 be redesigned to be more computationally efficient and spend less time switching threads?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7156
https://github.com/broadinstitute/gatk/issues/7158:17995,Availability,down,down,17995,"| || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || | ; 12:11:32.828 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_| ; 12:11:32.828 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_) ; 12:11:32.828 WARN Funcotator - |___/ ; 12:11:32.828 WARN Funcotator - --------------------------------------------------------------------------------; 12:11:32.828 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this ; 12:11:32.828 WARN Funcotator - run was misconfigured. ; 12:11:32.828 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 12:11:32.828 WARN Funcotator - ================================================================================; 12:11:32.829 INFO VcfFuncotationFactory - ClinVar_VCF 20180401 cache hits/total: 0/0; 12:11:32.829 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; 12:11:32.830 INFO Funcotator - Shutting down engine; [March 24, 2021 12:11:32 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.22 minutes.; Runtime.totalMemory()=1793064960; Tool returned:; true; (gatk) root@75181703d894:/gatk# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-di",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:19452,Deployability,update,updates,19452,"# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-dictionary-validation true --remove-filtered-variants false --five-prime-flank-size 5000 --three-prime-flank-size 0 --force-b37-to-hg19-reference-contig-conversion false --transcript-selection-mode CANONICAL --lookahead-cache-bp 100000 --min-num-bases-for-segment-funcotation 150 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.2.0.0"",Date=""March 24, 2021 12:11:32 PM GMT"">; ## Funcotator 4.2.0.0 | Date 20211124T121132 | Gencode 34 CANONICAL | Achilles 110303 | CGC full_2012_03-15 | ClinVar 12.03.20 | C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:11916,Modifiability,config,config,11916,; 12:11:28.277 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.426 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.771 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.877 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; 12:11:28.882 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.883 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.config; 12:11:28.905 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.906 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; WARNING 2021-03-24 12:11:28 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 12:11:28.910 INFO DataSourceUtils - Resolved data source file path: file:///gatk/cosmic_tissue.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cosmic_tissue/hg19/cosmic_tissue.tsv; 12:11:28.930 INFO DataSourceUtils - Resolved data source file path: file:///gatk/cosmic_fusion.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cosmic_fusion/hg19/cosmic_fusion.tsv; 12:11:28.932 INFO DataSourceUtils - R,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:13759,Modifiability,config,config,13759,/cosmic_fusion.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cosmic_fusion/hg19/cosmic_fusion.tsv; 12:11:28.932 INFO DataSourceUtils - Resolved data source file path: file:///gatk/gencode_xhgnc_v75_37.hg19.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/gencode_xhgnc/hg19/gencode_xhgnc_v75_37.hg19.tsv; 12:11:29.933 INFO DataSourceUtils - Resolved data source file path: file:///gatk/Cosmic.db -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cosmic/hg19/Cosmic.db; 12:11:30.002 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar : 100000; 12:11:30.004 INFO DataSourceUtils - Resolved data source file path: file:///gatk/clinvar_hgmd.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 12:11:30.005 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.config; 12:11:30.052 INFO DataSourceUtils - Resolved data source file path: file:///gatk/clinvar_hgmd.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 12:11:30.053 INFO DataSourceUtils - Resolved data source file path: file:///gatk/clinvar_hgmd.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; WARNING 2021-03-24 12:11:30 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 12:11:30.054 INFO DataSourceUtils - Resolved data source file path: file:///gatk/dnaRepairGenes.20180524T145835.csv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dna_repair_genes/hg19/dnaRepairGenes.20180524T145835.csv; 12:11:30.055 INFO DataSourceUtils - Resolved data source file path: file:///gatk/simple_uniprot_Dec012014.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/sim,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:2642,Performance,Load,Loading,2642,"-----------------------------------. funcotator output:. (gatk) root@75181703d894:/gatk# ./gatk Funcotator \; > --variant ./my_data/test_b37.vcf \; > --reference ./my_data/human_g1k_v37.fasta \; > --ref-version hg19 \; > --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s \; > --output ./my_data/variants.funcotated.maf \; > --output-file-format MAF \; > --disable-sequence-dictionary-validation; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Funcotator --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output ./my_data/variants.funcotated.maf --output-file-format MAF --disable-sequence-dictionary-validation; 12:11:19.732 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 24, 2021 12:11:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:11:19.904 INFO Funcotator - ------------------------------------------------------------; 12:11:19.904 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:11:19.904 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:11:19.905 INFO Funcotator - Executing as root@75181703d894 on Linux v4.15.0-132-generic amd64; 12:11:19.905 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 12:11:19.905 INFO Funcotator - Start Date/Time: March 24, 2021 12:11:19 PM GMT; 12:11:19.905 INFO Funcotator - ------------------------------------------------------------; 12:11:19.905 INFO Funcot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:9497,Performance,cache,cache,9497,l_2012-03-15.txt -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cancer_gene_census/hg19/CancerGeneCensus_Table_1_full_2012-03-15.txt; 12:11:20.417 INFO DataSourceUtils - Resolved data source file path: file:///gatk/clinvar_20180401.vcf -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 12:11:20.418 INFO DataSourceUtils - Resolved data source file path: file:///gatk/achilles_lineage_results.import.txt -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/achilles/hg19/achilles_lineage_results.import.txt; 12:11:20.419 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; 12:11:20.420 INFO DataSourceUtils - Resolved data source file path: file:///gatk/gencode.v34lift37.annotation.REORDERED.gtf -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/gencode/hg19/gencode.v34lift37.annotation.REORDERED.gtf; 12:11:20.420 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; 12:11:20.452 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/gencode/hg19/gencode.v34lift37.annotation.REORDERED.gtf; 12:11:20.517 INFO DataSourceUtils - Resolved data source file path: file:///gatk/gencode.v34lift37.pc_transcripts.fa -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/gencode/hg19/gencode.v34lift37.pc_transcripts.fa; 12:11:27.980 INFO DataSourceUtils - Resolved data source file path: file:///gatk/Familial_Cancer_Genes.no_dupes.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/familial/hg19/Familial_Cancer_Genes.no_dupes.tsv; 12:11:27.985 INFO DataSourceUtils - Resolved data source file path: file:///gatk/gencode_xrefseq_v75_37.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg19/gencode_xrefseq_v75_37.tsv; 12:11:28.126 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hgnc_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:10887,Performance,cache,cache,10887,taSources.v1.7.20200521s/gencode/hg19/gencode.v34lift37.pc_transcripts.fa; 12:11:27.980 INFO DataSourceUtils - Resolved data source file path: file:///gatk/Familial_Cancer_Genes.no_dupes.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/familial/hg19/Familial_Cancer_Genes.no_dupes.tsv; 12:11:27.985 INFO DataSourceUtils - Resolved data source file path: file:///gatk/gencode_xrefseq_v75_37.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg19/gencode_xrefseq_v75_37.tsv; 12:11:28.126 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hgnc_download_Nov302017.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/hgnc/hg19/hgnc_download_Nov302017.tsv; 12:11:28.270 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.270 INFO DataSourceUtils - Setting lookahead cache for data source: dbSNP : 100000; 12:11:28.277 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.426 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.771 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.877 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; 12:11:28.882 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.883 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.202005,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:11521,Performance,cache,cache,11521,> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/hgnc/hg19/hgnc_download_Nov302017.tsv; 12:11:28.270 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.270 INFO DataSourceUtils - Setting lookahead cache for data source: dbSNP : 100000; 12:11:28.277 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.426 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.771 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.877 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; 12:11:28.882 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.883 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.config; 12:11:28.905 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.906 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; WARNING 2021-03-24 12:11:28 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 12:11:28.910 INFO DataSourceUtils - Re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:13345,Performance,cache,cache,13345,g an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 12:11:28.910 INFO DataSourceUtils - Resolved data source file path: file:///gatk/cosmic_tissue.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cosmic_tissue/hg19/cosmic_tissue.tsv; 12:11:28.930 INFO DataSourceUtils - Resolved data source file path: file:///gatk/cosmic_fusion.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cosmic_fusion/hg19/cosmic_fusion.tsv; 12:11:28.932 INFO DataSourceUtils - Resolved data source file path: file:///gatk/gencode_xhgnc_v75_37.hg19.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/gencode_xhgnc/hg19/gencode_xhgnc_v75_37.hg19.tsv; 12:11:29.933 INFO DataSourceUtils - Resolved data source file path: file:///gatk/Cosmic.db -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cosmic/hg19/Cosmic.db; 12:11:30.002 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar : 100000; 12:11:30.004 INFO DataSourceUtils - Resolved data source file path: file:///gatk/clinvar_hgmd.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 12:11:30.005 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.config; 12:11:30.052 INFO DataSourceUtils - Resolved data source file path: file:///gatk/clinvar_hgmd.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 12:11:30.053 INFO DataSourceUtils - Resolved data source file path: file:///gatk/clinvar_hgmd.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; WARNING 2021-03-24 12:11:30 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:15330,Performance,cache,cache,15330,ressedInputStream; 12:11:30.054 INFO DataSourceUtils - Resolved data source file path: file:///gatk/dnaRepairGenes.20180524T145835.csv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dna_repair_genes/hg19/dnaRepairGenes.20180524T145835.csv; 12:11:30.055 INFO DataSourceUtils - Resolved data source file path: file:///gatk/simple_uniprot_Dec012014.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/simple_uniprot/hg19/simple_uniprot_Dec012014.tsv; 12:11:30.101 INFO DataSourceUtils - Resolved data source file path: file:///gatk/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cancer_gene_census/hg19/CancerGeneCensus_Table_1_full_2012-03-15.txt; 12:11:30.104 INFO DataSourceUtils - Resolved data source file path: file:///gatk/clinvar_20180401.vcf -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 12:11:30.104 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 12:11:30.106 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 12:11:30.163 INFO DataSourceUtils - Resolved data source file path: file:///gatk/clinvar_20180401.vcf -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 12:11:32.523 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 12:11:32.592 INFO DataSourceUtils - Resolved data source file path: file:///gatk/achilles_lineage_results.import.txt -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/achilles/hg19/achilles_lineage_results.import.txt; 12:11:32.594 INFO Funcotator - Initializing Funcotator Engine...; 12:11:32.595 INFO FuncotatorEngine - Using given VCF and Reference. No conversion required.; 12:11:32.595 INFO Funcotator - Creatin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:17851,Performance,cache,cache,17851,"27 WARN Funcotator - _ _ _ __ __ _ _ _ _ ; 12:11:32.827 WARN Funcotator - | || || | \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || | ; 12:11:32.828 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || | ; 12:11:32.828 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_| ; 12:11:32.828 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_) ; 12:11:32.828 WARN Funcotator - |___/ ; 12:11:32.828 WARN Funcotator - --------------------------------------------------------------------------------; 12:11:32.828 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this ; 12:11:32.828 WARN Funcotator - run was misconfigured. ; 12:11:32.828 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 12:11:32.828 WARN Funcotator - ================================================================================; 12:11:32.829 INFO VcfFuncotationFactory - ClinVar_VCF 20180401 cache hits/total: 0/0; 12:11:32.829 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; 12:11:32.830 INFO Funcotator - Shutting down engine; [March 24, 2021 12:11:32 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.22 minutes.; Runtime.totalMemory()=1793064960; Tool returned:; true; (gatk) root@75181703d894:/gatk# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:17932,Performance,cache,cache,17932," \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || | ; 12:11:32.828 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || | ; 12:11:32.828 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_| ; 12:11:32.828 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_) ; 12:11:32.828 WARN Funcotator - |___/ ; 12:11:32.828 WARN Funcotator - --------------------------------------------------------------------------------; 12:11:32.828 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this ; 12:11:32.828 WARN Funcotator - run was misconfigured. ; 12:11:32.828 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 12:11:32.828 WARN Funcotator - ================================================================================; 12:11:32.829 INFO VcfFuncotationFactory - ClinVar_VCF 20180401 cache hits/total: 0/0; 12:11:32.829 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; 12:11:32.830 INFO Funcotator - Shutting down engine; [March 24, 2021 12:11:32 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.22 minutes.; Runtime.totalMemory()=1793064960; Tool returned:; true; (gatk) root@75181703d894:/gatk# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:19223,Performance,cache,cache-bp,19223,"# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-dictionary-validation true --remove-filtered-variants false --five-prime-flank-size 5000 --three-prime-flank-size 0 --force-b37-to-hg19-reference-contig-conversion false --transcript-selection-mode CANONICAL --lookahead-cache-bp 100000 --min-num-bases-for-segment-funcotation 150 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.2.0.0"",Date=""March 24, 2021 12:11:32 PM GMT"">; ## Funcotator 4.2.0.0 | Date 20211124T121132 | Gencode 34 CANONICAL | Achilles 110303 | CGC full_2012_03-15 | ClinVar 12.03.20 | C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:2898,Safety,detect,detect,2898,"ncotator_dataSources.v1.7.20200521s \; > --output ./my_data/variants.funcotated.maf \; > --output-file-format MAF \; > --disable-sequence-dictionary-validation; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Funcotator --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output ./my_data/variants.funcotated.maf --output-file-format MAF --disable-sequence-dictionary-validation; 12:11:19.732 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 24, 2021 12:11:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:11:19.904 INFO Funcotator - ------------------------------------------------------------; 12:11:19.904 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:11:19.904 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:11:19.905 INFO Funcotator - Executing as root@75181703d894 on Linux v4.15.0-132-generic amd64; 12:11:19.905 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 12:11:19.905 INFO Funcotator - Start Date/Time: March 24, 2021 12:11:19 PM GMT; 12:11:19.905 INFO Funcotator - ------------------------------------------------------------; 12:11:19.905 INFO Funcotator - ------------------------------------------------------------; 12:11:19.906 INFO Funcotator - HTSJDK Version: 2.24.0; 12:11:19.906 INFO Funcotator - Picard Version: 2.25.0; 12:11:19.906 INFO Funcotator - Built for Spark Version: 2.4.5; 12:11:19.90",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:2042,Security,validat,validation,2042,".	GAAGA	G,GCA	.	.	.	GT:AD:DP	1/2:0,12,8:20	1/1:0,30,2:32; 5	112174757	.	GAAGA	G,GGA	.	.	.	GT:AD:DP	0/1:12,8,0:20	0/2:30,0,2:32; 5	112174757	.	GAAGA	G,GGA	.	.	.	GT:AD:DP	0/2:12,0,8:20	0/1:30,2,0:32; 6	41903782	.	AG	CA	.	.	.	GT:AD:DP	0/1:28,22:50	0/0:48,0:48; 7	116412043	.	G	C	.	.	.	GT:AD:DP	0/1:25,22:47	0/0:98,1:99; 13	28608242	.	A	AACTCCCATTTGAGATCATATTCATATTCTCTGAAATCAACGTAGAAGTACTCATTACCCCCTCGGGGGG	.	.	.	GT:AD:DP	0/1:10,10:20	0/0:11,0:11; 17	7579312	.	C	A	.	.	.	GT:AD:DP	0/1:20,22:42	0/0:18,1:1. ----------------------------------------------------------------------------------------------------------------------------------------------. funcotator output:. (gatk) root@75181703d894:/gatk# ./gatk Funcotator \; > --variant ./my_data/test_b37.vcf \; > --reference ./my_data/human_g1k_v37.fasta \; > --ref-version hg19 \; > --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s \; > --output ./my_data/variants.funcotated.maf \; > --output-file-format MAF \; > --disable-sequence-dictionary-validation; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Funcotator --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output ./my_data/variants.funcotated.maf --output-file-format MAF --disable-sequence-dictionary-validation; 12:11:19.732 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 24, 2021 12:11:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:11:19.904 INFO Funcotator - ----------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:2590,Security,validat,validation,2590,"--------------------------------------------------------------------------------------------------------. funcotator output:. (gatk) root@75181703d894:/gatk# ./gatk Funcotator \; > --variant ./my_data/test_b37.vcf \; > --reference ./my_data/human_g1k_v37.fasta \; > --ref-version hg19 \; > --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s \; > --output ./my_data/variants.funcotated.maf \; > --output-file-format MAF \; > --disable-sequence-dictionary-validation; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Funcotator --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output ./my_data/variants.funcotated.maf --output-file-format MAF --disable-sequence-dictionary-validation; 12:11:19.732 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 24, 2021 12:11:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:11:19.904 INFO Funcotator - ------------------------------------------------------------; 12:11:19.904 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:11:19.904 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:11:19.905 INFO Funcotator - Executing as root@75181703d894 on Linux v4.15.0-132-generic amd64; 12:11:19.905 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 12:11:19.905 INFO Funcotator - Start Date/Time: March 24, 2021 12:11:19 PM GMT; 12:11:19.905 INFO Funcotator - -----------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:4717,Security,validat,validation,4717,----------; 12:11:19.906 INFO Funcotator - HTSJDK Version: 2.24.0; 12:11:19.906 INFO Funcotator - Picard Version: 2.25.0; 12:11:19.906 INFO Funcotator - Built for Spark Version: 2.4.5; 12:11:19.906 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:11:19.906 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:11:19.906 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:11:19.907 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:11:19.907 INFO Funcotator - Deflater: IntelDeflater; 12:11:19.907 INFO Funcotator - Inflater: IntelInflater; 12:11:19.907 INFO Funcotator - GCS max retries/reopens: 20; 12:11:19.907 INFO Funcotator - Requester pays: disabled; 12:11:19.907 INFO Funcotator - Initializing engine; 12:11:20.348 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/test_b37.vcf; 12:11:20.368 INFO Funcotator - Done initializing engine; 12:11:20.368 INFO Funcotator - Skipping sequence dictionary validation.; 12:11:20.369 INFO Funcotator - Processing user transcripts/defaults/overrides...; 12:11:20.370 INFO Funcotator - Initializing data sources...; 12:11:20.375 INFO DataSourceUtils - Initializing data sources from directory: ./my_data/funcotator_dataSources.v1.7.20200521s; 12:11:20.376 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 12:11:20.376 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 12:11:20.377 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 12:11:20.388 INFO DataSourceUtils - Resolved data source file path: file:///gatk/gencode.v34lift37.annotation.REORDERED.gtf -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/gencode/hg19/gencode.v34lift37.annotation.REORDERED.gtf; 12:11:20.389 INFO DataSourceUtils - Resolved data source fi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:19014,Security,validat,validation,19014,"# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-dictionary-validation true --remove-filtered-variants false --five-prime-flank-size 5000 --three-prime-flank-size 0 --force-b37-to-hg19-reference-contig-conversion false --transcript-selection-mode CANONICAL --lookahead-cache-bp 100000 --min-num-bases-for-segment-funcotation 150 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.2.0.0"",Date=""March 24, 2021 12:11:32 PM GMT"">; ## Funcotator 4.2.0.0 | Date 20211124T121132 | Gencode 34 CANONICAL | Achilles 110303 | CGC full_2012_03-15 | ClinVar 12.03.20 | C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7158:19396,Security,validat,validation-stringency,19396,"# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-dictionary-validation true --remove-filtered-variants false --five-prime-flank-size 5000 --three-prime-flank-size 0 --force-b37-to-hg19-reference-contig-conversion false --transcript-selection-mode CANONICAL --lookahead-cache-bp 100000 --min-num-bases-for-segment-funcotation 150 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.2.0.0"",Date=""March 24, 2021 12:11:32 PM GMT"">; ## Funcotator 4.2.0.0 | Date 20211124T121132 | Gencode 34 CANONICAL | Achilles 110303 | CGC full_2012_03-15 | ClinVar 12.03.20 | C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158
https://github.com/broadinstitute/gatk/issues/7161:157,Deployability,release,release,157,"## Bug Report. ### Affected tool(s) or class(es); `GATK MarkDuplicatesSpark`; `GATK EstimateLibraryComplexity`. ### Affected version(s); - [x] Latest public release version 4.2.0.0. ### Description ; The metrics from `GATK MarkDuplicatesSpark` and `GATK EstimateLibraryComplexity` do not match, even though those from `GATK MarkDuplicatesSpark` are the same as `Picard MarkDuplicatesWithMateCigar`. #### Expected behavior; I'd expect that the metrics from `GATK MarkDuplicatesSpark` and `GATK EstimateLibraryComplexity` would be the same, since [here](https://gatk.broadinstitute.org/hc/en-us/articles/360050814112-MarkDuplicatesSpark) recommends to run `GATK MarkDuplicatesSpark` without metrics (it is faster) and run `GATK EstimateLibraryComplexity` afterwards. #### Actual behavior. EstimateLibraryComplexity ; ```; ## htsjdk.samtools.metrics.StringHeader; # EstimateLibraryComplexity INPUT=[temp/align/bwa_aln/c_lib1_L001.sorted.bam] OUTPUT=stats/align/estimate_library_complexity/c_lib1.metrics.txt MIN_IDENTICAL_BASES=5 MAX_DIFF_RATE=0.03 MIN_MEAN_QUALITY=20 MAX_GROUP_RATIO=500 MAX_READ_LENGTH=0 MIN_GROUP_COUNT=2 READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=2279706 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; ## htsjdk.samtools.metrics.StringHeader; # Started on: Wed Mar 24 21:31:32 CET 2021. ## METRICS CLASS picard.sam.DuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown 0 9951 0 0 0 0 0 0. ## HISTOGRAM java.lang.Integer; duplication_group_count Unknown; 1 9951; ```. MarkDuplicatesSpark; ```; #",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161
https://github.com/broadinstitute/gatk/issues/7161:2944,Energy Efficiency,reduce,reducers,2944,"READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown 0 9951 0 0 0 0 0 0. ## HISTOGRAM java.lang.Integer; duplication_group_count Unknown; 1 9951; ```. MarkDuplicatesSpark; ```; ## htsjdk.samtools.metrics.StringHeader; # MarkDuplicatesSpark --output temp/align/markduplicates/c_lib1.bam --metrics-file stats/align/markduplicates/c_lib1.metrics.txt --input temp/align/bwa_aln/c_lib1_L001.sorted.bam --read-validation-stringency LENIENT --spark-master local[8] --allow-multiple-sort-orders-in-input false --treat-unsorted-as-querygroup-ordered false --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --duplicate-tagging-policy DontTag --remove-all-duplicates false --remove-sequencing-duplicates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --use-nio false --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --create-output-bam-index true --create-output-bam-splitting-index true --splitting-index-granularity 4096 --create-output-variant-index true --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false; ## htsjdk.samtools.metrics.StringHeader; # Started on: March 24, 2021 9:31:36 PM CET. ## METRICS CLASS org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown Library 0 9998 0 0 0 0 0 0; ```. MarkDuplica",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161
https://github.com/broadinstitute/gatk/issues/7161:1139,Performance,optimiz,optimized,1139,"EstimateLibraryComplexity` do not match, even though those from `GATK MarkDuplicatesSpark` are the same as `Picard MarkDuplicatesWithMateCigar`. #### Expected behavior; I'd expect that the metrics from `GATK MarkDuplicatesSpark` and `GATK EstimateLibraryComplexity` would be the same, since [here](https://gatk.broadinstitute.org/hc/en-us/articles/360050814112-MarkDuplicatesSpark) recommends to run `GATK MarkDuplicatesSpark` without metrics (it is faster) and run `GATK EstimateLibraryComplexity` afterwards. #### Actual behavior. EstimateLibraryComplexity ; ```; ## htsjdk.samtools.metrics.StringHeader; # EstimateLibraryComplexity INPUT=[temp/align/bwa_aln/c_lib1_L001.sorted.bam] OUTPUT=stats/align/estimate_library_complexity/c_lib1.metrics.txt MIN_IDENTICAL_BASES=5 MAX_DIFF_RATE=0.03 MIN_MEAN_QUALITY=20 MAX_GROUP_RATIO=500 MAX_READ_LENGTH=0 MIN_GROUP_COUNT=2 READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=2279706 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; ## htsjdk.samtools.metrics.StringHeader; # Started on: Wed Mar 24 21:31:32 CET 2021. ## METRICS CLASS picard.sam.DuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown 0 9951 0 0 0 0 0 0. ## HISTOGRAM java.lang.Integer; duplication_group_count Unknown; 1 9951; ```. MarkDuplicatesSpark; ```; ## htsjdk.samtools.metrics.StringHeader; # MarkDuplicatesSpark --output temp/align/markduplicates/c_lib1.bam --metrics-file stats/align/markduplicates/c_lib1.metrics.txt --input temp/align/bwa_aln/c_lib1_L001.sorted.bam --read-validation-stringency LENIEN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161
https://github.com/broadinstitute/gatk/issues/7161:2577,Performance,optimiz,optimized,2577,"READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown 0 9951 0 0 0 0 0 0. ## HISTOGRAM java.lang.Integer; duplication_group_count Unknown; 1 9951; ```. MarkDuplicatesSpark; ```; ## htsjdk.samtools.metrics.StringHeader; # MarkDuplicatesSpark --output temp/align/markduplicates/c_lib1.bam --metrics-file stats/align/markduplicates/c_lib1.metrics.txt --input temp/align/bwa_aln/c_lib1_L001.sorted.bam --read-validation-stringency LENIENT --spark-master local[8] --allow-multiple-sort-orders-in-input false --treat-unsorted-as-querygroup-ordered false --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --duplicate-tagging-policy DontTag --remove-all-duplicates false --remove-sequencing-duplicates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --use-nio false --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --create-output-bam-index true --create-output-bam-splitting-index true --splitting-index-granularity 4096 --create-output-variant-index true --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false; ## htsjdk.samtools.metrics.StringHeader; # Started on: March 24, 2021 9:31:36 PM CET. ## METRICS CLASS org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown Library 0 9998 0 0 0 0 0 0; ```. MarkDuplica",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161
https://github.com/broadinstitute/gatk/issues/7161:4375,Performance,optimiz,optimized,4375,"ersion false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false; ## htsjdk.samtools.metrics.StringHeader; # Started on: March 24, 2021 9:31:36 PM CET. ## METRICS CLASS org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown Library 0 9998 0 0 0 0 0 0; ```. MarkDuplicatesWithMateCigar ; ```; ## htsjdk.samtools.metrics.StringHeader; # MarkDuplicatesWithMateCigar INPUT=[temp/align/bwa_aln/c_lib1_L001.sorted.bam] OUTPUT=temp/align/markduplicateswithmatecigar/c_lib1.bam METRICS_FILE=stats/align/markduplicateswithmatecigar/c_lib1.metrics.txt VALIDATION_STRINGENCY=LENIENT MINIMUM_DISTANCE=-1 SKIP_PAIRS_WITH_NO_MATE_CIGAR=true BLOCK_SIZE=100000 ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=TOTAL_MAPPED_REFERENCE_LENGTH PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicatesWithMateCigar READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; ## htsjdk.samtools.metrics.StringHeader; # Started on: Wed Mar 24 21:49:06 CET 2021. ## METRICS CLASS picard.sam.DuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown Library 0 9998 0 0 0 0 0 0; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161
https://github.com/broadinstitute/gatk/issues/7161:2227,Security,validat,validation-stringency,2227,"READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown 0 9951 0 0 0 0 0 0. ## HISTOGRAM java.lang.Integer; duplication_group_count Unknown; 1 9951; ```. MarkDuplicatesSpark; ```; ## htsjdk.samtools.metrics.StringHeader; # MarkDuplicatesSpark --output temp/align/markduplicates/c_lib1.bam --metrics-file stats/align/markduplicates/c_lib1.metrics.txt --input temp/align/bwa_aln/c_lib1_L001.sorted.bam --read-validation-stringency LENIENT --spark-master local[8] --allow-multiple-sort-orders-in-input false --treat-unsorted-as-querygroup-ordered false --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --duplicate-tagging-policy DontTag --remove-all-duplicates false --remove-sequencing-duplicates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --use-nio false --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --create-output-bam-index true --create-output-bam-splitting-index true --splitting-index-granularity 4096 --create-output-variant-index true --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false; ## htsjdk.samtools.metrics.StringHeader; # Started on: March 24, 2021 9:31:36 PM CET. ## METRICS CLASS org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown Library 0 9998 0 0 0 0 0 0; ```. MarkDuplica",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161
https://github.com/broadinstitute/gatk/issues/7161:2863,Security,validat,validation,2863,"READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown 0 9951 0 0 0 0 0 0. ## HISTOGRAM java.lang.Integer; duplication_group_count Unknown; 1 9951; ```. MarkDuplicatesSpark; ```; ## htsjdk.samtools.metrics.StringHeader; # MarkDuplicatesSpark --output temp/align/markduplicates/c_lib1.bam --metrics-file stats/align/markduplicates/c_lib1.metrics.txt --input temp/align/bwa_aln/c_lib1_L001.sorted.bam --read-validation-stringency LENIENT --spark-master local[8] --allow-multiple-sort-orders-in-input false --treat-unsorted-as-querygroup-ordered false --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --duplicate-tagging-policy DontTag --remove-all-duplicates false --remove-sequencing-duplicates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --use-nio false --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --create-output-bam-index true --create-output-bam-splitting-index true --splitting-index-granularity 4096 --create-output-variant-index true --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false; ## htsjdk.samtools.metrics.StringHeader; # Started on: March 24, 2021 9:31:36 PM CET. ## METRICS CLASS org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown Library 0 9998 0 0 0 0 0 0; ```. MarkDuplica",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161
https://github.com/broadinstitute/gatk/issues/7162:5132,Availability,Avail,Available,5132," - Inflater: IntelInflater; 08:33:37.135 INFO FilterAlignmentArtifacts - GCS max retries/reopens: 20; 08:33:37.135 INFO FilterAlignmentArtifacts - Requester pays: disabled; 08:33:37.136 WARN FilterAlignmentArtifacts -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 08:33:37.136 INFO FilterAlignmentArtifacts - Initializing engine; 08:33:37.531 INFO FeatureManager - Using codec VCFCodec to read file file:///data/filteredVCF/in2510-8.orientationFilter.vcf; 08:33:37.586 INFO FilterAlignmentArtifacts - Done initializing engine; 08:33:37.668 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 08:33:37.706 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 08:33:37.707 INFO IntelPairHmm - Available threads: 8; 08:33:37.707 INFO IntelPairHmm - Requested threads: 4; 08:33:37.707 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 08:33:37.708 INFO ProgressMeter - Starting traversal; 08:33:37.708 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007ff7b7dfe32d, pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:5483,Availability,error,error,5483,"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 08:33:37.136 INFO FilterAlignmentArtifacts - Initializing engine; 08:33:37.531 INFO FeatureManager - Using codec VCFCodec to read file file:///data/filteredVCF/in2510-8.orientationFilter.vcf; 08:33:37.586 INFO FilterAlignmentArtifacts - Done initializing engine; 08:33:37.668 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 08:33:37.706 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 08:33:37.707 INFO IntelPairHmm - Available threads: 8; 08:33:37.707 INFO IntelPairHmm - Requested threads: 4; 08:33:37.707 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 08:33:37.708 INFO ProgressMeter - Starting traversal; 08:33:37.708 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007ff7b7dfe32d, pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6117,Availability,error,error,6117,"FO IntelPairHmm - Available threads: 8; 08:33:37.707 INFO IntelPairHmm - Requested threads: 4; 08:33:37.707 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 08:33:37.708 INFO ProgressMeter - Starting traversal; 08:33:37.708 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007ff7b7dfe32d, pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.lo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7969,Availability,down,down,7969," [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be needed. Eliminating either the first or last line from the range will make the program work again. Did not attempt to remove lines from the middle of the range yet to see if they're necessary to cause the fault, but it's 2am and I should probably sleep.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:8418,Availability,fault,fault,8418," [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be needed. Eliminating either the first or last line from the range will make the program work again. Did not attempt to remove lines from the middle of the range yet to see if they're necessary to cause the fault, but it's 2am and I should probably sleep.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7759,Deployability,pipeline,pipeline,7759," [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be needed. Eliminating either the first or last line from the range will make the program work again. Did not attempt to remove lines from the middle of the range yet to see if they're necessary to cause the fault, but it's 2am and I should probably sleep.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:1856,Performance,Load,Loading,1856,"VCF/in2510-8.orientationFilter.vcf --input /data/rawVCF/mutectBAM/in2510-8.mutect2.bam --bwa-mem-index-image /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.img --output /data/alignmentArtifactFilteredVCF/in2510-8.orientationFilter.alignmentArtifactFilter.vcf; Using GATK jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterAlignmentArtifacts --reference /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.gz --variant /data/filteredVCF/in2510-8.orientationFilter.vcf --input /data/rawVCF/mutectBAM/in2510-8.mutect2.bam --bwa-mem-index-image /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.img --output /data/alignmentArtifactFilteredVCF/in2510-8.orientationFilter.alignmentArtifactFilter.vcf; 08:33:36.572 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 08:33:36.591 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 08:33:36.592 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 08:33:36.826 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 25, 2021 8:33:37 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 08:33:37.130 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 08:33:37.130 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.9.0-SNAPSHOT; 08:33:37.130 INFO FilterAlignment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:2019,Performance,Load,Loading,2019,"el.fa.img --output /data/alignmentArtifactFilteredVCF/in2510-8.orientationFilter.alignmentArtifactFilter.vcf; Using GATK jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterAlignmentArtifacts --reference /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.gz --variant /data/filteredVCF/in2510-8.orientationFilter.vcf --input /data/rawVCF/mutectBAM/in2510-8.mutect2.bam --bwa-mem-index-image /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.img --output /data/alignmentArtifactFilteredVCF/in2510-8.orientationFilter.alignmentArtifactFilter.vcf; 08:33:36.572 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 08:33:36.591 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 08:33:36.592 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 08:33:36.826 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 25, 2021 8:33:37 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 08:33:37.130 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 08:33:37.130 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.9.0-SNAPSHOT; 08:33:37.130 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 08:33:37.131 INFO FilterAlignmentArtifacts - Executing as gatk@1ff04a9b2ba9 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:2291,Performance,Load,Loading,2291,"-Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterAlignmentArtifacts --reference /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.gz --variant /data/filteredVCF/in2510-8.orientationFilter.vcf --input /data/rawVCF/mutectBAM/in2510-8.mutect2.bam --bwa-mem-index-image /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.img --output /data/alignmentArtifactFilteredVCF/in2510-8.orientationFilter.alignmentArtifactFilter.vcf; 08:33:36.572 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 08:33:36.591 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 08:33:36.592 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 08:33:36.826 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 25, 2021 8:33:37 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 08:33:37.130 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 08:33:37.130 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.9.0-SNAPSHOT; 08:33:37.130 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 08:33:37.131 INFO FilterAlignmentArtifacts - Executing as gatk@1ff04a9b2ba9 on Linux v5.4.72-microsoft-standard-WSL2 amd64; 08:33:37.131 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 08:33:37.131 INFO FilterAlignmentArtifacts - Start Date/Time: March 25, 2021 8:33:36 AM GMT; 08:33",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:4878,Performance,Load,Loading,4878,"aults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:33:37.133 INFO FilterAlignmentArtifacts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:33:37.134 INFO FilterAlignmentArtifacts - Deflater: IntelDeflater; 08:33:37.134 INFO FilterAlignmentArtifacts - Inflater: IntelInflater; 08:33:37.135 INFO FilterAlignmentArtifacts - GCS max retries/reopens: 20; 08:33:37.135 INFO FilterAlignmentArtifacts - Requester pays: disabled; 08:33:37.136 WARN FilterAlignmentArtifacts -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 08:33:37.136 INFO FilterAlignmentArtifacts - Initializing engine; 08:33:37.531 INFO FeatureManager - Using codec VCFCodec to read file file:///data/filteredVCF/in2510-8.orientationFilter.vcf; 08:33:37.586 INFO FilterAlignmentArtifacts - Done initializing engine; 08:33:37.668 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 08:33:37.706 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 08:33:37.707 INFO IntelPairHmm - Available threads: 8; 08:33:37.707 INFO IntelPairHmm - Requested threads: 4; 08:33:37.707 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 08:33:37.708 INFO ProgressMeter - Starting traversal; 08:33:37.708 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007ff7b7dfe32d, pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:5254,Performance,multi-thread,multi-threaded,5254,"rtifacts - Requester pays: disabled; 08:33:37.136 WARN FilterAlignmentArtifacts -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 08:33:37.136 INFO FilterAlignmentArtifacts - Initializing engine; 08:33:37.531 INFO FeatureManager - Using codec VCFCodec to read file file:///data/filteredVCF/in2510-8.orientationFilter.vcf; 08:33:37.586 INFO FilterAlignmentArtifacts - Done initializing engine; 08:33:37.668 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 08:33:37.706 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 08:33:37.707 INFO IntelPairHmm - Available threads: 8; 08:33:37.707 INFO IntelPairHmm - Requested threads: 4; 08:33:37.707 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 08:33:37.708 INFO ProgressMeter - Starting traversal; 08:33:37.708 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007ff7b7dfe32d, pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:2555,Safety,detect,detect,2555,"8.orientationFilter.vcf --input /data/rawVCF/mutectBAM/in2510-8.mutect2.bam --bwa-mem-index-image /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.img --output /data/alignmentArtifactFilteredVCF/in2510-8.orientationFilter.alignmentArtifactFilter.vcf; 08:33:36.572 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 08:33:36.591 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 08:33:36.592 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 08:33:36.826 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 25, 2021 8:33:37 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 08:33:37.130 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 08:33:37.130 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.9.0-SNAPSHOT; 08:33:37.130 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 08:33:37.131 INFO FilterAlignmentArtifacts - Executing as gatk@1ff04a9b2ba9 on Linux v5.4.72-microsoft-standard-WSL2 amd64; 08:33:37.131 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 08:33:37.131 INFO FilterAlignmentArtifacts - Start Date/Time: March 25, 2021 8:33:36 AM GMT; 08:33:37.131 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 08:33:37.132 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 08:33:37.133 INFO FilterAlignmentArtifacts - HTSJDK ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:5498,Safety,detect,detected,5498,"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 08:33:37.136 INFO FilterAlignmentArtifacts - Initializing engine; 08:33:37.531 INFO FeatureManager - Using codec VCFCodec to read file file:///data/filteredVCF/in2510-8.orientationFilter.vcf; 08:33:37.586 INFO FilterAlignmentArtifacts - Done initializing engine; 08:33:37.668 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 08:33:37.706 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 08:33:37.707 INFO IntelPairHmm - Available threads: 8; 08:33:37.707 INFO IntelPairHmm - Requested threads: 4; 08:33:37.707 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 08:33:37.708 INFO ProgressMeter - Starting traversal; 08:33:37.708 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007ff7b7dfe32d, pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6198,Testability,log,log,6198,"g the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 08:33:37.708 INFO ProgressMeter - Starting traversal; 08:33:37.708 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007ff7b7dfe32d, pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://gi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6482,Testability,log,log,6482," been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007ff7b7dfe32d, pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6554,Testability,log,log,6554,"c=0x00007ff7b7dfe32d, pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6575,Testability,log,log,6575," pid=849, tid=0x00007ff82e11d700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6647,Testability,log,log,6647,"ronment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy Gi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6668,Testability,log,log,6668," (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requir",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6740,Testability,log,log,6740,"it Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6761,Testability,log,log,6761,"b08 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6833,Testability,log,log,6833,"libgkl_smithwaterman5951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6854,Testability,log,log,6854,"951765478004985534.so+0x132d] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6926,Testability,log,log,6926,"nt, int, int, int*, int)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:6947,Testability,log,log,6947,"nt)+0x1bd; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7019,Testability,log,log,7019,". To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I incl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7040,Testability,log,log,7040,"ing, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the cras",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7112,Testability,log,log,7112," report file with more information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single l",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7133,Testability,log,log,7133,"e information is saved as:; # /home/gatk/hs_err_pid849.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two tha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7205,Testability,log,log,7205,"would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be nee",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7226,Testability,log,log,7226,"a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be needed. Eliminating eit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7298,Testability,log,log,7298,".jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be needed. Eliminating either the first or last line from the range will make the program work agai",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7319,Testability,log,log,7319,"pened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be needed. Eliminating either the first or last line from the range will make the program work again. Did not attempt t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:7391,Testability,log,log,7391,"ic frame for where to report the bug.; #; ```. **RELEVANT FILES**; [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be needed. Eliminating either the first or last line from the range will make the program work again. Did not attempt to remove lines from the middle of the range yet to see if they're neces",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/issues/7162:8052,Testability,log,logs,8052," [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be needed. Eliminating either the first or last line from the range will make the program work again. Did not attempt to remove lines from the middle of the range yet to see if they're necessary to cause the fault, but it's 2am and I should probably sleep.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162
https://github.com/broadinstitute/gatk/pull/7164:27,Security,secur,security,27,"For ingesting into the aou security boundary, use the optional service account json argument.; This also needs to localize some files manually.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7164
https://github.com/broadinstitute/gatk/pull/7167:39,Performance,load,load,39,"Chunk full list of .tsv files ready to load to bq into sets that are less than the 15tb limit set on each bq load. From the original; datatype_tsvs directory, each set is moved to its own directory, and when the load is complete, the data is moved into a done directory within each set. . Assuming pet tsvs and 1 set, at the start:; gs://bucket/pet_tsvs/pet_001_*. At end:; gs://bucket/pet_tsvs/set_1/done/pet_001_*. --; The output file, `bq_final_job_statuses.txt`, contains the following columns (and example data):; 1. bq load job ID : bqjob_r2715fbcab1fd0e44_00000178708f0abe_1; 2. set number:; 3. path to set data: gs://fc-13e1680e-eb3d-4102-975a-be0142ee9618/full_15tb_test_2/pet_tsvs/set_1/; 4. status of the bq load: SUCCESS/FAIL. What should be the best user experience in case of FAIL?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7167
https://github.com/broadinstitute/gatk/pull/7167:109,Performance,load,load,109,"Chunk full list of .tsv files ready to load to bq into sets that are less than the 15tb limit set on each bq load. From the original; datatype_tsvs directory, each set is moved to its own directory, and when the load is complete, the data is moved into a done directory within each set. . Assuming pet tsvs and 1 set, at the start:; gs://bucket/pet_tsvs/pet_001_*. At end:; gs://bucket/pet_tsvs/set_1/done/pet_001_*. --; The output file, `bq_final_job_statuses.txt`, contains the following columns (and example data):; 1. bq load job ID : bqjob_r2715fbcab1fd0e44_00000178708f0abe_1; 2. set number:; 3. path to set data: gs://fc-13e1680e-eb3d-4102-975a-be0142ee9618/full_15tb_test_2/pet_tsvs/set_1/; 4. status of the bq load: SUCCESS/FAIL. What should be the best user experience in case of FAIL?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7167
https://github.com/broadinstitute/gatk/pull/7167:212,Performance,load,load,212,"Chunk full list of .tsv files ready to load to bq into sets that are less than the 15tb limit set on each bq load. From the original; datatype_tsvs directory, each set is moved to its own directory, and when the load is complete, the data is moved into a done directory within each set. . Assuming pet tsvs and 1 set, at the start:; gs://bucket/pet_tsvs/pet_001_*. At end:; gs://bucket/pet_tsvs/set_1/done/pet_001_*. --; The output file, `bq_final_job_statuses.txt`, contains the following columns (and example data):; 1. bq load job ID : bqjob_r2715fbcab1fd0e44_00000178708f0abe_1; 2. set number:; 3. path to set data: gs://fc-13e1680e-eb3d-4102-975a-be0142ee9618/full_15tb_test_2/pet_tsvs/set_1/; 4. status of the bq load: SUCCESS/FAIL. What should be the best user experience in case of FAIL?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7167
https://github.com/broadinstitute/gatk/pull/7167:525,Performance,load,load,525,"Chunk full list of .tsv files ready to load to bq into sets that are less than the 15tb limit set on each bq load. From the original; datatype_tsvs directory, each set is moved to its own directory, and when the load is complete, the data is moved into a done directory within each set. . Assuming pet tsvs and 1 set, at the start:; gs://bucket/pet_tsvs/pet_001_*. At end:; gs://bucket/pet_tsvs/set_1/done/pet_001_*. --; The output file, `bq_final_job_statuses.txt`, contains the following columns (and example data):; 1. bq load job ID : bqjob_r2715fbcab1fd0e44_00000178708f0abe_1; 2. set number:; 3. path to set data: gs://fc-13e1680e-eb3d-4102-975a-be0142ee9618/full_15tb_test_2/pet_tsvs/set_1/; 4. status of the bq load: SUCCESS/FAIL. What should be the best user experience in case of FAIL?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7167
https://github.com/broadinstitute/gatk/pull/7167:719,Performance,load,load,719,"Chunk full list of .tsv files ready to load to bq into sets that are less than the 15tb limit set on each bq load. From the original; datatype_tsvs directory, each set is moved to its own directory, and when the load is complete, the data is moved into a done directory within each set. . Assuming pet tsvs and 1 set, at the start:; gs://bucket/pet_tsvs/pet_001_*. At end:; gs://bucket/pet_tsvs/set_1/done/pet_001_*. --; The output file, `bq_final_job_statuses.txt`, contains the following columns (and example data):; 1. bq load job ID : bqjob_r2715fbcab1fd0e44_00000178708f0abe_1; 2. set number:; 3. path to set data: gs://fc-13e1680e-eb3d-4102-975a-be0142ee9618/full_15tb_test_2/pet_tsvs/set_1/; 4. status of the bq load: SUCCESS/FAIL. What should be the best user experience in case of FAIL?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7167
https://github.com/broadinstitute/gatk/pull/7167:763,Usability,user experience,user experience,763,"Chunk full list of .tsv files ready to load to bq into sets that are less than the 15tb limit set on each bq load. From the original; datatype_tsvs directory, each set is moved to its own directory, and when the load is complete, the data is moved into a done directory within each set. . Assuming pet tsvs and 1 set, at the start:; gs://bucket/pet_tsvs/pet_001_*. At end:; gs://bucket/pet_tsvs/set_1/done/pet_001_*. --; The output file, `bq_final_job_statuses.txt`, contains the following columns (and example data):; 1. bq load job ID : bqjob_r2715fbcab1fd0e44_00000178708f0abe_1; 2. set number:; 3. path to set data: gs://fc-13e1680e-eb3d-4102-975a-be0142ee9618/full_15tb_test_2/pet_tsvs/set_1/; 4. status of the bq load: SUCCESS/FAIL. What should be the best user experience in case of FAIL?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7167
https://github.com/broadinstitute/gatk/issues/7169:166,Performance,perform,performing,166,"## Feature request. ### Tool(s) or class(es) involved; VariantRecalibrator. ### Description; Currently VariantRecalibrator only accepts vcf as input. Previously when performing Joint Genoptying using GenotypeGVCFs, the outputs were vcf hence this behavior made sense. . Now that we have GenomicsDB import (which is quite fast) we still have to use GenotypeGVCFs to extract a vcf for Variant Recalibrator. So, let's just skip a step and let VariantRecalibrator use GenomicsDB as an input!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7169
https://github.com/broadinstitute/gatk/pull/7170:18,Integrability,depend,dependency,18,We should fix the dependency issue and then re-enable them.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7170
https://github.com/broadinstitute/gatk/issues/7172:64,Integrability,depend,dependencies,64,"The spark dataproc tests seem to be failing, likely due to some dependencies changing on the backend outside of our control. I have disabled these tests temporarily while we sort out the issue. The failing tests are:. `DataprocIntegrationTest.markDuplicatesSparkOnDataproc`; `DataprocIntegrationTest.printReadSparkOnDataproc`. Once this is fixed, these tests should be re-enabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7172
https://github.com/broadinstitute/gatk/issues/7172:19,Testability,test,tests,19,"The spark dataproc tests seem to be failing, likely due to some dependencies changing on the backend outside of our control. I have disabled these tests temporarily while we sort out the issue. The failing tests are:. `DataprocIntegrationTest.markDuplicatesSparkOnDataproc`; `DataprocIntegrationTest.printReadSparkOnDataproc`. Once this is fixed, these tests should be re-enabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7172
https://github.com/broadinstitute/gatk/issues/7172:147,Testability,test,tests,147,"The spark dataproc tests seem to be failing, likely due to some dependencies changing on the backend outside of our control. I have disabled these tests temporarily while we sort out the issue. The failing tests are:. `DataprocIntegrationTest.markDuplicatesSparkOnDataproc`; `DataprocIntegrationTest.printReadSparkOnDataproc`. Once this is fixed, these tests should be re-enabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7172
https://github.com/broadinstitute/gatk/issues/7172:206,Testability,test,tests,206,"The spark dataproc tests seem to be failing, likely due to some dependencies changing on the backend outside of our control. I have disabled these tests temporarily while we sort out the issue. The failing tests are:. `DataprocIntegrationTest.markDuplicatesSparkOnDataproc`; `DataprocIntegrationTest.printReadSparkOnDataproc`. Once this is fixed, these tests should be re-enabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7172
https://github.com/broadinstitute/gatk/issues/7172:353,Testability,test,tests,353,"The spark dataproc tests seem to be failing, likely due to some dependencies changing on the backend outside of our control. I have disabled these tests temporarily while we sort out the issue. The failing tests are:. `DataprocIntegrationTest.markDuplicatesSparkOnDataproc`; `DataprocIntegrationTest.printReadSparkOnDataproc`. Once this is fixed, these tests should be re-enabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7172
https://github.com/broadinstitute/gatk/pull/7174:160,Deployability,Update,Update,160,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/246. ### **Commit Summary**; - Created AvroFileReader ; - Created a sampleAvroFile; - Update ExtractCohort and ExtractCohortEngine to accept a AvroFile. Testing:; Created a test for AvroFileReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7174
https://github.com/broadinstitute/gatk/pull/7174:227,Testability,Test,Testing,227,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/246. ### **Commit Summary**; - Created AvroFileReader ; - Created a sampleAvroFile; - Update ExtractCohort and ExtractCohortEngine to accept a AvroFile. Testing:; Created a test for AvroFileReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7174
https://github.com/broadinstitute/gatk/pull/7174:247,Testability,test,test,247,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/246. ### **Commit Summary**; - Created AvroFileReader ; - Created a sampleAvroFile; - Update ExtractCohort and ExtractCohortEngine to accept a AvroFile. Testing:; Created a test for AvroFileReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7174
https://github.com/broadinstitute/gatk/pull/7175:0,Deployability,update,updated,0,updated the query to calculate num hets and homvars; add in excess het and check threshold,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7175
https://github.com/broadinstitute/gatk/issues/7177:521,Availability,error,errorDepth,521,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/CalculateContamination.java; /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/ContaminationModel.java. ### Affected version(s); - [x] Latest public release version - v4.2.0.0 (also detected on previous versions) ; - [x] Latest master branch as of 03/30/2021. ### Description ; **ContaminationModel**; **Problem:**; Where errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination** , which can be misinterpreted by the end user. calculateContaminationFromHoms receives the list of pileups PileupSummary; It iterates from 0.4 INITIAL_MAF_THRESHOLD down to zero. In each iteration pileups are selected using multiple, different strategies.; When the stdError exit condition is met (i.e., stdError < (contamination* MIN_RELATIVE_ERROR +MIN_ABSOLUTE_ERROR)), it reports out the contamination and stdError values. The issue is that this stdError exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/issues/7177:790,Availability,down,down,790,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/CalculateContamination.java; /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/ContaminationModel.java. ### Affected version(s); - [x] Latest public release version - v4.2.0.0 (also detected on previous versions) ; - [x] Latest master branch as of 03/30/2021. ### Description ; **ContaminationModel**; **Problem:**; Where errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination** , which can be misinterpreted by the end user. calculateContaminationFromHoms receives the list of pileups PileupSummary; It iterates from 0.4 INITIAL_MAF_THRESHOLD down to zero. In each iteration pileups are selected using multiple, different strategies.; When the stdError exit condition is met (i.e., stdError < (contamination* MIN_RELATIVE_ERROR +MIN_ABSOLUTE_ERROR)), it reports out the contamination and stdError values. The issue is that this stdError exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/issues/7177:1811,Availability,error,errorDepth,1811,"n the stdError exit condition is met (i.e., stdError < (contamination* MIN_RELATIVE_ERROR +MIN_ABSOLUTE_ERROR)), it reports out the contamination and stdError values. The issue is that this stdError exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are tested. **contaminationOppositeDepth = Math.max(oppositeDepth - errorDepth, 0);**; **contamination = contaminationOppositeDepth / totalDepthWeightedByOppositeFrequency**. Solution proposed:; Currently, when errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination**; it seems to us that this should instead be interpreted as **unable to calculate contamination** because of the high error rate in these pileups. Improvements for current version:; In addition, we suggest calculating all contamination values over all strategies/iterations and outputting the highest contamination value (by MAF and/or strategy, as appropriate), rather than exiting after the first MAF iteration where stdError exit condition is met. ### Description what needs to be added or modified; Please see the code change attached, compare it to 4.2.0.0. The code ou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/issues/7177:2064,Availability,error,errorDepth,2064,"Error exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are tested. **contaminationOppositeDepth = Math.max(oppositeDepth - errorDepth, 0);**; **contamination = contaminationOppositeDepth / totalDepthWeightedByOppositeFrequency**. Solution proposed:; Currently, when errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination**; it seems to us that this should instead be interpreted as **unable to calculate contamination** because of the high error rate in these pileups. Improvements for current version:; In addition, we suggest calculating all contamination values over all strategies/iterations and outputting the highest contamination value (by MAF and/or strategy, as appropriate), rather than exiting after the first MAF iteration where stdError exit condition is met. ### Description what needs to be added or modified; Please see the code change attached, compare it to 4.2.0.0. The code outputs all pileups used in each iteration and shows the non zero contamination value if exist.; [Contamination.zip](https://github.com/broadinstitute/gatk/files/6232262/Contamination.zip). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/issues/7177:2207,Availability,error,errorDepth,2207,"Error exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are tested. **contaminationOppositeDepth = Math.max(oppositeDepth - errorDepth, 0);**; **contamination = contaminationOppositeDepth / totalDepthWeightedByOppositeFrequency**. Solution proposed:; Currently, when errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination**; it seems to us that this should instead be interpreted as **unable to calculate contamination** because of the high error rate in these pileups. Improvements for current version:; In addition, we suggest calculating all contamination values over all strategies/iterations and outputting the highest contamination value (by MAF and/or strategy, as appropriate), rather than exiting after the first MAF iteration where stdError exit condition is met. ### Description what needs to be added or modified; Please see the code change attached, compare it to 4.2.0.0. The code outputs all pileups used in each iteration and shows the non zero contamination value if exist.; [Contamination.zip](https://github.com/broadinstitute/gatk/files/6232262/Contamination.zip). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/issues/7177:2430,Availability,error,error,2430,"Error exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are tested. **contaminationOppositeDepth = Math.max(oppositeDepth - errorDepth, 0);**; **contamination = contaminationOppositeDepth / totalDepthWeightedByOppositeFrequency**. Solution proposed:; Currently, when errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination**; it seems to us that this should instead be interpreted as **unable to calculate contamination** because of the high error rate in these pileups. Improvements for current version:; In addition, we suggest calculating all contamination values over all strategies/iterations and outputting the highest contamination value (by MAF and/or strategy, as appropriate), rather than exiting after the first MAF iteration where stdError exit condition is met. ### Description what needs to be added or modified; Please see the code change attached, compare it to 4.2.0.0. The code outputs all pileups used in each iteration and shows the non zero contamination value if exist.; [Contamination.zip](https://github.com/broadinstitute/gatk/files/6232262/Contamination.zip). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/issues/7177:348,Deployability,release,release,348,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/CalculateContamination.java; /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/ContaminationModel.java. ### Affected version(s); - [x] Latest public release version - v4.2.0.0 (also detected on previous versions) ; - [x] Latest master branch as of 03/30/2021. ### Description ; **ContaminationModel**; **Problem:**; Where errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination** , which can be misinterpreted by the end user. calculateContaminationFromHoms receives the list of pileups PileupSummary; It iterates from 0.4 INITIAL_MAF_THRESHOLD down to zero. In each iteration pileups are selected using multiple, different strategies.; When the stdError exit condition is met (i.e., stdError < (contamination* MIN_RELATIVE_ERROR +MIN_ABSOLUTE_ERROR)), it reports out the contamination and stdError values. The issue is that this stdError exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/issues/7177:381,Safety,detect,detected,381,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/CalculateContamination.java; /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/ContaminationModel.java. ### Affected version(s); - [x] Latest public release version - v4.2.0.0 (also detected on previous versions) ; - [x] Latest master branch as of 03/30/2021. ### Description ; **ContaminationModel**; **Problem:**; Where errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination** , which can be misinterpreted by the end user. calculateContaminationFromHoms receives the list of pileups PileupSummary; It iterates from 0.4 INITIAL_MAF_THRESHOLD down to zero. In each iteration pileups are selected using multiple, different strategies.; When the stdError exit condition is met (i.e., stdError < (contamination* MIN_RELATIVE_ERROR +MIN_ABSOLUTE_ERROR)), it reports out the contamination and stdError values. The issue is that this stdError exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/issues/7177:1904,Testability,log,logic,1904,"n the stdError exit condition is met (i.e., stdError < (contamination* MIN_RELATIVE_ERROR +MIN_ABSOLUTE_ERROR)), it reports out the contamination and stdError values. The issue is that this stdError exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are tested. **contaminationOppositeDepth = Math.max(oppositeDepth - errorDepth, 0);**; **contamination = contaminationOppositeDepth / totalDepthWeightedByOppositeFrequency**. Solution proposed:; Currently, when errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination**; it seems to us that this should instead be interpreted as **unable to calculate contamination** because of the high error rate in these pileups. Improvements for current version:; In addition, we suggest calculating all contamination values over all strategies/iterations and outputting the highest contamination value (by MAF and/or strategy, as appropriate), rather than exiting after the first MAF iteration where stdError exit condition is met. ### Description what needs to be added or modified; Please see the code change attached, compare it to 4.2.0.0. The code ou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/issues/7177:2000,Testability,test,tested,2000,"n the stdError exit condition is met (i.e., stdError < (contamination* MIN_RELATIVE_ERROR +MIN_ABSOLUTE_ERROR)), it reports out the contamination and stdError values. The issue is that this stdError exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are tested. **contaminationOppositeDepth = Math.max(oppositeDepth - errorDepth, 0);**; **contamination = contaminationOppositeDepth / totalDepthWeightedByOppositeFrequency**. Solution proposed:; Currently, when errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination**; it seems to us that this should instead be interpreted as **unable to calculate contamination** because of the high error rate in these pileups. Improvements for current version:; In addition, we suggest calculating all contamination values over all strategies/iterations and outputting the highest contamination value (by MAF and/or strategy, as appropriate), rather than exiting after the first MAF iteration where stdError exit condition is met. ### Description what needs to be added or modified; Please see the code change attached, compare it to 4.2.0.0. The code ou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177
https://github.com/broadinstitute/gatk/pull/7180:136,Deployability,update,updated,136,"Previously we were relying on the gcloud package signing key retrieved; during build of the GATK base image. However, the base image is updated; so infrequently that it's possible that the key it uses has expired. To; address this, we now retrieve an updated key at the start of the Docker; build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7180
https://github.com/broadinstitute/gatk/pull/7180:251,Deployability,update,updated,251,"Previously we were relying on the gcloud package signing key retrieved; during build of the GATK base image. However, the base image is updated; so infrequently that it's possible that the key it uses has expired. To; address this, we now retrieve an updated key at the start of the Docker; build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7180
https://github.com/broadinstitute/gatk/pull/7181:244,Availability,down,down,244,"- added custom classes `ExtractCohortRecord` and `ExtractCohortFilterRecord` that implement `Locatable`; - refactored attribute building from these records; - now that the records are `Locatable`s, can use `OverlapDetector` to filter locations down to only desired intervals (including excluded sites); - removed queryMode `QUERY` and associated querying from options",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7181
https://github.com/broadinstitute/gatk/pull/7181:107,Modifiability,refactor,refactored,107,"- added custom classes `ExtractCohortRecord` and `ExtractCohortFilterRecord` that implement `Locatable`; - refactored attribute building from these records; - now that the records are `Locatable`s, can use `OverlapDetector` to filter locations down to only desired intervals (including excluded sites); - removed queryMode `QUERY` and associated querying from options",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7181
https://github.com/broadinstitute/gatk/issues/7182:623,Availability,down,down,623,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; CalibrateDragstrModel; ### Affected version(s); - [ ] Latest public release version [gatk/4.2.0.0]. ### Description . gatk 4.2.0.0 CalibrateDragstrModel produces the following stacktrace.... ```; 13:55:31.187 INFO CalibrateDragstrModel - Initializing engine; 13:55:33.395 INFO CalibrateDragstrModel - Done initializing engine; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:10993,Availability,down,down,10993," 5740000 672351.6; 13:53:40.253 INFO ProgressMeter - chr19:40111935 8.7 5871000 674471.0; 13:53:50.326 INFO ProgressMeter - chr20:33030731 8.9 5993000 675459.1; 13:54:00.362 INFO ProgressMeter - chr21:27087627 9.0 6111000 676013.0; 13:54:10.423 INFO ProgressMeter - chr22:41712333 9.2 6226000 676191.6; 13:54:20.447 INFO ProgressMeter - chrX:39799780 9.4 6342000 676514.9; 13:54:30.520 INFO ProgressMeter - chrX:91818371 9.5 6453000 676246.2; 13:54:40.591 INFO ProgressMeter - chrX:143619069 9.7 6568000 676399.8; 13:54:50.640 INFO ProgressMeter - chrUn_KI270743v1:125398 9.9 6674000 675662.2; 13:55:00.673 INFO ProgressMeter - chr20_KI270869v1_alt:62679 10.0 6792000 676161.8; 13:55:10.679 INFO ProgressMeter - chr19_GL949752v1_alt:485077 10.2 6910000 676673.7; 13:55:26.149 INFO ProgressMeter - HLA-DRB1*11:01:02:3272 10.5 6938356 662718.7; 13:55:26.149 INFO ProgressMeter - Traversal complete. Processed 6938356 total records in 10.5 minutes.; 13:55:26.149 INFO ComposeSTRTableFile - Shutting down engine; [April 4, 2021 1:55:26 PM EDT] org.broadinstitute.hellbender.tools.dragstr.ComposeSTRTableFile done. Elapsed time: 10.52 minutes.; Runtime.totalMemory()=1128792064; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:14611,Availability,down,down,14611,"SYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:55:31.186 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:55:31.186 INFO CalibrateDragstrModel - Deflater: IntelDeflater; 13:55:31.186 INFO CalibrateDragstrModel - Inflater: IntelInflater; 13:55:31.186 INFO CalibrateDragstrModel - GCS max retries/reopens: 20; 13:55:31.186 INFO CalibrateDragstrModel - Requester pays: disabled; 13:55:31.187 WARN CalibrateDragstrModel -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CalibrateDragstrModel is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:55:31.187 INFO CalibrateDragstrModel - Initializing engine; 13:55:33.395 INFO CalibrateDragstrModel - Done initializing engine; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:16734,Availability,error,error,16734,"erencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Expected behavior. Runs to completed and writes out model file. #### Actual behavior; _Tell us what happens instead_. The following error occurs....; ```; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.Abstr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:16960,Availability,down,down,16960,"se(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Expected behavior. Runs to completed and writes out model file. #### Actual behavior; _Tell us what happens instead_. The following error occurs....; ```; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:161,Deployability,release,release,161,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; CalibrateDragstrModel; ### Affected version(s); - [ ] Latest public release version [gatk/4.2.0.0]. ### Description . gatk 4.2.0.0 CalibrateDragstrModel produces the following stacktrace.... ```; 13:55:31.187 INFO CalibrateDragstrModel - Initializing engine; 13:55:33.395 INFO CalibrateDragstrModel - Done initializing engine; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:3483,Deployability,install,install,3483,"stitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is test script (test.sh) that is used.; ```; module load gatk; CRAM=$1; SAMPLE=$(basename $CRAM); SAMPLE=${SAMPLE/\.cram/}; mkdir -p gvcf.STR/$SAMPLE; mkdir -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:3783,Deployability,install,install,3783," -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.458 INFO ComposeSTRTableFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:44:55.458 INFO ComposeSTRTableFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:4212,Deployability,install,install,4212,"RAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.458 INFO ComposeSTRTableFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:44:55.458 INFO ComposeSTRTableFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:44:55.459 INFO ComposeSTRTableFile - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:44:55.459 INFO ComposeSTRTableFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:44:55.460 INFO ComposeSTRTableFile - Start Date/Time: April 4, 2021 1:44:55 PM EDT; 13:44:55.460 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.460 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:11212,Deployability,install,install,11212,"3.0; 13:54:10.423 INFO ProgressMeter - chr22:41712333 9.2 6226000 676191.6; 13:54:20.447 INFO ProgressMeter - chrX:39799780 9.4 6342000 676514.9; 13:54:30.520 INFO ProgressMeter - chrX:91818371 9.5 6453000 676246.2; 13:54:40.591 INFO ProgressMeter - chrX:143619069 9.7 6568000 676399.8; 13:54:50.640 INFO ProgressMeter - chrUn_KI270743v1:125398 9.9 6674000 675662.2; 13:55:00.673 INFO ProgressMeter - chr20_KI270869v1_alt:62679 10.0 6792000 676161.8; 13:55:10.679 INFO ProgressMeter - chr19_GL949752v1_alt:485077 10.2 6910000 676673.7; 13:55:26.149 INFO ProgressMeter - HLA-DRB1*11:01:02:3272 10.5 6938356 662718.7; 13:55:26.149 INFO ProgressMeter - Traversal complete. Processed 6938356 total records in 10.5 minutes.; 13:55:26.149 INFO ComposeSTRTableFile - Shutting down engine; [April 4, 2021 1:55:26 PM EDT] org.broadinstitute.hellbender.tools.dragstr.ComposeSTRTableFile done. Elapsed time: 10.52 minutes.; Runtime.totalMemory()=1128792064; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:11512,Deployability,install,install,11512,"INFO ProgressMeter - chrUn_KI270743v1:125398 9.9 6674000 675662.2; 13:55:00.673 INFO ProgressMeter - chr20_KI270869v1_alt:62679 10.0 6792000 676161.8; 13:55:10.679 INFO ProgressMeter - chr19_GL949752v1_alt:485077 10.2 6910000 676673.7; 13:55:26.149 INFO ProgressMeter - HLA-DRB1*11:01:02:3272 10.5 6938356 662718.7; 13:55:26.149 INFO ProgressMeter - Traversal complete. Processed 6938356 total records in 10.5 minutes.; 13:55:26.149 INFO ComposeSTRTableFile - Shutting down engine; [April 4, 2021 1:55:26 PM EDT] org.broadinstitute.hellbender.tools.dragstr.ComposeSTRTableFile done. Elapsed time: 10.52 minutes.; Runtime.totalMemory()=1128792064; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:55:31.182 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.183 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:55:31.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:12051,Deployability,install,install,12051,"der.tools.dragstr.ComposeSTRTableFile done. Elapsed time: 10.52 minutes.; Runtime.totalMemory()=1128792064; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:55:31.182 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.183 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:55:31.183 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:55:31.183 INFO CalibrateDragstrModel - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:55:31.184 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:55:31.184 INFO CalibrateDragstrModel - Start Date/Time: April 4, 2021 1:55:30 PM EDT; 13:55:31.184 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:1408,Integrability,wrap,wrapAndCopyInto,1408,"5:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:15396,Integrability,wrap,wrapAndCopyInto,15396,"5:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:17745,Integrability,wrap,wrapAndCopyInto,17745,"5:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:3549,Modifiability,variab,variable,3549,"titute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is test script (test.sh) that is used.; ```; module load gatk; CRAM=$1; SAMPLE=$(basename $CRAM); SAMPLE=${SAMPLE/\.cram/}; mkdir -p gvcf.STR/$SAMPLE; mkdir -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ----------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:11278,Modifiability,variab,variable,11278," 676191.6; 13:54:20.447 INFO ProgressMeter - chrX:39799780 9.4 6342000 676514.9; 13:54:30.520 INFO ProgressMeter - chrX:91818371 9.5 6453000 676246.2; 13:54:40.591 INFO ProgressMeter - chrX:143619069 9.7 6568000 676399.8; 13:54:50.640 INFO ProgressMeter - chrUn_KI270743v1:125398 9.9 6674000 675662.2; 13:55:00.673 INFO ProgressMeter - chr20_KI270869v1_alt:62679 10.0 6792000 676161.8; 13:55:10.679 INFO ProgressMeter - chr19_GL949752v1_alt:485077 10.2 6910000 676673.7; 13:55:26.149 INFO ProgressMeter - HLA-DRB1*11:01:02:3272 10.5 6938356 662718.7; 13:55:26.149 INFO ProgressMeter - Traversal complete. Processed 6938356 total records in 10.5 minutes.; 13:55:26.149 INFO ComposeSTRTableFile - Shutting down engine; [April 4, 2021 1:55:26 PM EDT] org.broadinstitute.hellbender.tools.dragstr.ComposeSTRTableFile done. Elapsed time: 10.52 minutes.; Runtime.totalMemory()=1128792064; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:2691,Performance,load,load,2691,"ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is test script (test.sh) that is used.; ```; module load gatk; CRAM=$1; SAMPLE=$(basename $CRAM); SAMPLE=${SAMPLE/\.cram/}; mkdir -p gvcf.STR/$SAMPLE; mkdir -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:4142,Performance,Load,Loading,4142,"h gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.458 INFO ComposeSTRTableFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:44:55.458 INFO ComposeSTRTableFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:44:55.459 INFO ComposeSTRTableFile - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:44:55.459 INFO ComposeSTRTableFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:44:55.460 INFO ComposeSTRTableFile - Start Date/Time: April 4, 2021 1:44:55 PM EDT; 13:44:55.460 INFO ComposeSTRTable",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:11981,Performance,Load,Loading,11981,"le - Shutting down engine; [April 4, 2021 1:55:26 PM EDT] org.broadinstitute.hellbender.tools.dragstr.ComposeSTRTableFile done. Elapsed time: 10.52 minutes.; Runtime.totalMemory()=1128792064; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:55:31.182 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.183 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:55:31.183 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:55:31.183 INFO CalibrateDragstrModel - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:55:31.184 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:55:31.184 INFO CalibrateDragstrModel - Start Date/Time: April 4, 2021 1:55:30 PM EDT; 13:55:31.184 INFO Cal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:4429,Safety,detect,detect,4429,"ealign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.458 INFO ComposeSTRTableFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:44:55.458 INFO ComposeSTRTableFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:44:55.459 INFO ComposeSTRTableFile - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:44:55.459 INFO ComposeSTRTableFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:44:55.460 INFO ComposeSTRTableFile - Start Date/Time: April 4, 2021 1:44:55 PM EDT; 13:44:55.460 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.460 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.461 INFO ComposeSTRTableFile - HTSJDK Version: 2.24.0; 13:44:55.461 INFO ComposeSTRTableFile - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:12268,Safety,detect,detect,12268,"in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:55:31.182 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.183 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:55:31.183 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:55:31.183 INFO CalibrateDragstrModel - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:55:31.184 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:55:31.184 INFO CalibrateDragstrModel - Start Date/Time: April 4, 2021 1:55:30 PM EDT; 13:55:31.184 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.184 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.185 INFO CalibrateDragstrModel - HTSJDK Version: 2.24.0; 13:55:31.185 INFO CalibrateDrag",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:2642,Testability,test,test,2642,"ractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is test script (test.sh) that is used.; ```; module load gatk; CRAM=$1; SAMPLE=$(basename $CRAM); SAMPLE=${SAMPLE/\.cram/}; mkdir -p gvcf.STR/$SAMPLE; mkdir -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_asy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:2655,Testability,test,test,2655,"ractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is test script (test.sh) that is used.; ```; module load gatk; CRAM=$1; SAMPLE=$(basename $CRAM); SAMPLE=${SAMPLE/\.cram/}; mkdir -p gvcf.STR/$SAMPLE; mkdir -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_asy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/issues/7182:3348,Testability,test,test,3348,"LineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is test script (test.sh) that is used.; ```; module load gatk; CRAM=$1; SAMPLE=$(basename $CRAM); SAMPLE=${SAMPLE/\.cram/}; mkdir -p gvcf.STR/$SAMPLE; mkdir -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.googl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182
https://github.com/broadinstitute/gatk/pull/7184:420,Availability,down,down,420,"addresses specops issues:; - #268 https://github.com/broadinstitute/dsp-spec-ops/issues/268; - #270 https://github.com/broadinstitute/dsp-spec-ops/issues/270. similar changes as the PR for ExtractCohort:; - added custom classes `ExtractFeaturesRecord` that implements `Locatable`; - refactored attribute building from these records; - now that the records are `Locatable`s, can use `OverlapDetector` to filter locations down to only desired intervals (including excluded sites). also:; - enable using intervals input (-L) rather than specifying min-location and max-location. updated WDL to support scattering using SplitIntervals (based off of CohortExtract); - add back AS_QD to headers (currently headers are shared between ExtractCohort and ExtractFeatures - AS_QD not needed for cohort but is needed for features)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7184
https://github.com/broadinstitute/gatk/pull/7184:576,Deployability,update,updated,576,"addresses specops issues:; - #268 https://github.com/broadinstitute/dsp-spec-ops/issues/268; - #270 https://github.com/broadinstitute/dsp-spec-ops/issues/270. similar changes as the PR for ExtractCohort:; - added custom classes `ExtractFeaturesRecord` that implements `Locatable`; - refactored attribute building from these records; - now that the records are `Locatable`s, can use `OverlapDetector` to filter locations down to only desired intervals (including excluded sites). also:; - enable using intervals input (-L) rather than specifying min-location and max-location. updated WDL to support scattering using SplitIntervals (based off of CohortExtract); - add back AS_QD to headers (currently headers are shared between ExtractCohort and ExtractFeatures - AS_QD not needed for cohort but is needed for features)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7184
https://github.com/broadinstitute/gatk/pull/7184:283,Modifiability,refactor,refactored,283,"addresses specops issues:; - #268 https://github.com/broadinstitute/dsp-spec-ops/issues/268; - #270 https://github.com/broadinstitute/dsp-spec-ops/issues/270. similar changes as the PR for ExtractCohort:; - added custom classes `ExtractFeaturesRecord` that implements `Locatable`; - refactored attribute building from these records; - now that the records are `Locatable`s, can use `OverlapDetector` to filter locations down to only desired intervals (including excluded sites). also:; - enable using intervals input (-L) rather than specifying min-location and max-location. updated WDL to support scattering using SplitIntervals (based off of CohortExtract); - add back AS_QD to headers (currently headers are shared between ExtractCohort and ExtractFeatures - AS_QD not needed for cohort but is needed for features)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7184
https://github.com/broadinstitute/gatk/issues/7185:282,Modifiability,polymorphi,polymorphic,282,"## Question. Why we use MIN_DP over DP for synthetic Ref allele depth for genotypes derived from hom-ref blocks? ; Would it make more sense to keep and use the average or median?. ```java. ## GenotypeGVCFsEngine.java:176 (about); ...; if (result.isPolymorphicInSamples()) {; // For polymorphic sites we need to make sure e.g. the SB tag is sent to the annotation engine and then removed later.; final VariantContext reannotated = annotationEngine.annotateContext(result, features, ref, null, a -> true);; return new VariantContextBuilder(reannotated).genotypes(; ==!==> cleanupGenotypeAnnotations(reannotated, false)).make();; } else if (includeNonVariants) {; ... ## Same file ln 436, method cleanupGenotypeAnnotations:; ...; // move the MIN_DP to DP; if ( oldGT.hasExtendedAttribute(GATKVCFConstants.MIN_DP_FORMAT_KEY) ) {; depth = parseInt(oldGT.getAnyAttribute(GATKVCFConstants.MIN_DP_FORMAT_KEY));; builder.DP(depth);; attrs.remove(GATKVCFConstants.MIN_DP_FORMAT_KEY);; }; ... ```. ### Tool(s) or class(es) involved; GenotypeGVCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7185
https://github.com/broadinstitute/gatk/issues/7187:203,Availability,error,error,203,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7187:218,Availability,Error,Error,218,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7187:939,Availability,error,error,939,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7187:112,Deployability,release,release,112,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7187:789,Modifiability,parameteriz,parameterization,789,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7187:776,Testability,log,log,776,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7187:817,Testability,log,log,817,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7187:836,Testability,log,log,836,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7187:906,Testability,log,log,906,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7187:991,Testability,log,log,991,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187
https://github.com/broadinstitute/gatk/issues/7189:326,Availability,error,error,326,"## Bug Report. Originally reported by @ldgauthier via slack. ### Affected tool(s) or class(es); CalibrateDragstrModel. ### Affected version(s); - [X ] Latest public release version [4.2.0.0]. ### Description ; An out-of-memory exception when running the aforementioned tool in at least one of many samples. Location where the error occur does not seem to be always the same but it was fixable by increasing memory over 15Gb. #### Steps to reproduce. Since I'm not sure the data is public I won't disclose its location nor ID in this issue. Let's call it the ""SAMPLE"" in ""SAMPLE.cram"":. ```; gatk --java-options ""-Dsamjdk.reference_fasta=Homo_sapiens_assembly38.fasta -Xmx2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" \; CalibrateDragstrModel \; -R Homo_sapiens_assembly38.fasta \; -I SAMPLE.cram \; -str Homo_sapiens_assembly38.str \; -O SAMPLE.final.cram.dragstr \; --parallel \; --verbosity DEBUG; ```. ```Homo_sapiens_assembly38.str``` depends only on the reference and ca be composed using this:. ```; gatk ComposeSTRFile -R Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.str; ```. #### Expected behavior; Completes without issues. #### Actual behavior; a Java Out-of-Memory error is throw and the execution finished without results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7189
https://github.com/broadinstitute/gatk/issues/7189:1188,Availability,error,error,1188,"## Bug Report. Originally reported by @ldgauthier via slack. ### Affected tool(s) or class(es); CalibrateDragstrModel. ### Affected version(s); - [X ] Latest public release version [4.2.0.0]. ### Description ; An out-of-memory exception when running the aforementioned tool in at least one of many samples. Location where the error occur does not seem to be always the same but it was fixable by increasing memory over 15Gb. #### Steps to reproduce. Since I'm not sure the data is public I won't disclose its location nor ID in this issue. Let's call it the ""SAMPLE"" in ""SAMPLE.cram"":. ```; gatk --java-options ""-Dsamjdk.reference_fasta=Homo_sapiens_assembly38.fasta -Xmx2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" \; CalibrateDragstrModel \; -R Homo_sapiens_assembly38.fasta \; -I SAMPLE.cram \; -str Homo_sapiens_assembly38.str \; -O SAMPLE.final.cram.dragstr \; --parallel \; --verbosity DEBUG; ```. ```Homo_sapiens_assembly38.str``` depends only on the reference and ca be composed using this:. ```; gatk ComposeSTRFile -R Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.str; ```. #### Expected behavior; Completes without issues. #### Actual behavior; a Java Out-of-Memory error is throw and the execution finished without results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7189
https://github.com/broadinstitute/gatk/issues/7189:165,Deployability,release,release,165,"## Bug Report. Originally reported by @ldgauthier via slack. ### Affected tool(s) or class(es); CalibrateDragstrModel. ### Affected version(s); - [X ] Latest public release version [4.2.0.0]. ### Description ; An out-of-memory exception when running the aforementioned tool in at least one of many samples. Location where the error occur does not seem to be always the same but it was fixable by increasing memory over 15Gb. #### Steps to reproduce. Since I'm not sure the data is public I won't disclose its location nor ID in this issue. Let's call it the ""SAMPLE"" in ""SAMPLE.cram"":. ```; gatk --java-options ""-Dsamjdk.reference_fasta=Homo_sapiens_assembly38.fasta -Xmx2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" \; CalibrateDragstrModel \; -R Homo_sapiens_assembly38.fasta \; -I SAMPLE.cram \; -str Homo_sapiens_assembly38.str \; -O SAMPLE.final.cram.dragstr \; --parallel \; --verbosity DEBUG; ```. ```Homo_sapiens_assembly38.str``` depends only on the reference and ca be composed using this:. ```; gatk ComposeSTRFile -R Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.str; ```. #### Expected behavior; Completes without issues. #### Actual behavior; a Java Out-of-Memory error is throw and the execution finished without results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7189
https://github.com/broadinstitute/gatk/issues/7189:938,Integrability,depend,depends,938,"## Bug Report. Originally reported by @ldgauthier via slack. ### Affected tool(s) or class(es); CalibrateDragstrModel. ### Affected version(s); - [X ] Latest public release version [4.2.0.0]. ### Description ; An out-of-memory exception when running the aforementioned tool in at least one of many samples. Location where the error occur does not seem to be always the same but it was fixable by increasing memory over 15Gb. #### Steps to reproduce. Since I'm not sure the data is public I won't disclose its location nor ID in this issue. Let's call it the ""SAMPLE"" in ""SAMPLE.cram"":. ```; gatk --java-options ""-Dsamjdk.reference_fasta=Homo_sapiens_assembly38.fasta -Xmx2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" \; CalibrateDragstrModel \; -R Homo_sapiens_assembly38.fasta \; -I SAMPLE.cram \; -str Homo_sapiens_assembly38.str \; -O SAMPLE.final.cram.dragstr \; --parallel \; --verbosity DEBUG; ```. ```Homo_sapiens_assembly38.str``` depends only on the reference and ca be composed using this:. ```; gatk ComposeSTRFile -R Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.str; ```. #### Expected behavior; Completes without issues. #### Actual behavior; a Java Out-of-Memory error is throw and the execution finished without results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7189
https://github.com/broadinstitute/gatk/pull/7193:20,Deployability,release,release,20,"Dependent on HTSJDK release after https://github.com/samtools/htsjdk/pull/1544. Fixes #7111 . - Added optional argument `--ignore-non-ref-in-types` to support correct handling of VariantContexts that contain a NON_REF allele; - Default behavior does not change; - Note that this only enables correct handling of GVCF input. The filtered output files are VCF (not GVCF) files, since reference blocks are not extended when a variant is filtered out; - Added integration test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7193
https://github.com/broadinstitute/gatk/pull/7193:456,Deployability,integrat,integration,456,"Dependent on HTSJDK release after https://github.com/samtools/htsjdk/pull/1544. Fixes #7111 . - Added optional argument `--ignore-non-ref-in-types` to support correct handling of VariantContexts that contain a NON_REF allele; - Default behavior does not change; - Note that this only enables correct handling of GVCF input. The filtered output files are VCF (not GVCF) files, since reference blocks are not extended when a variant is filtered out; - Added integration test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7193
https://github.com/broadinstitute/gatk/pull/7193:0,Integrability,Depend,Dependent,0,"Dependent on HTSJDK release after https://github.com/samtools/htsjdk/pull/1544. Fixes #7111 . - Added optional argument `--ignore-non-ref-in-types` to support correct handling of VariantContexts that contain a NON_REF allele; - Default behavior does not change; - Note that this only enables correct handling of GVCF input. The filtered output files are VCF (not GVCF) files, since reference blocks are not extended when a variant is filtered out; - Added integration test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7193
https://github.com/broadinstitute/gatk/pull/7193:456,Integrability,integrat,integration,456,"Dependent on HTSJDK release after https://github.com/samtools/htsjdk/pull/1544. Fixes #7111 . - Added optional argument `--ignore-non-ref-in-types` to support correct handling of VariantContexts that contain a NON_REF allele; - Default behavior does not change; - Note that this only enables correct handling of GVCF input. The filtered output files are VCF (not GVCF) files, since reference blocks are not extended when a variant is filtered out; - Added integration test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7193
https://github.com/broadinstitute/gatk/pull/7193:407,Modifiability,extend,extended,407,"Dependent on HTSJDK release after https://github.com/samtools/htsjdk/pull/1544. Fixes #7111 . - Added optional argument `--ignore-non-ref-in-types` to support correct handling of VariantContexts that contain a NON_REF allele; - Default behavior does not change; - Note that this only enables correct handling of GVCF input. The filtered output files are VCF (not GVCF) files, since reference blocks are not extended when a variant is filtered out; - Added integration test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7193
https://github.com/broadinstitute/gatk/pull/7193:468,Testability,test,test,468,"Dependent on HTSJDK release after https://github.com/samtools/htsjdk/pull/1544. Fixes #7111 . - Added optional argument `--ignore-non-ref-in-types` to support correct handling of VariantContexts that contain a NON_REF allele; - Default behavior does not change; - Note that this only enables correct handling of GVCF input. The filtered output files are VCF (not GVCF) files, since reference blocks are not extended when a variant is filtered out; - Added integration test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7193
https://github.com/broadinstitute/gatk/pull/7194:585,Integrability,message,message,585,"specops issue #269 https://github.com/broadinstitute/dsp-spec-ops/issues/269. - user can provide either [snps-truth-sensitivity-filter-level, indels-truth-sensitivity-filter-level] or [snps-lod-score-cutoff, indels-lod-score-cutoff], or neither, in which case default values of snps-truth-sensitivity-filter=99.7 and indels-truth-sensitivity-fitler=99.0 are used.; - regardless of lod score cutoffs or truth sensitivity cutoffs, the filtering string is either ""low_VQSLOD_SNP"" or ""low_VQSLOD_INDEL""; - if lod score cutoffs are provided, those are used for filtering. the header filter message looks like ""Site failed INDEL model VQSLOD cutoff of 0.0""; - if truth sensitivity cutoffs are provided, the corresponding lod scores are looked up from the tranche table in BigQuery. the header filter message in this case looks like ""Site failed INDEL model sensitivity cutoff (90.0), corresponding with VQSLOD cutoff of 0.0""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7194
https://github.com/broadinstitute/gatk/pull/7194:794,Integrability,message,message,794,"specops issue #269 https://github.com/broadinstitute/dsp-spec-ops/issues/269. - user can provide either [snps-truth-sensitivity-filter-level, indels-truth-sensitivity-filter-level] or [snps-lod-score-cutoff, indels-lod-score-cutoff], or neither, in which case default values of snps-truth-sensitivity-filter=99.7 and indels-truth-sensitivity-fitler=99.0 are used.; - regardless of lod score cutoffs or truth sensitivity cutoffs, the filtering string is either ""low_VQSLOD_SNP"" or ""low_VQSLOD_INDEL""; - if lod score cutoffs are provided, those are used for filtering. the header filter message looks like ""Site failed INDEL model VQSLOD cutoff of 0.0""; - if truth sensitivity cutoffs are provided, the corresponding lod scores are looked up from the tranche table in BigQuery. the header filter message in this case looks like ""Site failed INDEL model sensitivity cutoff (90.0), corresponding with VQSLOD cutoff of 0.0""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7194
https://github.com/broadinstitute/gatk/pull/7196:511,Deployability,update,updated,511,"specops issue #265 https://github.com/broadinstitute/dsp-spec-ops/issues/265. in addition to renaming the metadata table (in both CreateVariantIngestFiles tool and ImportGenomes wdl), this PR:; - removes interval_list_blob from the metadata/sample_info table; - adds missing QUALapprox field to the vet schema defaults (in ImportGenomes wdl). this was tested by running ImportGenomes.wdl in Terra and the sample_info table gets created & populated as expected. note: ImportArrays.wdl and array tooling were not updated",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7196
https://github.com/broadinstitute/gatk/pull/7196:352,Testability,test,tested,352,"specops issue #265 https://github.com/broadinstitute/dsp-spec-ops/issues/265. in addition to renaming the metadata table (in both CreateVariantIngestFiles tool and ImportGenomes wdl), this PR:; - removes interval_list_blob from the metadata/sample_info table; - adds missing QUALapprox field to the vet schema defaults (in ImportGenomes wdl). this was tested by running ImportGenomes.wdl in Terra and the sample_info table gets created & populated as expected. note: ImportArrays.wdl and array tooling were not updated",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7196
https://github.com/broadinstitute/gatk/pull/7197:359,Deployability,update,update,359,"Addresses [219](https://github.com/broadinstitute/dsp-spec-ops/issues/219). Major changes. - calculate site level metrics in `feature_extract.sql`; - extract metrics, apply thresholds, and set filter field in ExtractFeature; - CreateSiteFilteringFiles to translate from input VCF with filter fields into format for BQ loading, especially `location` fields; - update WDL to call CreateSiteFilteringFiles and upload results to BQ. Minor changes; - added call_GQ to alt_allele creation; - reduced memory requirements in WDL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7197
https://github.com/broadinstitute/gatk/pull/7197:486,Energy Efficiency,reduce,reduced,486,"Addresses [219](https://github.com/broadinstitute/dsp-spec-ops/issues/219). Major changes. - calculate site level metrics in `feature_extract.sql`; - extract metrics, apply thresholds, and set filter field in ExtractFeature; - CreateSiteFilteringFiles to translate from input VCF with filter fields into format for BQ loading, especially `location` fields; - update WDL to call CreateSiteFilteringFiles and upload results to BQ. Minor changes; - added call_GQ to alt_allele creation; - reduced memory requirements in WDL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7197
https://github.com/broadinstitute/gatk/pull/7197:318,Performance,load,loading,318,"Addresses [219](https://github.com/broadinstitute/dsp-spec-ops/issues/219). Major changes. - calculate site level metrics in `feature_extract.sql`; - extract metrics, apply thresholds, and set filter field in ExtractFeature; - CreateSiteFilteringFiles to translate from input VCF with filter fields into format for BQ loading, especially `location` fields; - update WDL to call CreateSiteFilteringFiles and upload results to BQ. Minor changes; - added call_GQ to alt_allele creation; - reduced memory requirements in WDL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7197
https://github.com/broadinstitute/gatk/issues/7199:989,Availability,error,errors,989," 25, referenceIndex:37. Run the command as follows: ; ```; ./gatk HaplotypeCallerSpark --native-pair-hmm-threads 40 -ERC GVCF -R hg38/fa/GRCh38.fna -I NA12878.sort.dup.BQSR.bam -L random.bed -O NA12878.g.vcf.gz ; ```; The contents of random.bed:; ```; chr1_KI270706v1_random 45744 46286; chr1_KI270706v1_random 69250 69770; chr4_GL000008v2_random 7167 7691; chr4_GL000008v2_random 131611 132154; chr4_GL000008v2_random 155105 155625; chr4_GL000008v2_random 177247 177767; chr4_GL000008v2_random 191280 191800; ```; And the weird thing is that when I removed one of these chromosomes chr1_KI270706v1_random, it works. . ----. ## Bug Report; The following errors were found while running the HaplotypeCallerSpark on WES data; ```; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO NewHadoopRDD: Input split: file:/data/phoenix-output/LUSH_WES/sz01-huyue-LUSH_WES-2021041210463878ad3/NA12878/LUSH_BQSR/NA12878.sort.dup.BQSR.bam:67108864+29181309; 21/04/13 07:32:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000001_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:24 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000001_0: Committed; 21/04/13 07:32:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 106). 762 bytes result sent to driver; 21/04/13 07:32:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 106) in 136 ms on localhost (executor driver) (1/3); 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:1295,Availability,failure,failures,1295," 25, referenceIndex:37. Run the command as follows: ; ```; ./gatk HaplotypeCallerSpark --native-pair-hmm-threads 40 -ERC GVCF -R hg38/fa/GRCh38.fna -I NA12878.sort.dup.BQSR.bam -L random.bed -O NA12878.g.vcf.gz ; ```; The contents of random.bed:; ```; chr1_KI270706v1_random 45744 46286; chr1_KI270706v1_random 69250 69770; chr4_GL000008v2_random 7167 7691; chr4_GL000008v2_random 131611 132154; chr4_GL000008v2_random 155105 155625; chr4_GL000008v2_random 177247 177767; chr4_GL000008v2_random 191280 191800; ```; And the weird thing is that when I removed one of these chromosomes chr1_KI270706v1_random, it works. . ----. ## Bug Report; The following errors were found while running the HaplotypeCallerSpark on WES data; ```; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO NewHadoopRDD: Input split: file:/data/phoenix-output/LUSH_WES/sz01-huyue-LUSH_WES-2021041210463878ad3/NA12878/LUSH_BQSR/NA12878.sort.dup.BQSR.bam:67108864+29181309; 21/04/13 07:32:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000001_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:24 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000001_0: Committed; 21/04/13 07:32:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 106). 762 bytes result sent to driver; 21/04/13 07:32:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 106) in 136 ms on localhost (executor driver) (1/3); 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:1543,Availability,failure,failures,1543," 25, referenceIndex:37. Run the command as follows: ; ```; ./gatk HaplotypeCallerSpark --native-pair-hmm-threads 40 -ERC GVCF -R hg38/fa/GRCh38.fna -I NA12878.sort.dup.BQSR.bam -L random.bed -O NA12878.g.vcf.gz ; ```; The contents of random.bed:; ```; chr1_KI270706v1_random 45744 46286; chr1_KI270706v1_random 69250 69770; chr4_GL000008v2_random 7167 7691; chr4_GL000008v2_random 131611 132154; chr4_GL000008v2_random 155105 155625; chr4_GL000008v2_random 177247 177767; chr4_GL000008v2_random 191280 191800; ```; And the weird thing is that when I removed one of these chromosomes chr1_KI270706v1_random, it works. . ----. ## Bug Report; The following errors were found while running the HaplotypeCallerSpark on WES data; ```; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO NewHadoopRDD: Input split: file:/data/phoenix-output/LUSH_WES/sz01-huyue-LUSH_WES-2021041210463878ad3/NA12878/LUSH_BQSR/NA12878.sort.dup.BQSR.bam:67108864+29181309; 21/04/13 07:32:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000001_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:24 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000001_0: Committed; 21/04/13 07:32:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 106). 762 bytes result sent to driver; 21/04/13 07:32:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 106) in 136 ms on localhost (executor driver) (1/3); 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:2474,Availability,failure,failures,2474,"864+29181309; 21/04/13 07:32:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000001_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:24 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000001_0: Committed; 21/04/13 07:32:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 106). 762 bytes result sent to driver; 21/04/13 07:32:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 106) in 136 ms on localhost (executor driver) (1/3); 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 ERROR Utils: Aborting task; java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(Indexing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:2722,Availability,failure,failures,2722,"864+29181309; 21/04/13 07:32:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000001_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:24 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000001_0: Committed; 21/04/13 07:32:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 106). 762 bytes result sent to driver; 21/04/13 07:32:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 106) in 136 ms on localhost (executor driver) (1/3); 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 ERROR Utils: Aborting task; java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(Indexing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:2970,Availability,failure,failures,2970,"864+29181309; 21/04/13 07:32:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000001_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:24 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000001_0: Committed; 21/04/13 07:32:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 106). 762 bytes result sent to driver; 21/04/13 07:32:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 106) in 136 ms on localhost (executor driver) (1/3); 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 ERROR Utils: Aborting task; java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(Indexing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:3218,Availability,failure,failures,3218,"864+29181309; 21/04/13 07:32:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000001_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:24 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000001_0: Committed; 21/04/13 07:32:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 106). 762 bytes result sent to driver; 21/04/13 07:32:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 106) in 136 ms on localhost (executor driver) (1/3); 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 ERROR Utils: Aborting task; java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(Indexing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:3253,Availability,ERROR,ERROR,3253,"864+29181309; 21/04/13 07:32:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000001_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:24 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000001_0: Committed; 21/04/13 07:32:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 106). 762 bytes result sent to driver; 21/04/13 07:32:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 106) in 136 ms on localhost (executor driver) (1/3); 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 ERROR Utils: Aborting task; java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(Indexing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:5388,Availability,ERROR,ERROR,5388,ryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:5485,Availability,ERROR,ERROR,5485,nal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.r,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:10803,Availability,ERROR,ERROR,10803,"9); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. 21/04/13 07:32:25 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:11280,Availability,failure,failure,11280,"nal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. 21/04/13 07:32:25 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurren",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:11337,Availability,failure,failure,11337,"ter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. 21/04/13 07:32:25 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:61",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:13875,Availability,ERROR,ERROR,13875,"t.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:13997,Availability,failure,failure,13997,"eaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:14054,Availability,failure,failure,14054,"fOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:61",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:24005,Availability,down,down,24005,"008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more; 21/04/13 07:32:25 INFO SparkUI: Stopped Spark web UI at http://wgs-cntech-online-it:4040; 21/04/13 07:32:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 21/04/13 07:32:25 INFO MemoryStore: MemoryStore cleared; 21/04/13 07:32:25 INFO BlockManager: BlockManager stopped; 21/04/13 07:32:25 INFO BlockManagerMaster: BlockManagerMaster stopped; 21/04/13 07:32:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 21/04/13 07:32:25 INFO SparkContext: Successfully stopped SparkContext; 07:32:25.095 INFO HaplotypeCallerSpark - Shutting down engine; ```. ### Affected tool(s) or class(es); HaplotypeCallerSpark. ### Affected version(s); - gatk-4.1.9.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:4821,Energy Efficiency,schedul,scheduler,4821,iter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkH,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:4892,Energy Efficiency,schedul,scheduler,4892,utFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWrite,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:5965,Energy Efficiency,schedul,scheduler,5965,"Runner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:6036,Energy Efficiency,schedul,scheduler,6036,"Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantconte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:8884,Energy Efficiency,schedul,scheduler,8884,"FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000002_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:25 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000002_0: Committed; 21/04/13 07:32:25 INFO Executor: Finished task 2.0 in stage 5.0 (TID 107). 848 bytes result sent to driver; 21/04/13 07:32:25 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 107) in 279 ms on localhost (executor driver) (2/3); 21/04/13 07:32:25 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:8955,Energy Efficiency,schedul,scheduler,8955,"r_000002_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:25 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000002_0: Committed; 21/04/13 07:32:25 INFO Executor: Finished task 2.0 in stage 5.0 (TID 107). 848 bytes result sent to driver; 21/04/13 07:32:25 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 107) in 279 ms on localhost (executor driver) (2/3); 21/04/13 07:32:25 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantconte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:11830,Energy Efficiency,schedul,scheduler,11830,"stage 5.0 failed 1 times; aborting job; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:11901,Energy Efficiency,schedul,scheduler,11901,"dulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantconte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:14547,Energy Efficiency,schedul,scheduler,14547,"backs(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:14618,Energy Efficiency,schedul,scheduler,14618,"ter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantconte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:16488,Energy Efficiency,schedul,scheduler,16488,iant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:16528,Energy Efficiency,schedul,scheduler,16528,(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:16626,Energy Efficiency,schedul,scheduler,16626,r.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:16723,Energy Efficiency,schedul,scheduler,16723,mat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:16974,Energy Efficiency,schedul,scheduler,16974,e(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:17054,Energy Efficiency,schedul,scheduler,17054,r$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:17159,Energy Efficiency,schedul,scheduler,17159,un$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:17307,Energy Efficiency,schedul,scheduler,17307,k.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:17395,Energy Efficiency,schedul,scheduler,17395,Task(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:17492,Energy Efficiency,schedul,scheduler,17492,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOpe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:17587,Energy Efficiency,schedul,scheduler,17587,.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:17750,Energy Efficiency,schedul,scheduler,17750,abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:21495,Energy Efficiency,schedul,scheduler,21495,"eProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:21566,Energy Efficiency,schedul,scheduler,21566,".hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantconte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:5173,Performance,concurren,concurrent,5173,:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:5257,Performance,concurren,concurrent,5257,pWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Exec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:6317,Performance,concurren,concurrent,6317,"617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:6401,Performance,concurren,concurrent,6401,"riter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:9236,Performance,concurren,concurrent,9236," 07:32:25 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 107) in 279 ms on localhost (executor driver) (2/3); 21/04/13 07:32:25 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:9320,Performance,concurren,concurrent,9320," localhost (executor driver) (2/3); 21/04/13 07:32:25 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:12182,Performance,concurren,concurrent,12182,"e 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:12266,Performance,concurren,concurrent,12266,"to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:14899,Performance,concurren,concurrent,14899,"ting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:14983,Performance,concurren,concurrent,14983,"to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:21847,Performance,concurren,concurrent,21847,"ram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:21931,Performance,concurren,concurrent,21931,":160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:3266,Safety,Abort,Aborting,3266,"864+29181309; 21/04/13 07:32:24 INFO FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000001_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:24 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000001_0: Committed; 21/04/13 07:32:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 106). 762 bytes result sent to driver; 21/04/13 07:32:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 106) in 136 ms on localhost (executor driver) (1/3); 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 21/04/13 07:32:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 21/04/13 07:32:24 ERROR Utils: Aborting task; java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(Indexing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:5457,Safety,abort,aborted,5457,ryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:10861,Safety,abort,aborting,10861,"(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. 21/04/13 07:32:25 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:11259,Safety,abort,aborted,11259,"nal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. 21/04/13 07:32:25 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurren",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:13900,Safety,Abort,Aborting,13900,"t.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:13976,Safety,abort,aborted,13976,"eaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:16658,Safety,abort,abortStage,16658,ter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:16755,Safety,abort,abortStage,16755,ite(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:16997,Safety,abort,abortStage,16997,; at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/issues/7199:23630,Usability,clear,cleared,23630,"008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more; 21/04/13 07:32:25 INFO SparkUI: Stopped Spark web UI at http://wgs-cntech-online-it:4040; 21/04/13 07:32:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 21/04/13 07:32:25 INFO MemoryStore: MemoryStore cleared; 21/04/13 07:32:25 INFO BlockManager: BlockManager stopped; 21/04/13 07:32:25 INFO BlockManagerMaster: BlockManagerMaster stopped; 21/04/13 07:32:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 21/04/13 07:32:25 INFO SparkContext: Successfully stopped SparkContext; 07:32:25.095 INFO HaplotypeCallerSpark - Shutting down engine; ```. ### Affected tool(s) or class(es); HaplotypeCallerSpark. ### Affected version(s); - gatk-4.1.9.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199
https://github.com/broadinstitute/gatk/pull/7200:478,Availability,error,error,478,"specops issue #273: https://github.com/broadinstitute/dsp-spec-ops/issues/273. - renamed `ngs_cohort_extract.py` -> `create_cohort_extract_data_table.py`; - run the script in a WDL (GvsPrepareCallset.wdl); - use a custom docker - include script for creating and pushing this docker to gcr.io; - enable running as a SA - this has been tested in Terra and works as expected. if using a dataset that requires SA access and the user does not provide a working SA key, they get this error: `User does not have bigquery.jobs.create permission in project specops-variantstore-sa-tests.`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7200
https://github.com/broadinstitute/gatk/pull/7200:409,Security,access,access,409,"specops issue #273: https://github.com/broadinstitute/dsp-spec-ops/issues/273. - renamed `ngs_cohort_extract.py` -> `create_cohort_extract_data_table.py`; - run the script in a WDL (GvsPrepareCallset.wdl); - use a custom docker - include script for creating and pushing this docker to gcr.io; - enable running as a SA - this has been tested in Terra and works as expected. if using a dataset that requires SA access and the user does not provide a working SA key, they get this error: `User does not have bigquery.jobs.create permission in project specops-variantstore-sa-tests.`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7200
https://github.com/broadinstitute/gatk/pull/7200:334,Testability,test,tested,334,"specops issue #273: https://github.com/broadinstitute/dsp-spec-ops/issues/273. - renamed `ngs_cohort_extract.py` -> `create_cohort_extract_data_table.py`; - run the script in a WDL (GvsPrepareCallset.wdl); - use a custom docker - include script for creating and pushing this docker to gcr.io; - enable running as a SA - this has been tested in Terra and works as expected. if using a dataset that requires SA access and the user does not provide a working SA key, they get this error: `User does not have bigquery.jobs.create permission in project specops-variantstore-sa-tests.`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7200
https://github.com/broadinstitute/gatk/pull/7200:572,Testability,test,tests,572,"specops issue #273: https://github.com/broadinstitute/dsp-spec-ops/issues/273. - renamed `ngs_cohort_extract.py` -> `create_cohort_extract_data_table.py`; - run the script in a WDL (GvsPrepareCallset.wdl); - use a custom docker - include script for creating and pushing this docker to gcr.io; - enable running as a SA - this has been tested in Terra and works as expected. if using a dataset that requires SA access and the user does not provide a working SA key, they get this error: `User does not have bigquery.jobs.create permission in project specops-variantstore-sa-tests.`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7200
https://github.com/broadinstitute/gatk/issues/7202:532,Availability,Error,Error,532,## Bug Report. ### Affected tool(s) or class(es). GermlineCNVCaller. ### Affected version(s). - [x] Latest public release version gatk 4.2.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . The same set of hdf5 works fine with another annotated_intervals.tsv . the stack trace:; ```; 11:52:33.788 INFO GermlineCNVCaller - Aggregating read-count file /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/; 20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0e1df603266809/B00HOTD.counts.hdf5 (229 / 347); HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 173 in H5Dread(): can'; t read data; major: Dataset; minor: Read failed; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 550 in H5D__read(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:1405,Availability,error,error,1405,1.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0e1df603266809/B00HOTD.counts.hdf5 (229 / 347); HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 173 in H5Dread(): can'; t read data; major: Dataset; minor: Read failed; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 550 in H5D__read(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D__contig; _readvv_sieve_cb(): block read failed; major: Dataset; minor: Read failed; #008: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request f,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:1820,Availability,error,error,1820,"d(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D__contig; _readvv_sieve_cb(): block read failed; major: Dataset; minor: Read failed; #008: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request failed; major: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:3025,Availability,error,error,3025,"/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request failed; major: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:3053,Availability,error,error,3053,"/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request failed; major: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:3251,Availability,Error,Error,3251,"/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request failed; major: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:3540,Availability,down,down,3540,"DF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hellbender.tools.copynumber.utils.HDF5Utils.readIntervals(HDF5Utils.java:62); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.lambda$new; $2(HDF5SimpleCountCollection.java:76); 	at htsjdk.samtools.util.Lazy.get(Lazy.java:25); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.getInterva; ls(HDF5SimpleCountCollection.java:85",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:114,Deployability,release,release,114,## Bug Report. ### Affected tool(s) or class(es). GermlineCNVCaller. ### Affected version(s). - [x] Latest public release version gatk 4.2.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . The same set of hdf5 works fine with another annotated_intervals.tsv . the stack trace:; ```; 11:52:33.788 INFO GermlineCNVCaller - Aggregating read-count file /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/; 20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0e1df603266809/B00HOTD.counts.hdf5 (229 / 347); HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 173 in H5Dread(): can'; t read data; major: Dataset; minor: Read failed; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 550 in H5D__read(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:3031,Integrability,message,message,3031,"/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request failed; major: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:3457,Integrability,rout,routine,3457,"or: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hellbender.tools.copynumber.utils.HDF5Utils.readIntervals(HDF5Utils.java:62); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.lambda$new; $2(HDF5SimpleCountCollection.java:76); 	at htsjdk.samtools.util.Lazy.get(Lazy.java:25); 	at org.broadinstitute.hellbender.too",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:3805,Modifiability,variab,variable,3805,"led: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hellbender.tools.copynumber.utils.HDF5Utils.readIntervals(HDF5Utils.java:62); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.lambda$new; $2(HDF5SimpleCountCollection.java:76); 	at htsjdk.samtools.util.Lazy.get(Lazy.java:25); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.getInterva; ls(HDF5SimpleCountCollection.java:85); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readHDF5(Simpl; eCountCollection.java:119); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readAndSubset(; Simp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:1581,Performance,perform,perform,1581,ive/HDF5-prefix/src/HDF5/src/H5Dio.c line 173 in H5Dread(): can'; t read data; major: Dataset; minor: Read failed; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 550 in H5D__read(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D__contig; _readvv_sieve_cb(): block read failed; major: Dataset; minor: Read failed; #008: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request failed; major: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; majo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:1785,Performance,perform,perform,1785,"d(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D__contig; _readvv_sieve_cb(): block read failed; major: Dataset; minor: Read failed; #008: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request failed; major: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:538,Safety,detect,detected,538,## Bug Report. ### Affected tool(s) or class(es). GermlineCNVCaller. ### Affected version(s). - [x] Latest public release version gatk 4.2.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . The same set of hdf5 works fine with another annotated_intervals.tsv . the stack trace:; ```; 11:52:33.788 INFO GermlineCNVCaller - Aggregating read-count file /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/; 20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0e1df603266809/B00HOTD.counts.hdf5 (229 / 347); HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 173 in H5Dread(): can'; t read data; major: Dataset; minor: Read failed; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 550 in H5D__read(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:3257,Safety,detect,detected,3257,"/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request failed; major: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:186,Testability,test,test,186,## Bug Report. ### Affected tool(s) or class(es). GermlineCNVCaller. ### Affected version(s). - [x] Latest public release version gatk 4.2.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . The same set of hdf5 works fine with another annotated_intervals.tsv . the stack trace:; ```; 11:52:33.788 INFO GermlineCNVCaller - Aggregating read-count file /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/; 20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0e1df603266809/B00HOTD.counts.hdf5 (229 / 347); HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 173 in H5Dread(): can'; t read data; major: Dataset; minor: Read failed; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 550 in H5D__read(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:4621,Usability,Simpl,SimpleCountCollection,4621,"r.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hellbender.tools.copynumber.utils.HDF5Utils.readIntervals(HDF5Utils.java:62); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.lambda$new; $2(HDF5SimpleCountCollection.java:76); 	at htsjdk.samtools.util.Lazy.get(Lazy.java:25); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.getInterva; ls(HDF5SimpleCountCollection.java:85); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readHDF5(Simpl; eCountCollection.java:119); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readAndSubset(; SimpleCountCollection.java:98); ```. with annotated_intervals.tsv . ```; $ grep -v '@' annotated_intervals.tsv | cat -n | head; 1	CONTIG	START	END	GC_CONTENT	SEGMENTAL_DUPLICATION_CONTENT; 2	chr1	10001	110000	0.422350	0.000000; 3	chr1	110001	177417	0.441046	0.000000; 4	chr1	227418	267719	0.391445	0.000000; 5	chr1	317720	417719	0.401850	0.000000; 6	chr1	417720	471368	0.471155	0.000000; 7	chr1	521369	621368	0.436950	0.000000; 8	chr1	621369	721368	0.428550	0.000000; 9	chr1	721369	821368	0.442210	0.000000; 10	chr1	821369	921368	0.606500	0.000000. $ grep -v '@' /annotated_intervals.tsv | cat -n | tail; 28717	chrY	28051429	28151428	0.372550	0.000000; 28718	chrY	28151429	28251428	0.380900	0.000000; 28719	chrY	28251429	28351428	0.390730	0.000000; 28720	chrY	28351429	28451428	0.381580	0.000000; 28721	chrY	28451429	28551428	0.394890	0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:4652,Usability,Simpl,Simpl,4652,"lapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hellbender.tools.copynumber.utils.HDF5Utils.readIntervals(HDF5Utils.java:62); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.lambda$new; $2(HDF5SimpleCountCollection.java:76); 	at htsjdk.samtools.util.Lazy.get(Lazy.java:25); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.getInterva; ls(HDF5SimpleCountCollection.java:85); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readHDF5(Simpl; eCountCollection.java:119); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readAndSubset(; SimpleCountCollection.java:98); ```. with annotated_intervals.tsv . ```; $ grep -v '@' annotated_intervals.tsv | cat -n | head; 1	CONTIG	START	END	GC_CONTENT	SEGMENTAL_DUPLICATION_CONTENT; 2	chr1	10001	110000	0.422350	0.000000; 3	chr1	110001	177417	0.441046	0.000000; 4	chr1	227418	267719	0.391445	0.000000; 5	chr1	317720	417719	0.401850	0.000000; 6	chr1	417720	471368	0.471155	0.000000; 7	chr1	521369	621368	0.436950	0.000000; 8	chr1	621369	721368	0.428550	0.000000; 9	chr1	721369	821368	0.442210	0.000000; 10	chr1	821369	921368	0.606500	0.000000. $ grep -v '@' /annotated_intervals.tsv | cat -n | tail; 28717	chrY	28051429	28151428	0.372550	0.000000; 28718	chrY	28151429	28251428	0.380900	0.000000; 28719	chrY	28251429	28351428	0.390730	0.000000; 28720	chrY	28351429	28451428	0.381580	0.000000; 28721	chrY	28451429	28551428	0.394890	0.000000; 28722	chrY	28551429",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:4758,Usability,Simpl,SimpleCountCollection,4758,rror: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hellbender.tools.copynumber.utils.HDF5Utils.readIntervals(HDF5Utils.java:62); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.lambda$new; $2(HDF5SimpleCountCollection.java:76); 	at htsjdk.samtools.util.Lazy.get(Lazy.java:25); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.getInterva; ls(HDF5SimpleCountCollection.java:85); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readHDF5(Simpl; eCountCollection.java:119); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readAndSubset(; SimpleCountCollection.java:98); ```. with annotated_intervals.tsv . ```; $ grep -v '@' annotated_intervals.tsv | cat -n | head; 1	CONTIG	START	END	GC_CONTENT	SEGMENTAL_DUPLICATION_CONTENT; 2	chr1	10001	110000	0.422350	0.000000; 3	chr1	110001	177417	0.441046	0.000000; 4	chr1	227418	267719	0.391445	0.000000; 5	chr1	317720	417719	0.401850	0.000000; 6	chr1	417720	471368	0.471155	0.000000; 7	chr1	521369	621368	0.436950	0.000000; 8	chr1	621369	721368	0.428550	0.000000; 9	chr1	721369	821368	0.442210	0.000000; 10	chr1	821369	921368	0.606500	0.000000. $ grep -v '@' /annotated_intervals.tsv | cat -n | tail; 28717	chrY	28051429	28151428	0.372550	0.000000; 28718	chrY	28151429	28251428	0.380900	0.000000; 28719	chrY	28251429	28351428	0.390730	0.000000; 28720	chrY	28351429	28451428	0.381580	0.000000; 28721	chrY	28451429	28551428	0.394890	0.000000; 28722	chrY	28551429	28651428	0.380360	0.000000; 28723	chrY	28651429	28751428	0.392380	0.000000; 28724	chrY	28751429	28819361	0.4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/issues/7202:4796,Usability,Simpl,SimpleCountCollection,4796, read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hellbender.tools.copynumber.utils.HDF5Utils.readIntervals(HDF5Utils.java:62); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.lambda$new; $2(HDF5SimpleCountCollection.java:76); 	at htsjdk.samtools.util.Lazy.get(Lazy.java:25); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.getInterva; ls(HDF5SimpleCountCollection.java:85); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readHDF5(Simpl; eCountCollection.java:119); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readAndSubset(; SimpleCountCollection.java:98); ```. with annotated_intervals.tsv . ```; $ grep -v '@' annotated_intervals.tsv | cat -n | head; 1	CONTIG	START	END	GC_CONTENT	SEGMENTAL_DUPLICATION_CONTENT; 2	chr1	10001	110000	0.422350	0.000000; 3	chr1	110001	177417	0.441046	0.000000; 4	chr1	227418	267719	0.391445	0.000000; 5	chr1	317720	417719	0.401850	0.000000; 6	chr1	417720	471368	0.471155	0.000000; 7	chr1	521369	621368	0.436950	0.000000; 8	chr1	621369	721368	0.428550	0.000000; 9	chr1	721369	821368	0.442210	0.000000; 10	chr1	821369	921368	0.606500	0.000000. $ grep -v '@' /annotated_intervals.tsv | cat -n | tail; 28717	chrY	28051429	28151428	0.372550	0.000000; 28718	chrY	28151429	28251428	0.380900	0.000000; 28719	chrY	28251429	28351428	0.390730	0.000000; 28720	chrY	28351429	28451428	0.381580	0.000000; 28721	chrY	28451429	28551428	0.394890	0.000000; 28722	chrY	28551429	28651428	0.380360	0.000000; 28723	chrY	28651429	28751428	0.392380	0.000000; 28724	chrY	28751429	28819361	0.421121	0.000000; 28725	chrY	588,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202
https://github.com/broadinstitute/gatk/pull/7204:5,Security,authenticat,authenticates,5,This authenticates us to dockerhub on travis builds that require docker.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7204
https://github.com/broadinstitute/gatk/pull/7205:76,Performance,load,load,76,"- `GvsImportGenomes.wdl` - renamed WDL, fixed issue with poorly returned bq load string; - `GvsCreateFilterSet.wdl` - renamed WDL, added SA support, fixed a lot of issues in ExtractFeatures tool surrounding permissions - BQ projectIDs are now being properly passed through, freq_table UDF defined in repo rather than in BQ; - `GvsPrepareCallset.wdl` (done in previous PR); - `GvsExtractCallset.wdl` - renamed WDL, added SA support; - SA testing README added. all 4 tested with SA in this Terra workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_sa_testing; testing also without SA in this workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_no_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7205
https://github.com/broadinstitute/gatk/pull/7205:437,Testability,test,testing,437,"- `GvsImportGenomes.wdl` - renamed WDL, fixed issue with poorly returned bq load string; - `GvsCreateFilterSet.wdl` - renamed WDL, added SA support, fixed a lot of issues in ExtractFeatures tool surrounding permissions - BQ projectIDs are now being properly passed through, freq_table UDF defined in repo rather than in BQ; - `GvsPrepareCallset.wdl` (done in previous PR); - `GvsExtractCallset.wdl` - renamed WDL, added SA support; - SA testing README added. all 4 tested with SA in this Terra workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_sa_testing; testing also without SA in this workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_no_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7205
https://github.com/broadinstitute/gatk/pull/7205:465,Testability,test,tested,465,"- `GvsImportGenomes.wdl` - renamed WDL, fixed issue with poorly returned bq load string; - `GvsCreateFilterSet.wdl` - renamed WDL, added SA support, fixed a lot of issues in ExtractFeatures tool surrounding permissions - BQ projectIDs are now being properly passed through, freq_table UDF defined in repo rather than in BQ; - `GvsPrepareCallset.wdl` (done in previous PR); - `GvsExtractCallset.wdl` - renamed WDL, added SA support; - SA testing README added. all 4 tested with SA in this Terra workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_sa_testing; testing also without SA in this workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_no_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7205
https://github.com/broadinstitute/gatk/pull/7205:577,Testability,test,testing,577,"- `GvsImportGenomes.wdl` - renamed WDL, fixed issue with poorly returned bq load string; - `GvsCreateFilterSet.wdl` - renamed WDL, added SA support, fixed a lot of issues in ExtractFeatures tool surrounding permissions - BQ projectIDs are now being properly passed through, freq_table UDF defined in repo rather than in BQ; - `GvsPrepareCallset.wdl` (done in previous PR); - `GvsExtractCallset.wdl` - renamed WDL, added SA support; - SA testing README added. all 4 tested with SA in this Terra workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_sa_testing; testing also without SA in this workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_no_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7205
https://github.com/broadinstitute/gatk/pull/7206:2,Deployability,update,updated,2,"- updated integration test (removed that argument so it defaults to the same thing); - tested that using ""NONE"" as an argument works",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7206
https://github.com/broadinstitute/gatk/pull/7206:10,Deployability,integrat,integration,10,"- updated integration test (removed that argument so it defaults to the same thing); - tested that using ""NONE"" as an argument works",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7206
https://github.com/broadinstitute/gatk/pull/7206:10,Integrability,integrat,integration,10,"- updated integration test (removed that argument so it defaults to the same thing); - tested that using ""NONE"" as an argument works",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7206
https://github.com/broadinstitute/gatk/pull/7206:22,Testability,test,test,22,"- updated integration test (removed that argument so it defaults to the same thing); - tested that using ""NONE"" as an argument works",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7206
https://github.com/broadinstitute/gatk/pull/7206:87,Testability,test,tested,87,"- updated integration test (removed that argument so it defaults to the same thing); - tested that using ""NONE"" as an argument works",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7206
https://github.com/broadinstitute/gatk/pull/7207:97,Testability,test,test,97,"for issue 286, this adds a clustering (hardcoded) option as the location; successful in a manual test of the WDL. <img width=""341"" alt=""Screen Shot 2021-04-20 at 4 48 28 PM"" src=""https://user-images.githubusercontent.com/6863459/115462177-541bc700-a1f8-11eb-8910-dffb217c3413.png"">; <img width=""467"" alt=""Screen Shot 2021-04-20 at 4 48 20 PM"" src=""https://user-images.githubusercontent.com/6863459/115462212-5f6ef280-a1f8-11eb-9714-6b8f9bf4637a.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7207
https://github.com/broadinstitute/gatk/pull/7208:142,Security,password,passwords,142,* Moving the artifactory key and username out of the .travis.yml and into the travis settings directly.; * This will make it easier to rotate passwords in the future without requiring a new commit and rebases.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7208
https://github.com/broadinstitute/gatk/pull/7209:20,Testability,test,tested,20,specops issue #287. tested with defined truth sensitivities here: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_no_sa/job_history/39f8c4ea-8e21-46d4-a6ba-8e570ce75f8b,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7209
https://github.com/broadinstitute/gatk/issues/7211:1318,Availability,error,error,1318,"s on the versions tested. This does not occur with version 4.1.4.1 where the program manages to process and output the entire VCF. Upon further testing, it seems that the FORMAT field AF causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariants.apply(LeftAlignAndTrimVariants.java:225); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferenceP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7211:2725,Integrability,wrap,wrapAndCopyInto,2725,ATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariants.apply(LeftAlignAndTrimVariants.java:225); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); 	at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7211:1463,Security,validat,validateArg,1463," causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariants.apply(LeftAlignAndTrimVariants.java:225); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7211:1543,Security,validat,validate,1543,"lem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariants.apply(LeftAlignAndTrimVariants.java:225); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$Ite",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7211:124,Testability,Test,Tested,124,"## Bug Report. ### Affected tool(s) or class(es); LeftAlignAndTrimVariants --split-multi-allelics. ### Affected version(s); Tested on versions 4.1.8.0, 4.1.9.0 and 4.2.0.0. ### Description ; GATK LeftAlignAndTrimVariants with option --split-multi-allelics crashes on multiallelic records on the versions tested. This does not occur with version 4.1.4.1 where the program manages to process and output the entire VCF. Upon further testing, it seems that the FORMAT field AF causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7211:304,Testability,test,tested,304,"## Bug Report. ### Affected tool(s) or class(es); LeftAlignAndTrimVariants --split-multi-allelics. ### Affected version(s); Tested on versions 4.1.8.0, 4.1.9.0 and 4.2.0.0. ### Description ; GATK LeftAlignAndTrimVariants with option --split-multi-allelics crashes on multiallelic records on the versions tested. This does not occur with version 4.1.4.1 where the program manages to process and output the entire VCF. Upon further testing, it seems that the FORMAT field AF causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7211:430,Testability,test,testing,430,"## Bug Report. ### Affected tool(s) or class(es); LeftAlignAndTrimVariants --split-multi-allelics. ### Affected version(s); Tested on versions 4.1.8.0, 4.1.9.0 and 4.2.0.0. ### Description ; GATK LeftAlignAndTrimVariants with option --split-multi-allelics crashes on multiallelic records on the versions tested. This does not occur with version 4.1.4.1 where the program manages to process and output the entire VCF. Upon further testing, it seems that the FORMAT field AF causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7211:526,Testability,test,test,526,"## Bug Report. ### Affected tool(s) or class(es); LeftAlignAndTrimVariants --split-multi-allelics. ### Affected version(s); Tested on versions 4.1.8.0, 4.1.9.0 and 4.2.0.0. ### Description ; GATK LeftAlignAndTrimVariants with option --split-multi-allelics crashes on multiallelic records on the versions tested. This does not occur with version 4.1.4.1 where the program manages to process and output the entire VCF. Upon further testing, it seems that the FORMAT field AF causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7211:988,Testability,test,test,988,"## Bug Report. ### Affected tool(s) or class(es); LeftAlignAndTrimVariants --split-multi-allelics. ### Affected version(s); Tested on versions 4.1.8.0, 4.1.9.0 and 4.2.0.0. ### Description ; GATK LeftAlignAndTrimVariants with option --split-multi-allelics crashes on multiallelic records on the versions tested. This does not occur with version 4.1.4.1 where the program manages to process and output the entire VCF. Upon further testing, it seems that the FORMAT field AF causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7211:1006,Testability,test,test,1006,"g Report. ### Affected tool(s) or class(es); LeftAlignAndTrimVariants --split-multi-allelics. ### Affected version(s); Tested on versions 4.1.8.0, 4.1.9.0 and 4.2.0.0. ### Description ; GATK LeftAlignAndTrimVariants with option --split-multi-allelics crashes on multiallelic records on the versions tested. This does not occur with version 4.1.4.1 where the program manages to process and output the entire VCF. Upon further testing, it seems that the FORMAT field AF causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVaria",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211
https://github.com/broadinstitute/gatk/issues/7213:1238,Availability,error,error,1238,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7213:0,Modifiability,Plugin,Plugins,0,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7213:116,Modifiability,plugin,plugins,116,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7213:157,Modifiability,plugin,plugins,157,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7213:453,Modifiability,plugin,plugin,453,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7213:517,Modifiability,extend,extends,517,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7213:752,Modifiability,extend,extends,752,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7213:1220,Modifiability,plugin,plugins,1220,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7213:1316,Modifiability,plugin,plugins,1316,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7213:1468,Modifiability,plugin,plugins,1468,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; @ArgumentCollection; public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213
https://github.com/broadinstitute/gatk/issues/7214:124,Availability,error,error,124,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214
https://github.com/broadinstitute/gatk/issues/7214:148,Availability,ERROR,ERROR,148,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214
https://github.com/broadinstitute/gatk/issues/7214:168,Availability,Error,Error,168,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214
https://github.com/broadinstitute/gatk/issues/7214:236,Availability,Error,Error,236,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214
https://github.com/broadinstitute/gatk/issues/7214:470,Availability,error,error,470,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214
https://github.com/broadinstitute/gatk/issues/7214:493,Availability,ERROR,ERROR,493,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214
https://github.com/broadinstitute/gatk/issues/7214:513,Availability,Error,Error,513,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214
https://github.com/broadinstitute/gatk/issues/7214:581,Availability,Error,Error,581,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214
https://github.com/broadinstitute/gatk/pull/7217:248,Availability,avail,available,248,"A while back I added a .dockstore.yml file to the gatk repo so that gatk workflows in the /script/ folder would be automatically synced in Dockstore after every push or release. This also allowed the workflows in every branch/release to be readily available in Terra. However, GATKs 700+ branches has been causing problems for Dockstore syncs and in some instances associated released tags to be missing ([forum discussion](https://discuss.dockstore.org/t/dockstore-could-not-find-a-workflow-in-git-using-yml-though-it-worked-previously/4255/5)). . This PR adds filters to the dockstore.yml so that only the master branch and the releases gets synced to dockstore, also any future branches arent automatically synced. If anyone wants to sync their branch theyll have to add their branch name to the dockstore.yml file in their branch.; More info on dockstore yml and filters can be found [here](https://docs.dockstore.org/en/develop/getting-started/github-apps/github-apps.html)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7217
https://github.com/broadinstitute/gatk/pull/7217:169,Deployability,release,release,169,"A while back I added a .dockstore.yml file to the gatk repo so that gatk workflows in the /script/ folder would be automatically synced in Dockstore after every push or release. This also allowed the workflows in every branch/release to be readily available in Terra. However, GATKs 700+ branches has been causing problems for Dockstore syncs and in some instances associated released tags to be missing ([forum discussion](https://discuss.dockstore.org/t/dockstore-could-not-find-a-workflow-in-git-using-yml-though-it-worked-previously/4255/5)). . This PR adds filters to the dockstore.yml so that only the master branch and the releases gets synced to dockstore, also any future branches arent automatically synced. If anyone wants to sync their branch theyll have to add their branch name to the dockstore.yml file in their branch.; More info on dockstore yml and filters can be found [here](https://docs.dockstore.org/en/develop/getting-started/github-apps/github-apps.html)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7217
https://github.com/broadinstitute/gatk/pull/7217:226,Deployability,release,release,226,"A while back I added a .dockstore.yml file to the gatk repo so that gatk workflows in the /script/ folder would be automatically synced in Dockstore after every push or release. This also allowed the workflows in every branch/release to be readily available in Terra. However, GATKs 700+ branches has been causing problems for Dockstore syncs and in some instances associated released tags to be missing ([forum discussion](https://discuss.dockstore.org/t/dockstore-could-not-find-a-workflow-in-git-using-yml-though-it-worked-previously/4255/5)). . This PR adds filters to the dockstore.yml so that only the master branch and the releases gets synced to dockstore, also any future branches arent automatically synced. If anyone wants to sync their branch theyll have to add their branch name to the dockstore.yml file in their branch.; More info on dockstore yml and filters can be found [here](https://docs.dockstore.org/en/develop/getting-started/github-apps/github-apps.html)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7217
https://github.com/broadinstitute/gatk/pull/7217:377,Deployability,release,released,377,"A while back I added a .dockstore.yml file to the gatk repo so that gatk workflows in the /script/ folder would be automatically synced in Dockstore after every push or release. This also allowed the workflows in every branch/release to be readily available in Terra. However, GATKs 700+ branches has been causing problems for Dockstore syncs and in some instances associated released tags to be missing ([forum discussion](https://discuss.dockstore.org/t/dockstore-could-not-find-a-workflow-in-git-using-yml-though-it-worked-previously/4255/5)). . This PR adds filters to the dockstore.yml so that only the master branch and the releases gets synced to dockstore, also any future branches arent automatically synced. If anyone wants to sync their branch theyll have to add their branch name to the dockstore.yml file in their branch.; More info on dockstore yml and filters can be found [here](https://docs.dockstore.org/en/develop/getting-started/github-apps/github-apps.html)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7217
https://github.com/broadinstitute/gatk/pull/7217:631,Deployability,release,releases,631,"A while back I added a .dockstore.yml file to the gatk repo so that gatk workflows in the /script/ folder would be automatically synced in Dockstore after every push or release. This also allowed the workflows in every branch/release to be readily available in Terra. However, GATKs 700+ branches has been causing problems for Dockstore syncs and in some instances associated released tags to be missing ([forum discussion](https://discuss.dockstore.org/t/dockstore-could-not-find-a-workflow-in-git-using-yml-though-it-worked-previously/4255/5)). . This PR adds filters to the dockstore.yml so that only the master branch and the releases gets synced to dockstore, also any future branches arent automatically synced. If anyone wants to sync their branch theyll have to add their branch name to the dockstore.yml file in their branch.; More info on dockstore yml and filters can be found [here](https://docs.dockstore.org/en/develop/getting-started/github-apps/github-apps.html)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7217
https://github.com/broadinstitute/gatk/issues/7218:907,Availability,error,error,907,"GenomicsDBImport; Latest public release version [4.2.0.0]. I am running GenomicsDBImport on 5X WGS of ~1000 human samples, which is parallelized for each chromosome with batch size = 400. The code I use is as following:. <pre>gatk --java-options ""-Xmx80G -Xms80G"" GenomicsDBImport \; --genomicsdb-workspace-path ""${outpath}/chr${region}"" \; -L $region \; --sample-name-map $sample_map \; -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa \; --batch-size 400 \; --reader-threads 5; </pre>. It seems that GenomicsDBImport crash after finishing 1 batch for large chromosomes. For example, here I simultaneously run for chr1-12 (not finished yet). For chr5-12, file size of 1 batch is less than 40GB and they successfully finished import batch 1 and running for batch 2 or 3. Thus, the file size for chr5-12 are 59GB now. However, for chr1-4, they just crash in batch 1 for very long time without any error. I have check the memory usage and there is still >35GB free memory for the compute node of each chromosome. Please see the followings for detail:. File size for all chromosomes, the GenomicsDB for chr1-4 is smaller:; <pre>[hcaoad@login-0 GenomicsDB]> du -h --max-depth=1; 59G ./chr10; 59G ./chr6; 50G ./chr2; 59G ./chr12; 59G ./chr9; 59G ./chr5; 59G ./chr7; 48G ./chr1; 59G ./chr11; 59G ./chr8; 40G ./chr4; 41G ./chr3; 647G .; </pre>. Files in GenomicsDB of chr1 batch 1. As you can see, no update for the database since Apr 20 13:34, while current time is Apr 21.; <pre>[hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> pwd; /home/hcaoad/scratch/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/1$1$249250621/.__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> date; Wed Apr 21 11:09:46 HKT 2021; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> ll -h; total 48G; -rwx------ 1 hcaoad boip 260M Apr 20 13:34 AD.tdb; -rwx--",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:4170,Availability,error,error,4170,"4 MLEAF.tdb; -rwx------ 1 hcaoad boip 80M Apr 20 13:34 MLEAF_var.tdb; -rwx------ 1 hcaoad boip 236M Apr 20 13:34 MQRankSum.tdb; -rwx------ 1 hcaoad boip 204M Apr 20 13:34 PGT.tdb; -rwx------ 1 hcaoad boip 6.8M Apr 20 13:34 PGT_var.tdb; -rwx------ 1 hcaoad boip 208M Apr 20 13:34 PID.tdb; -rwx------ 1 hcaoad boip 15M Apr 20 13:34 PID_var.tdb; -rwx------ 1 hcaoad boip 315M Apr 20 13:34 PL.tdb; -rwx------ 1 hcaoad boip 14G Apr 20 13:34 PL_var.tdb; -rwx------ 1 hcaoad boip 173M Apr 20 13:34 PS.tdb; -rwx------ 1 hcaoad boip 496M Apr 20 13:34 QUAL.tdb; -rwx------ 1 hcaoad boip 412M Apr 20 13:34 RAW_MQandDP.tdb; -rwx------ 1 hcaoad boip 358M Apr 20 13:34 ReadPosRankSum.tdb; -rwx------ 1 hcaoad boip 254M Apr 20 13:34 REF.tdb; -rwx------ 1 hcaoad boip 1.5G Apr 20 13:34 REF_var.tdb; -rwx------ 1 hcaoad boip 278M Apr 20 13:34 Samples.tdb; -rwx------ 1 hcaoad boip 256M Apr 20 13:34 Samples_var.tdb; -rwx------ 1 hcaoad boip 510M Apr 20 13:34 SB.tdb; </pre>; Log file for chr1, no error reported:; <pre>Using GATK jar /home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx80G -Xms80G -jar /home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar GenomicsDBImport --genomicsdb-workspace-path /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB//chr1 -L 1 --sample-name-map input/sample.map -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa --batch-size 400 --reader-threads 5; 14:48:08.923 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 16, 2021 2:48:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:48:09.08",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:32,Deployability,release,release,32,"GenomicsDBImport; Latest public release version [4.2.0.0]. I am running GenomicsDBImport on 5X WGS of ~1000 human samples, which is parallelized for each chromosome with batch size = 400. The code I use is as following:. <pre>gatk --java-options ""-Xmx80G -Xms80G"" GenomicsDBImport \; --genomicsdb-workspace-path ""${outpath}/chr${region}"" \; -L $region \; --sample-name-map $sample_map \; -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa \; --batch-size 400 \; --reader-threads 5; </pre>. It seems that GenomicsDBImport crash after finishing 1 batch for large chromosomes. For example, here I simultaneously run for chr1-12 (not finished yet). For chr5-12, file size of 1 batch is less than 40GB and they successfully finished import batch 1 and running for batch 2 or 3. Thus, the file size for chr5-12 are 59GB now. However, for chr1-4, they just crash in batch 1 for very long time without any error. I have check the memory usage and there is still >35GB free memory for the compute node of each chromosome. Please see the followings for detail:. File size for all chromosomes, the GenomicsDB for chr1-4 is smaller:; <pre>[hcaoad@login-0 GenomicsDB]> du -h --max-depth=1; 59G ./chr10; 59G ./chr6; 50G ./chr2; 59G ./chr12; 59G ./chr9; 59G ./chr5; 59G ./chr7; 48G ./chr1; 59G ./chr11; 59G ./chr8; 40G ./chr4; 41G ./chr3; 647G .; </pre>. Files in GenomicsDB of chr1 batch 1. As you can see, no update for the database since Apr 20 13:34, while current time is Apr 21.; <pre>[hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> pwd; /home/hcaoad/scratch/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/1$1$249250621/.__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> date; Wed Apr 21 11:09:46 HKT 2021; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> ll -h; total 48G; -rwx------ 1 hcaoad boip 260M Apr 20 13:34 AD.tdb; -rwx--",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:1405,Deployability,update,update,1405,"GRCh37/hs37d5.fa \; --batch-size 400 \; --reader-threads 5; </pre>. It seems that GenomicsDBImport crash after finishing 1 batch for large chromosomes. For example, here I simultaneously run for chr1-12 (not finished yet). For chr5-12, file size of 1 batch is less than 40GB and they successfully finished import batch 1 and running for batch 2 or 3. Thus, the file size for chr5-12 are 59GB now. However, for chr1-4, they just crash in batch 1 for very long time without any error. I have check the memory usage and there is still >35GB free memory for the compute node of each chromosome. Please see the followings for detail:. File size for all chromosomes, the GenomicsDB for chr1-4 is smaller:; <pre>[hcaoad@login-0 GenomicsDB]> du -h --max-depth=1; 59G ./chr10; 59G ./chr6; 50G ./chr2; 59G ./chr12; 59G ./chr9; 59G ./chr5; 59G ./chr7; 48G ./chr1; 59G ./chr11; 59G ./chr8; 40G ./chr4; 41G ./chr3; 647G .; </pre>. Files in GenomicsDB of chr1 batch 1. As you can see, no update for the database since Apr 20 13:34, while current time is Apr 21.; <pre>[hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> pwd; /home/hcaoad/scratch/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/1$1$249250621/.__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> date; Wed Apr 21 11:09:46 HKT 2021; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> ll -h; total 48G; -rwx------ 1 hcaoad boip 260M Apr 20 13:34 AD.tdb; -rwx------ 1 hcaoad boip 203M Apr 20 13:34 AD_var.tdb; -rwx------ 1 hcaoad boip 304M Apr 20 13:34 ALT.tdb; -rwx------ 1 hcaoad boip 146M Apr 20 13:34 ALT_var.tdb; -rwx------ 1 hcaoad boip 353M Apr 20 13:34 BaseQRankSum.tdb; -rwx------ 1 hcaoad boip 7.3G Apr 20 13:34 __coords.tdb; -rwx------ 1 hcaoad boip 132M Apr 20 13:34 DB.tdb; -rwx------ 1 hcaoad boip 3.4G Apr 20 13:34 DP_FORMAT.tdb; -rwx------ 1 hcaoad boip 295M Apr 20 13:34 DP.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:5673,Deployability,release,release-,5673,"le.map -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa --batch-size 400 --reader-threads 5; 14:48:08.923 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 16, 2021 2:48:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:48:09.080 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0; 14:48:09.081 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:48:09.081 INFO GenomicsDBImport - Executing as hcaoad@hhnode-ib-46 on Linux v3.10.0-1062.el7.x86_64 amd64; 14:48:09.081 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:48:09.081 INFO GenomicsDBImport - Start Date/Time: April 16, 2021 2:48:08 PM HKT; 14:48:09.081 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Version: 2.24.0; 14:48:09.081 INFO GenomicsDBImport - Picard Version: 2.25.0; 14:48:09.081 INFO GenomicsDBImport - Built for Spark Version: 2.4.5; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:48:09.082 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:48:09.082 INFO GenomicsDBImport - Deflater: IntelDeflater; 14:48:09.082 INFO GenomicsDBImport - Inflater: IntelInflater; 14:48:09.082 INFO GenomicsDBImport - GCS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:8034,Deployability,update,update,8034," - Picard Version: 2.25.0; 14:48:09.081 INFO GenomicsDBImport - Built for Spark Version: 2.4.5; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:48:09.082 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:48:09.082 INFO GenomicsDBImport - Deflater: IntelDeflater; 14:48:09.082 INFO GenomicsDBImport - Inflater: IntelInflater; 14:48:09.082 INFO GenomicsDBImport - GCS max retries/reopens: 20; 14:48:09.082 INFO GenomicsDBImport - Requester pays: disabled; 14:48:09.082 INFO GenomicsDBImport - Initializing engine; 14:48:09.524 INFO IntervalArgumentCollection - Processing 249250621 bp from intervals; 14:48:09.551 INFO GenomicsDBImport - Done initializing engine; 14:48:09.781 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 14:48:09.782 INFO GenomicsDBImport - Vid Map JSON file will be written to /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/vidmap.json; 14:48:09.782 INFO GenomicsDBImport - Callset Map JSON file will be written to /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/callset.json; 14:48:09.782 INFO GenomicsDBImport - Complete VCF Header will be written to /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/vcfheader.vcf; 14:48:09.782 INFO GenomicsDBImport - Importing to workspace - /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB/chr1; 14:48:09.783 INFO ProgressMeter - Starting traversal; 14:48:09.783 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 14:48:09.935 INFO GenomicsDBImport - Starting batch input file preload; 14:48:21.686 INFO GenomicsDBImport - Finished batch preload; 14:48:21.686 INFO GenomicsDBImport - Importing batch 1 with 400 samples; </pre>. I am now trying to solve this by reducing batch size, will update once it finished for batch1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:4827,Performance,Load,Loading,4827,"p 358M Apr 20 13:34 ReadPosRankSum.tdb; -rwx------ 1 hcaoad boip 254M Apr 20 13:34 REF.tdb; -rwx------ 1 hcaoad boip 1.5G Apr 20 13:34 REF_var.tdb; -rwx------ 1 hcaoad boip 278M Apr 20 13:34 Samples.tdb; -rwx------ 1 hcaoad boip 256M Apr 20 13:34 Samples_var.tdb; -rwx------ 1 hcaoad boip 510M Apr 20 13:34 SB.tdb; </pre>; Log file for chr1, no error reported:; <pre>Using GATK jar /home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx80G -Xms80G -jar /home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar GenomicsDBImport --genomicsdb-workspace-path /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB//chr1 -L 1 --sample-name-map input/sample.map -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa --batch-size 400 --reader-threads 5; 14:48:08.923 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 16, 2021 2:48:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:48:09.080 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0; 14:48:09.081 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:48:09.081 INFO GenomicsDBImport - Executing as hcaoad@hhnode-ib-46 on Linux v3.10.0-1062.el7.x86_64 amd64; 14:48:09.081 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:48:09.081 INFO GenomicsDBImport - Start Date/Time: April 16, 2021 2:48:08 PM HKT; 14:48:09.081 INFO GenomicsDBImport - ------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:5122,Safety,detect,detect,5122,"M Apr 20 13:34 SB.tdb; </pre>; Log file for chr1, no error reported:; <pre>Using GATK jar /home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx80G -Xms80G -jar /home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar GenomicsDBImport --genomicsdb-workspace-path /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB//chr1 -L 1 --sample-name-map input/sample.map -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa --batch-size 400 --reader-threads 5; 14:48:08.923 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 16, 2021 2:48:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:48:09.080 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0; 14:48:09.081 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:48:09.081 INFO GenomicsDBImport - Executing as hcaoad@hhnode-ib-46 on Linux v3.10.0-1062.el7.x86_64 amd64; 14:48:09.081 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:48:09.081 INFO GenomicsDBImport - Start Date/Time: April 16, 2021 2:48:08 PM HKT; 14:48:09.081 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Version: 2.24.0; 14:48:09.081 INFO GenomicsDBImport - Picard Version: 2.25.0; 14:48:09.081 INFO Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:1144,Testability,log,login-,1144,"ich is parallelized for each chromosome with batch size = 400. The code I use is as following:. <pre>gatk --java-options ""-Xmx80G -Xms80G"" GenomicsDBImport \; --genomicsdb-workspace-path ""${outpath}/chr${region}"" \; -L $region \; --sample-name-map $sample_map \; -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa \; --batch-size 400 \; --reader-threads 5; </pre>. It seems that GenomicsDBImport crash after finishing 1 batch for large chromosomes. For example, here I simultaneously run for chr1-12 (not finished yet). For chr5-12, file size of 1 batch is less than 40GB and they successfully finished import batch 1 and running for batch 2 or 3. Thus, the file size for chr5-12 are 59GB now. However, for chr1-4, they just crash in batch 1 for very long time without any error. I have check the memory usage and there is still >35GB free memory for the compute node of each chromosome. Please see the followings for detail:. File size for all chromosomes, the GenomicsDB for chr1-4 is smaller:; <pre>[hcaoad@login-0 GenomicsDB]> du -h --max-depth=1; 59G ./chr10; 59G ./chr6; 50G ./chr2; 59G ./chr12; 59G ./chr9; 59G ./chr5; 59G ./chr7; 48G ./chr1; 59G ./chr11; 59G ./chr8; 40G ./chr4; 41G ./chr3; 647G .; </pre>. Files in GenomicsDB of chr1 batch 1. As you can see, no update for the database since Apr 20 13:34, while current time is Apr 21.; <pre>[hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> pwd; /home/hcaoad/scratch/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/1$1$249250621/.__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> date; Wed Apr 21 11:09:46 HKT 2021; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> ll -h; total 48G; -rwx------ 1 hcaoad boip 260M Apr 20 13:34 AD.tdb; -rwx------ 1 hcaoad boip 203M Apr 20 13:34 AD_var.tdb; -rwx------ 1 hcaoad boip 304M Apr 20 13:34 ALT.tdb; -rwx------ 1 hcaoad boip",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:1493,Testability,log,login-,1493," </pre>. It seems that GenomicsDBImport crash after finishing 1 batch for large chromosomes. For example, here I simultaneously run for chr1-12 (not finished yet). For chr5-12, file size of 1 batch is less than 40GB and they successfully finished import batch 1 and running for batch 2 or 3. Thus, the file size for chr5-12 are 59GB now. However, for chr1-4, they just crash in batch 1 for very long time without any error. I have check the memory usage and there is still >35GB free memory for the compute node of each chromosome. Please see the followings for detail:. File size for all chromosomes, the GenomicsDB for chr1-4 is smaller:; <pre>[hcaoad@login-0 GenomicsDB]> du -h --max-depth=1; 59G ./chr10; 59G ./chr6; 50G ./chr2; 59G ./chr12; 59G ./chr9; 59G ./chr5; 59G ./chr7; 48G ./chr1; 59G ./chr11; 59G ./chr8; 40G ./chr4; 41G ./chr3; 647G .; </pre>. Files in GenomicsDB of chr1 batch 1. As you can see, no update for the database since Apr 20 13:34, while current time is Apr 21.; <pre>[hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> pwd; /home/hcaoad/scratch/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/1$1$249250621/.__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> date; Wed Apr 21 11:09:46 HKT 2021; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> ll -h; total 48G; -rwx------ 1 hcaoad boip 260M Apr 20 13:34 AD.tdb; -rwx------ 1 hcaoad boip 203M Apr 20 13:34 AD_var.tdb; -rwx------ 1 hcaoad boip 304M Apr 20 13:34 ALT.tdb; -rwx------ 1 hcaoad boip 146M Apr 20 13:34 ALT_var.tdb; -rwx------ 1 hcaoad boip 353M Apr 20 13:34 BaseQRankSum.tdb; -rwx------ 1 hcaoad boip 7.3G Apr 20 13:34 __coords.tdb; -rwx------ 1 hcaoad boip 132M Apr 20 13:34 DB.tdb; -rwx------ 1 hcaoad boip 3.4G Apr 20 13:34 DP_FORMAT.tdb; -rwx------ 1 hcaoad boip 295M Apr 20 13:34 DP.tdb; -rwx------ 1 hcaoad boip 8.1G Apr 20 13:34 END.tdb; -r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:1724,Testability,log,login-,1724," less than 40GB and they successfully finished import batch 1 and running for batch 2 or 3. Thus, the file size for chr5-12 are 59GB now. However, for chr1-4, they just crash in batch 1 for very long time without any error. I have check the memory usage and there is still >35GB free memory for the compute node of each chromosome. Please see the followings for detail:. File size for all chromosomes, the GenomicsDB for chr1-4 is smaller:; <pre>[hcaoad@login-0 GenomicsDB]> du -h --max-depth=1; 59G ./chr10; 59G ./chr6; 50G ./chr2; 59G ./chr12; 59G ./chr9; 59G ./chr5; 59G ./chr7; 48G ./chr1; 59G ./chr11; 59G ./chr8; 40G ./chr4; 41G ./chr3; 647G .; </pre>. Files in GenomicsDB of chr1 batch 1. As you can see, no update for the database since Apr 20 13:34, while current time is Apr 21.; <pre>[hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> pwd; /home/hcaoad/scratch/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/1$1$249250621/.__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> date; Wed Apr 21 11:09:46 HKT 2021; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> ll -h; total 48G; -rwx------ 1 hcaoad boip 260M Apr 20 13:34 AD.tdb; -rwx------ 1 hcaoad boip 203M Apr 20 13:34 AD_var.tdb; -rwx------ 1 hcaoad boip 304M Apr 20 13:34 ALT.tdb; -rwx------ 1 hcaoad boip 146M Apr 20 13:34 ALT_var.tdb; -rwx------ 1 hcaoad boip 353M Apr 20 13:34 BaseQRankSum.tdb; -rwx------ 1 hcaoad boip 7.3G Apr 20 13:34 __coords.tdb; -rwx------ 1 hcaoad boip 132M Apr 20 13:34 DB.tdb; -rwx------ 1 hcaoad boip 3.4G Apr 20 13:34 DP_FORMAT.tdb; -rwx------ 1 hcaoad boip 295M Apr 20 13:34 DP.tdb; -rwx------ 1 hcaoad boip 8.1G Apr 20 13:34 END.tdb; -rwx------ 1 hcaoad boip 223M Apr 20 13:34 ExcessHet.tdb; -rwx------ 1 hcaoad boip 158M Apr 20 13:34 FILTER.tdb; -rwx------ 1 hcaoad boip 0 Apr 20 13:34 FILTER_var.tdb; -rwx------ 1 hcaoad boip 3.7G Apr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:1847,Testability,log,login-,1847,"le size for chr5-12 are 59GB now. However, for chr1-4, they just crash in batch 1 for very long time without any error. I have check the memory usage and there is still >35GB free memory for the compute node of each chromosome. Please see the followings for detail:. File size for all chromosomes, the GenomicsDB for chr1-4 is smaller:; <pre>[hcaoad@login-0 GenomicsDB]> du -h --max-depth=1; 59G ./chr10; 59G ./chr6; 50G ./chr2; 59G ./chr12; 59G ./chr9; 59G ./chr5; 59G ./chr7; 48G ./chr1; 59G ./chr11; 59G ./chr8; 40G ./chr4; 41G ./chr3; 647G .; </pre>. Files in GenomicsDB of chr1 batch 1. As you can see, no update for the database since Apr 20 13:34, while current time is Apr 21.; <pre>[hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> pwd; /home/hcaoad/scratch/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/1$1$249250621/.__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> date; Wed Apr 21 11:09:46 HKT 2021; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> ll -h; total 48G; -rwx------ 1 hcaoad boip 260M Apr 20 13:34 AD.tdb; -rwx------ 1 hcaoad boip 203M Apr 20 13:34 AD_var.tdb; -rwx------ 1 hcaoad boip 304M Apr 20 13:34 ALT.tdb; -rwx------ 1 hcaoad boip 146M Apr 20 13:34 ALT_var.tdb; -rwx------ 1 hcaoad boip 353M Apr 20 13:34 BaseQRankSum.tdb; -rwx------ 1 hcaoad boip 7.3G Apr 20 13:34 __coords.tdb; -rwx------ 1 hcaoad boip 132M Apr 20 13:34 DB.tdb; -rwx------ 1 hcaoad boip 3.4G Apr 20 13:34 DP_FORMAT.tdb; -rwx------ 1 hcaoad boip 295M Apr 20 13:34 DP.tdb; -rwx------ 1 hcaoad boip 8.1G Apr 20 13:34 END.tdb; -rwx------ 1 hcaoad boip 223M Apr 20 13:34 ExcessHet.tdb; -rwx------ 1 hcaoad boip 158M Apr 20 13:34 FILTER.tdb; -rwx------ 1 hcaoad boip 0 Apr 20 13:34 FILTER_var.tdb; -rwx------ 1 hcaoad boip 3.7G Apr 20 13:34 GQ.tdb; -rwx------ 1 hcaoad boip 253M Apr 20 13:34 GT.tdb; -rwx------ 1 hcaoad boip 277M Apr 20",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/issues/7218:4148,Testability,Log,Log,4148,"4 MLEAF.tdb; -rwx------ 1 hcaoad boip 80M Apr 20 13:34 MLEAF_var.tdb; -rwx------ 1 hcaoad boip 236M Apr 20 13:34 MQRankSum.tdb; -rwx------ 1 hcaoad boip 204M Apr 20 13:34 PGT.tdb; -rwx------ 1 hcaoad boip 6.8M Apr 20 13:34 PGT_var.tdb; -rwx------ 1 hcaoad boip 208M Apr 20 13:34 PID.tdb; -rwx------ 1 hcaoad boip 15M Apr 20 13:34 PID_var.tdb; -rwx------ 1 hcaoad boip 315M Apr 20 13:34 PL.tdb; -rwx------ 1 hcaoad boip 14G Apr 20 13:34 PL_var.tdb; -rwx------ 1 hcaoad boip 173M Apr 20 13:34 PS.tdb; -rwx------ 1 hcaoad boip 496M Apr 20 13:34 QUAL.tdb; -rwx------ 1 hcaoad boip 412M Apr 20 13:34 RAW_MQandDP.tdb; -rwx------ 1 hcaoad boip 358M Apr 20 13:34 ReadPosRankSum.tdb; -rwx------ 1 hcaoad boip 254M Apr 20 13:34 REF.tdb; -rwx------ 1 hcaoad boip 1.5G Apr 20 13:34 REF_var.tdb; -rwx------ 1 hcaoad boip 278M Apr 20 13:34 Samples.tdb; -rwx------ 1 hcaoad boip 256M Apr 20 13:34 Samples_var.tdb; -rwx------ 1 hcaoad boip 510M Apr 20 13:34 SB.tdb; </pre>; Log file for chr1, no error reported:; <pre>Using GATK jar /home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx80G -Xms80G -jar /home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar GenomicsDBImport --genomicsdb-workspace-path /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB//chr1 -L 1 --sample-name-map input/sample.map -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa --batch-size 400 --reader-threads 5; 14:48:08.923 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 16, 2021 2:48:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:48:09.08",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218
https://github.com/broadinstitute/gatk/pull/7219:108,Energy Efficiency,efficient,efficient,108,"Rationale: MultiVariantWalkers, including iterators like MultiVariantWalkerGroupedOnStart, are a useful and efficient iteration pattern. However, it is often essential to know the FeatureInput source of the variant.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7219
https://github.com/broadinstitute/gatk/pull/7220:108,Energy Efficiency,efficient,efficient,108,"Rationale: MultiVariantWalkers, including iterators like MultiVariantWalkerGroupedOnStart, are a useful and efficient iteration pattern. However, it is often essential to know the FeatureInput source of the variant. . This PR would set the value of source only for MultiVariantDataSource. It does so by wrapping the iterator. I probably prefer the alternate approach proposed here: #7219 though, since it avoids re-creating the VC. If #7219 is merged we would close this PR. . @cmnbroad this is related to discussion on #6973.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7220
https://github.com/broadinstitute/gatk/pull/7220:303,Integrability,wrap,wrapping,303,"Rationale: MultiVariantWalkers, including iterators like MultiVariantWalkerGroupedOnStart, are a useful and efficient iteration pattern. However, it is often essential to know the FeatureInput source of the variant. . This PR would set the value of source only for MultiVariantDataSource. It does so by wrapping the iterator. I probably prefer the alternate approach proposed here: #7219 though, since it avoids re-creating the VC. If #7219 is merged we would close this PR. . @cmnbroad this is related to discussion on #6973.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7220
https://github.com/broadinstitute/gatk/pull/7220:405,Safety,avoid,avoids,405,"Rationale: MultiVariantWalkers, including iterators like MultiVariantWalkerGroupedOnStart, are a useful and efficient iteration pattern. However, it is often essential to know the FeatureInput source of the variant. . This PR would set the value of source only for MultiVariantDataSource. It does so by wrapping the iterator. I probably prefer the alternate approach proposed here: #7219 though, since it avoids re-creating the VC. If #7219 is merged we would close this PR. . @cmnbroad this is related to discussion on #6973.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7220
https://github.com/broadinstitute/gatk/pull/7221:109,Testability,test,test,109,"Dont just use the default of 6--pass a param for the value instead (in the wdl and gatk tool). I ran a smoke test and the filtering completed successfully, but I'm not totally sure how to make sure it used the passed in param over the default. How can I check this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7221
https://github.com/broadinstitute/gatk/issues/7222:197,Energy Efficiency,meter,meter,197,## Bug Report. ### Affected tool(s) or class(es); _GenomicsDBImport_. ### Affected version(s); - 4.2.0.0; - after git revision 2b949f0dda49f495ce8fc7e3b8528cbc534e4819. ### Description ; _Progress meter does not accurately describe genomic locus being processed._. #### Steps to reproduce; _Run GenomicsDBImport (e.g. using -L to subset to a specific genomic locus)_. #### Expected behavior; _Progress meter should display the locus that is currently being processed during the import_. #### Actual behavior; _Progress meter displays unmaped locus. See [this forum post](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077628671-GenomicsDBImport-output-unmapped-in-ProgressMeter-and-running-very-slow-with-WGS-data-of-large-sample-size) for an example_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7222
https://github.com/broadinstitute/gatk/issues/7222:402,Energy Efficiency,meter,meter,402,## Bug Report. ### Affected tool(s) or class(es); _GenomicsDBImport_. ### Affected version(s); - 4.2.0.0; - after git revision 2b949f0dda49f495ce8fc7e3b8528cbc534e4819. ### Description ; _Progress meter does not accurately describe genomic locus being processed._. #### Steps to reproduce; _Run GenomicsDBImport (e.g. using -L to subset to a specific genomic locus)_. #### Expected behavior; _Progress meter should display the locus that is currently being processed during the import_. #### Actual behavior; _Progress meter displays unmaped locus. See [this forum post](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077628671-GenomicsDBImport-output-unmapped-in-ProgressMeter-and-running-very-slow-with-WGS-data-of-large-sample-size) for an example_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7222
https://github.com/broadinstitute/gatk/issues/7222:519,Energy Efficiency,meter,meter,519,## Bug Report. ### Affected tool(s) or class(es); _GenomicsDBImport_. ### Affected version(s); - 4.2.0.0; - after git revision 2b949f0dda49f495ce8fc7e3b8528cbc534e4819. ### Description ; _Progress meter does not accurately describe genomic locus being processed._. #### Steps to reproduce; _Run GenomicsDBImport (e.g. using -L to subset to a specific genomic locus)_. #### Expected behavior; _Progress meter should display the locus that is currently being processed during the import_. #### Actual behavior; _Progress meter displays unmaped locus. See [this forum post](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077628671-GenomicsDBImport-output-unmapped-in-ProgressMeter-and-running-very-slow-with-WGS-data-of-large-sample-size) for an example_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7222
https://github.com/broadinstitute/gatk/pull/7223:70,Energy Efficiency,reduce,reduce,70,"We want to move all the production GVCFs to the ""reblocked"" format to reduce the storage footprint. The new format doesn't list PLs for hom ref genotypes, so some changes to GenotypeGVCFs need to be made.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7223
https://github.com/broadinstitute/gatk/pull/7224:71,Deployability,release,releases,71,"Moving to [GenomicsDB 1.4.1 ](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.1)release will allow for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the `--genomicsdb-use-gcs-hdfs-connector` option. Using the native client with gcs allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc. The defaults are all hardcoded to match what is in gatk at present. It also helps with performance issues with gcs, see #7070. This version also contains fixes for #7089, although it will require additional support from gatk(will be part of a separate PR).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7224
https://github.com/broadinstitute/gatk/pull/7224:91,Deployability,release,release,91,"Moving to [GenomicsDB 1.4.1 ](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.1)release will allow for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the `--genomicsdb-use-gcs-hdfs-connector` option. Using the native client with gcs allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc. The defaults are all hardcoded to match what is in gatk at present. It also helps with performance issues with gcs, see #7070. This version also contains fixes for #7089, although it will require additional support from gatk(will be part of a separate PR).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7224
https://github.com/broadinstitute/gatk/pull/7224:463,Modifiability,config,configuring,463,"Moving to [GenomicsDB 1.4.1 ](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.1)release will allow for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the `--genomicsdb-use-gcs-hdfs-connector` option. Using the native client with gcs allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc. The defaults are all hardcoded to match what is in gatk at present. It also helps with performance issues with gcs, see #7070. This version also contains fixes for #7089, although it will require additional support from gatk(will be part of a separate PR).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7224
https://github.com/broadinstitute/gatk/pull/7224:580,Performance,perform,performance,580,"Moving to [GenomicsDB 1.4.1 ](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.1)release will allow for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the `--genomicsdb-use-gcs-hdfs-connector` option. Using the native client with gcs allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc. The defaults are all hardcoded to match what is in gatk at present. It also helps with performance issues with gcs, see #7070. This version also contains fixes for #7089, although it will require additional support from gatk(will be part of a separate PR).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7224
https://github.com/broadinstitute/gatk/pull/7224:413,Security,authenticat,authentication,413,"Moving to [GenomicsDB 1.4.1 ](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.1)release will allow for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the `--genomicsdb-use-gcs-hdfs-connector` option. Using the native client with gcs allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc. The defaults are all hardcoded to match what is in gatk at present. It also helps with performance issues with gcs, see #7070. This version also contains fixes for #7089, although it will require additional support from gatk(will be part of a separate PR).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7224
https://github.com/broadinstitute/gatk/issues/7225:19,Availability,error,error,19,"Hi,; I've got some error messages about ""tranches"" when I'm running VariantRecalibrator. Here is my command lines:; ```; gatk=/public/home/fan_lab/shali/gatk/gatk-4.1.8.1/gatk; reference=/public/home/fan_lab/shali/gatk/gatk_bundle/Homo_sapiens_assembly38.fasta; GATK_bundle=/public/home/fan_lab/shali/gatk/gatk_bundle. indir=/public/home/fan_lab/shali/NGS_new; outdir=/public/home/fan_lab/shali/NGS_new. gatk VariantRecalibrator \; -R $reference \; --variant $outdir/population/24samples.HC.vcf.gz \; --resource:hapmap,known=false,training=true,truth=true,prior=15.0 $GATK_bundle/hapmap_3.3.hg38.vcf.gz \; --resource:omni,known=false,training=true,truth=false,prior=12.0 $GATK_bundle/1000G_omni2.5.hg38.vcf.gz \; --resource:1000G,known=false,training=true,truth=false,prior=10.0 $GATK_bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource:dbsnp,known=true,training=false,truth=false,prior=6.0 $GATK_bundle/dbsnp_146.hg38.vcf.gz \; -an DP -an QD -an FS -an SOR -an ReadPosRankSum -an MQRankSum \; --mode SNP \; --tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 95.0 -tranche 90.0 \; --rscript-file $outdir/population/24samples.HC.snps.plots.R \; --tranches-file $outdir/poplation/24samples.HC.snps.tranches \; -O $outdir/poplation/24samples.HC.snps.recal; ```; The mission ended quickly.When I look at the log file, I find the following error message:; ```; A USER ERROR has occurred: /public/home/fan_lab/shali/NGS_new/poplation/24samples.HC.snps.tranches; ```; Any suggestions for me? Look forward to your reply. Thanks ever so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7225
https://github.com/broadinstitute/gatk/issues/7225:1352,Availability,error,error,1352,"Hi,; I've got some error messages about ""tranches"" when I'm running VariantRecalibrator. Here is my command lines:; ```; gatk=/public/home/fan_lab/shali/gatk/gatk-4.1.8.1/gatk; reference=/public/home/fan_lab/shali/gatk/gatk_bundle/Homo_sapiens_assembly38.fasta; GATK_bundle=/public/home/fan_lab/shali/gatk/gatk_bundle. indir=/public/home/fan_lab/shali/NGS_new; outdir=/public/home/fan_lab/shali/NGS_new. gatk VariantRecalibrator \; -R $reference \; --variant $outdir/population/24samples.HC.vcf.gz \; --resource:hapmap,known=false,training=true,truth=true,prior=15.0 $GATK_bundle/hapmap_3.3.hg38.vcf.gz \; --resource:omni,known=false,training=true,truth=false,prior=12.0 $GATK_bundle/1000G_omni2.5.hg38.vcf.gz \; --resource:1000G,known=false,training=true,truth=false,prior=10.0 $GATK_bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource:dbsnp,known=true,training=false,truth=false,prior=6.0 $GATK_bundle/dbsnp_146.hg38.vcf.gz \; -an DP -an QD -an FS -an SOR -an ReadPosRankSum -an MQRankSum \; --mode SNP \; --tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 95.0 -tranche 90.0 \; --rscript-file $outdir/population/24samples.HC.snps.plots.R \; --tranches-file $outdir/poplation/24samples.HC.snps.tranches \; -O $outdir/poplation/24samples.HC.snps.recal; ```; The mission ended quickly.When I look at the log file, I find the following error message:; ```; A USER ERROR has occurred: /public/home/fan_lab/shali/NGS_new/poplation/24samples.HC.snps.tranches; ```; Any suggestions for me? Look forward to your reply. Thanks ever so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7225
https://github.com/broadinstitute/gatk/issues/7225:1380,Availability,ERROR,ERROR,1380,"Hi,; I've got some error messages about ""tranches"" when I'm running VariantRecalibrator. Here is my command lines:; ```; gatk=/public/home/fan_lab/shali/gatk/gatk-4.1.8.1/gatk; reference=/public/home/fan_lab/shali/gatk/gatk_bundle/Homo_sapiens_assembly38.fasta; GATK_bundle=/public/home/fan_lab/shali/gatk/gatk_bundle. indir=/public/home/fan_lab/shali/NGS_new; outdir=/public/home/fan_lab/shali/NGS_new. gatk VariantRecalibrator \; -R $reference \; --variant $outdir/population/24samples.HC.vcf.gz \; --resource:hapmap,known=false,training=true,truth=true,prior=15.0 $GATK_bundle/hapmap_3.3.hg38.vcf.gz \; --resource:omni,known=false,training=true,truth=false,prior=12.0 $GATK_bundle/1000G_omni2.5.hg38.vcf.gz \; --resource:1000G,known=false,training=true,truth=false,prior=10.0 $GATK_bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource:dbsnp,known=true,training=false,truth=false,prior=6.0 $GATK_bundle/dbsnp_146.hg38.vcf.gz \; -an DP -an QD -an FS -an SOR -an ReadPosRankSum -an MQRankSum \; --mode SNP \; --tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 95.0 -tranche 90.0 \; --rscript-file $outdir/population/24samples.HC.snps.plots.R \; --tranches-file $outdir/poplation/24samples.HC.snps.tranches \; -O $outdir/poplation/24samples.HC.snps.recal; ```; The mission ended quickly.When I look at the log file, I find the following error message:; ```; A USER ERROR has occurred: /public/home/fan_lab/shali/NGS_new/poplation/24samples.HC.snps.tranches; ```; Any suggestions for me? Look forward to your reply. Thanks ever so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7225
https://github.com/broadinstitute/gatk/issues/7225:25,Integrability,message,messages,25,"Hi,; I've got some error messages about ""tranches"" when I'm running VariantRecalibrator. Here is my command lines:; ```; gatk=/public/home/fan_lab/shali/gatk/gatk-4.1.8.1/gatk; reference=/public/home/fan_lab/shali/gatk/gatk_bundle/Homo_sapiens_assembly38.fasta; GATK_bundle=/public/home/fan_lab/shali/gatk/gatk_bundle. indir=/public/home/fan_lab/shali/NGS_new; outdir=/public/home/fan_lab/shali/NGS_new. gatk VariantRecalibrator \; -R $reference \; --variant $outdir/population/24samples.HC.vcf.gz \; --resource:hapmap,known=false,training=true,truth=true,prior=15.0 $GATK_bundle/hapmap_3.3.hg38.vcf.gz \; --resource:omni,known=false,training=true,truth=false,prior=12.0 $GATK_bundle/1000G_omni2.5.hg38.vcf.gz \; --resource:1000G,known=false,training=true,truth=false,prior=10.0 $GATK_bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource:dbsnp,known=true,training=false,truth=false,prior=6.0 $GATK_bundle/dbsnp_146.hg38.vcf.gz \; -an DP -an QD -an FS -an SOR -an ReadPosRankSum -an MQRankSum \; --mode SNP \; --tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 95.0 -tranche 90.0 \; --rscript-file $outdir/population/24samples.HC.snps.plots.R \; --tranches-file $outdir/poplation/24samples.HC.snps.tranches \; -O $outdir/poplation/24samples.HC.snps.recal; ```; The mission ended quickly.When I look at the log file, I find the following error message:; ```; A USER ERROR has occurred: /public/home/fan_lab/shali/NGS_new/poplation/24samples.HC.snps.tranches; ```; Any suggestions for me? Look forward to your reply. Thanks ever so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7225
https://github.com/broadinstitute/gatk/issues/7225:1358,Integrability,message,message,1358,"Hi,; I've got some error messages about ""tranches"" when I'm running VariantRecalibrator. Here is my command lines:; ```; gatk=/public/home/fan_lab/shali/gatk/gatk-4.1.8.1/gatk; reference=/public/home/fan_lab/shali/gatk/gatk_bundle/Homo_sapiens_assembly38.fasta; GATK_bundle=/public/home/fan_lab/shali/gatk/gatk_bundle. indir=/public/home/fan_lab/shali/NGS_new; outdir=/public/home/fan_lab/shali/NGS_new. gatk VariantRecalibrator \; -R $reference \; --variant $outdir/population/24samples.HC.vcf.gz \; --resource:hapmap,known=false,training=true,truth=true,prior=15.0 $GATK_bundle/hapmap_3.3.hg38.vcf.gz \; --resource:omni,known=false,training=true,truth=false,prior=12.0 $GATK_bundle/1000G_omni2.5.hg38.vcf.gz \; --resource:1000G,known=false,training=true,truth=false,prior=10.0 $GATK_bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource:dbsnp,known=true,training=false,truth=false,prior=6.0 $GATK_bundle/dbsnp_146.hg38.vcf.gz \; -an DP -an QD -an FS -an SOR -an ReadPosRankSum -an MQRankSum \; --mode SNP \; --tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 95.0 -tranche 90.0 \; --rscript-file $outdir/population/24samples.HC.snps.plots.R \; --tranches-file $outdir/poplation/24samples.HC.snps.tranches \; -O $outdir/poplation/24samples.HC.snps.recal; ```; The mission ended quickly.When I look at the log file, I find the following error message:; ```; A USER ERROR has occurred: /public/home/fan_lab/shali/NGS_new/poplation/24samples.HC.snps.tranches; ```; Any suggestions for me? Look forward to your reply. Thanks ever so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7225
https://github.com/broadinstitute/gatk/issues/7225:1321,Testability,log,log,1321,"Hi,; I've got some error messages about ""tranches"" when I'm running VariantRecalibrator. Here is my command lines:; ```; gatk=/public/home/fan_lab/shali/gatk/gatk-4.1.8.1/gatk; reference=/public/home/fan_lab/shali/gatk/gatk_bundle/Homo_sapiens_assembly38.fasta; GATK_bundle=/public/home/fan_lab/shali/gatk/gatk_bundle. indir=/public/home/fan_lab/shali/NGS_new; outdir=/public/home/fan_lab/shali/NGS_new. gatk VariantRecalibrator \; -R $reference \; --variant $outdir/population/24samples.HC.vcf.gz \; --resource:hapmap,known=false,training=true,truth=true,prior=15.0 $GATK_bundle/hapmap_3.3.hg38.vcf.gz \; --resource:omni,known=false,training=true,truth=false,prior=12.0 $GATK_bundle/1000G_omni2.5.hg38.vcf.gz \; --resource:1000G,known=false,training=true,truth=false,prior=10.0 $GATK_bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource:dbsnp,known=true,training=false,truth=false,prior=6.0 $GATK_bundle/dbsnp_146.hg38.vcf.gz \; -an DP -an QD -an FS -an SOR -an ReadPosRankSum -an MQRankSum \; --mode SNP \; --tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 95.0 -tranche 90.0 \; --rscript-file $outdir/population/24samples.HC.snps.plots.R \; --tranches-file $outdir/poplation/24samples.HC.snps.tranches \; -O $outdir/poplation/24samples.HC.snps.recal; ```; The mission ended quickly.When I look at the log file, I find the following error message:; ```; A USER ERROR has occurred: /public/home/fan_lab/shali/NGS_new/poplation/24samples.HC.snps.tranches; ```; Any suggestions for me? Look forward to your reply. Thanks ever so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7225
https://github.com/broadinstitute/gatk/pull/7226:818,Availability,error,error,818,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/pull/7226:1371,Availability,error,errored,1371,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/pull/7226:737,Performance,Load,LoadTables,737,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/pull/7226:1001,Performance,load,load,1001,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/pull/7226:1183,Performance,load,load,1183,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/pull/7226:1292,Performance,load,loaded,1292,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/pull/7226:75,Safety,predict,predictable,75,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/pull/7226:265,Testability,test,test,265,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/pull/7226:316,Testability,assert,assert,316,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/pull/7226:943,Testability,test,tested,943,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226
https://github.com/broadinstitute/gatk/issues/7229:51,Availability,error,error,51,When I am trying to run gatk HallotypeCaller I got error below. Can someone give me some advices?. INFO: Failed to detect whether we are running on Google Compute Engine.; 12:55:36.413 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.413 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:55:36.414 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:55:36.414 INFO HaplotypeCaller - Executing as linux@Paulina on Linux v5.4.0-67-generic amd64; 12:55:36.414 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v11.0.10+9-Ubuntu-0ubuntu1.18.04; 12:55:36.414 INFO HaplotypeCaller - Start Date/Time: 24 April 2021 at 12:55:36 CEST; 12:55:36.414 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.414 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Version: 2.24.0; 12:55:36.414 INFO HaplotypeCaller - Picard Version: 2.25.0; 12:55:36.414 INFO HaplotypeCaller - Built for Spark Version: 2.4.5; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical p,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229
https://github.com/broadinstitute/gatk/issues/7229:2162,Availability,Down,Downloads,2162," HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 12:55:36.523 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229
https://github.com/broadinstitute/gatk/issues/7229:2355,Availability,Down,Downloads,2355,"Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 12:55:36.523 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229
https://github.com/broadinstitute/gatk/issues/7229:2577,Availability,Avail,Available,2577,"aller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 12:55:36.523 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:55:36.588 INFO ProgressMeter - unmapped 0.0 1 3333.3; 12:55:36.588 INFO ProgressMeter - Traversal complete. Processed 1 total regi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229
https://github.com/broadinstitute/gatk/issues/7229:3912,Availability,down,down,3912,aryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:55:36.588 INFO ProgressMeter - unmapped 0.0 1 3333.3; 12:55:36.588 INFO ProgressMeter - Traversal complete. Processed 1 total regions in 0.0 minutes.; 12:55:36.588 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 12:55:36.589 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 12:55:36.589 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 12:55:36.589 INFO HaplotypeCaller - Shutting down engine; [24 April 2021 at 12:55:36 CEST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=528482304,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229
https://github.com/broadinstitute/gatk/issues/7229:2112,Performance,Load,Loading,2112,"- Built for Spark Version: 2.4.5; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 12:55:36.523 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229
https://github.com/broadinstitute/gatk/issues/7229:2299,Performance,Load,Loading,2299,"S : false; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 12:55:36.523 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229
https://github.com/broadinstitute/gatk/issues/7229:2700,Performance,multi-thread,multi-threaded,2700,"quester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 12:55:36.523 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:55:36.588 INFO ProgressMeter - unmapped 0.0 1 3333.3; 12:55:36.588 INFO ProgressMeter - Traversal complete. Processed 1 total regions in 0.0 minutes.; 12:55:36.588 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 12:55:36.589 INFO PairHMM - Total c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229
https://github.com/broadinstitute/gatk/issues/7229:115,Safety,detect,detect,115,When I am trying to run gatk HallotypeCaller I got error below. Can someone give me some advices?. INFO: Failed to detect whether we are running on Google Compute Engine.; 12:55:36.413 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.413 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:55:36.414 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:55:36.414 INFO HaplotypeCaller - Executing as linux@Paulina on Linux v5.4.0-67-generic amd64; 12:55:36.414 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v11.0.10+9-Ubuntu-0ubuntu1.18.04; 12:55:36.414 INFO HaplotypeCaller - Start Date/Time: 24 April 2021 at 12:55:36 CEST; 12:55:36.414 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.414 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Version: 2.24.0; 12:55:36.414 INFO HaplotypeCaller - Picard Version: 2.25.0; 12:55:36.414 INFO HaplotypeCaller - Built for Spark Version: 2.4.5; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical p,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229
https://github.com/broadinstitute/gatk/issues/7232:1610,Availability,recover,recover-all-dangling-branches,1610,"hat have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads: (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:8,Energy Efficiency,Adapt,Adaptive,8,"Mutect2 Adaptive Pruning issue as discussed in GATK OH meeting. ; Here is the original post:. This request was created from a contribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:3114,Energy Efficiency,reduce,reduced,3114,"l with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads: (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 INFO AssemblyResultSet - Trimming active region AssemblyRegion chr12:**2538**0238-**2538**0327 active?=true nReads=19912 with 2 haplotypes. 08:01:24.154 INFO AssemblyResultSet - Trimmed region to chr12:**2538**0255-**2538**0295 and reduced number of haplotypes from 2 to only 2. 08:01:25.383 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. I have tried troubleshooting with the steps stated in this \[blog\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant). However, it does not change the output vcf. I used the force-calling mode by giving the above call in an input vcf and the call did appear in the vcf file. **chr12** 25398285 . C A . . AS\_SB\_TABLE=4312,3630|14,8;DP=8096;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=22;POPAF=7.30;TLOD=14.69 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:7942,22:2.576e-03:7964:3609,8:4268,13:4312,3630,14,8. However, I cannot rely on force-calling mutations on a set of calls. I am unsure if I am missing out more calls that are not showing up. Are there any parameters I need to tune so that I do not miss calls like above?<br><br><i>(created from <a href='https://broadinstitute.ze",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:1881,Integrability,message,messages,1881,"event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads: (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads: (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:8,Modifiability,Adapt,Adaptive,8,"Mutect2 Adaptive Pruning issue as discussed in GATK OH meeting. ; Here is the original post:. This request was created from a contribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:3992,Performance,tune,tune,3992,"12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads: (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 INFO AssemblyResultSet - Trimming active region AssemblyRegion chr12:**2538**0238-**2538**0327 active?=true nReads=19912 with 2 haplotypes. 08:01:24.154 INFO AssemblyResultSet - Trimmed region to chr12:**2538**0255-**2538**0295 and reduced number of haplotypes from 2 to only 2. 08:01:25.383 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. I have tried troubleshooting with the steps stated in this \[blog\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant). However, it does not change the output vcf. I used the force-calling mode by giving the above call in an input vcf and the call did appear in the vcf file. **chr12** 25398285 . C A . . AS\_SB\_TABLE=4312,3630|14,8;DP=8096;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=22;POPAF=7.30;TLOD=14.69 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:7942,22:2.576e-03:7964:3609,8:4268,13:4312,3630,14,8. However, I cannot rely on force-calling mutations on a set of calls. I am unsure if I am missing out more calls that are not showing up. Are there any parameters I need to tune so that I do not miss calls like above?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/136765'>Zendesk ticket #136765</a>)<br>gz#136765</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:631,Safety,detect,detect,631,"Mutect2 Adaptive Pruning issue as discussed in GATK OH meeting. ; Here is the original post:. This request was created from a contribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:891,Safety,detect,detected,891,"Mutect2 Adaptive Pruning issue as discussed in GATK OH meeting. ; Here is the original post:. This request was created from a contribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:1610,Safety,recover,recover-all-dangling-branches,1610,"hat have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads: (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:2122,Safety,detect,detected,2122," full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads: (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads: (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 INFO AssemblyResultSet - Trimming active region AssemblyRegion chr12:**2538**0238-**2538**0327 active?=true nReads=19912 with 2 haplotypes. 08:01:24.154 INFO AssemblyResultSet - Trimmed region to chr12:**2538**0255-**2538**0295 and reduced",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:2577,Safety,detect,detection,2577,"t sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads: (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads: (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 INFO AssemblyResultSet - Trimming active region AssemblyRegion chr12:**2538**0238-**2538**0327 active?=true nReads=19912 with 2 haplotypes. 08:01:24.154 INFO AssemblyResultSet - Trimmed region to chr12:**2538**0255-**2538**0295 and reduced number of haplotypes from 2 to only 2. 08:01:25.383 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. I have tried troubleshooting with the steps stated in this \[blog\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant). However, it does not change the output vcf. I used the force-calling mode by giving the above call in an input vcf and the call did appear in the vcf file. *",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:1151,Testability,log,log,1151,"ontribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads: (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:1877,Testability,log,log,1877,"event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads: (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads: (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:2558,Testability,log,logs,2558,"t sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads: (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads: (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 INFO AssemblyResultSet - Trimming active region AssemblyRegion chr12:**2538**0238-**2538**0327 active?=true nReads=19912 with 2 haplotypes. 08:01:24.154 INFO AssemblyResultSet - Trimmed region to chr12:**2538**0255-**2538**0295 and reduced number of haplotypes from 2 to only 2. 08:01:25.383 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. I have tried troubleshooting with the steps stated in this \[blog\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant). However, it does not change the output vcf. I used the force-calling mode by giving the above call in an input vcf and the call did appear in the vcf file. *",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:275,Usability,clear,clear-expected-variant-not-show-up-in-the-,275,"Mutect2 Adaptive Pruning issue as discussed in GATK OH meeting. ; Here is the original post:. This request was created from a contribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:414,Usability,clear,clear-expected-variant-not-show-up-in-the-,414,"Mutect2 Adaptive Pruning issue as discussed in GATK OH meeting. ; Here is the original post:. This request was created from a contribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/issues/7232:710,Usability,clear,clear,710,"Mutect2 Adaptive Pruning issue as discussed in GATK OH meeting. ; Here is the original post:. This request was created from a contribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232
https://github.com/broadinstitute/gatk/pull/7233:616,Deployability,update,update,616,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:674,Deployability,update,update,674,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:728,Deployability,update,update,728,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:769,Deployability,update,update,769,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:1058,Deployability,update,update,1058,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:1116,Deployability,update,update,1116,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:1174,Deployability,update,update,1174,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:1228,Deployability,update,update,1228,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:752,Security,validat,validate,752,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:1201,Security,validat,validate,1201,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:974,Testability,test,test,974,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/pull/7233:1332,Testability,test,test,1332,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233
https://github.com/broadinstitute/gatk/issues/7234:47,Availability,error,error,47,"Hi,. I tried to build a gCNV model and got the error `python exited with 139` but can#t figure out what is the cause of this error. Can you please help me with that error message? I attended the whole command and output here. ```; (gatk4.2.0.0) k-hg-srv3:/media/Data/AnnotationDBs/CNV/Genom/hdf5 # gatk GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.hdf5 -I 0834-19.hdf5 -I 1080-20.hdf5 -I 1331-18.hdf5 -I 1460-18.hdf5 -I 1498-18.hdf5 -I 1576-20.hdf5 -I 1592-20.hdf5 -I 1716-15.hdf5 -I 1985-20.hdf5 -I 2167-20.hdf5 -I 0038-21.hdf5 -I 0094-21.hdf5 -I 0139-18.hdf5 -I 0345-20.hdf5 -I 0641-18.hdf5 -I 0949-20.hdf5 -I 1081-20.hdf5 -I 1416-20.hdf5 -I 1491-20.hdf5 -I 1553-18.hdf5 -I 1577-20.hdf5 -I 1600-20.hdf5 -I 1720-20.hdf5 -I 1995-20.hdf5 --contig-ploidy-calls ../ploidy-calls/ --annotated-intervals ../Genom.annotated.tsv --interval-merging-rule OVERLAPPING_ONLY --output /media/Data/AnnotationDBs/CNV/Genom --output-prefix CNV --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:125,Availability,error,error,125,"Hi,. I tried to build a gCNV model and got the error `python exited with 139` but can#t figure out what is the cause of this error. Can you please help me with that error message? I attended the whole command and output here. ```; (gatk4.2.0.0) k-hg-srv3:/media/Data/AnnotationDBs/CNV/Genom/hdf5 # gatk GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.hdf5 -I 0834-19.hdf5 -I 1080-20.hdf5 -I 1331-18.hdf5 -I 1460-18.hdf5 -I 1498-18.hdf5 -I 1576-20.hdf5 -I 1592-20.hdf5 -I 1716-15.hdf5 -I 1985-20.hdf5 -I 2167-20.hdf5 -I 0038-21.hdf5 -I 0094-21.hdf5 -I 0139-18.hdf5 -I 0345-20.hdf5 -I 0641-18.hdf5 -I 0949-20.hdf5 -I 1081-20.hdf5 -I 1416-20.hdf5 -I 1491-20.hdf5 -I 1553-18.hdf5 -I 1577-20.hdf5 -I 1600-20.hdf5 -I 1720-20.hdf5 -I 1995-20.hdf5 --contig-ploidy-calls ../ploidy-calls/ --annotated-intervals ../Genom.annotated.tsv --interval-merging-rule OVERLAPPING_ONLY --output /media/Data/AnnotationDBs/CNV/Genom --output-prefix CNV --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:165,Availability,error,error,165,"Hi,. I tried to build a gCNV model and got the error `python exited with 139` but can#t figure out what is the cause of this error. Can you please help me with that error message? I attended the whole command and output here. ```; (gatk4.2.0.0) k-hg-srv3:/media/Data/AnnotationDBs/CNV/Genom/hdf5 # gatk GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.hdf5 -I 0834-19.hdf5 -I 1080-20.hdf5 -I 1331-18.hdf5 -I 1460-18.hdf5 -I 1498-18.hdf5 -I 1576-20.hdf5 -I 1592-20.hdf5 -I 1716-15.hdf5 -I 1985-20.hdf5 -I 2167-20.hdf5 -I 0038-21.hdf5 -I 0094-21.hdf5 -I 0139-18.hdf5 -I 0345-20.hdf5 -I 0641-18.hdf5 -I 0949-20.hdf5 -I 1081-20.hdf5 -I 1416-20.hdf5 -I 1491-20.hdf5 -I 1553-18.hdf5 -I 1577-20.hdf5 -I 1600-20.hdf5 -I 1720-20.hdf5 -I 1995-20.hdf5 --contig-ploidy-calls ../ploidy-calls/ --annotated-intervals ../Genom.annotated.tsv --interval-merging-rule OVERLAPPING_ONLY --output /media/Data/AnnotationDBs/CNV/Genom --output-prefix CNV --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:9653,Availability,down,down,9653,"ating read-count file 0345-20.hdf5 (34 / 44); 17:30:33.666 INFO GermlineCNVCaller - Aggregating read-count file 0641-18.hdf5 (35 / 44); 17:30:36.381 INFO GermlineCNVCaller - Aggregating read-count file 0949-20.hdf5 (36 / 44); 17:30:39.115 INFO GermlineCNVCaller - Aggregating read-count file 1081-20.hdf5 (37 / 44); 17:30:41.937 INFO GermlineCNVCaller - Aggregating read-count file 1416-20.hdf5 (38 / 44); 17:30:44.527 INFO GermlineCNVCaller - Aggregating read-count file 1491-20.hdf5 (39 / 44); 17:30:47.177 INFO GermlineCNVCaller - Aggregating read-count file 1553-18.hdf5 (40 / 44); 17:30:49.901 INFO GermlineCNVCaller - Aggregating read-count file 1577-20.hdf5 (41 / 44); 17:30:52.733 INFO GermlineCNVCaller - Aggregating read-count file 1600-20.hdf5 (42 / 44); 17:30:55.582 INFO GermlineCNVCaller - Aggregating read-count file 1720-20.hdf5 (43 / 44); 17:30:58.449 INFO GermlineCNVCaller - Aggregating read-count file 1995-20.hdf5 (44 / 44); 17:40:55.029 INFO GermlineCNVCaller - Shutting down engine; [April 27, 2021 at 5:40:55 PM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 12.44 minutes.; Runtime.totalMemory()=11144265728; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 139; Command Line: python /media/Data/tmp/cohort_denoising_calling.4407709016674150942.py --ploidy_calls_path=/media/Data/AnnotationDBs/CNV/Genom/ploidy-calls --output_calls_path=/media/Data/AnnotationDBs/CNV/Genom/CNV-calls --output_tracking_path=/media/Data/AnnotationDBs/CNV/Genom/CNV-tracking --random_seed=1984 --modeling_interval_list=/media/Data/tmp/intervals8808982738430140650.tsv --output_model_path=/media/Data/AnnotationDBs/CNV/Genom/CNV-model --enable_explicit_gc_bias_modeling=True --read_count_tsv_files /media/Data/tmp/0028-2117281826103999737636.tsv /media/Data/tmp/0045-2114178578414165773973.tsv /media/Data/tmp/0098-187303293573572392847.tsv /media/Data/tmp/0156-2116276973316025582383.tsv /media/Data/tmp/042",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:171,Integrability,message,message,171,"Hi,. I tried to build a gCNV model and got the error `python exited with 139` but can#t figure out what is the cause of this error. Can you please help me with that error message? I attended the whole command and output here. ```; (gatk4.2.0.0) k-hg-srv3:/media/Data/AnnotationDBs/CNV/Genom/hdf5 # gatk GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.hdf5 -I 0834-19.hdf5 -I 1080-20.hdf5 -I 1331-18.hdf5 -I 1460-18.hdf5 -I 1498-18.hdf5 -I 1576-20.hdf5 -I 1592-20.hdf5 -I 1716-15.hdf5 -I 1985-20.hdf5 -I 2167-20.hdf5 -I 0038-21.hdf5 -I 0094-21.hdf5 -I 0139-18.hdf5 -I 0345-20.hdf5 -I 0641-18.hdf5 -I 0949-20.hdf5 -I 1081-20.hdf5 -I 1416-20.hdf5 -I 1491-20.hdf5 -I 1553-18.hdf5 -I 1577-20.hdf5 -I 1600-20.hdf5 -I 1720-20.hdf5 -I 1995-20.hdf5 --contig-ploidy-calls ../ploidy-calls/ --annotated-intervals ../Genom.annotated.tsv --interval-merging-rule OVERLAPPING_ONLY --output /media/Data/AnnotationDBs/CNV/Genom --output-prefix CNV --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:13931,Modifiability,variab,variable,13931,"e --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 17:31:06.046 INFO cohort_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 17:31:16.537 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 17:31:23.180 INFO cohort_denoising_calling - Loading 44 read counts file(s)...; 17:34:27.362 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 17:40:48.713 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)... Stderr:; at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:340); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:2648,Performance,Load,Loading,2648,"HORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.hdf5 -I 0834-19.hdf5 -I 1080-20.hdf5 -I 1331-18.hdf5 -I 1460-18.hdf5 -I 1498-18.hdf5 -I 1576-20.hdf5 -I 1592-20.hdf5 -I 1716-15.hdf5 -I 1985-20.hdf5 -I 2167-20.hdf5 -I 0038-21.hdf5 -I 0094-21.hdf5 -I 0139-18.hdf5 -I 0345-20.hdf5 -I 0641-18.hdf5 -I 0949-20.hdf5 -I 1081-20.hdf5 -I 1416-20.hdf5 -I 1491-20.hdf5 -I 1553-18.hdf5 -I 1577-20.hdf5 -I 1600-20.hdf5 -I 1720-20.hdf5 -I 1995-20.hdf5 --contig-ploidy-calls ../ploidy-calls/ --annotated-intervals ../Genom.annotated.tsv --interval-merging-rule OVERLAPPING_ONLY --output /media/Data/AnnotationDBs/CNV/Genom --output-prefix CNV --tmp-dir /media/Data/tmp/; 17:28:28.660 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 27, 2021 5:28:28 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:28:28.779 INFO GermlineCNVCaller - ------------------------------------------------------------; 17:28:28.779 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:28:28.779 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:28:28.779 INFO GermlineCNVCaller - Executing as root@k-hg-srv3 on Linux v5.3.18-24.37-default amd64; 17:28:28.780 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v11.0.8+10-suse-3.45.1-x8664; 17:28:28.780 INFO GermlineCNVCaller - Start Date/Time: April 27, 2021 at 5:28:28 PM CEST; 17:28:28.780 INFO GermlineCNVCaller - --------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:5465,Performance,perform,performed,5465,2 INFO GermlineCNVCaller - Inflater: IntelInflater; 17:28:28.782 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 17:28:28.782 INFO GermlineCNVCaller - Requester pays: disabled; 17:28:28.782 INFO GermlineCNVCaller - Initializing engine; 17:28:34.716 INFO GermlineCNVCaller - Done initializing engine; 17:28:34.723 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 17:28:35.689 INFO FeatureManager - Using codec IntervalListCodec to read file file:///media/Data/AnnotationDBs/CNV/Genom/hdf5/../Genom.filtered.interval_list; 17:28:42.892 INFO IntervalArgumentCollection - Processing 2741406000 bp from intervals; 17:28:43.237 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 17:28:51.740 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 17:28:57.410 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 17:28:57.410 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 17:28:57.940 INFO GermlineCNVCaller - Aggregating read-count file 0028-21.hdf5 (1 / 44); 17:29:00.837 INFO GermlineCNVCaller - Aggregating read-count file 0045-21.hdf5 (2 / 44); 17:29:03.690 INFO GermlineCNVCaller - Aggregating read-count file 0098-18.hdf5 (3 / 44); 17:29:06.658 INFO GermlineCNVCaller - Aggregating read-count file 0156-21.hdf5 (4 / 44); 17:29:09.435 INFO GermlineCNVCaller - Aggregating read-count file 0429-20.hdf5 (5 / 44); 17:29:12.235 INFO GermlineCNVCaller - Aggregating read-count file 0779-18.hdf5 (6 / 44); 17:29:14.939 INFO GermlineCNVCaller - Aggregating read-count file 1030-20.hdf5 (7 / 44); 17:29:17.822 INFO GermlineCNVCaller - Aggregating read-count file 1098-13.hdf5 (8 / 44); 17:29:20.668 INFO GermlineCNVCaller - Aggregating,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:13983,Performance,optimiz,optimizer,13983,"e --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 17:31:06.046 INFO cohort_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 17:31:16.537 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 17:31:23.180 INFO cohort_denoising_calling - Loading 44 read counts file(s)...; 17:34:27.362 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 17:40:48.713 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)... Stderr:; at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:340); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:14278,Performance,Load,Loading,14278,"dian_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 17:31:06.046 INFO cohort_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 17:31:16.537 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 17:31:23.180 INFO cohort_denoising_calling - Loading 44 read counts file(s)...; 17:34:27.362 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 17:40:48.713 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)... Stderr:; at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:340); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:14359,Performance,Load,Loading,14359,"iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 17:31:06.046 INFO cohort_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 17:31:16.537 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 17:31:23.180 INFO cohort_denoising_calling - Loading 44 read counts file(s)...; 17:34:27.362 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 17:40:48.713 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)... Stderr:; at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:340); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Thanks for any help.; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:2930,Safety,detect,detect,2930," -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.hdf5 -I 0834-19.hdf5 -I 1080-20.hdf5 -I 1331-18.hdf5 -I 1460-18.hdf5 -I 1498-18.hdf5 -I 1576-20.hdf5 -I 1592-20.hdf5 -I 1716-15.hdf5 -I 1985-20.hdf5 -I 2167-20.hdf5 -I 0038-21.hdf5 -I 0094-21.hdf5 -I 0139-18.hdf5 -I 0345-20.hdf5 -I 0641-18.hdf5 -I 0949-20.hdf5 -I 1081-20.hdf5 -I 1416-20.hdf5 -I 1491-20.hdf5 -I 1553-18.hdf5 -I 1577-20.hdf5 -I 1600-20.hdf5 -I 1720-20.hdf5 -I 1995-20.hdf5 --contig-ploidy-calls ../ploidy-calls/ --annotated-intervals ../Genom.annotated.tsv --interval-merging-rule OVERLAPPING_ONLY --output /media/Data/AnnotationDBs/CNV/Genom --output-prefix CNV --tmp-dir /media/Data/tmp/; 17:28:28.660 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 27, 2021 5:28:28 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:28:28.779 INFO GermlineCNVCaller - ------------------------------------------------------------; 17:28:28.779 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:28:28.779 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:28:28.779 INFO GermlineCNVCaller - Executing as root@k-hg-srv3 on Linux v5.3.18-24.37-default amd64; 17:28:28.780 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v11.0.8+10-suse-3.45.1-x8664; 17:28:28.780 INFO GermlineCNVCaller - Start Date/Time: April 27, 2021 at 5:28:28 PM CEST; 17:28:28.780 INFO GermlineCNVCaller - ------------------------------------------------------------; 17:28:28.780 INFO GermlineCNVCaller - ------------------------------------------------------------; 17:28:28.781 INFO GermlineCNVCaller - HTSJDK Version: 2.24.0; 17:28:28.781 INFO GermlineCNVCaller - Picard Version: 2.25.0; 17:28:28.781 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:5312,Security,validat,validating,5312,YNC_IO_WRITE_FOR_TRIBBLE : false; 17:28:28.782 INFO GermlineCNVCaller - Deflater: IntelDeflater; 17:28:28.782 INFO GermlineCNVCaller - Inflater: IntelInflater; 17:28:28.782 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 17:28:28.782 INFO GermlineCNVCaller - Requester pays: disabled; 17:28:28.782 INFO GermlineCNVCaller - Initializing engine; 17:28:34.716 INFO GermlineCNVCaller - Done initializing engine; 17:28:34.723 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 17:28:35.689 INFO FeatureManager - Using codec IntervalListCodec to read file file:///media/Data/AnnotationDBs/CNV/Genom/hdf5/../Genom.filtered.interval_list; 17:28:42.892 INFO IntervalArgumentCollection - Processing 2741406000 bp from intervals; 17:28:43.237 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 17:28:51.740 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 17:28:57.410 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 17:28:57.410 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 17:28:57.940 INFO GermlineCNVCaller - Aggregating read-count file 0028-21.hdf5 (1 / 44); 17:29:00.837 INFO GermlineCNVCaller - Aggregating read-count file 0045-21.hdf5 (2 / 44); 17:29:03.690 INFO GermlineCNVCaller - Aggregating read-count file 0098-18.hdf5 (3 / 44); 17:29:06.658 INFO GermlineCNVCaller - Aggregating read-count file 0156-21.hdf5 (4 / 44); 17:29:09.435 INFO GermlineCNVCaller - Aggregating read-count file 0429-20.hdf5 (5 / 44); 17:29:12.235 INFO GermlineCNVCaller - Aggregating read-count file 0779-18.hdf5 (6 / 44); 17:29:14.939 INFO GermlineCNVCaller - Aggregating read-count file 1030-20.hdf5 (7 / 44); 17:29:17.822 INFO GermlineCNV,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:5591,Security,Validat,Validating,5591,.782 INFO GermlineCNVCaller - Initializing engine; 17:28:34.716 INFO GermlineCNVCaller - Done initializing engine; 17:28:34.723 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 17:28:35.689 INFO FeatureManager - Using codec IntervalListCodec to read file file:///media/Data/AnnotationDBs/CNV/Genom/hdf5/../Genom.filtered.interval_list; 17:28:42.892 INFO IntervalArgumentCollection - Processing 2741406000 bp from intervals; 17:28:43.237 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 17:28:51.740 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 17:28:57.410 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 17:28:57.410 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 17:28:57.940 INFO GermlineCNVCaller - Aggregating read-count file 0028-21.hdf5 (1 / 44); 17:29:00.837 INFO GermlineCNVCaller - Aggregating read-count file 0045-21.hdf5 (2 / 44); 17:29:03.690 INFO GermlineCNVCaller - Aggregating read-count file 0098-18.hdf5 (3 / 44); 17:29:06.658 INFO GermlineCNVCaller - Aggregating read-count file 0156-21.hdf5 (4 / 44); 17:29:09.435 INFO GermlineCNVCaller - Aggregating read-count file 0429-20.hdf5 (5 / 44); 17:29:12.235 INFO GermlineCNVCaller - Aggregating read-count file 0779-18.hdf5 (6 / 44); 17:29:14.939 INFO GermlineCNVCaller - Aggregating read-count file 1030-20.hdf5 (7 / 44); 17:29:17.822 INFO GermlineCNVCaller - Aggregating read-count file 1098-13.hdf5 (8 / 44); 17:29:20.668 INFO GermlineCNVCaller - Aggregating read-count file 1450-20.hdf5 (9 / 44); 17:29:23.485 INFO GermlineCNVCaller - Aggregating read-count file 1495-17.hdf5 (10 / 44); 17:29:26.245 INFO GermlineCNVCaller - Aggregating read-count ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:4826,Testability,log,logger,4826,mlineCNVCaller - HTSJDK Version: 2.24.0; 17:28:28.781 INFO GermlineCNVCaller - Picard Version: 2.25.0; 17:28:28.781 INFO GermlineCNVCaller - Built for Spark Version: 2.4.5; 17:28:28.781 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:28:28.781 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:28:28.781 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:28:28.782 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:28:28.782 INFO GermlineCNVCaller - Deflater: IntelDeflater; 17:28:28.782 INFO GermlineCNVCaller - Inflater: IntelInflater; 17:28:28.782 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 17:28:28.782 INFO GermlineCNVCaller - Requester pays: disabled; 17:28:28.782 INFO GermlineCNVCaller - Initializing engine; 17:28:34.716 INFO GermlineCNVCaller - Done initializing engine; 17:28:34.723 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 17:28:35.689 INFO FeatureManager - Using codec IntervalListCodec to read file file:///media/Data/AnnotationDBs/CNV/Genom/hdf5/../Genom.filtered.interval_list; 17:28:42.892 INFO IntervalArgumentCollection - Processing 2741406000 bp from intervals; 17:28:43.237 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 17:28:51.740 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 17:28:57.410 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 17:28:57.410 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 17:28:57.940 INFO GermlineCNVCaller - Aggregating read-count file 0028-21.hdf5 (1 / 44); 17:29:00.837 INFO GermlineCNVCaller - Aggregating read-count file ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7234:4952,Testability,log,logging,4952,ler - Built for Spark Version: 2.4.5; 17:28:28.781 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:28:28.781 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:28:28.781 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:28:28.782 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:28:28.782 INFO GermlineCNVCaller - Deflater: IntelDeflater; 17:28:28.782 INFO GermlineCNVCaller - Inflater: IntelInflater; 17:28:28.782 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 17:28:28.782 INFO GermlineCNVCaller - Requester pays: disabled; 17:28:28.782 INFO GermlineCNVCaller - Initializing engine; 17:28:34.716 INFO GermlineCNVCaller - Done initializing engine; 17:28:34.723 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 17:28:35.689 INFO FeatureManager - Using codec IntervalListCodec to read file file:///media/Data/AnnotationDBs/CNV/Genom/hdf5/../Genom.filtered.interval_list; 17:28:42.892 INFO IntervalArgumentCollection - Processing 2741406000 bp from intervals; 17:28:43.237 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 17:28:51.740 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 17:28:57.410 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 17:28:57.410 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 17:28:57.940 INFO GermlineCNVCaller - Aggregating read-count file 0028-21.hdf5 (1 / 44); 17:29:00.837 INFO GermlineCNVCaller - Aggregating read-count file 0045-21.hdf5 (2 / 44); 17:29:03.690 INFO GermlineCNVCaller - Aggregating read-count file 0098-18.hdf5 (3 / 44); 17:29:06.658 INFO Germ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234
https://github.com/broadinstitute/gatk/issues/7235:468,Testability,log,log,468,During the discussion in fixing a bug in the GenotypeQuality score #7120 it came to light that when using posteriors for genotyping that the calculation in getGQLog10FromPosteriors() for computing the GQ is doing something a little different from the other codepath for calculating the GQ score. Specifically it is calculating the GQ as the difference in score between the first (selected) genotype and the second best genotype (after normalization for both) plus the log math addition of the other scores in the array. What this means in practice is that the score will be less than the first minus the second calculation especially if there were other marginal options with similar scores to the calls. This approach was implemented for the DRAGEN-GATK pipleine and currently will only change the scores there. Its worth discussing if we need this extra step or can rely on the old GQ calculation. See #7120 for more discussion.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7235
https://github.com/broadinstitute/gatk/pull/7236:25,Availability,down,down,25,This also start the mark down for how to find cost info for the aou prod project,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7236
https://github.com/broadinstitute/gatk/pull/7237:191,Testability,test,test-drive,191,"Addresses. https://github.com/broadinstitute/dsp-spec-ops/issues/280. Analysis has been done and delivered, this is primarily documentation of how to do it in the future. If someone wants to test-drive the instructions, there is a GVS VCF at. ```; gs://broad-dsp-spec-ops/scratch/bigquery-jointcalling/comparison-v3/gvs.chr20.vcf.gz*; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7237
https://github.com/broadinstitute/gatk/pull/7238:12,Deployability,update,updated,12,"@cmnbroad I updated VariantQC and identified one minor difference in behavior associated with VariantEvalEngine. Contig stratification assigns level based on all the contigs. If user-supplied contigs are given, it should defer to these. This PR addresses this, and adds a test case. Note: I put the getContigNames() method into VariantEvalEngine, but it would also be possible to keep this in Config, but expose a getter for userSuppliedIntervals. It seemed marginally better to keep that private.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238
https://github.com/broadinstitute/gatk/pull/7238:393,Modifiability,Config,Config,393,"@cmnbroad I updated VariantQC and identified one minor difference in behavior associated with VariantEvalEngine. Contig stratification assigns level based on all the contigs. If user-supplied contigs are given, it should defer to these. This PR addresses this, and adds a test case. Note: I put the getContigNames() method into VariantEvalEngine, but it would also be possible to keep this in Config, but expose a getter for userSuppliedIntervals. It seemed marginally better to keep that private.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238
https://github.com/broadinstitute/gatk/pull/7238:405,Security,expose,expose,405,"@cmnbroad I updated VariantQC and identified one minor difference in behavior associated with VariantEvalEngine. Contig stratification assigns level based on all the contigs. If user-supplied contigs are given, it should defer to these. This PR addresses this, and adds a test case. Note: I put the getContigNames() method into VariantEvalEngine, but it would also be possible to keep this in Config, but expose a getter for userSuppliedIntervals. It seemed marginally better to keep that private.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238
https://github.com/broadinstitute/gatk/pull/7238:272,Testability,test,test,272,"@cmnbroad I updated VariantQC and identified one minor difference in behavior associated with VariantEvalEngine. Contig stratification assigns level based on all the contigs. If user-supplied contigs are given, it should defer to these. This PR addresses this, and adds a test case. Note: I put the getContigNames() method into VariantEvalEngine, but it would also be possible to keep this in Config, but expose a getter for userSuppliedIntervals. It seemed marginally better to keep that private.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238
https://github.com/broadinstitute/gatk/pull/7240:225,Deployability,integrat,integration,225,"Rationale: certain evaluators use a pedigree. This PR is a minor change that lets VariantEvalArgCollection supply the PedigreeValidationType. It defaults to the current behavior, which is to always use STRICT. It includes an integration test to cover this feature.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7240
https://github.com/broadinstitute/gatk/pull/7240:225,Integrability,integrat,integration,225,"Rationale: certain evaluators use a pedigree. This PR is a minor change that lets VariantEvalArgCollection supply the PedigreeValidationType. It defaults to the current behavior, which is to always use STRICT. It includes an integration test to cover this feature.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7240
https://github.com/broadinstitute/gatk/pull/7240:237,Testability,test,test,237,"Rationale: certain evaluators use a pedigree. This PR is a minor change that lets VariantEvalArgCollection supply the PedigreeValidationType. It defaults to the current behavior, which is to always use STRICT. It includes an integration test to cover this feature.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7240
https://github.com/broadinstitute/gatk/pull/7241:0,Testability,test,tested,0,tested without SA: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_no_sa/job_history/235c2316-d31a-42a5-8f25-a0397b8be42e; tested with SA: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_sa_testing/job_history/81aa9029-fb58-453b-a962-bad4fde09a2e. note this worked fine with localization optional for the SA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7241
https://github.com/broadinstitute/gatk/pull/7241:143,Testability,test,tested,143,tested without SA: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_no_sa/job_history/235c2316-d31a-42a5-8f25-a0397b8be42e; tested with SA: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_sa_testing/job_history/81aa9029-fb58-453b-a962-bad4fde09a2e. note this worked fine with localization optional for the SA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7241
https://github.com/broadinstitute/gatk/pull/7243:724,Deployability,pipeline,pipeline,724,"Reworks classes used by `JointGermlineCNVSegmentationIntegration` for SV clustering and defragmentation. The design of `SVClusterEngine` has been overhauled to enable the implementation of `CNVDefragmenter` and `BinnedCNVDefragmenter` subclasses. Logic for producing representative records from a collection of clustered SVs has been separated into an `SVCollapser` class, which provides enhanced functionality for handling genotypes for SVs more generally. A number of bugs, particularly with max-clique clustering, have been fixed, as well as a parameter swap bug in `JointGermlineCNVSegmentationIntegration`. This is the first of a series of PRs for an experimental Java-based implementation of some modules in `gatk-sv` pipeline, including SV vcf merging, clustering, evidence aggregation, and genotyping.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243
https://github.com/broadinstitute/gatk/pull/7243:388,Modifiability,enhance,enhanced,388,"Reworks classes used by `JointGermlineCNVSegmentationIntegration` for SV clustering and defragmentation. The design of `SVClusterEngine` has been overhauled to enable the implementation of `CNVDefragmenter` and `BinnedCNVDefragmenter` subclasses. Logic for producing representative records from a collection of clustered SVs has been separated into an `SVCollapser` class, which provides enhanced functionality for handling genotypes for SVs more generally. A number of bugs, particularly with max-clique clustering, have been fixed, as well as a parameter swap bug in `JointGermlineCNVSegmentationIntegration`. This is the first of a series of PRs for an experimental Java-based implementation of some modules in `gatk-sv` pipeline, including SV vcf merging, clustering, evidence aggregation, and genotyping.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243
https://github.com/broadinstitute/gatk/pull/7243:247,Testability,Log,Logic,247,"Reworks classes used by `JointGermlineCNVSegmentationIntegration` for SV clustering and defragmentation. The design of `SVClusterEngine` has been overhauled to enable the implementation of `CNVDefragmenter` and `BinnedCNVDefragmenter` subclasses. Logic for producing representative records from a collection of clustered SVs has been separated into an `SVCollapser` class, which provides enhanced functionality for handling genotypes for SVs more generally. A number of bugs, particularly with max-clique clustering, have been fixed, as well as a parameter swap bug in `JointGermlineCNVSegmentationIntegration`. This is the first of a series of PRs for an experimental Java-based implementation of some modules in `gatk-sv` pipeline, including SV vcf merging, clustering, evidence aggregation, and genotyping.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243
https://github.com/broadinstitute/gatk/pull/7245:253,Availability,down,down,253,"Addresses https://github.com/broadinstitute/dsp-spec-ops/issues/307. - Increase headroom on VM above Java; - Increase disk space (incidental, not related to OOM); - parameterized Gnarly usage, default to false; - --emit-pls set to false no longer pulls down PLs. Compared results against baseline and saw no changes in GIAB results using ACMG cohort",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7245
https://github.com/broadinstitute/gatk/pull/7245:165,Modifiability,parameteriz,parameterized,165,"Addresses https://github.com/broadinstitute/dsp-spec-ops/issues/307. - Increase headroom on VM above Java; - Increase disk space (incidental, not related to OOM); - parameterized Gnarly usage, default to false; - --emit-pls set to false no longer pulls down PLs. Compared results against baseline and saw no changes in GIAB results using ACMG cohort",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7245
https://github.com/broadinstitute/gatk/issues/7247:453,Availability,error,error,453,"## Bug Report. ### Tool; FilterAlignmentArtifacts. ### Affected version(s); 4.2.0.0 and 4.1.9.0, run from local jar or docker. ### Description ; FilterAlignmentArtifacts crushes repetitively in the same position of the input mutect2 vcf `m2.vcf.gz` (chrX:63457865). But, it finishes task successfully when only the last variant from the output file is present in the input vcf file. ; I cut the input vcf around the troublesome variant to reproduce the error on a smaller input and: ; 1. Error did not occur when the input was very small ; 2. FilterAlignmentArtifacts finished run at different variant (chrX:73769127) when analyzing the smaller input (`test.vcf.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/k",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:488,Availability,Error,Error,488,"## Bug Report. ### Tool; FilterAlignmentArtifacts. ### Affected version(s); 4.2.0.0 and 4.1.9.0, run from local jar or docker. ### Description ; FilterAlignmentArtifacts crushes repetitively in the same position of the input mutect2 vcf `m2.vcf.gz` (chrX:63457865). But, it finishes task successfully when only the last variant from the output file is present in the input vcf file. ; I cut the input vcf around the troublesome variant to reproduce the error on a smaller input and: ; 1. Error did not occur when the input was very small ; 2. FilterAlignmentArtifacts finished run at different variant (chrX:73769127) when analyzing the smaller input (`test.vcf.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/k",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:1431,Availability,error,error,1431,"en the input was very small ; 2. FilterAlignmentArtifacts finished run at different variant (chrX:73769127) when analyzing the smaller input (`test.vcf.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:2024,Availability,error,error,2024,"f.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs (small: test.cf.gz and initial: m2.vcf.gz), bamout and ""hs_err_pid.logs"" to `gs://iseq/kt/strange-bug/` ; I hope you can access them. ; Best,; Kasia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:2453,Availability,toler,tolerance,2453,"f.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs (small: test.cf.gz and initial: m2.vcf.gz), bamout and ""hs_err_pid.logs"" to `gs://iseq/kt/strange-bug/` ; I hope you can access them. ; Best,; Kasia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:1446,Safety,detect,detected,1446,"en the input was very small ; 2. FilterAlignmentArtifacts finished run at different variant (chrX:73769127) when analyzing the smaller input (`test.vcf.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:2633,Security,access,access,2633,"f.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs (small: test.cf.gz and initial: m2.vcf.gz), bamout and ""hs_err_pid.logs"" to `gs://iseq/kt/strange-bug/` ; I hope you can access them. ; Best,; Kasia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:653,Testability,test,test,653,"## Bug Report. ### Tool; FilterAlignmentArtifacts. ### Affected version(s); 4.2.0.0 and 4.1.9.0, run from local jar or docker. ### Description ; FilterAlignmentArtifacts crushes repetitively in the same position of the input mutect2 vcf `m2.vcf.gz` (chrX:63457865). But, it finishes task successfully when only the last variant from the output file is present in the input vcf file. ; I cut the input vcf around the troublesome variant to reproduce the error on a smaller input and: ; 1. Error did not occur when the input was very small ; 2. FilterAlignmentArtifacts finished run at different variant (chrX:73769127) when analyzing the smaller input (`test.vcf.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/k",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:673,Testability,log,log,673,"## Bug Report. ### Tool; FilterAlignmentArtifacts. ### Affected version(s); 4.2.0.0 and 4.1.9.0, run from local jar or docker. ### Description ; FilterAlignmentArtifacts crushes repetitively in the same position of the input mutect2 vcf `m2.vcf.gz` (chrX:63457865). But, it finishes task successfully when only the last variant from the output file is present in the input vcf file. ; I cut the input vcf around the troublesome variant to reproduce the error on a smaller input and: ; 1. Error did not occur when the input was very small ; 2. FilterAlignmentArtifacts finished run at different variant (chrX:73769127) when analyzing the smaller input (`test.vcf.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/k",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:1000,Testability,log,log,1000,"rt. ### Tool; FilterAlignmentArtifacts. ### Affected version(s); 4.2.0.0 and 4.1.9.0, run from local jar or docker. ### Description ; FilterAlignmentArtifacts crushes repetitively in the same position of the input mutect2 vcf `m2.vcf.gz` (chrX:63457865). But, it finishes task successfully when only the last variant from the output file is present in the input vcf file. ; I cut the input vcf around the troublesome variant to reproduce the error on a smaller input and: ; 1. Error did not occur when the input was very small ; 2. FilterAlignmentArtifacts finished run at different variant (chrX:73769127) when analyzing the smaller input (`test.vcf.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.1598",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:2105,Testability,log,log,2105,"f.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs (small: test.cf.gz and initial: m2.vcf.gz), bamout and ""hs_err_pid.logs"" to `gs://iseq/kt/strange-bug/` ; I hope you can access them. ; Best,; Kasia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:2520,Testability,test,test,2520,"f.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs (small: test.cf.gz and initial: m2.vcf.gz), bamout and ""hs_err_pid.logs"" to `gs://iseq/kt/strange-bug/` ; I hope you can access them. ; Best,; Kasia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/issues/7247:2579,Testability,log,logs,2579,"f.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs (small: test.cf.gz and initial: m2.vcf.gz), bamout and ""hs_err_pid.logs"" to `gs://iseq/kt/strange-bug/` ; I hope you can access them. ; Best,; Kasia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247
https://github.com/broadinstitute/gatk/pull/7248:38,Energy Efficiency,reduce,reduce,38,"The overarching goal of this PR is to reduce or eliminate the effect of cohort size on the filtering of variants for a specific sample. As an example this means the filtering for the genotypes for a GIAB sample should be the same whether you make a VCF of the full cohort and then subset to the GIAB sample (expensive) or you just make a callset with just the GIAB sample. This is good for users since their results won't get ""better"" with more samples that they don't care about in their VCF. - calculate and store LowQual filter as a part of Filter Set creation; - use LowQual filter from filter set rather than recalculating it from QUALapprox at extract time; - flag (default is true) to perform VQSLod filtering at the sample/genotype level. Before/After results showing minimal impact are at:; https://docs.google.com/spreadsheets/d/1LUrssKHBCwIzbA_9M3b01Ul0urMbOOmv4Z703dHwiyg/edit#gid=398306713",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7248
https://github.com/broadinstitute/gatk/pull/7248:692,Performance,perform,perform,692,"The overarching goal of this PR is to reduce or eliminate the effect of cohort size on the filtering of variants for a specific sample. As an example this means the filtering for the genotypes for a GIAB sample should be the same whether you make a VCF of the full cohort and then subset to the GIAB sample (expensive) or you just make a callset with just the GIAB sample. This is good for users since their results won't get ""better"" with more samples that they don't care about in their VCF. - calculate and store LowQual filter as a part of Filter Set creation; - use LowQual filter from filter set rather than recalculating it from QUALapprox at extract time; - flag (default is true) to perform VQSLod filtering at the sample/genotype level. Before/After results showing minimal impact are at:; https://docs.google.com/spreadsheets/d/1LUrssKHBCwIzbA_9M3b01Ul0urMbOOmv4Z703dHwiyg/edit#gid=398306713",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7248
https://github.com/broadinstitute/gatk/issues/7249:75,Availability,error,error,75,"Hi, . I am using `GATK ASEReadCounter` from GATK 4.2.0.0 and encounter the error . `A USER ERROR has occurred: More then one variant context at position: chrX:2774793`. As I googled, that means that there are 2 different entries with the same genomic coordinate in my VCF file. ; But by doing a `grep -P 'chrX\t2774793' 16-98_WGS.vcf | cut -f1-5` for my vcf file I only get one line:. `chrX 2774793 . G *,C`. The whole command I am using is as following . ```; gatk ASEReadCounter ; -R /media/Data/Referenzgenome/HG19/HG19.karyo.fasta ; -I /media/Data/Marco/16-98/16-98_iPSC_A.recal.rh.bam ; -I /media/Data/Marco/16-98/16-98_iPSC_B.recal.rh.bam ; -V /media/Data/Marco/16-98/16-98_WGS.vcf ; -O /media/Data/Marco/16-98/16-98_Out_iPSC_A_B.table ; -L chrX; ```. Do you have any idea how to solve this?; Thanks in advance ; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7249
https://github.com/broadinstitute/gatk/issues/7249:91,Availability,ERROR,ERROR,91,"Hi, . I am using `GATK ASEReadCounter` from GATK 4.2.0.0 and encounter the error . `A USER ERROR has occurred: More then one variant context at position: chrX:2774793`. As I googled, that means that there are 2 different entries with the same genomic coordinate in my VCF file. ; But by doing a `grep -P 'chrX\t2774793' 16-98_WGS.vcf | cut -f1-5` for my vcf file I only get one line:. `chrX 2774793 . G *,C`. The whole command I am using is as following . ```; gatk ASEReadCounter ; -R /media/Data/Referenzgenome/HG19/HG19.karyo.fasta ; -I /media/Data/Marco/16-98/16-98_iPSC_A.recal.rh.bam ; -I /media/Data/Marco/16-98/16-98_iPSC_B.recal.rh.bam ; -V /media/Data/Marco/16-98/16-98_WGS.vcf ; -O /media/Data/Marco/16-98/16-98_Out_iPSC_A_B.table ; -L chrX; ```. Do you have any idea how to solve this?; Thanks in advance ; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7249
https://github.com/broadinstitute/gatk/issues/7250:3551,Availability,down,down,3551,"46:05.153 INFO CNNScoreVariants - Inflater: IntelInflater; 10:46:05.153 INFO CNNScoreVariants - GCS max retries/reopens: 20; 10:46:05.153 INFO CNNScoreVariants - Requester pays: disabled; 10:46:05.153 INFO CNNScoreVariants - Initializing engine; 10:46:05.598 INFO FeatureManager - Using codec VCFCodec to read file file:///lustre/scratch/scratch/regmova/tmp/TEST_DATA/TR017_GERMLINE_VARIANTS/TR017.GL.vcf.gz; 10:46:05.638 INFO CNNScoreVariants - Done initializing engine; 10:46:05.639 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 10:46:35.436 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/1d_cnn_mix_train_full_bn.8208762367402959162.json and weights:/tmp/1d_cnn_mix_train_full_bn.2787226329292768726.hd5; 10:46:35.438 INFO CNNScoreVariants - Done scoring variants with CNN.; 10:46:35.438 INFO CNNScoreVariants - Shutting down engine; [12 May 2021 10:46:35 BST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.51 minutes.; Runtime.totalMemory()=2132279296; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: A nack was received from the Python process (most likely caused by a raised exception caused by): nkm received. ```; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/regmova/miniconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/models.py"", line 22, in start_session_get_args_and_model; K.clear_session(). AttributeError: module 'keras.backend' has no attribute 'clear_session'; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForAck(StreamingPythonScriptExecutor.java:222); 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:183); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNSco",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250
https://github.com/broadinstitute/gatk/issues/7250:5663,Deployability,upgrade,upgraded,5663,".python.PythonScriptExecutorException: A nack was received from the Python process (most likely caused by a raised exception caused by): nkm received. ```; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/regmova/miniconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/models.py"", line 22, in start_session_get_args_and_model; K.clear_session(). AttributeError: module 'keras.backend' has no attribute 'clear_session'; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForAck(StreamingPythonScriptExecutor.java:222); 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:183); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.initializePythonArgsAndModel(CNNScoreVariants.java:557); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:317); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1056); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289) ; ```. #### Steps to reproduce; `conda activate gatk; gatk CNNScoreVariants -V VCF.vcf.gz -R reference.fa -O VCF.CNNscored.vcf `. #### Expected behaviour; CNNScoreVariants should generate an annotated VCF. #### Actual behavior; CNNScoreVariants crashes. #### What I tried; I ran into this issue with an older build based on v4.1.9. I upgraded to v4.2.0, removed and built the Conda environment again, but the issue persisted. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250
https://github.com/broadinstitute/gatk/issues/7250:876,Performance,Load,Loading,876,"## Bug Report. ### Affected tool(s) or class(es). CNNScoreVariants. ### Affected version(s); - [x] Latest master branch as of [12/05/2021]. Same issue for an older master branch in v4.1.9. ### Description. **Issue with vqsr_cnn package in Conda environment.; AttributeError: module 'keras.backend' has no attribute 'clear_session'**. > Using GATK jar /lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /lustre/home/regmova/tools/gatk/build ; >/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar CNNScoreVariants -V TR017.GL.vcf.gz -R /home/regmova; >/Scratch/RefGenome/hs37d5.fa -O TR017.CNNscored.vcf; 10:46:04.904 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 12, 2021 10:46:05 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.152 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.0.0-19-ge60cdf8-SNAPSHOT; 10:46:05.152 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:46:05.152 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_92-b14; 10:46:05.152 INFO CNNScoreVariants - Start Date/Time: 12 May 2021 10:46:04 BST; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Version: 2.24.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250
https://github.com/broadinstitute/gatk/issues/7250:3066,Performance,Load,Loading,3066,".0; 10:46:05.153 INFO CNNScoreVariants - Built for Spark Version: 2.4.5; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:46:05.153 INFO CNNScoreVariants - Deflater: IntelDeflater; 10:46:05.153 INFO CNNScoreVariants - Inflater: IntelInflater; 10:46:05.153 INFO CNNScoreVariants - GCS max retries/reopens: 20; 10:46:05.153 INFO CNNScoreVariants - Requester pays: disabled; 10:46:05.153 INFO CNNScoreVariants - Initializing engine; 10:46:05.598 INFO FeatureManager - Using codec VCFCodec to read file file:///lustre/scratch/scratch/regmova/tmp/TEST_DATA/TR017_GERMLINE_VARIANTS/TR017.GL.vcf.gz; 10:46:05.638 INFO CNNScoreVariants - Done initializing engine; 10:46:05.639 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 10:46:35.436 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/1d_cnn_mix_train_full_bn.8208762367402959162.json and weights:/tmp/1d_cnn_mix_train_full_bn.2787226329292768726.hd5; 10:46:35.438 INFO CNNScoreVariants - Done scoring variants with CNN.; 10:46:35.438 INFO CNNScoreVariants - Shutting down engine; [12 May 2021 10:46:35 BST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.51 minutes.; Runtime.totalMemory()=2132279296; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: A nack was received from the Python process (most likely caused by a raised exception caused by): nkm received. ```; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/regmova/miniconda3/envs/gatk/lib/python3.6/site-packages/vq",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250
https://github.com/broadinstitute/gatk/issues/7250:1190,Safety,detect,detect,1190,".9. ### Description. **Issue with vqsr_cnn package in Conda environment.; AttributeError: module 'keras.backend' has no attribute 'clear_session'**. > Using GATK jar /lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /lustre/home/regmova/tools/gatk/build ; >/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar CNNScoreVariants -V TR017.GL.vcf.gz -R /home/regmova; >/Scratch/RefGenome/hs37d5.fa -O TR017.CNNscored.vcf; 10:46:04.904 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 12, 2021 10:46:05 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.152 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.0.0-19-ge60cdf8-SNAPSHOT; 10:46:05.152 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:46:05.152 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_92-b14; 10:46:05.152 INFO CNNScoreVariants - Start Date/Time: 12 May 2021 10:46:04 BST; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Version: 2.24.1; 10:46:05.153 INFO CNNScoreVariants - Picard Version: 2.25.0; 10:46:05.153 INFO CNNScoreVariants - Built for Spark Version: 2.4.5; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Default",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250
https://github.com/broadinstitute/gatk/pull/7252:51,Performance,load,loaded,51,"if there are tables that do not have samples to be loaded, it will not generate the output files. so make them optional",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7252
https://github.com/broadinstitute/gatk/pull/7253:126,Deployability,pipeline,pipeline,126,This now outputs median coverage in addition to the mean coverage which was already being output before from the Mitochondria pipeline. @ahaessly Could you please take a look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7253
https://github.com/broadinstitute/gatk/issues/7254:198,Deployability,release,release,198,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Picard IlluminaBasecallsToSam and IlluminaBasecallsToFastq. ### Affected version(s); - [X] Latest public release version [4.2.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; ""This bug has been fixed in Picard release https://github.com/broadinstitute/picard/releases/tag/2.25.4 - The version of gatk that you are using (4.2.0.0) was packaged with Picard https://github.com/broadinstitute/picard/releases/tag/2.25.0 in it (which has the bug)."" See [IlluminaBasecallsToSam and IlluminaBasecallsToFastq do not demultiplex NovaSeq barcoded reads](https://github.com/broadinstitute/picard/issues/1679) for details. The GATK release needs to be updated to contain Picard 2.25.4 or better. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; Run tools with NovaSeq run directory as input and many dual index barcodes. #### Expected behavior; _Tell us what should happen_; Reads having each of the dual index barcodes should be demultiplexed to separate files. #### Actual behavior; _Tell us what happens instead_; Reads having each of the dual index barcodes are all in the UNKNOWN files. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254
https://github.com/broadinstitute/gatk/issues/7254:430,Deployability,release,release,430,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Picard IlluminaBasecallsToSam and IlluminaBasecallsToFastq. ### Affected version(s); - [X] Latest public release version [4.2.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; ""This bug has been fixed in Picard release https://github.com/broadinstitute/picard/releases/tag/2.25.4 - The version of gatk that you are using (4.2.0.0) was packaged with Picard https://github.com/broadinstitute/picard/releases/tag/2.25.0 in it (which has the bug)."" See [IlluminaBasecallsToSam and IlluminaBasecallsToFastq do not demultiplex NovaSeq barcoded reads](https://github.com/broadinstitute/picard/issues/1679) for details. The GATK release needs to be updated to contain Picard 2.25.4 or better. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; Run tools with NovaSeq run directory as input and many dual index barcodes. #### Expected behavior; _Tell us what should happen_; Reads having each of the dual index barcodes should be demultiplexed to separate files. #### Actual behavior; _Tell us what happens instead_; Reads having each of the dual index barcodes are all in the UNKNOWN files. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254
https://github.com/broadinstitute/gatk/issues/7254:479,Deployability,release,releases,479,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Picard IlluminaBasecallsToSam and IlluminaBasecallsToFastq. ### Affected version(s); - [X] Latest public release version [4.2.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; ""This bug has been fixed in Picard release https://github.com/broadinstitute/picard/releases/tag/2.25.4 - The version of gatk that you are using (4.2.0.0) was packaged with Picard https://github.com/broadinstitute/picard/releases/tag/2.25.0 in it (which has the bug)."" See [IlluminaBasecallsToSam and IlluminaBasecallsToFastq do not demultiplex NovaSeq barcoded reads](https://github.com/broadinstitute/picard/issues/1679) for details. The GATK release needs to be updated to contain Picard 2.25.4 or better. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; Run tools with NovaSeq run directory as input and many dual index barcodes. #### Expected behavior; _Tell us what should happen_; Reads having each of the dual index barcodes should be demultiplexed to separate files. #### Actual behavior; _Tell us what happens instead_; Reads having each of the dual index barcodes are all in the UNKNOWN files. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254
https://github.com/broadinstitute/gatk/issues/7254:616,Deployability,release,releases,616,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Picard IlluminaBasecallsToSam and IlluminaBasecallsToFastq. ### Affected version(s); - [X] Latest public release version [4.2.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; ""This bug has been fixed in Picard release https://github.com/broadinstitute/picard/releases/tag/2.25.4 - The version of gatk that you are using (4.2.0.0) was packaged with Picard https://github.com/broadinstitute/picard/releases/tag/2.25.0 in it (which has the bug)."" See [IlluminaBasecallsToSam and IlluminaBasecallsToFastq do not demultiplex NovaSeq barcoded reads](https://github.com/broadinstitute/picard/issues/1679) for details. The GATK release needs to be updated to contain Picard 2.25.4 or better. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; Run tools with NovaSeq run directory as input and many dual index barcodes. #### Expected behavior; _Tell us what should happen_; Reads having each of the dual index barcodes should be demultiplexed to separate files. #### Actual behavior; _Tell us what happens instead_; Reads having each of the dual index barcodes are all in the UNKNOWN files. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254
https://github.com/broadinstitute/gatk/issues/7254:840,Deployability,release,release,840,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Picard IlluminaBasecallsToSam and IlluminaBasecallsToFastq. ### Affected version(s); - [X] Latest public release version [4.2.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; ""This bug has been fixed in Picard release https://github.com/broadinstitute/picard/releases/tag/2.25.4 - The version of gatk that you are using (4.2.0.0) was packaged with Picard https://github.com/broadinstitute/picard/releases/tag/2.25.0 in it (which has the bug)."" See [IlluminaBasecallsToSam and IlluminaBasecallsToFastq do not demultiplex NovaSeq barcoded reads](https://github.com/broadinstitute/picard/issues/1679) for details. The GATK release needs to be updated to contain Picard 2.25.4 or better. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; Run tools with NovaSeq run directory as input and many dual index barcodes. #### Expected behavior; _Tell us what should happen_; Reads having each of the dual index barcodes should be demultiplexed to separate files. #### Actual behavior; _Tell us what happens instead_; Reads having each of the dual index barcodes are all in the UNKNOWN files. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254
https://github.com/broadinstitute/gatk/issues/7254:860,Deployability,update,updated,860,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Picard IlluminaBasecallsToSam and IlluminaBasecallsToFastq. ### Affected version(s); - [X] Latest public release version [4.2.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; ""This bug has been fixed in Picard release https://github.com/broadinstitute/picard/releases/tag/2.25.4 - The version of gatk that you are using (4.2.0.0) was packaged with Picard https://github.com/broadinstitute/picard/releases/tag/2.25.0 in it (which has the bug)."" See [IlluminaBasecallsToSam and IlluminaBasecallsToFastq do not demultiplex NovaSeq barcoded reads](https://github.com/broadinstitute/picard/issues/1679) for details. The GATK release needs to be updated to contain Picard 2.25.4 or better. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; Run tools with NovaSeq run directory as input and many dual index barcodes. #### Expected behavior; _Tell us what should happen_; Reads having each of the dual index barcodes should be demultiplexed to separate files. #### Actual behavior; _Tell us what happens instead_; Reads having each of the dual index barcodes are all in the UNKNOWN files. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254
https://github.com/broadinstitute/gatk/issues/7254:267,Testability,test,test,267,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Picard IlluminaBasecallsToSam and IlluminaBasecallsToFastq. ### Affected version(s); - [X] Latest public release version [4.2.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; ""This bug has been fixed in Picard release https://github.com/broadinstitute/picard/releases/tag/2.25.4 - The version of gatk that you are using (4.2.0.0) was packaged with Picard https://github.com/broadinstitute/picard/releases/tag/2.25.0 in it (which has the bug)."" See [IlluminaBasecallsToSam and IlluminaBasecallsToFastq do not demultiplex NovaSeq barcoded reads](https://github.com/broadinstitute/picard/issues/1679) for details. The GATK release needs to be updated to contain Picard 2.25.4 or better. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; Run tools with NovaSeq run directory as input and many dual index barcodes. #### Expected behavior; _Tell us what should happen_; Reads having each of the dual index barcodes should be demultiplexed to separate files. #### Actual behavior; _Tell us what happens instead_; Reads having each of the dual index barcodes are all in the UNKNOWN files. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254
https://github.com/broadinstitute/gatk/issues/7254:367,Testability,log,logs,367,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Picard IlluminaBasecallsToSam and IlluminaBasecallsToFastq. ### Affected version(s); - [X] Latest public release version [4.2.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; ""This bug has been fixed in Picard release https://github.com/broadinstitute/picard/releases/tag/2.25.4 - The version of gatk that you are using (4.2.0.0) was packaged with Picard https://github.com/broadinstitute/picard/releases/tag/2.25.0 in it (which has the bug)."" See [IlluminaBasecallsToSam and IlluminaBasecallsToFastq do not demultiplex NovaSeq barcoded reads](https://github.com/broadinstitute/picard/issues/1679) for details. The GATK release needs to be updated to contain Picard 2.25.4 or better. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; Run tools with NovaSeq run directory as input and many dual index barcodes. #### Expected behavior; _Tell us what should happen_; Reads having each of the dual index barcodes should be demultiplexed to separate files. #### Actual behavior; _Tell us what happens instead_; Reads having each of the dual index barcodes are all in the UNKNOWN files. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254
https://github.com/broadinstitute/gatk/pull/7255:158,Deployability,release,release,158,We have received a request to incorporate some of the picard tool improvements in newer versions of picard. This is a bump in preparation of an eventual GATK release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7255
https://github.com/broadinstitute/gatk/pull/7256:168,Security,authenticat,authenticate,168,* It turned out we weren't logging in to dockerhub on the wdl test cases in travis because I misunderstood how the DOCKER_TEST flag was used.; * Now we unconditionally authenticate to dockerhub in all test shards instead of trying to pick only the relevant ones.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7256
https://github.com/broadinstitute/gatk/pull/7256:27,Testability,log,logging,27,* It turned out we weren't logging in to dockerhub on the wdl test cases in travis because I misunderstood how the DOCKER_TEST flag was used.; * Now we unconditionally authenticate to dockerhub in all test shards instead of trying to pick only the relevant ones.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7256
https://github.com/broadinstitute/gatk/pull/7256:62,Testability,test,test,62,* It turned out we weren't logging in to dockerhub on the wdl test cases in travis because I misunderstood how the DOCKER_TEST flag was used.; * Now we unconditionally authenticate to dockerhub in all test shards instead of trying to pick only the relevant ones.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7256
https://github.com/broadinstitute/gatk/pull/7256:201,Testability,test,test,201,* It turned out we weren't logging in to dockerhub on the wdl test cases in travis because I misunderstood how the DOCKER_TEST flag was used.; * Now we unconditionally authenticate to dockerhub in all test shards instead of trying to pick only the relevant ones.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7256
https://github.com/broadinstitute/gatk/pull/7257:98,Deployability,update,update,98,This PR contains the GATK code necessary to enable some features present in the recent GenomicsDB update:; - Fixes https://github.com/broadinstitute/gatk/issues/7222; - Adds tests for https://github.com/broadinstitute/gatk/issues/7089; - Fixed an issue identified by @kcibul where the combine operation for certain fields needs to take care to not remap missing fields to NON_REF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7257
https://github.com/broadinstitute/gatk/pull/7257:174,Testability,test,tests,174,This PR contains the GATK code necessary to enable some features present in the recent GenomicsDB update:; - Fixes https://github.com/broadinstitute/gatk/issues/7222; - Adds tests for https://github.com/broadinstitute/gatk/issues/7089; - Fixed an issue identified by @kcibul where the combine operation for certain fields needs to take care to not remap missing fields to NON_REF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7257
https://github.com/broadinstitute/gatk/issues/7258:665,Availability,error,error,665,"Hello, I made a PoN with my samples and created an hdf5 PoN. This was made using . ##Preprocess; ``; gatk PreprocessIntervals -R ref/hs37d5.fa --bin-length 10000 --padding 0 -O preprocessed_intervals.interval_list; ``. ##annotate; ``; gatk AnnotateIntervals -R ref/hs37d5.fa -L preprocessed_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O annotated_intervals.tsv; ``. ##PoN; ``; gatk --java-options ""-Xmx6500m"" CreateReadCountPanelOfNormals -I MD0078B1.counts.hdf5 -I MD1341B1.counts.hdf5 --minimum-interval-median-percentile 5.0 -O sandbox/cnvponC.pon.hdf5; ``. When I use DenoiseReadCounts on the .counts.hdf5 for the tumour samples, I get an error.; This is the command I used: ; ``; gatk DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; ``. I know that some of these errors are expected but I don't see any other errors and I'm not sure why it stopped running. Any help would be appreciated thank you!. ##Affected Version: gatk/4.0.1.2. ##Bug Report. Using GATK jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; 20:08:44.839 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - The Genome Analysis Toolkit (GATK) v4.0.1.2; 20:08:45.222 INFO D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:924,Availability,error,errors,924,"Hello, I made a PoN with my samples and created an hdf5 PoN. This was made using . ##Preprocess; ``; gatk PreprocessIntervals -R ref/hs37d5.fa --bin-length 10000 --padding 0 -O preprocessed_intervals.interval_list; ``. ##annotate; ``; gatk AnnotateIntervals -R ref/hs37d5.fa -L preprocessed_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O annotated_intervals.tsv; ``. ##PoN; ``; gatk --java-options ""-Xmx6500m"" CreateReadCountPanelOfNormals -I MD0078B1.counts.hdf5 -I MD1341B1.counts.hdf5 --minimum-interval-median-percentile 5.0 -O sandbox/cnvponC.pon.hdf5; ``. When I use DenoiseReadCounts on the .counts.hdf5 for the tumour samples, I get an error.; This is the command I used: ; ``; gatk DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; ``. I know that some of these errors are expected but I don't see any other errors and I'm not sure why it stopped running. Any help would be appreciated thank you!. ##Affected Version: gatk/4.0.1.2. ##Bug Report. Using GATK jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; 20:08:44.839 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - The Genome Analysis Toolkit (GATK) v4.0.1.2; 20:08:45.222 INFO D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:970,Availability,error,errors,970,"Hello, I made a PoN with my samples and created an hdf5 PoN. This was made using . ##Preprocess; ``; gatk PreprocessIntervals -R ref/hs37d5.fa --bin-length 10000 --padding 0 -O preprocessed_intervals.interval_list; ``. ##annotate; ``; gatk AnnotateIntervals -R ref/hs37d5.fa -L preprocessed_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O annotated_intervals.tsv; ``. ##PoN; ``; gatk --java-options ""-Xmx6500m"" CreateReadCountPanelOfNormals -I MD0078B1.counts.hdf5 -I MD1341B1.counts.hdf5 --minimum-interval-median-percentile 5.0 -O sandbox/cnvponC.pon.hdf5; ``. When I use DenoiseReadCounts on the .counts.hdf5 for the tumour samples, I get an error.; This is the command I used: ; ``; gatk DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; ``. I know that some of these errors are expected but I don't see any other errors and I'm not sure why it stopped running. Any help would be appreciated thank you!. ##Affected Version: gatk/4.0.1.2. ##Bug Report. Using GATK jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; 20:08:44.839 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - The Genome Analysis Toolkit (GATK) v4.0.1.2; 20:08:45.222 INFO D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:3922,Availability,Error,Error,3922,"seReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:08:45.223 INFO DenoiseReadCounts - Deflater: IntelDeflater; 20:08:45.223 INFO DenoiseReadCounts - Inflater: IntelInflater; 20:08:45.223 INFO DenoiseReadCounts - GCS max retries/reopens: 20; 20:08:45.223 INFO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:4572,Availability,down,down,4572,"lizing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY mode: Not an HDF5 file; at org.broadinstitute.hdf5.HDF5File.open(HDF5File.java:490); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:82); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:66); at org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts.doWork(DenoiseReadCounts.java:188); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:3348,Deployability,patch,patch,3348,"Counts - Start Date/Time: May 18, 2021 8:08:44 PM EDT; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - HTSJDK Version: 2.14.1; 20:08:45.223 INFO DenoiseReadCounts - Picard Version: 2.17.2; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:08:45.223 INFO DenoiseReadCounts - Deflater: IntelDeflater; 20:08:45.223 INFO DenoiseReadCounts - Inflater: IntelInflater; 20:08:45.223 INFO DenoiseReadCounts - GCS max retries/reopens: 20; 20:08:45.223 INFO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:553,Modifiability,sandbox,sandbox,553,"Hello, I made a PoN with my samples and created an hdf5 PoN. This was made using . ##Preprocess; ``; gatk PreprocessIntervals -R ref/hs37d5.fa --bin-length 10000 --padding 0 -O preprocessed_intervals.interval_list; ``. ##annotate; ``; gatk AnnotateIntervals -R ref/hs37d5.fa -L preprocessed_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O annotated_intervals.tsv; ``. ##PoN; ``; gatk --java-options ""-Xmx6500m"" CreateReadCountPanelOfNormals -I MD0078B1.counts.hdf5 -I MD1341B1.counts.hdf5 --minimum-interval-median-percentile 5.0 -O sandbox/cnvponC.pon.hdf5; ``. When I use DenoiseReadCounts on the .counts.hdf5 for the tumour samples, I get an error.; This is the command I used: ; ``; gatk DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; ``. I know that some of these errors are expected but I don't see any other errors and I'm not sure why it stopped running. Any help would be appreciated thank you!. ##Affected Version: gatk/4.0.1.2. ##Bug Report. Using GATK jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; 20:08:44.839 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - The Genome Analysis Toolkit (GATK) v4.0.1.2; 20:08:45.222 INFO D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:1647,Performance,Load,Loading,1647,"r samples, I get an error.; This is the command I used: ; ``; gatk DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; ``. I know that some of these errors are expected but I don't see any other errors and I'm not sure why it stopped running. Any help would be appreciated thank you!. ##Affected Version: gatk/4.0.1.2. ##Bug Report. Using GATK jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; 20:08:44.839 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - The Genome Analysis Toolkit (GATK) v4.0.1.2; 20:08:45.222 INFO DenoiseReadCounts - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:08:45.222 INFO DenoiseReadCounts - Executing as lnegm@qlogin11.ccm.sickkids.ca on Linux v3.10.0-957.27.2.el7.x86_64 amd64; 20:08:45.222 INFO DenoiseReadCounts - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_91-b14; 20:08:45.222 INFO DenoiseReadCounts - Start Date/Time: May 18, 2021 8:08:44 PM EDT; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:3928,Safety,detect,detected,3928,"seReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:08:45.223 INFO DenoiseReadCounts - Deflater: IntelDeflater; 20:08:45.223 INFO DenoiseReadCounts - Inflater: IntelInflater; 20:08:45.223 INFO DenoiseReadCounts - GCS max retries/reopens: 20; 20:08:45.223 INFO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:4104,Security,access,accessibilty,4104,"eReadCounts - Deflater: IntelDeflater; 20:08:45.223 INFO DenoiseReadCounts - Inflater: IntelInflater; 20:08:45.223 INFO DenoiseReadCounts - GCS max retries/reopens: 20; 20:08:45.223 INFO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY mode: Not an HDF5 file; at org.broadinstitute.hdf5.HDF5File.open(HDF5File.java:490); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:82); at org.broadinstitute.hdf5.HDF5File.<init",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:4296,Security,access,accessibilty,4296,"FO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY mode: Not an HDF5 file; at org.broadinstitute.hdf5.HDF5File.open(HDF5File.java:490); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:82); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:66); at org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts.doWork(DenoiseReadCounts.java:188); at org.broadinstitute.hellbender.cmdline.CommandLineProgram",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:4486,Security,access,accessibilty,4486,"5.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY mode: Not an HDF5 file; at org.broadinstitute.hdf5.HDF5File.open(HDF5File.java:490); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:82); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:66); at org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts.doWork(DenoiseReadCounts.java:188); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:553,Testability,sandbox,sandbox,553,"Hello, I made a PoN with my samples and created an hdf5 PoN. This was made using . ##Preprocess; ``; gatk PreprocessIntervals -R ref/hs37d5.fa --bin-length 10000 --padding 0 -O preprocessed_intervals.interval_list; ``. ##annotate; ``; gatk AnnotateIntervals -R ref/hs37d5.fa -L preprocessed_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O annotated_intervals.tsv; ``. ##PoN; ``; gatk --java-options ""-Xmx6500m"" CreateReadCountPanelOfNormals -I MD0078B1.counts.hdf5 -I MD1341B1.counts.hdf5 --minimum-interval-median-percentile 5.0 -O sandbox/cnvponC.pon.hdf5; ``. When I use DenoiseReadCounts on the .counts.hdf5 for the tumour samples, I get an error.; This is the command I used: ; ``; gatk DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; ``. I know that some of these errors are expected but I don't see any other errors and I'm not sure why it stopped running. Any help would be appreciated thank you!. ##Affected Version: gatk/4.0.1.2. ##Bug Report. Using GATK jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; 20:08:44.839 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - The Genome Analysis Toolkit (GATK) v4.0.1.2; 20:08:45.222 INFO D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:3634,Testability,log,logger,3634,-----------------; 20:08:45.222 INFO DenoiseReadCounts - HTSJDK Version: 2.14.1; 20:08:45.223 INFO DenoiseReadCounts - Picard Version: 2.17.2; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:08:45.223 INFO DenoiseReadCounts - Deflater: IntelDeflater; 20:08:45.223 INFO DenoiseReadCounts - Inflater: IntelInflater; 20:08:45.223 INFO DenoiseReadCounts - GCS max retries/reopens: 20; 20:08:45.223 INFO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/issues/7258:3760,Testability,log,logging,3760,"DenoiseReadCounts - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:08:45.223 INFO DenoiseReadCounts - Deflater: IntelDeflater; 20:08:45.223 INFO DenoiseReadCounts - Inflater: IntelInflater; 20:08:45.223 INFO DenoiseReadCounts - GCS max retries/reopens: 20; 20:08:45.223 INFO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258
https://github.com/broadinstitute/gatk/pull/7260:77,Modifiability,Refactor,Refactored,77,"Conceptually -- two things happen in this PR. . 1. Removed ""arrays"" code; 2. Refactored package names into. ...gvs; |- ingest; |- filtering; |- extract; \- common. All tests pass (can be run with `./gradlew test --tests ""org.broadinstitute.hellbender.tools.gvs*""`). I will run a full workflow in Terra (import, train, extract) and verify. BEFORE we merge, we'll the existing ""ah_var_store"" to indicate this is where array code lives",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7260
https://github.com/broadinstitute/gatk/pull/7260:168,Testability,test,tests,168,"Conceptually -- two things happen in this PR. . 1. Removed ""arrays"" code; 2. Refactored package names into. ...gvs; |- ingest; |- filtering; |- extract; \- common. All tests pass (can be run with `./gradlew test --tests ""org.broadinstitute.hellbender.tools.gvs*""`). I will run a full workflow in Terra (import, train, extract) and verify. BEFORE we merge, we'll the existing ""ah_var_store"" to indicate this is where array code lives",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7260
https://github.com/broadinstitute/gatk/pull/7260:207,Testability,test,test,207,"Conceptually -- two things happen in this PR. . 1. Removed ""arrays"" code; 2. Refactored package names into. ...gvs; |- ingest; |- filtering; |- extract; \- common. All tests pass (can be run with `./gradlew test --tests ""org.broadinstitute.hellbender.tools.gvs*""`). I will run a full workflow in Terra (import, train, extract) and verify. BEFORE we merge, we'll the existing ""ah_var_store"" to indicate this is where array code lives",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7260
https://github.com/broadinstitute/gatk/pull/7260:214,Testability,test,tests,214,"Conceptually -- two things happen in this PR. . 1. Removed ""arrays"" code; 2. Refactored package names into. ...gvs; |- ingest; |- filtering; |- extract; \- common. All tests pass (can be run with `./gradlew test --tests ""org.broadinstitute.hellbender.tools.gvs*""`). I will run a full workflow in Terra (import, train, extract) and verify. BEFORE we merge, we'll the existing ""ah_var_store"" to indicate this is where array code lives",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7260
https://github.com/broadinstitute/gatk/pull/7261:566,Deployability,update,update,566,"@fleharty this is a rebased version of the 17cadfa399643877c70ba830d0b4abf9e5b159a9 branch used to generate the Pf7 CNV call set. There are two minor changes: a) one to remove spurious negative dCR estimates reported by gCNV, which were negatively affecting genotyping of HRP2/3 deletions, and b) updating sklearn to the version used for clustering, so that we can reproduce everything exactly using just the GATK Docker. The latter change probably isn't absolutely necessary, but it doesn't seem to break anything so I'm going to go ahead with it. We might want to update to an even more recent version later on (especially if we make any breaking/non-refactoring improvements to the malaria genotyping code after the initial PR), but unfortunately this slightly changes the clustering assignment for a few samples. @mwalker174 @asmirnov239 we discussed the first change some time ago, but just a heads up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7261
https://github.com/broadinstitute/gatk/pull/7261:653,Modifiability,refactor,refactoring,653,"@fleharty this is a rebased version of the 17cadfa399643877c70ba830d0b4abf9e5b159a9 branch used to generate the Pf7 CNV call set. There are two minor changes: a) one to remove spurious negative dCR estimates reported by gCNV, which were negatively affecting genotyping of HRP2/3 deletions, and b) updating sklearn to the version used for clustering, so that we can reproduce everything exactly using just the GATK Docker. The latter change probably isn't absolutely necessary, but it doesn't seem to break anything so I'm going to go ahead with it. We might want to update to an even more recent version later on (especially if we make any breaking/non-refactoring improvements to the malaria genotyping code after the initial PR), but unfortunately this slightly changes the clustering assignment for a few samples. @mwalker174 @asmirnov239 we discussed the first change some time ago, but just a heads up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7261
https://github.com/broadinstitute/gatk/issues/7265:42,Safety,predict,predictions,42,"Funcotator is producing erroneous protein predictions for some variants. . A few include the following from `HG38` using `funcotator_dataSources.v1.6.20190124g.tar.gz`:. ```; chr7	48227340	.	CTTT	ATGA	82.64	PASS	AC=1;AF=0.500;AN=2;BaseQRankSum=0.157;CNN_1D=-3.565;DP=30;ExcessHet=3.0103;FS=9.874;FUNCOTATION=[ABCA13|hg38|chr7|48227340|48227343|MISSENSE||ONP|CTTT|CTTT|ATGA|g.chr7:48227340_48227343CTTT>ATGA|ENST00000435803.5|+|6|571_574|c.547_550CTTT>ATGA|c.(547-552)CtTTct>AtGAct|p.183_184LS>MT|0.34405940594059403|GGATTTTCTACTTTTACTGCCGAG||||||||||||||||||||||||||||||false||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||false|false||];MBQ=30,30;MFRL=291,574;MLEAC=1;MLEAF=0.500;MMQ=60,60;MPOS=46;MQ=60.00;MQRankSum=0.000;QD=2.75;ReadPosRankSum=1.314;SOR=3.248	GT:AD:DP:GQ:PL	0/1:26,4:30:90:90,0,1080; chr7	48227340	.	C	A	67.77	CNN_2D_SNP_Tranche_99.90_100.00	AC=1;AF=0.500;AN=2;BaseQRankSum=0.202;CNN_2D=-5.539;DP=27;ExcessHet=3.0103;FS=10.098;FUNCOTATION=[ABCA13|hg38|chr7|48227340|48227340|MISSENSE||SNP|C|C|A|g.chr7:48227340C>A|ENST00000435803.5|+|6|571|c.547C>A|c.(547-549)Cag>Aag|p.Q183K|0.34413965087281795|GGATTTTCTACTTTTACTGCC||||||||||||||||||||||||||||||false|||Unknown|Unknown|Unknown|Unknown];MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=2.51;SOR=3.243	GT:AD:DP:GQ:MBQ:MFRL:MMQ:MPOS:PL	0/1:23,4:27:96:30,30:291,574:60:-2147483648:96,0,7386; chr7	48227342	.	T	G	70.77	CNN_2D_SNP_Tranche_99.90_100.00	AC=1;AF=0.500;AN=2;BaseQRankSum=-0.848;CNN_2D=-5.148;DP=27;ExcessHet=3.0103;FS=10.098;FUNCOTATION=[ABCA13|hg38|chr7|48227342|48227342|MISSENSE||SNP|T|T|G|g.chr7:48227342T>G|ENST00000435803.5|+|6|573|c.549T>G|c.(547-549)caT>caG|p.H183Q|0.34413965087281795|ATTTTCTACTTTTACTGCCGA||||||||||||||||||||||||||||||false|||Unknown|Unknown|Unknown|Unknown];MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=2.62;ReadPosRankSum=1.468;SOR=3.243	GT:AD:DP:GQ:MBQ:MFRL:MMQ:MPOS:PL	0/1:23,4:27:99:30,25:291,574:60:47:99,0,7263; chr7	48227343	.	T	A	70.77	CNN_2D_SNP_Tranche_9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7265
https://github.com/broadinstitute/gatk/issues/7266:690,Availability,Error,Error,690,"## Bug Report. ### Affected tool(s) or class(es); ""gatk PostprocessGermlineCNVCalls"". ### Affected version(s); - docker (broadinstitute/gatk, 4.1.9.0 version). ### Description ; #### Steps to reproduce. This is my command line. ; gatk PostprocessGermlineCNVCalls \; --calls-shard-path CALLS_01 \; --model-shard-path MODEL_01 \; --allosomal-contig chrX \; --allosomal-contig chrY \; --autosomal-ref-copy-number 2 \; --contig-ploidy-calls contig-ploidy-calls \; --sample-index 9 \; --output-genotyped-intervals genotyped-intervals-2016001038.vcf.gz \; --output-genotyped-segments genotyped-segments-2016001038.vcf.gz \; --output-denoised-copy-ratios denoised_copy_ratios-2016001038.tsv. #### Error message ; ![image](https://user-images.githubusercontent.com/33537478/119120932-cab21b80-ba67-11eb-9cd3-bb0af7192799.png). #### Expected behavior; generation of following files; genotyped-intervals-2016001038.vcf.gz, genotyped-segments-2016001038.vcf.gz, denoised_copy_ratios-2016001038.tsv . #### Actual behavior; when making a ""genotyped-segments-2016001038.vcf.gz"" file, tool emits this error message. . ![image](https://user-images.githubusercontent.com/33537478/119120932-cab21b80-ba67-11eb-9cd3-bb0af7192799.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7266
https://github.com/broadinstitute/gatk/issues/7266:1086,Availability,error,error,1086,"## Bug Report. ### Affected tool(s) or class(es); ""gatk PostprocessGermlineCNVCalls"". ### Affected version(s); - docker (broadinstitute/gatk, 4.1.9.0 version). ### Description ; #### Steps to reproduce. This is my command line. ; gatk PostprocessGermlineCNVCalls \; --calls-shard-path CALLS_01 \; --model-shard-path MODEL_01 \; --allosomal-contig chrX \; --allosomal-contig chrY \; --autosomal-ref-copy-number 2 \; --contig-ploidy-calls contig-ploidy-calls \; --sample-index 9 \; --output-genotyped-intervals genotyped-intervals-2016001038.vcf.gz \; --output-genotyped-segments genotyped-segments-2016001038.vcf.gz \; --output-denoised-copy-ratios denoised_copy_ratios-2016001038.tsv. #### Error message ; ![image](https://user-images.githubusercontent.com/33537478/119120932-cab21b80-ba67-11eb-9cd3-bb0af7192799.png). #### Expected behavior; generation of following files; genotyped-intervals-2016001038.vcf.gz, genotyped-segments-2016001038.vcf.gz, denoised_copy_ratios-2016001038.tsv . #### Actual behavior; when making a ""genotyped-segments-2016001038.vcf.gz"" file, tool emits this error message. . ![image](https://user-images.githubusercontent.com/33537478/119120932-cab21b80-ba67-11eb-9cd3-bb0af7192799.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7266
https://github.com/broadinstitute/gatk/issues/7266:696,Integrability,message,message,696,"## Bug Report. ### Affected tool(s) or class(es); ""gatk PostprocessGermlineCNVCalls"". ### Affected version(s); - docker (broadinstitute/gatk, 4.1.9.0 version). ### Description ; #### Steps to reproduce. This is my command line. ; gatk PostprocessGermlineCNVCalls \; --calls-shard-path CALLS_01 \; --model-shard-path MODEL_01 \; --allosomal-contig chrX \; --allosomal-contig chrY \; --autosomal-ref-copy-number 2 \; --contig-ploidy-calls contig-ploidy-calls \; --sample-index 9 \; --output-genotyped-intervals genotyped-intervals-2016001038.vcf.gz \; --output-genotyped-segments genotyped-segments-2016001038.vcf.gz \; --output-denoised-copy-ratios denoised_copy_ratios-2016001038.tsv. #### Error message ; ![image](https://user-images.githubusercontent.com/33537478/119120932-cab21b80-ba67-11eb-9cd3-bb0af7192799.png). #### Expected behavior; generation of following files; genotyped-intervals-2016001038.vcf.gz, genotyped-segments-2016001038.vcf.gz, denoised_copy_ratios-2016001038.tsv . #### Actual behavior; when making a ""genotyped-segments-2016001038.vcf.gz"" file, tool emits this error message. . ![image](https://user-images.githubusercontent.com/33537478/119120932-cab21b80-ba67-11eb-9cd3-bb0af7192799.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7266
https://github.com/broadinstitute/gatk/issues/7266:1092,Integrability,message,message,1092,"## Bug Report. ### Affected tool(s) or class(es); ""gatk PostprocessGermlineCNVCalls"". ### Affected version(s); - docker (broadinstitute/gatk, 4.1.9.0 version). ### Description ; #### Steps to reproduce. This is my command line. ; gatk PostprocessGermlineCNVCalls \; --calls-shard-path CALLS_01 \; --model-shard-path MODEL_01 \; --allosomal-contig chrX \; --allosomal-contig chrY \; --autosomal-ref-copy-number 2 \; --contig-ploidy-calls contig-ploidy-calls \; --sample-index 9 \; --output-genotyped-intervals genotyped-intervals-2016001038.vcf.gz \; --output-genotyped-segments genotyped-segments-2016001038.vcf.gz \; --output-denoised-copy-ratios denoised_copy_ratios-2016001038.tsv. #### Error message ; ![image](https://user-images.githubusercontent.com/33537478/119120932-cab21b80-ba67-11eb-9cd3-bb0af7192799.png). #### Expected behavior; generation of following files; genotyped-intervals-2016001038.vcf.gz, genotyped-segments-2016001038.vcf.gz, denoised_copy_ratios-2016001038.tsv . #### Actual behavior; when making a ""genotyped-segments-2016001038.vcf.gz"" file, tool emits this error message. . ![image](https://user-images.githubusercontent.com/33537478/119120932-cab21b80-ba67-11eb-9cd3-bb0af7192799.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7266
https://github.com/broadinstitute/gatk/issues/7269:2014,Availability,down,download,2014,". Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. Yes samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1 tabix samtools; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. #### Expected behavior; _Tell us what should happen_; All `PASS` var counts are the same; #### Actual behavior; _Tell us what happens instead_; `PASS` var counts vary slightly +/- samtools and year docker built; ----; Thank you for your time!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/issues/7269:2486,Availability,down,download,2486,". Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. Yes samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1 tabix samtools; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. #### Expected behavior; _Tell us what should happen_; All `PASS` var counts are the same; #### Actual behavior; _Tell us what happens instead_; `PASS` var counts vary slightly +/- samtools and year docker built; ----; Thank you for your time!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/issues/7269:1066,Deployability,update,update,1066,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Mutect2; ### Affected version(s); - [ ] 4.1.1.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; We have run mutect2 on the same sample using the same input crams, references, and intervals. The only discernible difference is the docker image used that we built. The difference between the first and second one is that the first one was built without samtools, the second one with samtools. The third is exactly the same as the second except it was re-built about a year or more later. Looking at a count of the `PASS` results based on each:. |Run type |var count|; |---------------------------|---------|; |docker no samtools | 8265 |; |docker yes samtools | 8283 |; |docker yes samtools rebuilt | 8273 |; |docker no samtools recently built | 8271 |. Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/issues/7269:1888,Deployability,update,update,1888,". Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. Yes samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1 tabix samtools; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. #### Expected behavior; _Tell us what should happen_; All `PASS` var counts are the same; #### Actual behavior; _Tell us what happens instead_; `PASS` var counts vary slightly +/- samtools and year docker built; ----; Thank you for your time!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/issues/7269:1902,Deployability,install,install,1902,". Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. Yes samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1 tabix samtools; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. #### Expected behavior; _Tell us what should happen_; All `PASS` var counts are the same; #### Actual behavior; _Tell us what happens instead_; `PASS` var counts vary slightly +/- samtools and year docker built; ----; Thank you for your time!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/issues/7269:2005,Deployability,release,releases,2005,". Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. Yes samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1 tabix samtools; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. #### Expected behavior; _Tell us what should happen_; All `PASS` var counts are the same; #### Actual behavior; _Tell us what happens instead_; `PASS` var counts vary slightly +/- samtools and year docker built; ----; Thank you for your time!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/issues/7269:2345,Deployability,update,update,2345,". Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. Yes samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1 tabix samtools; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. #### Expected behavior; _Tell us what should happen_; All `PASS` var counts are the same; #### Actual behavior; _Tell us what happens instead_; `PASS` var counts vary slightly +/- samtools and year docker built; ----; Thank you for your time!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/issues/7269:2359,Deployability,install,install,2359,". Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. Yes samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1 tabix samtools; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. #### Expected behavior; _Tell us what should happen_; All `PASS` var counts are the same; #### Actual behavior; _Tell us what happens instead_; `PASS` var counts vary slightly +/- samtools and year docker built; ----; Thank you for your time!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/issues/7269:2477,Deployability,release,releases,2477,". Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. Yes samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1 tabix samtools; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. #### Expected behavior; _Tell us what should happen_; All `PASS` var counts are the same; #### Actual behavior; _Tell us what happens instead_; `PASS` var counts vary slightly +/- samtools and year docker built; ----; Thank you for your time!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/issues/7269:234,Testability,log,logs,234,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Mutect2; ### Affected version(s); - [ ] 4.1.1.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; We have run mutect2 on the same sample using the same input crams, references, and intervals. The only discernible difference is the docker image used that we built. The difference between the first and second one is that the first one was built without samtools, the second one with samtools. The third is exactly the same as the second except it was re-built about a year or more later. Looking at a count of the `PASS` results based on each:. |Run type |var count|; |---------------------------|---------|; |docker no samtools | 8265 |; |docker yes samtools | 8283 |; |docker yes samtools rebuilt | 8273 |; |docker no samtools recently built | 8271 |. Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269
https://github.com/broadinstitute/gatk/pull/7270:117,Deployability,update,updated,117,"We found some links that needed to be fixed in the Funcotator, ASEReadCounter, and VariantRecalibrator tool docs. We updated them or removed the links as needed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7270
https://github.com/broadinstitute/gatk/pull/7271:308,Availability,avail,available,308,"Using underlying functionality from GenomicsDB to validate/specify cloud url's for GenomicsDB workspaces. This allows for the specification of s3 and azure blob storage uri's in addition to gcs for GenomicsDB workspaces. Currently, there are no tests for s3/az uri's, this is just experimental functionality available if needed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7271
https://github.com/broadinstitute/gatk/pull/7271:50,Security,validat,validate,50,"Using underlying functionality from GenomicsDB to validate/specify cloud url's for GenomicsDB workspaces. This allows for the specification of s3 and azure blob storage uri's in addition to gcs for GenomicsDB workspaces. Currently, there are no tests for s3/az uri's, this is just experimental functionality available if needed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7271
https://github.com/broadinstitute/gatk/pull/7271:245,Testability,test,tests,245,"Using underlying functionality from GenomicsDB to validate/specify cloud url's for GenomicsDB workspaces. This allows for the specification of s3 and azure blob storage uri's in addition to gcs for GenomicsDB workspaces. Currently, there are no tests for s3/az uri's, this is just experimental functionality available if needed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7271
https://github.com/broadinstitute/gatk/pull/7272:35,Availability,reliab,reliability,35,A collection of changes to enhance reliability and ease-of-use. Users no longer have to make a table containing the sample names to extract in GvsPrepareCallset (which was painful) and they don't have to re-supply that same table when rendering the VCF in GvsExtractCallset (which was error prone). GvsPrepareCallet now takes a file of sample names as a parameter as well as an export table _prefix_. The main `sample_info` table is then subset to the sample names in the supplied file and stored in the table `{export_prefix}__SAMPLES`. The export table is created and now named `{export_prefix}__DATA`. GvsExtractCallset now only needs to take this export prefix and is able to get the sample list and data it needs from these tables. @ericsong -- does this fit the AoU use case well?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7272
https://github.com/broadinstitute/gatk/pull/7272:285,Availability,error,error,285,A collection of changes to enhance reliability and ease-of-use. Users no longer have to make a table containing the sample names to extract in GvsPrepareCallset (which was painful) and they don't have to re-supply that same table when rendering the VCF in GvsExtractCallset (which was error prone). GvsPrepareCallet now takes a file of sample names as a parameter as well as an export table _prefix_. The main `sample_info` table is then subset to the sample names in the supplied file and stored in the table `{export_prefix}__SAMPLES`. The export table is created and now named `{export_prefix}__DATA`. GvsExtractCallset now only needs to take this export prefix and is able to get the sample list and data it needs from these tables. @ericsong -- does this fit the AoU use case well?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7272
https://github.com/broadinstitute/gatk/pull/7272:27,Modifiability,enhance,enhance,27,A collection of changes to enhance reliability and ease-of-use. Users no longer have to make a table containing the sample names to extract in GvsPrepareCallset (which was painful) and they don't have to re-supply that same table when rendering the VCF in GvsExtractCallset (which was error prone). GvsPrepareCallet now takes a file of sample names as a parameter as well as an export table _prefix_. The main `sample_info` table is then subset to the sample names in the supplied file and stored in the table `{export_prefix}__SAMPLES`. The export table is created and now named `{export_prefix}__DATA`. GvsExtractCallset now only needs to take this export prefix and is able to get the sample list and data it needs from these tables. @ericsong -- does this fit the AoU use case well?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7272
https://github.com/broadinstitute/gatk/pull/7273:62,Performance,load,loaded,62,query the partition table to see if samples have already been loaded,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7273
https://github.com/broadinstitute/gatk/pull/7275:8,Testability,test,tested,8,"Locally tested and the exclude-filtered flag removes filtered rows; <img width=""1007"" alt=""Screen Shot 2021-05-25 at 7 09 22 PM"" src=""https://user-images.githubusercontent.com/6863459/119579588-b8e4b700-bd8c-11eb-858f-8a4c8bad152d.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7275
https://github.com/broadinstitute/gatk/issues/7276:1295,Availability,down,down,1295,"loci, an exception is raised on FilterMutectCalls. This issue was reported https://github.com/broadinstitute/gatk/issues/6237, and I have bumped GATK to several versions, including latest, but the issue persists. This exception is not raised when the `--alleles` flag is excluded. ### Affected tool(s) or class(es); Mutect2, FilterMutectCalls. ### Affected version(s); - [ ] 4.2.0.0; - [ ] 4.1.9.0; - [ ] 4.1.4.1; - [ ] _not_ 4.1.5.0 (but unfiltered vcf is not correct due to missing info fields so we are not using this version). ### Description ; here is the stack trace; ```; 20:29:45.346 INFO FilterMutectCalls - Done initializing engine; 20:29:45.399 INFO ProgressMeter - Starting traversal; 20:29:45.399 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:29:45.400 INFO FilterMutectCalls - Starting pass 0 through the variants; 20:29:45.580 INFO FilterMutectCalls - Finished pass 0 through the variants; 20:29:45.630 INFO FilterMutectCalls - Shutting down engine; [May 25, 2021 8:29:45 PM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=462946304; java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276
https://github.com/broadinstitute/gatk/issues/7276:2404,Performance,perform,performEMIteration,2404,apsed time: 0.01 minutes.; Runtime.totalMemory()=462946304; java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276
https://github.com/broadinstitute/gatk/issues/7276:1626,Testability,log,logSumExp,1626,"ls. ### Affected version(s); - [ ] 4.2.0.0; - [ ] 4.1.9.0; - [ ] 4.1.4.1; - [ ] _not_ 4.1.5.0 (but unfiltered vcf is not correct due to missing info fields so we are not using this version). ### Description ; here is the stack trace; ```; 20:29:45.346 INFO FilterMutectCalls - Done initializing engine; 20:29:45.399 INFO ProgressMeter - Starting traversal; 20:29:45.399 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:29:45.400 INFO FilterMutectCalls - Starting pass 0 through the variants; 20:29:45.580 INFO FilterMutectCalls - Finished pass 0 through the variants; 20:29:45.630 INFO FilterMutectCalls - Shutting down engine; [May 25, 2021 8:29:45 PM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=462946304; java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276
https://github.com/broadinstitute/gatk/issues/7276:2545,Usability,learn,learnAndClearAccumulatedData,2545,at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276
https://github.com/broadinstitute/gatk/issues/7276:2695,Usability,learn,learnParameters,2695,"ls.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). ```. #### Steps to reproduce; Here are the commands run. Can provide additional details if needed.; ```; gatk \; --java-options ""-Xmx32g"" \; M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276
https://github.com/broadinstitute/gatk/issues/7280:2358,Deployability,update,updates,2358,"ols.github.io/hts-specs/VCFv4.2.pdf) doesn't actually say what format should the SB field be, so overriding it seems to be a bug in htsjdk?. #### Steps to reproduce; ```; cat sb-good-tiny.vcf; ##fileformat=VCFv4.2; ##INFO=<ID=SB,Number=.,Type=Integer,Description="""">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	10146	.	AC	A	.	.	SB=5,2,18,29. gatk ApplyVQSR -O sb-recalibrated-tiny.vcf -V sb-good-tiny.vcf --recal-file recalibration. cat sb-recalibrated-tiny.vcf; ##fileformat=VCFv4.2; ##FILTER=<ID=LOW_VQSLOD,Description=""VQSLOD < 0.0"">; ##FILTER=<ID=PASS,Description=""Site contains at least one allele that passes filters"">; ##GATKCommandLine=<ID=ApplyVQSR,CommandLine=""ApplyVQSR --recal-file /Users/vlad/tmp/sb/recalibration --output sb-recalibrated-tiny-renamed4.vcf --variant sb-good-tiny-renamed4.vcf --use-allele-specific-annotations false --ignore-all-filters false --exclude-filtered false --mode SNP --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.1.9.0"",Date=""31 May 2021 12:07:54 PM"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=NEGATIVE_TRAIN_SITE,Number=0,Type=Flag,Description=""This variant was used to build the negative",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7280
https://github.com/broadinstitute/gatk/issues/7280:3696,Performance,perform,performing,3696,"rval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.1.9.0"",Date=""31 May 2021 12:07:54 PM"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=NEGATIVE_TRAIN_SITE,Number=0,Type=Flag,Description=""This variant was used to build the negative training set of bad variants"">; ##INFO=<ID=POSITIVE_TRAIN_SITE,Number=0,Type=Flag,Description=""This variant was used to build the positive training set of good variants"">; ##INFO=<ID=SB,Number=1,Type=Float,Description=""Strand Bias"">; ##INFO=<ID=VQSLOD,Number=1,Type=Float,Description=""Log odds of being a true variant versus being false under the trained gaussian mixture model"">; ##INFO=<ID=culprit,Number=1,Type=String,Description=""The annotation which was the worst performing in the Gaussian mixture model, likely the reason why the variant was filtered out"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38>; ##source=ApplyVQSR; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	10146	.	AC	A	.	.	SB=5,2,18,29; ```. It can be reproduced with any `recalibration`/`recalibration.idx` pair of files. #### Expected behavior; SB INFO header is either not overwritten, or the correct Type and Number are given. #### Actual behavior; SB INFO header is overwritten with an incorrect Type and Number.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7280
https://github.com/broadinstitute/gatk/issues/7280:1127,Safety,detect,detect,1127,"h htsjdk). ### Affected version(s); - GATK version: 4.1.9.0. ### Description ; Running `ApplyVQSR` on a VCF with a pre-existing SB field makes it overwrite it with an incorrect header. Original header:. ```; ##INFO=<ID=SB,Number=.,Type=Integer,Description="""">; ``` . New header:. ```; ##INFO=<ID=SB,Number=1,Type=Float,Description=""Strand Bias"">; ```. Even though the `INFO` values stay in the format of an array of integers: `SB=5,2,18,29`. My understanding is the replacement of the header actually [happens in htsjdk](https://github.com/samtools/htsjdk/blob/7719274fe370a51a24e6067de21bbe7e18c160a9/src/main/java/htsjdk/variant/vcf/VCFStandardHeaderLines.java#L187), as it has SB in a set of [standard fields](https://github.com/samtools/htsjdk/blob/7719274fe370a51a24e6067de21bbe7e18c160a9/src/main/java/htsjdk/variant/vcf/VCFStandardHeaderLines.java#L175). I see that [many](https://github.com/broadinstitute/gatk/search?q=Per-sample+component+statistics+which+comprise+the+Fisher%27s+Exact+Test+to+detect+strand+bias) GATK tools add SB with Number=1,Type=Integer, and the [VCF spec](https://samtools.github.io/hts-specs/VCFv4.2.pdf) doesn't actually say what format should the SB field be, so overriding it seems to be a bug in htsjdk?. #### Steps to reproduce; ```; cat sb-good-tiny.vcf; ##fileformat=VCFv4.2; ##INFO=<ID=SB,Number=.,Type=Integer,Description="""">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	10146	.	AC	A	.	.	SB=5,2,18,29. gatk ApplyVQSR -O sb-recalibrated-tiny.vcf -V sb-good-tiny.vcf --recal-file recalibration. cat sb-recalibrated-tiny.vcf; ##fileformat=VCFv4.2; ##FILTER=<ID=LOW_VQSLOD,Description=""VQSLOD < 0.0"">; ##FILTER=<ID=PASS,Description=""Site contains at least one allele that passes filters"">; ##GATKCommandLine=<ID=ApplyVQSR,CommandLine=""ApplyVQSR --recal-file /Users/vlad/tmp/sb/recalibration --output sb-recalibrated-tiny-renamed4.vcf --variant sb-good-tiny-renamed4.vcf --use-allele-specific-annotations fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7280
https://github.com/broadinstitute/gatk/issues/7280:2302,Security,validat,validation-stringency,2302,"ols.github.io/hts-specs/VCFv4.2.pdf) doesn't actually say what format should the SB field be, so overriding it seems to be a bug in htsjdk?. #### Steps to reproduce; ```; cat sb-good-tiny.vcf; ##fileformat=VCFv4.2; ##INFO=<ID=SB,Number=.,Type=Integer,Description="""">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	10146	.	AC	A	.	.	SB=5,2,18,29. gatk ApplyVQSR -O sb-recalibrated-tiny.vcf -V sb-good-tiny.vcf --recal-file recalibration. cat sb-recalibrated-tiny.vcf; ##fileformat=VCFv4.2; ##FILTER=<ID=LOW_VQSLOD,Description=""VQSLOD < 0.0"">; ##FILTER=<ID=PASS,Description=""Site contains at least one allele that passes filters"">; ##GATKCommandLine=<ID=ApplyVQSR,CommandLine=""ApplyVQSR --recal-file /Users/vlad/tmp/sb/recalibration --output sb-recalibrated-tiny-renamed4.vcf --variant sb-good-tiny-renamed4.vcf --use-allele-specific-annotations false --ignore-all-filters false --exclude-filtered false --mode SNP --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.1.9.0"",Date=""31 May 2021 12:07:54 PM"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=NEGATIVE_TRAIN_SITE,Number=0,Type=Flag,Description=""This variant was used to build the negative",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7280
https://github.com/broadinstitute/gatk/issues/7280:2401,Security,validat,validation,2401,"f --recal-file recalibration. cat sb-recalibrated-tiny.vcf; ##fileformat=VCFv4.2; ##FILTER=<ID=LOW_VQSLOD,Description=""VQSLOD < 0.0"">; ##FILTER=<ID=PASS,Description=""Site contains at least one allele that passes filters"">; ##GATKCommandLine=<ID=ApplyVQSR,CommandLine=""ApplyVQSR --recal-file /Users/vlad/tmp/sb/recalibration --output sb-recalibrated-tiny-renamed4.vcf --variant sb-good-tiny-renamed4.vcf --use-allele-specific-annotations false --ignore-all-filters false --exclude-filtered false --mode SNP --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.1.9.0"",Date=""31 May 2021 12:07:54 PM"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=NEGATIVE_TRAIN_SITE,Number=0,Type=Flag,Description=""This variant was used to build the negative training set of bad variants"">; ##INFO=<ID=POSITIVE_TRAIN_SITE,Number=0,Type=Flag,Description=""This variant was used to build the positive training set of good variants"">; ##INFO=<ID=SB,Number=1,Type=Float,Description=""Strand Bias"">; ##INFO=<ID=VQSLOD,Number=1,Type=Float,Description=""Log odds of being a true variant versus being false under the trained gaussian mixture model"">; ##INFO=<ID=culprit,Number=1,Type=String,Description=""The annotation which w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7280
https://github.com/broadinstitute/gatk/issues/7280:1119,Testability,Test,Test,1119,"h htsjdk). ### Affected version(s); - GATK version: 4.1.9.0. ### Description ; Running `ApplyVQSR` on a VCF with a pre-existing SB field makes it overwrite it with an incorrect header. Original header:. ```; ##INFO=<ID=SB,Number=.,Type=Integer,Description="""">; ``` . New header:. ```; ##INFO=<ID=SB,Number=1,Type=Float,Description=""Strand Bias"">; ```. Even though the `INFO` values stay in the format of an array of integers: `SB=5,2,18,29`. My understanding is the replacement of the header actually [happens in htsjdk](https://github.com/samtools/htsjdk/blob/7719274fe370a51a24e6067de21bbe7e18c160a9/src/main/java/htsjdk/variant/vcf/VCFStandardHeaderLines.java#L187), as it has SB in a set of [standard fields](https://github.com/samtools/htsjdk/blob/7719274fe370a51a24e6067de21bbe7e18c160a9/src/main/java/htsjdk/variant/vcf/VCFStandardHeaderLines.java#L175). I see that [many](https://github.com/broadinstitute/gatk/search?q=Per-sample+component+statistics+which+comprise+the+Fisher%27s+Exact+Test+to+detect+strand+bias) GATK tools add SB with Number=1,Type=Integer, and the [VCF spec](https://samtools.github.io/hts-specs/VCFv4.2.pdf) doesn't actually say what format should the SB field be, so overriding it seems to be a bug in htsjdk?. #### Steps to reproduce; ```; cat sb-good-tiny.vcf; ##fileformat=VCFv4.2; ##INFO=<ID=SB,Number=.,Type=Integer,Description="""">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	10146	.	AC	A	.	.	SB=5,2,18,29. gatk ApplyVQSR -O sb-recalibrated-tiny.vcf -V sb-good-tiny.vcf --recal-file recalibration. cat sb-recalibrated-tiny.vcf; ##fileformat=VCFv4.2; ##FILTER=<ID=LOW_VQSLOD,Description=""VQSLOD < 0.0"">; ##FILTER=<ID=PASS,Description=""Site contains at least one allele that passes filters"">; ##GATKCommandLine=<ID=ApplyVQSR,CommandLine=""ApplyVQSR --recal-file /Users/vlad/tmp/sb/recalibration --output sb-recalibrated-tiny-renamed4.vcf --variant sb-good-tiny-renamed4.vcf --use-allele-specific-annotations fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7280
https://github.com/broadinstitute/gatk/issues/7280:3512,Testability,Log,Log,3512,"rval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.1.9.0"",Date=""31 May 2021 12:07:54 PM"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=NEGATIVE_TRAIN_SITE,Number=0,Type=Flag,Description=""This variant was used to build the negative training set of bad variants"">; ##INFO=<ID=POSITIVE_TRAIN_SITE,Number=0,Type=Flag,Description=""This variant was used to build the positive training set of good variants"">; ##INFO=<ID=SB,Number=1,Type=Float,Description=""Strand Bias"">; ##INFO=<ID=VQSLOD,Number=1,Type=Float,Description=""Log odds of being a true variant versus being false under the trained gaussian mixture model"">; ##INFO=<ID=culprit,Number=1,Type=String,Description=""The annotation which was the worst performing in the Gaussian mixture model, likely the reason why the variant was filtered out"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38>; ##source=ApplyVQSR; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	10146	.	AC	A	.	.	SB=5,2,18,29; ```. It can be reproduced with any `recalibration`/`recalibration.idx` pair of files. #### Expected behavior; SB INFO header is either not overwritten, or the correct Type and Number are given. #### Actual behavior; SB INFO header is overwritten with an incorrect Type and Number.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7280
https://github.com/broadinstitute/gatk/issues/7281:6337,Availability,Avail,Available,6337,ce sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to /tmp/libgkl_utils9151568277466250840.so; 11:35:41.777 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:35:41.802 DEBUG NativeLibraryLoader - Extracting libgkl_pairhmm_omp.so to /tmp/libgkl_pairhmm_omp8179002917276126697.so; 11:35:41.847 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:35:41.848 INFO IntelPairHmm - Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache mi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:8068,Availability,Recover,Recovered,8068,he miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:8141,Availability,Recover,Recovered,8141,he - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:9192,Availability,Recover,Recovered,9192,DoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:9267,Availability,Recover,Recovered,9267, Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:10072,Availability,Recover,Recovered,10072,rocessing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dan,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:10145,Availability,Recover,Recovered,10145,; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:10910,Availability,Recover,Recovered,10910,eads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:10983,Availability,Recover,Recovered,10983,rM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:11056,Availability,Recover,Recovered,11056,ngGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:11131,Availability,Recover,Recovered,11131,Graph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false num,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:13043,Availability,Recover,Recovered,13043,6:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:13117,Availability,Recover,Recovered,13117,tive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 7,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:14167,Availability,Recover,Recovered,14167,ToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:14242,Availability,Recover,Recovered,14242,5.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:15455,Availability,Recover,Recovered,15455,e coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:15528,Availability,Recover,Recovered,15528,t 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16272,Availability,Recover,Recovered,16272,36 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16345,Availability,Recover,Recovered,16345,embly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 danglin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16419,Availability,Recover,Recovered,16419,97 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16494,Availability,Recover,Recovered,16494, DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:17251,Availability,Recover,Recovered,17251,readingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing as,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:17326,Availability,Recover,Recovered,17326,adingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.48,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:19103,Availability,Recover,Recovered,19103,; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.489 DEBUG Mutect2 - Processing assembly region at chrM:11085-11384 isActive: false numReads: 0; 12:05:51.501 DEBUG Mutect2 - Processing assembly region at chrM:11385-11684 isActive: false numReads: 0; 12:05:51.513 DEBUG Mutect2 - Processing assembly region at chrM:11685-11984 isActive: false numReads: 0; 12:05:51.526 DEBUG Mutect2 - Processing assembly region at chrM:11985-12284 isActive: false numReads: 0; 12:06:02.022 DEBUG Mutect2 - Processing assembly region at chrM:12285-12584 isActive: false numReads: 0; 12:06:03.941 DEBUG Mutect2 - Processing assembly region at chrM:12585-12729 isActive: false numReads: 44205; 12:06:04.330 DEBUG Mutect2 - Processing assembly region at chrM:12730-13020 isActive: true numReads: 88386; 12:06:10.995 DEBUG ReadThreadingGraph - Recovered 11 of 15 dangling tails; 12:06:11.087 DEBUG ReadThreadingGraph - Recovered 7 of 36 dangling heads; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG Rea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:19178,Availability,Recover,Recovered,19178,84 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.489 DEBUG Mutect2 - Processing assembly region at chrM:11085-11384 isActive: false numReads: 0; 12:05:51.501 DEBUG Mutect2 - Processing assembly region at chrM:11385-11684 isActive: false numReads: 0; 12:05:51.513 DEBUG Mutect2 - Processing assembly region at chrM:11685-11984 isActive: false numReads: 0; 12:05:51.526 DEBUG Mutect2 - Processing assembly region at chrM:11985-12284 isActive: false numReads: 0; 12:06:02.022 DEBUG Mutect2 - Processing assembly region at chrM:12285-12584 isActive: false numReads: 0; 12:06:03.941 DEBUG Mutect2 - Processing assembly region at chrM:12585-12729 isActive: false numReads: 44205; 12:06:04.330 DEBUG Mutect2 - Processing assembly region at chrM:12730-13020 isActive: true numReads: 88386; 12:06:10.995 DEBUG ReadThreadingGraph - Recovered 11 of 15 dangling tails; 12:06:11.087 DEBUG ReadThreadingGraph - Recovered 7 of 36 dangling heads; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG Re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20128,Availability,Recover,Recovered,20128,ls; 12:06:11.087 DEBUG ReadThreadingGraph - Recovered 7 of 36 dangling heads; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20203,Availability,Recover,Recovered,20203,s; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20767,Availability,Recover,Recovered,20767, DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20842,Availability,Recover,Recovered,20842,alse numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at ch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:22201,Availability,Recover,Recovered,22201,:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:22274,Availability,Recover,Recovered,22274,tect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:22347,Availability,Recover,Recovered,22347,mReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignment,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:22420,Availability,Recover,Recovered,22420, chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:23971,Availability,down,down,23971,"tive Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:13:58.943 INFO ProgressMeter - chrM:15445 38.3 63 1.6; 12:13:58.946 INFO ProgressMeter - Traversal complete. Processed 63 total regions in 38.3 minutes.; 12:13:59.105 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.7153035790000002; 12:13:59.110 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 1084.6708644550001; 12:13:59.114 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 54.84 sec; 12:13:59.118 INFO Mutect2 - Shutting down engine; [May 31, 2021 12:13:59 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 38.32 minutes.; Runtime.totalMemory()=18715508736; ```. From the log, we see that Mutect2 finished in 40 minutes. In the meanwhile the `g.vcf`, `g.vcf.idx` and `g.vcf.stats` files are generated and contain non-empty contents. However, the program keeps running for hours and still has not finished. Therefore I wonder if Mutect2 is stuck with some post-processing that is less documented. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3302,Deployability,Configurat,Configuration,3302,SSION_LEVEL : 2; 11:35:40.188 INFO Mutect2 - HTSJDK Defaults.CREATE_INDEX : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3286,Modifiability,Config,ConfigFactory,3286,SSION_LEVEL : 2; 11:35:40.188 INFO Mutect2 - HTSJDK Defaults.CREATE_INDEX : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3302,Modifiability,Config,Configuration,3302,SSION_LEVEL : 2; 11:35:40.188 INFO Mutect2 - HTSJDK Defaults.CREATE_INDEX : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3350,Modifiability,Config,ConfigFactory,3350,.CREATE_INDEX : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3406,Modifiability,Config,ConfigFactory,3406,"aults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3473,Modifiability,Config,ConfigFactory,3473,"TOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3552,Modifiability,Config,ConfigFactory,3552," Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3631,Modifiability,Config,ConfigFactory,3631," Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3710,Modifiability,Config,ConfigFactory,3710,"s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG Conf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3789,Modifiability,Config,ConfigFactory,3789,"1:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	clou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3855,Modifiability,Config,ConfigFactory,3855,"; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3931,Modifiability,Config,ConfigFactory,3931,"L; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:3999,Modifiability,Config,ConfigFactory,3999,"_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4075,Modifiability,Config,ConfigFactory,4075,"WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Request",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4145,Modifiability,Config,ConfigFactory,4145,".USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4218,Modifiability,Config,ConfigFactory,4218,"DK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4288,Modifiability,Config,ConfigFactory,4288,"ctory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4360,Modifiability,Config,ConfigFactory,4360,"ries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4491,Modifiability,Config,ConfigFactory,4491,"_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4598,Modifiability,Config,ConfigFactory,4598,"35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4713,Modifiability,Config,ConfigFactory,4713,"use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-packa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4775,Modifiability,Config,ConfigFactory,4775,"tory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:4842,Modifiability,Config,ConfigFactory,4842,"tory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:8355,Modifiability,Extend,Extended,8355, 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 >,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:9488,Modifiability,Extend,Extended,9488, Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:10278,Modifiability,Extend,Extended,10278,angling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine -,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:11354,Modifiability,Extend,Extended,11354,coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:13432,Modifiability,Extend,Extended,13432, - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Eng,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:14377,Modifiability,Extend,Extended,14377,6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isAct,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:15849,Modifiability,Extend,Extended,15849,mbly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16628,Modifiability,Extend,Extended,16628,anding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:17460,Modifiability,Extend,Extended,17460,BUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.489 DEBUG Mutect2 - Processing assembly region at chrM:11085-11384 isActive: false numReads: 0; 12:05:51.501 DEBUG Mutect2 - Processing asse,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:19314,Modifiability,Extend,Extended,19314,05:51.489 DEBUG Mutect2 - Processing assembly region at chrM:11085-11384 isActive: false numReads: 0; 12:05:51.501 DEBUG Mutect2 - Processing assembly region at chrM:11385-11684 isActive: false numReads: 0; 12:05:51.513 DEBUG Mutect2 - Processing assembly region at chrM:11685-11984 isActive: false numReads: 0; 12:05:51.526 DEBUG Mutect2 - Processing assembly region at chrM:11985-12284 isActive: false numReads: 0; 12:06:02.022 DEBUG Mutect2 - Processing assembly region at chrM:12285-12584 isActive: false numReads: 0; 12:06:03.941 DEBUG Mutect2 - Processing assembly region at chrM:12585-12729 isActive: false numReads: 44205; 12:06:04.330 DEBUG Mutect2 - Processing assembly region at chrM:12730-13020 isActive: true numReads: 88386; 12:06:10.995 DEBUG ReadThreadingGraph - Recovered 11 of 15 dangling tails; 12:06:11.087 DEBUG ReadThreadingGraph - Recovered 7 of 36 dangling heads; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20339,Modifiability,Extend,Extended,20339,0; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20978,Modifiability,Extend,Extended,20978,341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:22555,Modifiability,Extend,Extended,22555,umReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:13:58.943 INFO ProgressMeter - chrM:15445 38.3 63 1.6; 12:13:58.946 INFO ProgressM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:5658,Performance,Load,Loading,5658,ute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to /tmp/libgkl_utils9151568277466250840.so; 11:35:41.777 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:35:41.802 DEBUG NativeLibraryLoader - Extracting libgkl_pairhmm_omp.so to /tmp/libgkl_pairhmm_omp8179002917276126697.so; 11:35:41.847 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:35:41.848 INFO IntelPairHmm - Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:5946,Performance,Load,Loading,5946,:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to /tmp/libgkl_utils9151568277466250840.so; 11:35:41.777 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:35:41.802 DEBUG NativeLibraryLoader - Extracting libgkl_pairhmm_omp.so to /tmp/libgkl_pairhmm_omp8179002917276126697.so; 11:35:41.847 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:35:41.848 INFO IntelPairHmm - Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:6460,Performance,multi-thread,multi-threaded,6460,ence sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to /tmp/libgkl_utils9151568277466250840.so; 11:35:41.777 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:35:41.802 DEBUG NativeLibraryLoader - Extracting libgkl_pairhmm_omp.so to /tmp/libgkl_pairhmm_omp8179002917276126697.so; 11:35:41.847 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:35:41.848 INFO IntelPairHmm - Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:7070,Performance,cache,cache,7070,bgkl_pairhmm_omp.so; 11:35:41.802 DEBUG NativeLibraryLoader - Extracting libgkl_pairhmm_omp.so to /tmp/libgkl_pairhmm_omp8179002917276126697.so; 11:35:41.847 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:35:41.848 INFO IntelPairHmm - Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recove,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:7151,Performance,cache,cache,7151,airhmm_omp.so to /tmp/libgkl_pairhmm_omp8179002917276126697.so; 11:35:41.847 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:35:41.848 INFO IntelPairHmm - Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:7333,Performance,cache,cache,7333,Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBU,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:7415,Performance,cache,cache,7415,41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:7501,Performance,cache,cache,7501,mplementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:7587,Performance,cache,cache,7587,ode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:7675,Performance,cache,cache,7675,quent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly regi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:8220,Performance,cache,cache,8220,ng assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tail,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:9348,Performance,cache,cache,9348, Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:11212,Performance,cache,cache,11212, Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:12354,Performance,cache,cache,12354, Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:12444,Performance,cache,cache,12444,603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:12535,Performance,cache,cache,12535,2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:13197,Performance,cache,cache,13197,11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:13287,Performance,cache,cache,13287,numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:14963,Performance,cache,cache,14963,ion at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:15263,Performance,cache,cache,15263,eads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:15608,Performance,cache,cache,15608,tect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:15701,Performance,cache,cache,15701, chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotyp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16950,Performance,cache,cache,16950,:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.44,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:1354,Safety,detect,detect,1354,"unning:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar Mutect2 --input unique.bam --output unique.mutect2.g.vcf --reference chrM.fa --use-jdk-inflater true --use-jdk-deflater true --mitochondria-mode --emit-ref-confidence GVCF --disable-read-filter ReadLengthReadFilter --disable-read-filter NotDuplicateReadFilter --disable-read-filter MappingQualityAvailableReadFilter --read-filter null --dont-use-soft-clipped-bases true --max-reads-per-alignment-start 0 --min-base-quality-score 0 --soft-clip-low-quality-ends false --verbosity DEBUG; Picked up _JAVA_OPTIONS: -Xmx128g; 11:35:39.890 WARN GATKReadFilterPluginDescriptor - Values were supplied for (ReadLengthReadFilter) that is also disabled; May 31, 2021 11:35:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:35:40.175 INFO Mutect2 - ------------------------------------------------------------; 11:35:40.176 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:35:40.176 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:35:40.177 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_144-b01; 11:35:40.177 INFO Mutect2 - Start Date/Time: May 31, 2021 11:35:39 AM EDT; 11:35:40.177 INFO Mutect2 - ------------------------------------------------------------; 11:35:40.177 INFO Mutect2 - ------------------------------------------------------------; 11:35:40.178 INFO Mutect2 - HTSJDK Version: 2.24.0; 11:35:40.179 INFO Mutect2 - Picard Version: 2.25.0; 11:35:40.179 INFO Mutect2 - Built for Spark Version: 2.4.5; 11:35:40.187 INFO Mutect2 - HTSJDK Defaults.BUFFER_SIZE : 131072; 11:35:40.188 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:35:40.188 INFO Mutect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:8068,Safety,Recover,Recovered,8068,he miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:8141,Safety,Recover,Recovered,8141,he - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:9192,Safety,Recover,Recovered,9192,DoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:9267,Safety,Recover,Recovered,9267, Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:10072,Safety,Recover,Recovered,10072,rocessing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dan,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:10145,Safety,Recover,Recovered,10145,; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:10910,Safety,Recover,Recovered,10910,eads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:10983,Safety,Recover,Recovered,10983,rM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:11056,Safety,Recover,Recovered,11056,ngGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:11131,Safety,Recover,Recovered,11131,Graph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false num,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:13043,Safety,Recover,Recovered,13043,6:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:13117,Safety,Recover,Recovered,13117,tive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 7,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:14167,Safety,Recover,Recovered,14167,ToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:14242,Safety,Recover,Recovered,14242,5.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:15455,Safety,Recover,Recovered,15455,e coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:15528,Safety,Recover,Recovered,15528,t 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16272,Safety,Recover,Recovered,16272,36 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16345,Safety,Recover,Recovered,16345,embly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 danglin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16419,Safety,Recover,Recovered,16419,97 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:16494,Safety,Recover,Recovered,16494, DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:17251,Safety,Recover,Recovered,17251,readingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing as,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:17326,Safety,Recover,Recovered,17326,adingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.48,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:19103,Safety,Recover,Recovered,19103,; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.489 DEBUG Mutect2 - Processing assembly region at chrM:11085-11384 isActive: false numReads: 0; 12:05:51.501 DEBUG Mutect2 - Processing assembly region at chrM:11385-11684 isActive: false numReads: 0; 12:05:51.513 DEBUG Mutect2 - Processing assembly region at chrM:11685-11984 isActive: false numReads: 0; 12:05:51.526 DEBUG Mutect2 - Processing assembly region at chrM:11985-12284 isActive: false numReads: 0; 12:06:02.022 DEBUG Mutect2 - Processing assembly region at chrM:12285-12584 isActive: false numReads: 0; 12:06:03.941 DEBUG Mutect2 - Processing assembly region at chrM:12585-12729 isActive: false numReads: 44205; 12:06:04.330 DEBUG Mutect2 - Processing assembly region at chrM:12730-13020 isActive: true numReads: 88386; 12:06:10.995 DEBUG ReadThreadingGraph - Recovered 11 of 15 dangling tails; 12:06:11.087 DEBUG ReadThreadingGraph - Recovered 7 of 36 dangling heads; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG Rea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:19178,Safety,Recover,Recovered,19178,84 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.489 DEBUG Mutect2 - Processing assembly region at chrM:11085-11384 isActive: false numReads: 0; 12:05:51.501 DEBUG Mutect2 - Processing assembly region at chrM:11385-11684 isActive: false numReads: 0; 12:05:51.513 DEBUG Mutect2 - Processing assembly region at chrM:11685-11984 isActive: false numReads: 0; 12:05:51.526 DEBUG Mutect2 - Processing assembly region at chrM:11985-12284 isActive: false numReads: 0; 12:06:02.022 DEBUG Mutect2 - Processing assembly region at chrM:12285-12584 isActive: false numReads: 0; 12:06:03.941 DEBUG Mutect2 - Processing assembly region at chrM:12585-12729 isActive: false numReads: 44205; 12:06:04.330 DEBUG Mutect2 - Processing assembly region at chrM:12730-13020 isActive: true numReads: 88386; 12:06:10.995 DEBUG ReadThreadingGraph - Recovered 11 of 15 dangling tails; 12:06:11.087 DEBUG ReadThreadingGraph - Recovered 7 of 36 dangling heads; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG Re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20128,Safety,Recover,Recovered,20128,ls; 12:06:11.087 DEBUG ReadThreadingGraph - Recovered 7 of 36 dangling heads; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20203,Safety,Recover,Recovered,20203,s; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20767,Safety,Recover,Recovered,20767, DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:20842,Safety,Recover,Recovered,20842,alse numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at ch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:22201,Safety,Recover,Recovered,22201,:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:22274,Safety,Recover,Recovered,22274,tect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:22347,Safety,Recover,Recovered,22347,mReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignment,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:22420,Safety,Recover,Recovered,22420, chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:262,Testability,log,log,262,"Dear GATK4 deveopers,. Thanks for developing this software. I'm trying to call SNVs from a BAM file using Mutect2. The BAM file is fairly small (~30MB), containing ~400,000 75bp paired-end reads mapped to mouse mitochondrial genome. Below is the command and the log:; ```; Using GATK jar /home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar Mutect2 --input unique.bam --output unique.mutect2.g.vcf --reference chrM.fa --use-jdk-inflater true --use-jdk-deflater true --mitochondria-mode --emit-ref-confidence GVCF --disable-read-filter ReadLengthReadFilter --disable-read-filter NotDuplicateReadFilter --disable-read-filter MappingQualityAvailableReadFilter --read-filter null --dont-use-soft-clipped-bases true --max-reads-per-alignment-start 0 --min-base-quality-score 0 --soft-clip-low-quality-ends false --verbosity DEBUG; Picked up _JAVA_OPTIONS: -Xmx128g; 11:35:39.890 WARN GATKReadFilterPluginDescriptor - Values were supplied for (ReadLengthReadFilter) that is also disabled; May 31, 2021 11:35:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:35:40.175 INFO Mutect2 - ------------------------------------------------------------; 11:35:40.176 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:35:40.176 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:35:40.177 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_144-b01; 11:35:40.177 INFO Mutect2 - Start Date/Time: May 31, 2021 11:35:39 AM EDT; 11:35:40.177 INFO Mutect2 - ------------------------------------------------------------; 11:35:40.177 INFO Mutect2 - -----------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7281:24159,Testability,log,log,24159,"tive Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:13:58.943 INFO ProgressMeter - chrM:15445 38.3 63 1.6; 12:13:58.946 INFO ProgressMeter - Traversal complete. Processed 63 total regions in 38.3 minutes.; 12:13:59.105 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.7153035790000002; 12:13:59.110 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 1084.6708644550001; 12:13:59.114 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 54.84 sec; 12:13:59.118 INFO Mutect2 - Shutting down engine; [May 31, 2021 12:13:59 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 38.32 minutes.; Runtime.totalMemory()=18715508736; ```. From the log, we see that Mutect2 finished in 40 minutes. In the meanwhile the `g.vcf`, `g.vcf.idx` and `g.vcf.stats` files are generated and contain non-empty contents. However, the program keeps running for hours and still has not finished. Therefore I wonder if Mutect2 is stuck with some post-processing that is less documented. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281
https://github.com/broadinstitute/gatk/issues/7285:903,Availability,down,downsampling,903,"This is a follow up from a discussion from the GATK Office Hours meeting. The user has not found that the --dont-use-soft-clip-bases parameter is the culprit of this difference. The user is sending in a bug report. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078111952-The-depth-of-SNV-InDels-calculated-by-Mutect2-in-GATK4-2-0-0-is-much-lower-than-real-sequencing-depth-why-](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078111952-The-depth-of-SNV-InDels-calculated-by-Mutect2-in-GATK4-2-0-0-is-much-lower-than-real-sequencing-depth-why-). \--. Hi GATK team,. GATK version: 4.2.0.0. Sample: Target region sequencing, human cancer, everage depth 1000X. I'm using Mutect2 in GATK4.2.0.0 to call somatic SNV/InDels. What confused me is that I found the depth of each location emitted from ""AD"" field in vcf is much lower than the real depth. I think I have disabled downsampling by set ""--max-reads-per-alignment-start"" to 0. The command line I used is as follow:. gatk Mutect2-R reference.fa-I tumor.bam--panel-of-normals pon.vcf.gz -L target.bed-O sample.snvIndels.vcf--callable-depth 30--f1r2-tar-gz sample.f1r2.tar.gz--min-base-quality-score 25 --max-reads-per-alignment-start 0 --minimum-allele-fraction 0.002--dont-use-soft-clipped-bases--force-active--mitochondria-mode--enable-all-annotations. For example, a mutated point information in vcf called by GATK4.2.0.0-Mutect2 is:. 1 24868045 . A G . . AC=1;AF=0.500;AN=2;AS\_MQ=60.00;AS\_SB\_TABLE=51,50|46,23;AS\_UNIQ\_ALT\_READ\_COUNT=69;BaseQRankSum=0.561;ClippingRankSum=-1.473;DP=179;ECNT=2;FS=13.849;LikelihoodRankSum=-0.392;MBQ=37,37;MFRL=236,239;MMQ=60,60;MPOS=44;MQ=60.00;MQ0=0;MQRankSum=0.000;NCC=0;NCount=0;OCM=0;PON;POPAF=7.30;REF\_BASES=GCTCAGCAGAACAGACCCAGA;ReadPosRankSum=1.335;SOR=1.545;Samples=HD786\_4-1;TLOD=230.09 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:**101,69**:0.408:170:54,30:45,39:51,50,46,23. But the information of the same point in vcf called by GATK4.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7285
https://github.com/broadinstitute/gatk/issues/7285:2598,Availability,down,downsampling,2598,"on emitted from ""AD"" field in vcf is much lower than the real depth. I think I have disabled downsampling by set ""--max-reads-per-alignment-start"" to 0. The command line I used is as follow:. gatk Mutect2-R reference.fa-I tumor.bam--panel-of-normals pon.vcf.gz -L target.bed-O sample.snvIndels.vcf--callable-depth 30--f1r2-tar-gz sample.f1r2.tar.gz--min-base-quality-score 25 --max-reads-per-alignment-start 0 --minimum-allele-fraction 0.002--dont-use-soft-clipped-bases--force-active--mitochondria-mode--enable-all-annotations. For example, a mutated point information in vcf called by GATK4.2.0.0-Mutect2 is:. 1 24868045 . A G . . AC=1;AF=0.500;AN=2;AS\_MQ=60.00;AS\_SB\_TABLE=51,50|46,23;AS\_UNIQ\_ALT\_READ\_COUNT=69;BaseQRankSum=0.561;ClippingRankSum=-1.473;DP=179;ECNT=2;FS=13.849;LikelihoodRankSum=-0.392;MBQ=37,37;MFRL=236,239;MMQ=60,60;MPOS=44;MQ=60.00;MQ0=0;MQRankSum=0.000;NCC=0;NCount=0;OCM=0;PON;POPAF=7.30;REF\_BASES=GCTCAGCAGAACAGACCCAGA;ReadPosRankSum=1.335;SOR=1.545;Samples=HD786\_4-1;TLOD=230.09 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:**101,69**:0.408:170:54,30:45,39:51,50,46,23. But the information of the same point in vcf called by GATK4.1.9.0-Mutect2 **using the same command** is:. 1 24868045 . A G . . AC=1;AF=0.500;AN=2;AS\_MQ=60.00;AS\_SB\_TABLE=299,290|183,155;AS\_UNIQ\_ALT\_READ\_COUNT=328;BaseQRankSum=1.715;ClippingRankSum=-0.613;DP=934;ECNT=2;FS=1.802;LikelihoodRankSum=0.052;MBQ=20,20;MFRL=153,145;MMQ=60,60;MPOS=39;MQ=60.00;MQ0=0;MQRankSum=0.000;NCC=0;NCount=0;OCM=0;PON;POPAF=7.30;REF\_BASES=GCTCAGCAGAACAGACCCAGA;ReadPosRankSum=3.636;SOR=0.837;Samples=HD786\_4-1;TLOD=779.29 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:**589,338**:0.371:927:290,167:293,171:299,290,183,155. This ""580+338"" is exactly the true depth. Is there any other downsampling or filter in GATK4.2.0.0-Mutect2 but not in GATK4.1.9.0-Mutect2?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/159302'>Zendesk ticket #159302</a>)<br>gz#159302</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7285
https://github.com/broadinstitute/gatk/issues/7287:656,Modifiability,inherit,inherited,656,"My lab has a variety of custom walkers. Many subclass MultiVariantWalkerGroupedOnStart, which is a useful iteration pattern. We tend to scatter/gather on our cluster, where each job is given an interval set. When doing this, handling variant spanning those borders is critical. We just had an issue around this, which stems from MultiVariantWalkerGroupedOnStart and the fact that ignoreIntervalsOutsideStart defaults to false. For our usage, we almost never want this to be true, and it's a really subtle problem if the user doesnt remember to set this. So my question is: is there a best-practice way for subclasses to override / remove or set default on inherited arguments? Granted, individual walkers could simply change the value of ignoreIntervalsOutsideStart during the init phase, but I dont like that solution since it basically leaves an useless/ignored argument. . thanks in advance for any ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7287
https://github.com/broadinstitute/gatk/issues/7287:711,Usability,simpl,simply,711,"My lab has a variety of custom walkers. Many subclass MultiVariantWalkerGroupedOnStart, which is a useful iteration pattern. We tend to scatter/gather on our cluster, where each job is given an interval set. When doing this, handling variant spanning those borders is critical. We just had an issue around this, which stems from MultiVariantWalkerGroupedOnStart and the fact that ignoreIntervalsOutsideStart defaults to false. For our usage, we almost never want this to be true, and it's a really subtle problem if the user doesnt remember to set this. So my question is: is there a best-practice way for subclasses to override / remove or set default on inherited arguments? Granted, individual walkers could simply change the value of ignoreIntervalsOutsideStart during the init phase, but I dont like that solution since it basically leaves an useless/ignored argument. . thanks in advance for any ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7287
https://github.com/broadinstitute/gatk/pull/7288:179,Availability,down,down,179,"Two primary sets of changes. 1. split out the combined ""CREATE TABLE AS... SELECT... join PET + VET"" into 3 separate items. CREATE, INSERT vet, INSERT pet; 2. To keep our shuffle down we are not joining in sample_id at query time, since we already have the id -> name mapping in ExtractCohort... we just needed to use it (should reduce costs slightly also). Testing. Tested on the GVS tieout set. As expected the only difference in the cohort extract tables is that we are no longer seeing mis-joined VET information at `*` sites (which is a nice side benefits). Otherwise tables tie out exactly in SQL. In addition, I ran a full GIAB tieout before and after and the results are identical",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7288
https://github.com/broadinstitute/gatk/pull/7288:329,Energy Efficiency,reduce,reduce,329,"Two primary sets of changes. 1. split out the combined ""CREATE TABLE AS... SELECT... join PET + VET"" into 3 separate items. CREATE, INSERT vet, INSERT pet; 2. To keep our shuffle down we are not joining in sample_id at query time, since we already have the id -> name mapping in ExtractCohort... we just needed to use it (should reduce costs slightly also). Testing. Tested on the GVS tieout set. As expected the only difference in the cohort extract tables is that we are no longer seeing mis-joined VET information at `*` sites (which is a nice side benefits). Otherwise tables tie out exactly in SQL. In addition, I ran a full GIAB tieout before and after and the results are identical",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7288
https://github.com/broadinstitute/gatk/pull/7288:358,Testability,Test,Testing,358,"Two primary sets of changes. 1. split out the combined ""CREATE TABLE AS... SELECT... join PET + VET"" into 3 separate items. CREATE, INSERT vet, INSERT pet; 2. To keep our shuffle down we are not joining in sample_id at query time, since we already have the id -> name mapping in ExtractCohort... we just needed to use it (should reduce costs slightly also). Testing. Tested on the GVS tieout set. As expected the only difference in the cohort extract tables is that we are no longer seeing mis-joined VET information at `*` sites (which is a nice side benefits). Otherwise tables tie out exactly in SQL. In addition, I ran a full GIAB tieout before and after and the results are identical",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7288
https://github.com/broadinstitute/gatk/pull/7288:367,Testability,Test,Tested,367,"Two primary sets of changes. 1. split out the combined ""CREATE TABLE AS... SELECT... join PET + VET"" into 3 separate items. CREATE, INSERT vet, INSERT pet; 2. To keep our shuffle down we are not joining in sample_id at query time, since we already have the id -> name mapping in ExtractCohort... we just needed to use it (should reduce costs slightly also). Testing. Tested on the GVS tieout set. As expected the only difference in the cohort extract tables is that we are no longer seeing mis-joined VET information at `*` sites (which is a nice side benefits). Otherwise tables tie out exactly in SQL. In addition, I ran a full GIAB tieout before and after and the results are identical",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7288
https://github.com/broadinstitute/gatk/issues/7289:169,Deployability,release,release,169,## Bug Report. ### Affected tool(s) or class(es). HC java.lang.IllegalStateException: Padded span must contain active span. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description. ```; Runtime.totalMemory()=2494038016; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289
https://github.com/broadinstitute/gatk/issues/7289:2122,Deployability,update,updates,2122,must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. see attachement. ```; gatk HaplotypeCaller -I jeter.bam -L jeter.bed --seconds-between-progress-updates 600 --minimum-mapping-quality 30 -R hs37d5_all_chr.fasta -O jeter.vcf.gz; ```. #### Expected behavior. no exception. #### Actual behavior. ```; java.lang.IllegalStateException: Padded span must contain active span; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289
https://github.com/broadinstitute/gatk/issues/7289:931,Performance,load,loadNextAssemblyRegion,931,## Bug Report. ### Affected tool(s) or class(es). HC java.lang.IllegalStateException: Padded span must contain active span. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description. ```; Runtime.totalMemory()=2494038016; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289
https://github.com/broadinstitute/gatk/issues/7289:420,Security,validat,validate,420,## Bug Report. ### Affected tool(s) or class(es). HC java.lang.IllegalStateException: Padded span must contain active span. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description. ```; Runtime.totalMemory()=2494038016; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289
https://github.com/broadinstitute/gatk/issues/7289:239,Testability,test,test,239,## Bug Report. ### Affected tool(s) or class(es). HC java.lang.IllegalStateException: Padded span must contain active span. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description. ```; Runtime.totalMemory()=2494038016; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289
https://github.com/broadinstitute/gatk/issues/7290:934,Modifiability,extend,extend,934,"If the assembler has not discovered an allele it could be simply that the samples does not have that allele but it could also be the case that it failed to assemble due to some edge case. . If the former it makes sense to assign depth 0 but for the latter that would be misleading and a ""I don't know"" output (""."") would be more appropriate. For example, what about those HOM-REF sites that end up with PL=0,0,0 because the reference-confidence-model found reads that don't support the reference sequence yet the assembly did not produce a concrete alternative. Fast forward and the same sample is joint-genotyped with in a cohort with other samples for which HC assembled the alternative haplotype/allele (correctly). Then we will assign AD=0 to those alternative alleles in the original (no-quite)-hom-ref sample. . I think the better answer would be AD=""."" in light of the lack of confidence on the hom-ref call. . Would this even extend to cases where we are confident on hom-ref? Unless any single read is exactly the reference at that site there is a potential for that allele to have gone unnoticed. . Would make sense that if someone wants to know the AD for every alt. allele at a sample where some weren't discovered in, he must re-run HC in GGA mode with the full list of alt alleles?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7290
https://github.com/broadinstitute/gatk/issues/7290:58,Usability,simpl,simply,58,"If the assembler has not discovered an allele it could be simply that the samples does not have that allele but it could also be the case that it failed to assemble due to some edge case. . If the former it makes sense to assign depth 0 but for the latter that would be misleading and a ""I don't know"" output (""."") would be more appropriate. For example, what about those HOM-REF sites that end up with PL=0,0,0 because the reference-confidence-model found reads that don't support the reference sequence yet the assembly did not produce a concrete alternative. Fast forward and the same sample is joint-genotyped with in a cohort with other samples for which HC assembled the alternative haplotype/allele (correctly). Then we will assign AD=0 to those alternative alleles in the original (no-quite)-hom-ref sample. . I think the better answer would be AD=""."" in light of the lack of confidence on the hom-ref call. . Would this even extend to cases where we are confident on hom-ref? Unless any single read is exactly the reference at that site there is a potential for that allele to have gone unnoticed. . Would make sense that if someone wants to know the AD for every alt. allele at a sample where some weren't discovered in, he must re-run HC in GGA mode with the full list of alt alleles?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7290
https://github.com/broadinstitute/gatk/issues/7292:301,Availability,down,downloaded,301,"Hi, GATK contributors. Thank you for the great software!. I had looked for the answer several days. . Question:; Several samples are new sequenced and `g.vcf` was called using `GATK` with `-ERC GVCF` models.; I want to add this new sample variants into the existing VCF file (no g.vcf avaliable) that downloaded from other researchers.; I have tested some commands like `CombineGVCFs`, `MergeVcfs` but all failed.; Could you give some advices?; Any respone would be helpful. Thanks ~",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7292
https://github.com/broadinstitute/gatk/issues/7292:344,Testability,test,tested,344,"Hi, GATK contributors. Thank you for the great software!. I had looked for the answer several days. . Question:; Several samples are new sequenced and `g.vcf` was called using `GATK` with `-ERC GVCF` models.; I want to add this new sample variants into the existing VCF file (no g.vcf avaliable) that downloaded from other researchers.; I have tested some commands like `CombineGVCFs`, `MergeVcfs` but all failed.; Could you give some advices?; Any respone would be helpful. Thanks ~",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7292
https://github.com/broadinstitute/gatk/pull/7293:1111,Availability,error,error,1111,"There are params in Extract Cohort that can be tightened up. Extract Tool has two params that are not used by Extract Features and are _already_ in Extract Cohort. The filter_set_name is used in the BQ filtering tables and looks like we can set it to be completely required for any type of filtering. There are 3 BQ filter tables -- 2 are needed for filtering (no matter what?) and 1 (tranches) is needed for thresholding and sensitivity calculations?. Genotype level filtering is true by default, but this doesn't seem like it should effect things after this cleanup. Though technically it should only be true if a filter_set_name has been specified. I will add another comment in the body of the code, but I would like to add this safety gate explicitly. Disable gnarly doesn't need to be a passed in param---so we'll rip it out for now. SNP and INDEL truth sensitivity and SNP and INDEL Lod scores are cumbersome to have to worry about passing in, but I dont see a better alternative. Should there be additional validation on these (where if they are specified, but no filter_set_name is, then they throw an error?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7293
https://github.com/broadinstitute/gatk/pull/7293:733,Safety,safe,safety,733,"There are params in Extract Cohort that can be tightened up. Extract Tool has two params that are not used by Extract Features and are _already_ in Extract Cohort. The filter_set_name is used in the BQ filtering tables and looks like we can set it to be completely required for any type of filtering. There are 3 BQ filter tables -- 2 are needed for filtering (no matter what?) and 1 (tranches) is needed for thresholding and sensitivity calculations?. Genotype level filtering is true by default, but this doesn't seem like it should effect things after this cleanup. Though technically it should only be true if a filter_set_name has been specified. I will add another comment in the body of the code, but I would like to add this safety gate explicitly. Disable gnarly doesn't need to be a passed in param---so we'll rip it out for now. SNP and INDEL truth sensitivity and SNP and INDEL Lod scores are cumbersome to have to worry about passing in, but I dont see a better alternative. Should there be additional validation on these (where if they are specified, but no filter_set_name is, then they throw an error?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7293
https://github.com/broadinstitute/gatk/pull/7293:1015,Security,validat,validation,1015,"There are params in Extract Cohort that can be tightened up. Extract Tool has two params that are not used by Extract Features and are _already_ in Extract Cohort. The filter_set_name is used in the BQ filtering tables and looks like we can set it to be completely required for any type of filtering. There are 3 BQ filter tables -- 2 are needed for filtering (no matter what?) and 1 (tranches) is needed for thresholding and sensitivity calculations?. Genotype level filtering is true by default, but this doesn't seem like it should effect things after this cleanup. Though technically it should only be true if a filter_set_name has been specified. I will add another comment in the body of the code, but I would like to add this safety gate explicitly. Disable gnarly doesn't need to be a passed in param---so we'll rip it out for now. SNP and INDEL truth sensitivity and SNP and INDEL Lod scores are cumbersome to have to worry about passing in, but I dont see a better alternative. Should there be additional validation on these (where if they are specified, but no filter_set_name is, then they throw an error?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7293
https://github.com/broadinstitute/gatk/issues/7294:435,Availability,error,error,435,"## Feature request. ### Tool involved - GATK cnv_somatic_pair_workflow version 4.2.0.0; Link to main wdl - https://dockstore.org/workflows/github.com/broadinstitute/gatk/cnv_somatic_pair_workflow:4.2.0.0?tab=files. ### Description; Requesting for addition of a new feature called the `IndexFeatureFile` to be added as initial step in the `Funcotate_Segment` task of the `cnv_somatic_pair_workflow` wdl . **Detailed description:** ; An error was encountered while running the CNVSomaticPairWorkflow: `A USER ERROR has occurred: Input /cromwell_root/fc-a21facc8-da03-4987-bb5b-dfadbfda2747/a923baec-ddd9-429a-b046-1f03c5ebda64/CNVSomaticPairWorkflow/42ba0311-ba93-4fdd-9b5e-bd7348c0ad42/call-CallCopyRatioSegmentsTumor/AMP-18-003-TIS.called.seg must support random access to enable traversal by intervals. If it's a file, please index it using the bundled tool IndexFeatureFile`. - The FuncotateSegments task is asking an index file for the seg file, which is unusual to create an index for seg; - The same command from the cnv wdl was tested on-prem (Broad server using ish) by transferring all required files on-prem, this was done to replicate the error and find a solution without wasting compute money or resources on Terra; - To fix the issue, we initially ran Indexfilefeature tool (on-prem) for creating index file for the seg file, using the following command. ; `gatk IndexFeatureFile -I seg file`; - And then ran the main command `./gatk --java-options -Xmx2000m FuncotateSegments --data-sources-path data_sources_directory --ref-version hg19 --output-file-format SEG -R fasta_file_path --segments seg_file_path -O output_file_path/{basename}.seg.funcotated.tsv -L interval_list_path --transcript-selection-mode CANONICAL`. After this was complete, annotated file was generated correctly. In conclusion - the error identified is that `Indexfilefeature` tool had to be run as the first step. Requesting this change to be incorporated into `cnv_somatic_funcotate_seg_workflow.wdl`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7294
https://github.com/broadinstitute/gatk/issues/7294:508,Availability,ERROR,ERROR,508,"## Feature request. ### Tool involved - GATK cnv_somatic_pair_workflow version 4.2.0.0; Link to main wdl - https://dockstore.org/workflows/github.com/broadinstitute/gatk/cnv_somatic_pair_workflow:4.2.0.0?tab=files. ### Description; Requesting for addition of a new feature called the `IndexFeatureFile` to be added as initial step in the `Funcotate_Segment` task of the `cnv_somatic_pair_workflow` wdl . **Detailed description:** ; An error was encountered while running the CNVSomaticPairWorkflow: `A USER ERROR has occurred: Input /cromwell_root/fc-a21facc8-da03-4987-bb5b-dfadbfda2747/a923baec-ddd9-429a-b046-1f03c5ebda64/CNVSomaticPairWorkflow/42ba0311-ba93-4fdd-9b5e-bd7348c0ad42/call-CallCopyRatioSegmentsTumor/AMP-18-003-TIS.called.seg must support random access to enable traversal by intervals. If it's a file, please index it using the bundled tool IndexFeatureFile`. - The FuncotateSegments task is asking an index file for the seg file, which is unusual to create an index for seg; - The same command from the cnv wdl was tested on-prem (Broad server using ish) by transferring all required files on-prem, this was done to replicate the error and find a solution without wasting compute money or resources on Terra; - To fix the issue, we initially ran Indexfilefeature tool (on-prem) for creating index file for the seg file, using the following command. ; `gatk IndexFeatureFile -I seg file`; - And then ran the main command `./gatk --java-options -Xmx2000m FuncotateSegments --data-sources-path data_sources_directory --ref-version hg19 --output-file-format SEG -R fasta_file_path --segments seg_file_path -O output_file_path/{basename}.seg.funcotated.tsv -L interval_list_path --transcript-selection-mode CANONICAL`. After this was complete, annotated file was generated correctly. In conclusion - the error identified is that `Indexfilefeature` tool had to be run as the first step. Requesting this change to be incorporated into `cnv_somatic_funcotate_seg_workflow.wdl`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7294
https://github.com/broadinstitute/gatk/issues/7294:1151,Availability,error,error,1151,"## Feature request. ### Tool involved - GATK cnv_somatic_pair_workflow version 4.2.0.0; Link to main wdl - https://dockstore.org/workflows/github.com/broadinstitute/gatk/cnv_somatic_pair_workflow:4.2.0.0?tab=files. ### Description; Requesting for addition of a new feature called the `IndexFeatureFile` to be added as initial step in the `Funcotate_Segment` task of the `cnv_somatic_pair_workflow` wdl . **Detailed description:** ; An error was encountered while running the CNVSomaticPairWorkflow: `A USER ERROR has occurred: Input /cromwell_root/fc-a21facc8-da03-4987-bb5b-dfadbfda2747/a923baec-ddd9-429a-b046-1f03c5ebda64/CNVSomaticPairWorkflow/42ba0311-ba93-4fdd-9b5e-bd7348c0ad42/call-CallCopyRatioSegmentsTumor/AMP-18-003-TIS.called.seg must support random access to enable traversal by intervals. If it's a file, please index it using the bundled tool IndexFeatureFile`. - The FuncotateSegments task is asking an index file for the seg file, which is unusual to create an index for seg; - The same command from the cnv wdl was tested on-prem (Broad server using ish) by transferring all required files on-prem, this was done to replicate the error and find a solution without wasting compute money or resources on Terra; - To fix the issue, we initially ran Indexfilefeature tool (on-prem) for creating index file for the seg file, using the following command. ; `gatk IndexFeatureFile -I seg file`; - And then ran the main command `./gatk --java-options -Xmx2000m FuncotateSegments --data-sources-path data_sources_directory --ref-version hg19 --output-file-format SEG -R fasta_file_path --segments seg_file_path -O output_file_path/{basename}.seg.funcotated.tsv -L interval_list_path --transcript-selection-mode CANONICAL`. After this was complete, annotated file was generated correctly. In conclusion - the error identified is that `Indexfilefeature` tool had to be run as the first step. Requesting this change to be incorporated into `cnv_somatic_funcotate_seg_workflow.wdl`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7294
https://github.com/broadinstitute/gatk/issues/7294:1820,Availability,error,error,1820,"## Feature request. ### Tool involved - GATK cnv_somatic_pair_workflow version 4.2.0.0; Link to main wdl - https://dockstore.org/workflows/github.com/broadinstitute/gatk/cnv_somatic_pair_workflow:4.2.0.0?tab=files. ### Description; Requesting for addition of a new feature called the `IndexFeatureFile` to be added as initial step in the `Funcotate_Segment` task of the `cnv_somatic_pair_workflow` wdl . **Detailed description:** ; An error was encountered while running the CNVSomaticPairWorkflow: `A USER ERROR has occurred: Input /cromwell_root/fc-a21facc8-da03-4987-bb5b-dfadbfda2747/a923baec-ddd9-429a-b046-1f03c5ebda64/CNVSomaticPairWorkflow/42ba0311-ba93-4fdd-9b5e-bd7348c0ad42/call-CallCopyRatioSegmentsTumor/AMP-18-003-TIS.called.seg must support random access to enable traversal by intervals. If it's a file, please index it using the bundled tool IndexFeatureFile`. - The FuncotateSegments task is asking an index file for the seg file, which is unusual to create an index for seg; - The same command from the cnv wdl was tested on-prem (Broad server using ish) by transferring all required files on-prem, this was done to replicate the error and find a solution without wasting compute money or resources on Terra; - To fix the issue, we initially ran Indexfilefeature tool (on-prem) for creating index file for the seg file, using the following command. ; `gatk IndexFeatureFile -I seg file`; - And then ran the main command `./gatk --java-options -Xmx2000m FuncotateSegments --data-sources-path data_sources_directory --ref-version hg19 --output-file-format SEG -R fasta_file_path --segments seg_file_path -O output_file_path/{basename}.seg.funcotated.tsv -L interval_list_path --transcript-selection-mode CANONICAL`. After this was complete, annotated file was generated correctly. In conclusion - the error identified is that `Indexfilefeature` tool had to be run as the first step. Requesting this change to be incorporated into `cnv_somatic_funcotate_seg_workflow.wdl`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7294
https://github.com/broadinstitute/gatk/issues/7294:764,Security,access,access,764,"## Feature request. ### Tool involved - GATK cnv_somatic_pair_workflow version 4.2.0.0; Link to main wdl - https://dockstore.org/workflows/github.com/broadinstitute/gatk/cnv_somatic_pair_workflow:4.2.0.0?tab=files. ### Description; Requesting for addition of a new feature called the `IndexFeatureFile` to be added as initial step in the `Funcotate_Segment` task of the `cnv_somatic_pair_workflow` wdl . **Detailed description:** ; An error was encountered while running the CNVSomaticPairWorkflow: `A USER ERROR has occurred: Input /cromwell_root/fc-a21facc8-da03-4987-bb5b-dfadbfda2747/a923baec-ddd9-429a-b046-1f03c5ebda64/CNVSomaticPairWorkflow/42ba0311-ba93-4fdd-9b5e-bd7348c0ad42/call-CallCopyRatioSegmentsTumor/AMP-18-003-TIS.called.seg must support random access to enable traversal by intervals. If it's a file, please index it using the bundled tool IndexFeatureFile`. - The FuncotateSegments task is asking an index file for the seg file, which is unusual to create an index for seg; - The same command from the cnv wdl was tested on-prem (Broad server using ish) by transferring all required files on-prem, this was done to replicate the error and find a solution without wasting compute money or resources on Terra; - To fix the issue, we initially ran Indexfilefeature tool (on-prem) for creating index file for the seg file, using the following command. ; `gatk IndexFeatureFile -I seg file`; - And then ran the main command `./gatk --java-options -Xmx2000m FuncotateSegments --data-sources-path data_sources_directory --ref-version hg19 --output-file-format SEG -R fasta_file_path --segments seg_file_path -O output_file_path/{basename}.seg.funcotated.tsv -L interval_list_path --transcript-selection-mode CANONICAL`. After this was complete, annotated file was generated correctly. In conclusion - the error identified is that `Indexfilefeature` tool had to be run as the first step. Requesting this change to be incorporated into `cnv_somatic_funcotate_seg_workflow.wdl`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7294
https://github.com/broadinstitute/gatk/issues/7294:1036,Testability,test,tested,1036,"## Feature request. ### Tool involved - GATK cnv_somatic_pair_workflow version 4.2.0.0; Link to main wdl - https://dockstore.org/workflows/github.com/broadinstitute/gatk/cnv_somatic_pair_workflow:4.2.0.0?tab=files. ### Description; Requesting for addition of a new feature called the `IndexFeatureFile` to be added as initial step in the `Funcotate_Segment` task of the `cnv_somatic_pair_workflow` wdl . **Detailed description:** ; An error was encountered while running the CNVSomaticPairWorkflow: `A USER ERROR has occurred: Input /cromwell_root/fc-a21facc8-da03-4987-bb5b-dfadbfda2747/a923baec-ddd9-429a-b046-1f03c5ebda64/CNVSomaticPairWorkflow/42ba0311-ba93-4fdd-9b5e-bd7348c0ad42/call-CallCopyRatioSegmentsTumor/AMP-18-003-TIS.called.seg must support random access to enable traversal by intervals. If it's a file, please index it using the bundled tool IndexFeatureFile`. - The FuncotateSegments task is asking an index file for the seg file, which is unusual to create an index for seg; - The same command from the cnv wdl was tested on-prem (Broad server using ish) by transferring all required files on-prem, this was done to replicate the error and find a solution without wasting compute money or resources on Terra; - To fix the issue, we initially ran Indexfilefeature tool (on-prem) for creating index file for the seg file, using the following command. ; `gatk IndexFeatureFile -I seg file`; - And then ran the main command `./gatk --java-options -Xmx2000m FuncotateSegments --data-sources-path data_sources_directory --ref-version hg19 --output-file-format SEG -R fasta_file_path --segments seg_file_path -O output_file_path/{basename}.seg.funcotated.tsv -L interval_list_path --transcript-selection-mode CANONICAL`. After this was complete, annotated file was generated correctly. In conclusion - the error identified is that `Indexfilefeature` tool had to be run as the first step. Requesting this change to be incorporated into `cnv_somatic_funcotate_seg_workflow.wdl`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7294
https://github.com/broadinstitute/gatk/issues/7297:382,Availability,Error,Error-running-DenoiseReadCounts-on-,382,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:504,Availability,Error,Error-running-DenoiseReadCounts-on-,504,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:583,Availability,error,error,583,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:875,Availability,error,error,875,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:893,Availability,ERROR,ERROR,893,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:1153,Availability,error,error,1153,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:1167,Availability,ERROR,ERROR,1167,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:1479,Availability,error,error,1479,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:1374,Deployability,install,installed,1374,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:920,Performance,load,load,920,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:1194,Performance,load,load,1194,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7297:881,Testability,log,log,881,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297
https://github.com/broadinstitute/gatk/issues/7298:545,Availability,error,error,545,"It looks like this is a bug with 4.2.0.0 because the same Mutect2 output has no issues with FilterMutectCalls 4.1.6.0. . This request was created from a contribution made by Qihan Long on June 04, 2021 03:21 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0 ; ; b) Exact command used:. gatk FilterMutectCalls \\ ; ; \-R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa \\ ; ; \-V somatic\_mutation/Mutect2/test.vcf.gz \\ ; ; \-O somatic\_mutation/FilterMutectCalls/test.vcf.gz. c) Entire error log:. I used the ""--enable-all-annotations"" option within Mutect2 to get a vcf file with abundant information. However, the following FilterMutectCalls step seemed to be intolerant of some information within previous step's vcf file record. The intolerated record within vcf listed below:. chr1 6197724 . C CT,CTT,CTTT . . AC=1,1,1;AF=0.167,0.167,0.167;AN=6;AS\_BaseQRankSum=-6.431;AS\_MQ=60.00,60.00,60.00;AS\_MQRankSum=0.000;AS\_ReadPosRankSum=5.751;AS\_SB\_TABLE=42,880|3,164|3,32|0,14;**AS\_UNIQ\_ALT\_READ\_COUNT=167|35|14**;BQHIST=5,1,0,0,1,11,2,0,0,0,14,2,0,0,1,15,1,0,0,0,16,1,0,0,0,17,0,2,0,0,18,2,0,0,1,19,6,0,1,0,20,25,0,2,2,21,13,0,1,2,22,20,0,3,3,23,2,1,2,1,24,6,0,2,0,25,21,1,4,7,26,33,0,5,6,27,18,0,0,7,28,29,0,0,4,29,26,2,4,8,30,161,4,5,51,31,263,2,3,51,32,129,2,3,22,33,41,0,0,0,34,15,0,0,0,35,20,0,0,0,36,19,0,0,0,37,12,0,0,0,38,1,0,0,0,39,9,0,0,0,41,18,0,0,0,44,26,0,0,0;BaseQRankSum=-6.431;ClippingRankSum=-7.714;DP=1323;ECNT=1;FS=0.000;LikelihoodRankSum=-7.886;MBQ=31,30,26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:855,Availability,error,error,855,"It looks like this is a bug with 4.2.0.0 because the same Mutect2 output has no issues with FilterMutectCalls 4.1.6.0. . This request was created from a contribution made by Qihan Long on June 04, 2021 03:21 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0 ; ; b) Exact command used:. gatk FilterMutectCalls \\ ; ; \-R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa \\ ; ; \-V somatic\_mutation/Mutect2/test.vcf.gz \\ ; ; \-O somatic\_mutation/FilterMutectCalls/test.vcf.gz. c) Entire error log:. I used the ""--enable-all-annotations"" option within Mutect2 to get a vcf file with abundant information. However, the following FilterMutectCalls step seemed to be intolerant of some information within previous step's vcf file record. The intolerated record within vcf listed below:. chr1 6197724 . C CT,CTT,CTTT . . AC=1,1,1;AF=0.167,0.167,0.167;AN=6;AS\_BaseQRankSum=-6.431;AS\_MQ=60.00,60.00,60.00;AS\_MQRankSum=0.000;AS\_ReadPosRankSum=5.751;AS\_SB\_TABLE=42,880|3,164|3,32|0,14;**AS\_UNIQ\_ALT\_READ\_COUNT=167|35|14**;BQHIST=5,1,0,0,1,11,2,0,0,0,14,2,0,0,1,15,1,0,0,0,16,1,0,0,0,17,0,2,0,0,18,2,0,0,1,19,6,0,1,0,20,25,0,2,2,21,13,0,1,2,22,20,0,3,3,23,2,1,2,1,24,6,0,2,0,25,21,1,4,7,26,33,0,5,6,27,18,0,0,7,28,29,0,0,4,29,26,2,4,8,30,161,4,5,51,31,263,2,3,51,32,129,2,3,22,33,41,0,0,0,34,15,0,0,0,35,20,0,0,0,36,19,0,0,0,37,12,0,0,0,38,1,0,0,0,39,9,0,0,0,41,18,0,0,0,44,26,0,0,0;BaseQRankSum=-6.431;ClippingRankSum=-7.714;DP=1323;ECNT=1;FS=0.000;LikelihoodRankSum=-7.886;MBQ=31,30,26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:2363,Availability,error,error,2363,"**;BQHIST=5,1,0,0,1,11,2,0,0,0,14,2,0,0,1,15,1,0,0,0,16,1,0,0,0,17,0,2,0,0,18,2,0,0,1,19,6,0,1,0,20,25,0,2,2,21,13,0,1,2,22,20,0,3,3,23,2,1,2,1,24,6,0,2,0,25,21,1,4,7,26,33,0,5,6,27,18,0,0,7,28,29,0,0,4,29,26,2,4,8,30,161,4,5,51,31,263,2,3,51,32,129,2,3,22,33,41,0,0,0,34,15,0,0,0,35,20,0,0,0,36,19,0,0,0,37,12,0,0,0,38,1,0,0,0,39,9,0,0,0,41,18,0,0,0,44,26,0,0,0;BaseQRankSum=-6.431;ClippingRankSum=-7.714;DP=1323;ECNT=1;FS=0.000;LikelihoodRankSum=-7.886;MBQ=31,30,26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30.97;OCM=0;POPAF=6.00,6.00,6.00;REF\_BASES=GAACTTGCTTCTTTTTTTTGC;RPA=8,9,10,11;RU=T;ReadPosRankSum=5.751;SOR=1.152;STR;Samples=TCGA-NJ-A55R-01A-11R-A262-07;TLOD=284.47,51.82,3.50 GT:AD:AF:DP:F1R2:F2R1:SB 0/1/2/3:819,166,35,14:0.161,0.034,0.014:1034:365,76,17,4:440,87,17,8:16,803,6,209 0/0:103,1,0,0:0.017,8.250e-03,8.221e-03:104:50,1,0,0:52,0,0,0:26,77,0,1. The error log that FilterMutectCalls emited was listed below:. Using GATK jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar FilterMutectCalls -R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa -V somatic\_mutation/Mutect2/test.vcf.gz -O somatic\_mutation/FilterMutectCalls/test.vcf.gz ; ; 11:03:39.517 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jun 04, 2021 11:03:49 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:03:49.968 INFO FilterMutectCalls - --------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:5592,Availability,down,down,5592,"rMutectCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 11:03:49.970 INFO FilterMutectCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 11:03:49.970 INFO FilterMutectCalls - Deflater: IntelDeflater ; ; 11:03:49.971 INFO FilterMutectCalls - Inflater: IntelInflater ; ; 11:03:49.971 INFO FilterMutectCalls - GCS max retries/reopens: 20 ; ; 11:03:49.971 INFO FilterMutectCalls - Requester pays: disabled ; ; 11:03:49.971 INFO FilterMutectCalls - Initializing engine ; ; 11:03:50.504 INFO FeatureManager - Using codec VCFCodec to read file file:///home/lqh/somatic\_mutation/Mutect2/test.vcf.gz ; ; 11:03:50.696 INFO FilterMutectCalls - Done initializing engine ; ; 11:03:50.840 INFO ProgressMeter - Starting traversal ; ; 11:03:50.840 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute ; ; 11:03:50.841 INFO FilterMutectCalls - Starting pass 0 through the variants ; ; 11:03:51.014 INFO FilterMutectCalls - Shutting down engine ; ; \[June 4, 2021 11:03:51 AM CST\] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.19 minutes. ; ; Runtime.totalMemory()=625999872 ; ; java.lang.NumberFormatException: **For input string: ""167|35|14""** ; ; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ; ; at java.lang.Integer.parseInt(Integer.java:580) ; ; at java.lang.Integer.valueOf(Integer.java:766) ; ; at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:288) ; ; at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ; at java.util.Collections$2.tryAdvance(Collections.java:4717) ; ; at java.util.Collections$2.forEachRemaining(Collections.java:4725) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:7422,Availability,error,errorProbabilities,7422,) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:7538,Availability,Error,ErrorProbabilities,7538,eam.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:7570,Availability,Error,ErrorProbabilities,7570,luateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:8298,Availability,Error,ErrorProbabilities,8298,(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ; ; at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ; at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ; at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:8324,Availability,Error,ErrorProbabilities,8324,16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ; ; at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ; at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ; at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPip,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:6551,Energy Efficiency,Reduce,ReduceOps,6551,"FO FilterMutectCalls - Shutting down engine ; ; \[June 4, 2021 11:03:51 AM CST\] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.19 minutes. ; ; Runtime.totalMemory()=625999872 ; ; java.lang.NumberFormatException: **For input string: ""167|35|14""** ; ; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ; ; at java.lang.Integer.parseInt(Integer.java:580) ; ; at java.lang.Integer.valueOf(Integer.java:766) ; ; at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:288) ; ; at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ; at java.util.Collections$2.tryAdvance(Collections.java:4717) ; ; at java.util.Collections$2.forEachRemaining(Collections.java:4725) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lamb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:6561,Energy Efficiency,Reduce,ReduceOp,6561,"FO FilterMutectCalls - Shutting down engine ; ; \[June 4, 2021 11:03:51 AM CST\] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.19 minutes. ; ; Runtime.totalMemory()=625999872 ; ; java.lang.NumberFormatException: **For input string: ""167|35|14""** ; ; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ; ; at java.lang.Integer.parseInt(Integer.java:580) ; ; at java.lang.Integer.valueOf(Integer.java:766) ; ; at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:288) ; ; at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ; at java.util.Collections$2.tryAdvance(Collections.java:4717) ; ; at java.util.Collections$2.forEachRemaining(Collections.java:4725) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lamb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:6589,Energy Efficiency,Reduce,ReduceOps,6589,"hutting down engine ; ; \[June 4, 2021 11:03:51 AM CST\] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.19 minutes. ; ; Runtime.totalMemory()=625999872 ; ; java.lang.NumberFormatException: **For input string: ""167|35|14""** ; ; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ; ; at java.lang.Integer.parseInt(Integer.java:580) ; ; at java.lang.Integer.valueOf(Integer.java:766) ; ; at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:288) ; ; at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ; at java.util.Collections$2.tryAdvance(Collections.java:4717) ; ; at java.util.Collections$2.forEachRemaining(Collections.java:4725) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:7695,Energy Efficiency,Reduce,ReduceOps,7695,am.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(M,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:7726,Energy Efficiency,Reduce,ReduceOps,7726,.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:8017,Energy Efficiency,Reduce,ReduceOps,8017,ntext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ; ; at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:8027,Energy Efficiency,Reduce,ReduceOp,8027,ntext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ; ; at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:8055,Energy Efficiency,Reduce,ReduceOps,8055,rg.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ; ; at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ; at java.util.It,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:6484,Integrability,wrap,wrapAndCopyInto,6484,"s - Starting pass 0 through the variants ; ; 11:03:51.014 INFO FilterMutectCalls - Shutting down engine ; ; \[June 4, 2021 11:03:51 AM CST\] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.19 minutes. ; ; Runtime.totalMemory()=625999872 ; ; java.lang.NumberFormatException: **For input string: ""167|35|14""** ; ; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ; ; at java.lang.Integer.parseInt(Integer.java:580) ; ; at java.lang.Integer.valueOf(Integer.java:766) ; ; at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:288) ; ; at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ; at java.util.Collections$2.tryAdvance(Collections.java:4717) ; ; at java.util.Collections$2.forEachRemaining(Collections.java:4725) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:7950,Integrability,wrap,wrapAndCopyInto,7950,ariantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ; ; at java.util,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:9303,Integrability,wrap,wrapAndCopyInto,9303,nit>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ; ; at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ; at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ; at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ;,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:2975,Performance,Load,Loading,2975,"NCount=0;NLOD=27.80,30.51,30.97;OCM=0;POPAF=6.00,6.00,6.00;REF\_BASES=GAACTTGCTTCTTTTTTTTGC;RPA=8,9,10,11;RU=T;ReadPosRankSum=5.751;SOR=1.152;STR;Samples=TCGA-NJ-A55R-01A-11R-A262-07;TLOD=284.47,51.82,3.50 GT:AD:AF:DP:F1R2:F2R1:SB 0/1/2/3:819,166,35,14:0.161,0.034,0.014:1034:365,76,17,4:440,87,17,8:16,803,6,209 0/0:103,1,0,0:0.017,8.250e-03,8.221e-03:104:50,1,0,0:52,0,0,0:26,77,0,1. The error log that FilterMutectCalls emited was listed below:. Using GATK jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar FilterMutectCalls -R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa -V somatic\_mutation/Mutect2/test.vcf.gz -O somatic\_mutation/FilterMutectCalls/test.vcf.gz ; ; 11:03:39.517 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jun 04, 2021 11:03:49 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:03:49.968 INFO FilterMutectCalls - ------------------------------------------------------------ ; ; 11:03:49.969 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 11:03:49.969 INFO FilterMutectCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:03:49.969 INFO FilterMutectCalls - Executing as lqh@master on Linux v5.6.14-1.el7.elrepo.x86\_64 amd64 ; ; 11:03:49.969 INFO FilterMutectCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0\_152-b16 ; ; 11:03:49.969 INFO FilterMutectCalls - Start Date/Time: June 4, 2021 11:03:39 A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:3266,Safety,detect,detect,3266,"4:440,87,17,8:16,803,6,209 0/0:103,1,0,0:0.017,8.250e-03,8.221e-03:104:50,1,0,0:52,0,0,0:26,77,0,1. The error log that FilterMutectCalls emited was listed below:. Using GATK jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar FilterMutectCalls -R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa -V somatic\_mutation/Mutect2/test.vcf.gz -O somatic\_mutation/FilterMutectCalls/test.vcf.gz ; ; 11:03:39.517 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jun 04, 2021 11:03:49 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:03:49.968 INFO FilterMutectCalls - ------------------------------------------------------------ ; ; 11:03:49.969 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 11:03:49.969 INFO FilterMutectCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:03:49.969 INFO FilterMutectCalls - Executing as lqh@master on Linux v5.6.14-1.el7.elrepo.x86\_64 amd64 ; ; 11:03:49.969 INFO FilterMutectCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0\_152-b16 ; ; 11:03:49.969 INFO FilterMutectCalls - Start Date/Time: June 4, 2021 11:03:39 AM CST ; ; 11:03:49.969 INFO FilterMutectCalls - ------------------------------------------------------------ ; ; 11:03:49.969 INFO FilterMutectCalls - ------------------------------------------------------------ ; ; 11:03:49.970 INFO FilterMutectCalls - HTSJDK Version: 2.24.0 ; ; 11:03",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:773,Testability,test,test,773,"It looks like this is a bug with 4.2.0.0 because the same Mutect2 output has no issues with FilterMutectCalls 4.1.6.0. . This request was created from a contribution made by Qihan Long on June 04, 2021 03:21 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0 ; ; b) Exact command used:. gatk FilterMutectCalls \\ ; ; \-R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa \\ ; ; \-V somatic\_mutation/Mutect2/test.vcf.gz \\ ; ; \-O somatic\_mutation/FilterMutectCalls/test.vcf.gz. c) Entire error log:. I used the ""--enable-all-annotations"" option within Mutect2 to get a vcf file with abundant information. However, the following FilterMutectCalls step seemed to be intolerant of some information within previous step's vcf file record. The intolerated record within vcf listed below:. chr1 6197724 . C CT,CTT,CTTT . . AC=1,1,1;AF=0.167,0.167,0.167;AN=6;AS\_BaseQRankSum=-6.431;AS\_MQ=60.00,60.00,60.00;AS\_MQRankSum=0.000;AS\_ReadPosRankSum=5.751;AS\_SB\_TABLE=42,880|3,164|3,32|0,14;**AS\_UNIQ\_ALT\_READ\_COUNT=167|35|14**;BQHIST=5,1,0,0,1,11,2,0,0,0,14,2,0,0,1,15,1,0,0,0,16,1,0,0,0,17,0,2,0,0,18,2,0,0,1,19,6,0,1,0,20,25,0,2,2,21,13,0,1,2,22,20,0,3,3,23,2,1,2,1,24,6,0,2,0,25,21,1,4,7,26,33,0,5,6,27,18,0,0,7,28,29,0,0,4,29,26,2,4,8,30,161,4,5,51,31,263,2,3,51,32,129,2,3,22,33,41,0,0,0,34,15,0,0,0,35,20,0,0,0,36,19,0,0,0,37,12,0,0,0,38,1,0,0,0,39,9,0,0,0,41,18,0,0,0,44,26,0,0,0;BaseQRankSum=-6.431;ClippingRankSum=-7.714;DP=1323;ECNT=1;FS=0.000;LikelihoodRankSum=-7.886;MBQ=31,30,26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:832,Testability,test,test,832,"It looks like this is a bug with 4.2.0.0 because the same Mutect2 output has no issues with FilterMutectCalls 4.1.6.0. . This request was created from a contribution made by Qihan Long on June 04, 2021 03:21 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0 ; ; b) Exact command used:. gatk FilterMutectCalls \\ ; ; \-R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa \\ ; ; \-V somatic\_mutation/Mutect2/test.vcf.gz \\ ; ; \-O somatic\_mutation/FilterMutectCalls/test.vcf.gz. c) Entire error log:. I used the ""--enable-all-annotations"" option within Mutect2 to get a vcf file with abundant information. However, the following FilterMutectCalls step seemed to be intolerant of some information within previous step's vcf file record. The intolerated record within vcf listed below:. chr1 6197724 . C CT,CTT,CTTT . . AC=1,1,1;AF=0.167,0.167,0.167;AN=6;AS\_BaseQRankSum=-6.431;AS\_MQ=60.00,60.00,60.00;AS\_MQRankSum=0.000;AS\_ReadPosRankSum=5.751;AS\_SB\_TABLE=42,880|3,164|3,32|0,14;**AS\_UNIQ\_ALT\_READ\_COUNT=167|35|14**;BQHIST=5,1,0,0,1,11,2,0,0,0,14,2,0,0,1,15,1,0,0,0,16,1,0,0,0,17,0,2,0,0,18,2,0,0,1,19,6,0,1,0,20,25,0,2,2,21,13,0,1,2,22,20,0,3,3,23,2,1,2,1,24,6,0,2,0,25,21,1,4,7,26,33,0,5,6,27,18,0,0,7,28,29,0,0,4,29,26,2,4,8,30,161,4,5,51,31,263,2,3,51,32,129,2,3,22,33,41,0,0,0,34,15,0,0,0,35,20,0,0,0,36,19,0,0,0,37,12,0,0,0,38,1,0,0,0,39,9,0,0,0,41,18,0,0,0,44,26,0,0,0;BaseQRankSum=-6.431;ClippingRankSum=-7.714;DP=1323;ECNT=1;FS=0.000;LikelihoodRankSum=-7.886;MBQ=31,30,26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:861,Testability,log,log,861,"It looks like this is a bug with 4.2.0.0 because the same Mutect2 output has no issues with FilterMutectCalls 4.1.6.0. . This request was created from a contribution made by Qihan Long on June 04, 2021 03:21 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0 ; ; b) Exact command used:. gatk FilterMutectCalls \\ ; ; \-R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa \\ ; ; \-V somatic\_mutation/Mutect2/test.vcf.gz \\ ; ; \-O somatic\_mutation/FilterMutectCalls/test.vcf.gz. c) Entire error log:. I used the ""--enable-all-annotations"" option within Mutect2 to get a vcf file with abundant information. However, the following FilterMutectCalls step seemed to be intolerant of some information within previous step's vcf file record. The intolerated record within vcf listed below:. chr1 6197724 . C CT,CTT,CTTT . . AC=1,1,1;AF=0.167,0.167,0.167;AN=6;AS\_BaseQRankSum=-6.431;AS\_MQ=60.00,60.00,60.00;AS\_MQRankSum=0.000;AS\_ReadPosRankSum=5.751;AS\_SB\_TABLE=42,880|3,164|3,32|0,14;**AS\_UNIQ\_ALT\_READ\_COUNT=167|35|14**;BQHIST=5,1,0,0,1,11,2,0,0,0,14,2,0,0,1,15,1,0,0,0,16,1,0,0,0,17,0,2,0,0,18,2,0,0,1,19,6,0,1,0,20,25,0,2,2,21,13,0,1,2,22,20,0,3,3,23,2,1,2,1,24,6,0,2,0,25,21,1,4,7,26,33,0,5,6,27,18,0,0,7,28,29,0,0,4,29,26,2,4,8,30,161,4,5,51,31,263,2,3,51,32,129,2,3,22,33,41,0,0,0,34,15,0,0,0,35,20,0,0,0,36,19,0,0,0,37,12,0,0,0,38,1,0,0,0,39,9,0,0,0,41,18,0,0,0,44,26,0,0,0;BaseQRankSum=-6.431;ClippingRankSum=-7.714;DP=1323;ECNT=1;FS=0.000;LikelihoodRankSum=-7.886;MBQ=31,30,26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:2369,Testability,log,log,2369,"**;BQHIST=5,1,0,0,1,11,2,0,0,0,14,2,0,0,1,15,1,0,0,0,16,1,0,0,0,17,0,2,0,0,18,2,0,0,1,19,6,0,1,0,20,25,0,2,2,21,13,0,1,2,22,20,0,3,3,23,2,1,2,1,24,6,0,2,0,25,21,1,4,7,26,33,0,5,6,27,18,0,0,7,28,29,0,0,4,29,26,2,4,8,30,161,4,5,51,31,263,2,3,51,32,129,2,3,22,33,41,0,0,0,34,15,0,0,0,35,20,0,0,0,36,19,0,0,0,37,12,0,0,0,38,1,0,0,0,39,9,0,0,0,41,18,0,0,0,44,26,0,0,0;BaseQRankSum=-6.431;ClippingRankSum=-7.714;DP=1323;ECNT=1;FS=0.000;LikelihoodRankSum=-7.886;MBQ=31,30,26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30.97;OCM=0;POPAF=6.00,6.00,6.00;REF\_BASES=GAACTTGCTTCTTTTTTTTGC;RPA=8,9,10,11;RU=T;ReadPosRankSum=5.751;SOR=1.152;STR;Samples=TCGA-NJ-A55R-01A-11R-A262-07;TLOD=284.47,51.82,3.50 GT:AD:AF:DP:F1R2:F2R1:SB 0/1/2/3:819,166,35,14:0.161,0.034,0.014:1034:365,76,17,4:440,87,17,8:16,803,6,209 0/0:103,1,0,0:0.017,8.250e-03,8.221e-03:104:50,1,0,0:52,0,0,0:26,77,0,1. The error log that FilterMutectCalls emited was listed below:. Using GATK jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar FilterMutectCalls -R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa -V somatic\_mutation/Mutect2/test.vcf.gz -O somatic\_mutation/FilterMutectCalls/test.vcf.gz ; ; 11:03:39.517 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jun 04, 2021 11:03:49 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:03:49.968 INFO FilterMutectCalls - --------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:2868,Testability,test,test,2868,"26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30.97;OCM=0;POPAF=6.00,6.00,6.00;REF\_BASES=GAACTTGCTTCTTTTTTTTGC;RPA=8,9,10,11;RU=T;ReadPosRankSum=5.751;SOR=1.152;STR;Samples=TCGA-NJ-A55R-01A-11R-A262-07;TLOD=284.47,51.82,3.50 GT:AD:AF:DP:F1R2:F2R1:SB 0/1/2/3:819,166,35,14:0.161,0.034,0.014:1034:365,76,17,4:440,87,17,8:16,803,6,209 0/0:103,1,0,0:0.017,8.250e-03,8.221e-03:104:50,1,0,0:52,0,0,0:26,77,0,1. The error log that FilterMutectCalls emited was listed below:. Using GATK jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar FilterMutectCalls -R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa -V somatic\_mutation/Mutect2/test.vcf.gz -O somatic\_mutation/FilterMutectCalls/test.vcf.gz ; ; 11:03:39.517 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jun 04, 2021 11:03:49 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:03:49.968 INFO FilterMutectCalls - ------------------------------------------------------------ ; ; 11:03:49.969 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 11:03:49.969 INFO FilterMutectCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:03:49.969 INFO FilterMutectCalls - Executing as lqh@master on Linux v5.6.14-1.el7.elrepo.x86\_64 amd64 ; ; 11:03:49.969 INFO FilterMutectCalls - Java runtime: Java HotSpot(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:2919,Testability,test,test,2919,";MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30.97;OCM=0;POPAF=6.00,6.00,6.00;REF\_BASES=GAACTTGCTTCTTTTTTTTGC;RPA=8,9,10,11;RU=T;ReadPosRankSum=5.751;SOR=1.152;STR;Samples=TCGA-NJ-A55R-01A-11R-A262-07;TLOD=284.47,51.82,3.50 GT:AD:AF:DP:F1R2:F2R1:SB 0/1/2/3:819,166,35,14:0.161,0.034,0.014:1034:365,76,17,4:440,87,17,8:16,803,6,209 0/0:103,1,0,0:0.017,8.250e-03,8.221e-03:104:50,1,0,0:52,0,0,0:26,77,0,1. The error log that FilterMutectCalls emited was listed below:. Using GATK jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar FilterMutectCalls -R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa -V somatic\_mutation/Mutect2/test.vcf.gz -O somatic\_mutation/FilterMutectCalls/test.vcf.gz ; ; 11:03:39.517 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/lqh/software/GATK-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jun 04, 2021 11:03:49 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:03:49.968 INFO FilterMutectCalls - ------------------------------------------------------------ ; ; 11:03:49.969 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 11:03:49.969 INFO FilterMutectCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:03:49.969 INFO FilterMutectCalls - Executing as lqh@master on Linux v5.6.14-1.el7.elrepo.x86\_64 amd64 ; ; 11:03:49.969 INFO FilterMutectCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0\_152-b16 ; ; 11:03",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:5223,Testability,test,test,5223,"----------- ; ; 11:03:49.970 INFO FilterMutectCalls - HTSJDK Version: 2.24.0 ; ; 11:03:49.970 INFO FilterMutectCalls - Picard Version: 2.25.0 ; ; 11:03:49.970 INFO FilterMutectCalls - Built for Spark Version: 2.4.5 ; ; 11:03:49.970 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 11:03:49.970 INFO FilterMutectCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 11:03:49.970 INFO FilterMutectCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 11:03:49.970 INFO FilterMutectCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 11:03:49.970 INFO FilterMutectCalls - Deflater: IntelDeflater ; ; 11:03:49.971 INFO FilterMutectCalls - Inflater: IntelInflater ; ; 11:03:49.971 INFO FilterMutectCalls - GCS max retries/reopens: 20 ; ; 11:03:49.971 INFO FilterMutectCalls - Requester pays: disabled ; ; 11:03:49.971 INFO FilterMutectCalls - Initializing engine ; ; 11:03:50.504 INFO FeatureManager - Using codec VCFCodec to read file file:///home/lqh/somatic\_mutation/Mutect2/test.vcf.gz ; ; 11:03:50.696 INFO FilterMutectCalls - Done initializing engine ; ; 11:03:50.840 INFO ProgressMeter - Starting traversal ; ; 11:03:50.840 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute ; ; 11:03:50.841 INFO FilterMutectCalls - Starting pass 0 through the variants ; ; 11:03:51.014 INFO FilterMutectCalls - Shutting down engine ; ; \[June 4, 2021 11:03:51 AM CST\] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.19 minutes. ; ; Runtime.totalMemory()=625999872 ; ; java.lang.NumberFormatException: **For input string: ""167|35|14""** ; ; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ; ; at java.lang.Integer.parseInt(Integer.java:580) ; ; at java.lang.Integer.valueOf(Integer.java:766) ; ; at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:288) ; ; at java.util.stream.R",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/issues/7298:10557,Testability,test,tested,10557,"e.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289). What's worth noted is that I tested FilterMutectCalls within GATK-4.1.6.0, it surly can accept the vcf record above and output normal records as below:. chr1 6197724 . C CT,CTT,CTTT . multiallelic AC=1,1,1;AF=0.167,0.167,0.167;AN=6;AS\_BaseQRankSum=-6.431;AS\_MQ=60.00,60.00,60.00;AS\_MQRankSum=0.000;AS\_ReadPosRankSum=5.751;AS\_SB\_TABLE=42,880|3,164|3,32|0,14;**AS\_UNIQ\_ALT\_READ\_COUNT=167|35|14**;BQHIST=5,1,0,0,1,11,2,0,0,0,14,2,0,0,1,15,1,0,0,0,16,1,0,0,0,17,0,2,0,0,18,2,0,0,1,19,6,0,1,0,20,25,0,2,2,21,13,0,1,2,22,20,0,3,3,23,2,1,2,1,24,6,0,2,0,25,21,1,4,7,26,33,0,5,6,27,18,0,0,7,28,29,0,0,4,29,26,2,4,8,30,161,4,5,51,31,263,2,3,51,32,129,2,3,22,33,41,0,0,0,34,15,0,0,0,35,20,0,0,0,36,19,0,0,0,37,12,0,0,0,38,1,0,0,0,39,9,0,0,0,41,18,0,0,0,44,26,0,0,0;BaseQRankSum=-6.431;CONTQ=93;ClippingRankSum=-7.714;DP=1323;ECNT=1;FS=0.000;GERMQ=93;LikelihoodRankSum=-7.886;MBQ=31,30,26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30.9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298
https://github.com/broadinstitute/gatk/pull/7299:125,Security,access,access,125,"If the sample file is created by extracting a table from BQ, the file might be in a bucket that only the service account can access. Add an option for using the service account to pull the file.; Also, expose the service account input at the workflow (not the task) level",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7299
https://github.com/broadinstitute/gatk/pull/7299:202,Security,expose,expose,202,"If the sample file is created by extracting a table from BQ, the file might be in a bucket that only the service account can access. Add an option for using the service account to pull the file.; Also, expose the service account input at the workflow (not the task) level",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7299
https://github.com/broadinstitute/gatk/issues/7300:201,Security,validat,validation,201,In the work on #7295 it became clear that there are a lot of overlapping overloads of the `createGenomeLoc()` method that has already caused some confusion since some overloads will skip the reference validation step. Somebody should audit all of the uses of `GenomeLocParser` and evaluate where validation is and isn't appropriate (possibly if you want an unvalidated genomeLoc use a SimpleInterval?) and wire them accordingly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7300
https://github.com/broadinstitute/gatk/issues/7300:234,Security,audit,audit,234,In the work on #7295 it became clear that there are a lot of overlapping overloads of the `createGenomeLoc()` method that has already caused some confusion since some overloads will skip the reference validation step. Somebody should audit all of the uses of `GenomeLocParser` and evaluate where validation is and isn't appropriate (possibly if you want an unvalidated genomeLoc use a SimpleInterval?) and wire them accordingly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7300
https://github.com/broadinstitute/gatk/issues/7300:296,Security,validat,validation,296,In the work on #7295 it became clear that there are a lot of overlapping overloads of the `createGenomeLoc()` method that has already caused some confusion since some overloads will skip the reference validation step. Somebody should audit all of the uses of `GenomeLocParser` and evaluate where validation is and isn't appropriate (possibly if you want an unvalidated genomeLoc use a SimpleInterval?) and wire them accordingly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7300
https://github.com/broadinstitute/gatk/issues/7300:31,Usability,clear,clear,31,In the work on #7295 it became clear that there are a lot of overlapping overloads of the `createGenomeLoc()` method that has already caused some confusion since some overloads will skip the reference validation step. Somebody should audit all of the uses of `GenomeLocParser` and evaluate where validation is and isn't appropriate (possibly if you want an unvalidated genomeLoc use a SimpleInterval?) and wire them accordingly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7300
https://github.com/broadinstitute/gatk/issues/7300:385,Usability,Simpl,SimpleInterval,385,In the work on #7295 it became clear that there are a lot of overlapping overloads of the `createGenomeLoc()` method that has already caused some confusion since some overloads will skip the reference validation step. Somebody should audit all of the uses of `GenomeLocParser` and evaluate where validation is and isn't appropriate (possibly if you want an unvalidated genomeLoc use a SimpleInterval?) and wire them accordingly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7300
https://github.com/broadinstitute/gatk/pull/7301:302,Modifiability,variab,variable,302,"This is related to #7287. . By default MultiVariantWalkerGroupedOnStart will iterate over any variant that spans the user-provided intervals. This is not what one would typically want when running scatter/gather jobs, since variants spanning interval borders would be included in both jobs. There is a variable/argument for IGNORE_VARIANTS_THAT_START_OUTSIDE_INTERVAL, but it's private and therefore subclasses cant read it. I would like our MultiVariantWalkerGroupedOnStart to view this value and at least log a warning if the current job hasUserSuppliedIntervals(), and ignoreIntervalsOutsideStart=false. In this PR I just make that variable protected, but I could also add formal getter/setters if you prefer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7301
https://github.com/broadinstitute/gatk/pull/7301:635,Modifiability,variab,variable,635,"This is related to #7287. . By default MultiVariantWalkerGroupedOnStart will iterate over any variant that spans the user-provided intervals. This is not what one would typically want when running scatter/gather jobs, since variants spanning interval borders would be included in both jobs. There is a variable/argument for IGNORE_VARIANTS_THAT_START_OUTSIDE_INTERVAL, but it's private and therefore subclasses cant read it. I would like our MultiVariantWalkerGroupedOnStart to view this value and at least log a warning if the current job hasUserSuppliedIntervals(), and ignoreIntervalsOutsideStart=false. In this PR I just make that variable protected, but I could also add formal getter/setters if you prefer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7301
https://github.com/broadinstitute/gatk/pull/7301:507,Testability,log,log,507,"This is related to #7287. . By default MultiVariantWalkerGroupedOnStart will iterate over any variant that spans the user-provided intervals. This is not what one would typically want when running scatter/gather jobs, since variants spanning interval borders would be included in both jobs. There is a variable/argument for IGNORE_VARIANTS_THAT_START_OUTSIDE_INTERVAL, but it's private and therefore subclasses cant read it. I would like our MultiVariantWalkerGroupedOnStart to view this value and at least log a warning if the current job hasUserSuppliedIntervals(), and ignoreIntervalsOutsideStart=false. In this PR I just make that variable protected, but I could also add formal getter/setters if you prefer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7301
https://github.com/broadinstitute/gatk/pull/7303:261,Availability,robust,robust,261,"Similar to AD, the new annotation (DD) captures the depth of each allele supporting evidence; or reads, however it does so by following a variational Bayes approach looking into the; likelihoods rather than applying a fix threshold. . This turns out to be more robust in some instances. To get the new non-standard annotation in HC you need to add -A AllelePseudoDepth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7303
https://github.com/broadinstitute/gatk/issues/7304:1099,Availability,error,error,1099,"ke this comes up because of an underlying assumption that the data is diploid. It would help out users if we could make some sort of workaround possible with this tool. This request was created from a contribution made by Samantha Zarate on May 28, 2021 14:24 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078069451-VariantEval-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078069451-VariantEval-IndexOutOfBoundsException). \--. a) GATK version used: v4.1.9 ; ; b) Exact command used:. /gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar \\ ; ; VariantEval \\ ; ; \-R /$PATH\_TO\_REFERENCE/chm13/t2t-chm13.20200921.withGRCh38chrY.chrEBV.chrYKI270740v1r.fasta \\ ; ; \--eval /$PATH\_TO\_VCF/1kgp.chrX.recalibrated.snp\_indel.pass.vcf.gz \\ ; ; \--pedigree /$PATH\_TO\_PED/1kgp\_trios.ped \\ ; ; \-no-ev -no-st -ST Family \\ ; ; \-EV MendelianViolationEvaluator \\ ; ; \-O 1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table. c) Entire error log:. 19:35:29.408 INFO VariantEval - ------------------------------------------------------------ 19:35:29.408 INFO VariantEval - The Genome Analysis Toolkit (GATK) v4.1.9.0 19:35:29.409 INFO VariantEval - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) 19:35:29.409 INFO VariantEval - Executing as root@0b79b5044551 on Linux v5.4.104+ amd64 19:35:29.409 INFO VariantEval - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 19:35:29.409 INFO VariantEval - Start Date/Time: May 27, 2021 7:35:29 PM UTC 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.410 INFO VariantEval - HTSJDK Version: 2.23.0 19:35:29.410 INFO VariantEval - Picard Version: 2.23.3 19:35:29.410 INFO VariantEval - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 19:35:29.410 INFO VariantEval - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/issues/7304:3626,Availability,down,down,3626,"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Warning: VariantEval is a BETA tool and is not yet ready for use in production !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\[0m 19:35:29.411 INFO VariantEval - Initializing engine 19:35:29.755 INFO FeatureManager - Using codec VCFCodec to read file file:///cromwell\_root/fc-47de7dae-e8e6-429c-b760-b4ba49136eee/joint\_vcfs/recalibrated/PASS/1kgp.chrX.recalibrated.snp\_indel.pass.vcf.gz 19:35:29.835 INFO VariantEval - Done initializing engine 19:35:29.836 INFO PedReader - Reading PED file /cromwell\_root/fc-47de7dae-e8e6-429c-b760-b4ba49136eee/resources/1kgp/1kgp\_trios.ped with missing fields: \[\] 19:35:29.854 INFO PedReader - Phenotype is other? true 19:35:32.686 INFO VariantEval - Creating 1881 combinatorial stratification states 19:35:32.742 INFO ProgressMeter - Starting traversal 19:35:32.742 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute 19:36:01.819 INFO VariantEval - Shutting down engine \[May 27, 2021 7:36:01 PM UTC\] org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval done. Elapsed time: 0.54 minutes. Runtime.totalMemory()=4964483072 java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 at java.util.ArrayList.rangeCheck(ArrayList.java:653) at java.util.ArrayList.get(ArrayList.java:429) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.isViolation(MendelianViolation.java:180) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.updateViolations(MendelianViolation.java:122) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.countFamilyViolations(MendelianViolation.java:148) at org.broadinstitute.hellbender.tools.walkers.varianteval.evaluators.MendelianViolationEvaluator.update1(MendelianViolationEvaluator.java:122) at org.broadinstitute.hellbender.tools.walkers.varianteval.util.EvaluationContext.apply(EvaluationContext.java:74) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.process",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/issues/7304:8603,Availability,error,error,8603,"2021/05/27 19:36:04 Delocalizing output /cromwell\_root/memory\_retry\_rc -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/memory\_retry\_rc 2021/05/27 19:36:04 Delocalizing output /cromwell\_root/rc -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/rc 2021/05/27 19:36:05 Delocalizing output /cromwell\_root/stdout -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/stdout 2021/05/27 19:36:06 Delocalizing output /cromwell\_root/stderr -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/stderr 2021/05/27 19:36:08 Delocalizing output /cromwell\_root/1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table Required file output '/cromwell\_root/1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table' does not exist. I am running VariantEval to detect Mendelian violations in large joint genotyped VCF files, so I'm running it on a per-chromosome basis. This error only occurs for the chromosome X file, and it only occurs with this FASTA file (GRCh38 on chrX does not cause this issue). IndexFeatureFile is run just before this error, which has also led to successful runs in other chromosomes, so that's not the issue. Any insight on this issue would be appreciated.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161363'>Zendesk ticket #161363</a>)<br>gz#161363</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/issues/7304:8773,Availability,error,error,8773,"2021/05/27 19:36:04 Delocalizing output /cromwell\_root/memory\_retry\_rc -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/memory\_retry\_rc 2021/05/27 19:36:04 Delocalizing output /cromwell\_root/rc -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/rc 2021/05/27 19:36:05 Delocalizing output /cromwell\_root/stdout -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/stdout 2021/05/27 19:36:06 Delocalizing output /cromwell\_root/stderr -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/stderr 2021/05/27 19:36:08 Delocalizing output /cromwell\_root/1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table Required file output '/cromwell\_root/1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table' does not exist. I am running VariantEval to detect Mendelian violations in large joint genotyped VCF files, so I'm running it on a per-chromosome basis. This error only occurs for the chromosome X file, and it only occurs with this FASTA file (GRCh38 on chrX does not cause this issue). IndexFeatureFile is run just before this error, which has also led to successful runs in other chromosomes, so that's not the issue. Any insight on this issue would be appreciated.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161363'>Zendesk ticket #161363</a>)<br>gz#161363</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/issues/7304:1606,Deployability,release,release-,1606,": v4.1.9 ; ; b) Exact command used:. /gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar \\ ; ; VariantEval \\ ; ; \-R /$PATH\_TO\_REFERENCE/chm13/t2t-chm13.20200921.withGRCh38chrY.chrEBV.chrYKI270740v1r.fasta \\ ; ; \--eval /$PATH\_TO\_VCF/1kgp.chrX.recalibrated.snp\_indel.pass.vcf.gz \\ ; ; \--pedigree /$PATH\_TO\_PED/1kgp\_trios.ped \\ ; ; \-no-ev -no-st -ST Family \\ ; ; \-EV MendelianViolationEvaluator \\ ; ; \-O 1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table. c) Entire error log:. 19:35:29.408 INFO VariantEval - ------------------------------------------------------------ 19:35:29.408 INFO VariantEval - The Genome Analysis Toolkit (GATK) v4.1.9.0 19:35:29.409 INFO VariantEval - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) 19:35:29.409 INFO VariantEval - Executing as root@0b79b5044551 on Linux v5.4.104+ amd64 19:35:29.409 INFO VariantEval - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 19:35:29.409 INFO VariantEval - Start Date/Time: May 27, 2021 7:35:29 PM UTC 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.410 INFO VariantEval - HTSJDK Version: 2.23.0 19:35:29.410 INFO VariantEval - Picard Version: 2.23.3 19:35:29.410 INFO VariantEval - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 19:35:29.410 INFO VariantEval - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false 19:35:29.410 INFO VariantEval - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true 19:35:29.410 INFO VariantEval - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false 19:35:29.410 INFO VariantEval - Deflater: IntelDeflater 19:35:29.410 INFO VariantEval - Inflater: IntelInflater 19:35:29.410 INFO VariantEval - GCS max retries/reopens: 20 19:35:29.410 INFO VariantEval - Requester pays: disabled 19:35:29.411 WARN VariantEval - \[1m\[31m !!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/issues/7304:4134,Deployability,update,updateViolations,4134,"O PedReader - Reading PED file /cromwell\_root/fc-47de7dae-e8e6-429c-b760-b4ba49136eee/resources/1kgp/1kgp\_trios.ped with missing fields: \[\] 19:35:29.854 INFO PedReader - Phenotype is other? true 19:35:32.686 INFO VariantEval - Creating 1881 combinatorial stratification states 19:35:32.742 INFO ProgressMeter - Starting traversal 19:35:32.742 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute 19:36:01.819 INFO VariantEval - Shutting down engine \[May 27, 2021 7:36:01 PM UTC\] org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval done. Elapsed time: 0.54 minutes. Runtime.totalMemory()=4964483072 java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 at java.util.ArrayList.rangeCheck(ArrayList.java:653) at java.util.ArrayList.get(ArrayList.java:429) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.isViolation(MendelianViolation.java:180) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.updateViolations(MendelianViolation.java:122) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.countFamilyViolations(MendelianViolation.java:148) at org.broadinstitute.hellbender.tools.walkers.varianteval.evaluators.MendelianViolationEvaluator.update1(MendelianViolationEvaluator.java:122) at org.broadinstitute.hellbender.tools.walkers.varianteval.util.EvaluationContext.apply(EvaluationContext.java:74) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.processComp(VariantEval.java:596) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.doApply(VariantEval.java:562) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.callDoApply(VariantEval.java:497) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.addVariant(VariantEval.java:478) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.access$100(VariantEval.java:469) at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/issues/7304:5898,Integrability,wrap,wrapAndCopyInto,5898,nder.tools.walkers.varianteval.VariantEval$PositionAggregator.addVariant(VariantEval.java:478) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.access$100(VariantEval.java:469) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.apply(VariantEval.java:511) at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:120) at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.Iterator.forEachRemaining(Iterator.java:116) at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418) at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:118) at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049) at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) at org.broadinstitute.hellbender.M,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/issues/7304:8489,Safety,detect,detect,8489,"2021/05/27 19:36:04 Delocalizing output /cromwell\_root/memory\_retry\_rc -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/memory\_retry\_rc 2021/05/27 19:36:04 Delocalizing output /cromwell\_root/rc -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/rc 2021/05/27 19:36:05 Delocalizing output /cromwell\_root/stdout -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/stdout 2021/05/27 19:36:06 Delocalizing output /cromwell\_root/stderr -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/stderr 2021/05/27 19:36:08 Delocalizing output /cromwell\_root/1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table -> gs://fc-51aefb1c-4e8e-4dcb-a59c-62e318ea351a/b4b25c64-84dc-4902-ae53-89bff2091e92/mendelian\_analysis/fd25ef2c-8ed6-4b2a-9df9-b7287511aee8/call-mendelian\_analysis/1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table Required file output '/cromwell\_root/1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table' does not exist. I am running VariantEval to detect Mendelian violations in large joint genotyped VCF files, so I'm running it on a per-chromosome basis. This error only occurs for the chromosome X file, and it only occurs with this FASTA file (GRCh38 on chrX does not cause this issue). IndexFeatureFile is run just before this error, which has also led to successful runs in other chromosomes, so that's not the issue. Any insight on this issue would be appreciated.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161363'>Zendesk ticket #161363</a>)<br>gz#161363</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/issues/7304:5099,Security,access,access,5099,ples.MendelianViolation.updateViolations(MendelianViolation.java:122) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.countFamilyViolations(MendelianViolation.java:148) at org.broadinstitute.hellbender.tools.walkers.varianteval.evaluators.MendelianViolationEvaluator.update1(MendelianViolationEvaluator.java:122) at org.broadinstitute.hellbender.tools.walkers.varianteval.util.EvaluationContext.apply(EvaluationContext.java:74) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.processComp(VariantEval.java:596) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.doApply(VariantEval.java:562) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.callDoApply(VariantEval.java:497) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.addVariant(VariantEval.java:478) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.access$100(VariantEval.java:469) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.apply(VariantEval.java:511) at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$traverse$1(MultiVariantWalker.java:120) at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.Iterator.forEachRemaining(Iterator.java:116) at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/issues/7304:1105,Testability,log,log,1105,"ke this comes up because of an underlying assumption that the data is diploid. It would help out users if we could make some sort of workaround possible with this tool. This request was created from a contribution made by Samantha Zarate on May 28, 2021 14:24 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078069451-VariantEval-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078069451-VariantEval-IndexOutOfBoundsException). \--. a) GATK version used: v4.1.9 ; ; b) Exact command used:. /gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar \\ ; ; VariantEval \\ ; ; \-R /$PATH\_TO\_REFERENCE/chm13/t2t-chm13.20200921.withGRCh38chrY.chrEBV.chrYKI270740v1r.fasta \\ ; ; \--eval /$PATH\_TO\_VCF/1kgp.chrX.recalibrated.snp\_indel.pass.vcf.gz \\ ; ; \--pedigree /$PATH\_TO\_PED/1kgp\_trios.ped \\ ; ; \-no-ev -no-st -ST Family \\ ; ; \-EV MendelianViolationEvaluator \\ ; ; \-O 1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table. c) Entire error log:. 19:35:29.408 INFO VariantEval - ------------------------------------------------------------ 19:35:29.408 INFO VariantEval - The Genome Analysis Toolkit (GATK) v4.1.9.0 19:35:29.409 INFO VariantEval - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) 19:35:29.409 INFO VariantEval - Executing as root@0b79b5044551 on Linux v5.4.104+ amd64 19:35:29.409 INFO VariantEval - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 19:35:29.409 INFO VariantEval - Start Date/Time: May 27, 2021 7:35:29 PM UTC 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.410 INFO VariantEval - HTSJDK Version: 2.23.0 19:35:29.410 INFO VariantEval - Picard Version: 2.23.3 19:35:29.410 INFO VariantEval - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 19:35:29.410 INFO VariantEval - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304
https://github.com/broadinstitute/gatk/pull/7307:260,Deployability,release,releases,260,"In the ""Related Annotations"" section of the annotations tool docs, the links were broken redirecting to the GATK homepage. We decided to remove these links since there is not a good way to link to the mentioned tool without the link becoming outdated with new releases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7307
https://github.com/broadinstitute/gatk/pull/7309:6,Deployability,update,updates,6,Found updates for broken links in the tool docs and replaced them or removed them.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7309
https://github.com/broadinstitute/gatk/issues/7311:1457,Availability,error,error,1457,"of GATK. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:35.527 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:11:35.528 INFO CombineGVCFs - Executing as jpac1984@p0002.ten.osc.edu on Linux v3.10.0-1160.21.1.el7.x86_64 amd64; 20:11:35.529 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 20:11:35.529 INFO CombineGVCFs - Start Date/Time: June 13, 2021 8:11:34 PM GMT; 20:11:35.529 INFO CombineGVCFs - --",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:3587,Availability,down,down,3587,"-----------; 20:11:35.530 INFO CombineGVCFs - HTSJDK Version: 2.24.0; 20:11:35.530 INFO CombineGVCFs - Picard Version: 2.25.0; 20:11:35.530 INFO CombineGVCFs - Built for Spark Version: 2.4.5; 20:11:35.530 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 20:11:35.530 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:11:35.531 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:11:35.531 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:11:35.531 INFO CombineGVCFs - Deflater: IntelDeflater; 20:11:35.531 INFO CombineGVCFs - Inflater: IntelInflater; 20:11:35.531 INFO CombineGVCFs - GCS max retries/reopens: 20; 20:11:35.531 INFO CombineGVCFs - Requester pays: disabled; 20:11:35.531 INFO CombineGVCFs - Initializing engine; 20:11:35.957 INFO FeatureManager - Using codec VCFCodec to read file file:///fs/scratch/PHS0338/appz/elprep-v5.0.2/PA113.vcf.gz; 20:11:35.969 INFO CombineGVCFs - Shutting down engine; [June 13, 2021 8:11:35 PM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=1772093440; ***********************************************************************; A USER ERROR has occurred: An index is required but was not found for file /fs/scratch/PHS0338/appz/elprep-v5.0.2/PA113.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input. ***********************************************************************. Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar CombineGVCFs -R /users/PHS0338/jpac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:3837,Availability,ERROR,ERROR,3837,"COMPRESSION_LEVEL : 2; 20:11:35.530 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:11:35.531 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:11:35.531 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:11:35.531 INFO CombineGVCFs - Deflater: IntelDeflater; 20:11:35.531 INFO CombineGVCFs - Inflater: IntelInflater; 20:11:35.531 INFO CombineGVCFs - GCS max retries/reopens: 20; 20:11:35.531 INFO CombineGVCFs - Requester pays: disabled; 20:11:35.531 INFO CombineGVCFs - Initializing engine; 20:11:35.957 INFO FeatureManager - Using codec VCFCodec to read file file:///fs/scratch/PHS0338/appz/elprep-v5.0.2/PA113.vcf.gz; 20:11:35.969 INFO CombineGVCFs - Shutting down engine; [June 13, 2021 8:11:35 PM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=1772093440; ***********************************************************************; A USER ERROR has occurred: An index is required but was not found for file /fs/scratch/PHS0338/appz/elprep-v5.0.2/PA113.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input. ***********************************************************************. Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz --variant IN33corr.vcf.gz --variant AL82.vcf.gz -O test.vcf.gz. ------; The CombineGVCFs websi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:143,Deployability,release,release,143,"----. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; CombineGVCFs. ### Affected version(s); - [ ] Latest public release version [version?] Yes; - [ ] Latest master branch as of [date of test?] singularity. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; I am trying to combine GVCFs for joint-calllings and I am using the latest singularity release of GATK. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:444,Deployability,release,release,444,"----. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; CombineGVCFs. ### Affected version(s); - [ ] Latest public release version [version?] Yes; - [ ] Latest master branch as of [date of test?] singularity. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; I am trying to combine GVCFs for joint-calllings and I am using the latest singularity release of GATK. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:1509,Performance,Load,Loading,1509,"duce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:35.527 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:11:35.528 INFO CombineGVCFs - Executing as jpac1984@p0002.ten.osc.edu on Linux v3.10.0-1160.21.1.el7.x86_64 amd64; 20:11:35.529 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 20:11:35.529 INFO CombineGVCFs - Start Date/Time: June 13, 2021 8:11:34 PM GMT; 20:11:35.529 INFO CombineGVCFs - ---------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:1764,Safety,detect,detect,1764,"arity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:35.527 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:11:35.528 INFO CombineGVCFs - Executing as jpac1984@p0002.ten.osc.edu on Linux v3.10.0-1160.21.1.el7.x86_64 amd64; 20:11:35.529 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 20:11:35.529 INFO CombineGVCFs - Start Date/Time: June 13, 2021 8:11:34 PM GMT; 20:11:35.529 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.529 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.530 INFO CombineGVCFs - HTSJDK Version: 2.24.0; 20:11:35.530 INFO CombineGVCFs - Picard Version: 2.25.0; 20:11:35.530 INFO CombineGVCFs - Built",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:217,Testability,test,test,217,"----. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; CombineGVCFs. ### Affected version(s); - [ ] Latest public release version [version?] Yes; - [ ] Latest master branch as of [date of test?] singularity. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; I am trying to combine GVCFs for joint-calllings and I am using the latest singularity release of GATK. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:329,Testability,log,logs,329,"----. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; CombineGVCFs. ### Affected version(s); - [ ] Latest public release version [version?] Yes; - [ ] Latest master branch as of [date of test?] singularity. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; I am trying to combine GVCFs for joint-calllings and I am using the latest singularity release of GATK. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:1065,Testability,test,test,1065,"pecial parameters?_; CombineGVCFs. ### Affected version(s); - [ ] Latest public release version [version?] Yes; - [ ] Latest master branch as of [date of test?] singularity. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; I am trying to combine GVCFs for joint-calllings and I am using the latest singularity release of GATK. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:35.527 INFO CombineGVCFs - For support and documentation go to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:1463,Testability,log,log,1463,"of GATK. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:35.527 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:11:35.528 INFO CombineGVCFs - Executing as jpac1984@p0002.ten.osc.edu on Linux v3.10.0-1160.21.1.el7.x86_64 amd64; 20:11:35.529 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 20:11:35.529 INFO CombineGVCFs - Start Date/Time: June 13, 2021 8:11:34 PM GMT; 20:11:35.529 INFO CombineGVCFs - --",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7311:4793,Testability,test,test,4793,"Inflater; 20:11:35.531 INFO CombineGVCFs - GCS max retries/reopens: 20; 20:11:35.531 INFO CombineGVCFs - Requester pays: disabled; 20:11:35.531 INFO CombineGVCFs - Initializing engine; 20:11:35.957 INFO FeatureManager - Using codec VCFCodec to read file file:///fs/scratch/PHS0338/appz/elprep-v5.0.2/PA113.vcf.gz; 20:11:35.969 INFO CombineGVCFs - Shutting down engine; [June 13, 2021 8:11:35 PM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=1772093440; ***********************************************************************; A USER ERROR has occurred: An index is required but was not found for file /fs/scratch/PHS0338/appz/elprep-v5.0.2/PA113.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input. ***********************************************************************. Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz --variant IN33corr.vcf.gz --variant AL82.vcf.gz -O test.vcf.gz. ------; The CombineGVCFs website - https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs - does not mention anything about this????. What did I miss or was not even mentioned and what should I do?; There is no reference about this ""https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs"" in the website, https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311
https://github.com/broadinstitute/gatk/issues/7312:17,Availability,error,error,17,"I bumped into an error in a PR of mine due to a recent update in master. While I should make the code of the PR more robust I think that the approach take to compose approximate likehoods in ```VariantAnnotator.makeLikelihoods``` can and should be improved. Currently uses -Infility as ""unlikely"" lk (I would say rather ""impossible"" lk) and 0 as ""likely"" based on whether the read pileup does not match the allele or it does match the allele. . IMO the ""unlikely"" lk should never be less than the mapping quality of the read. And it can be further reduced by the base quality in case of an snp or the indel error probrability; by default is 45 Phred yet as part of the integration with Illumina/DRAGEN Dragstr, at least in germline, we can come out with indel penalties that are tailred to the reference, read context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7312
https://github.com/broadinstitute/gatk/issues/7312:117,Availability,robust,robust,117,"I bumped into an error in a PR of mine due to a recent update in master. While I should make the code of the PR more robust I think that the approach take to compose approximate likehoods in ```VariantAnnotator.makeLikelihoods``` can and should be improved. Currently uses -Infility as ""unlikely"" lk (I would say rather ""impossible"" lk) and 0 as ""likely"" based on whether the read pileup does not match the allele or it does match the allele. . IMO the ""unlikely"" lk should never be less than the mapping quality of the read. And it can be further reduced by the base quality in case of an snp or the indel error probrability; by default is 45 Phred yet as part of the integration with Illumina/DRAGEN Dragstr, at least in germline, we can come out with indel penalties that are tailred to the reference, read context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7312
https://github.com/broadinstitute/gatk/issues/7312:607,Availability,error,error,607,"I bumped into an error in a PR of mine due to a recent update in master. While I should make the code of the PR more robust I think that the approach take to compose approximate likehoods in ```VariantAnnotator.makeLikelihoods``` can and should be improved. Currently uses -Infility as ""unlikely"" lk (I would say rather ""impossible"" lk) and 0 as ""likely"" based on whether the read pileup does not match the allele or it does match the allele. . IMO the ""unlikely"" lk should never be less than the mapping quality of the read. And it can be further reduced by the base quality in case of an snp or the indel error probrability; by default is 45 Phred yet as part of the integration with Illumina/DRAGEN Dragstr, at least in germline, we can come out with indel penalties that are tailred to the reference, read context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7312
https://github.com/broadinstitute/gatk/issues/7312:55,Deployability,update,update,55,"I bumped into an error in a PR of mine due to a recent update in master. While I should make the code of the PR more robust I think that the approach take to compose approximate likehoods in ```VariantAnnotator.makeLikelihoods``` can and should be improved. Currently uses -Infility as ""unlikely"" lk (I would say rather ""impossible"" lk) and 0 as ""likely"" based on whether the read pileup does not match the allele or it does match the allele. . IMO the ""unlikely"" lk should never be less than the mapping quality of the read. And it can be further reduced by the base quality in case of an snp or the indel error probrability; by default is 45 Phred yet as part of the integration with Illumina/DRAGEN Dragstr, at least in germline, we can come out with indel penalties that are tailred to the reference, read context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7312
https://github.com/broadinstitute/gatk/issues/7312:669,Deployability,integrat,integration,669,"I bumped into an error in a PR of mine due to a recent update in master. While I should make the code of the PR more robust I think that the approach take to compose approximate likehoods in ```VariantAnnotator.makeLikelihoods``` can and should be improved. Currently uses -Infility as ""unlikely"" lk (I would say rather ""impossible"" lk) and 0 as ""likely"" based on whether the read pileup does not match the allele or it does match the allele. . IMO the ""unlikely"" lk should never be less than the mapping quality of the read. And it can be further reduced by the base quality in case of an snp or the indel error probrability; by default is 45 Phred yet as part of the integration with Illumina/DRAGEN Dragstr, at least in germline, we can come out with indel penalties that are tailred to the reference, read context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7312
https://github.com/broadinstitute/gatk/issues/7312:548,Energy Efficiency,reduce,reduced,548,"I bumped into an error in a PR of mine due to a recent update in master. While I should make the code of the PR more robust I think that the approach take to compose approximate likehoods in ```VariantAnnotator.makeLikelihoods``` can and should be improved. Currently uses -Infility as ""unlikely"" lk (I would say rather ""impossible"" lk) and 0 as ""likely"" based on whether the read pileup does not match the allele or it does match the allele. . IMO the ""unlikely"" lk should never be less than the mapping quality of the read. And it can be further reduced by the base quality in case of an snp or the indel error probrability; by default is 45 Phred yet as part of the integration with Illumina/DRAGEN Dragstr, at least in germline, we can come out with indel penalties that are tailred to the reference, read context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7312
https://github.com/broadinstitute/gatk/issues/7312:669,Integrability,integrat,integration,669,"I bumped into an error in a PR of mine due to a recent update in master. While I should make the code of the PR more robust I think that the approach take to compose approximate likehoods in ```VariantAnnotator.makeLikelihoods``` can and should be improved. Currently uses -Infility as ""unlikely"" lk (I would say rather ""impossible"" lk) and 0 as ""likely"" based on whether the read pileup does not match the allele or it does match the allele. . IMO the ""unlikely"" lk should never be less than the mapping quality of the read. And it can be further reduced by the base quality in case of an snp or the indel error probrability; by default is 45 Phred yet as part of the integration with Illumina/DRAGEN Dragstr, at least in germline, we can come out with indel penalties that are tailred to the reference, read context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7312
https://github.com/broadinstitute/gatk/issues/7313:1306,Deployability,release,release,1306,"to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Edit: This was posted accidentally while sw",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7313
https://github.com/broadinstitute/gatk/issues/7313:1376,Testability,test,test,1376,"to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Edit: This was posted accidentally while sw",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7313
https://github.com/broadinstitute/gatk/issues/7313:1476,Testability,log,logs,1476,"ay comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Edit: This was posted accidentally while switching tabs. I'm sorry for the inconvenience. You can delete the issue since it's of no use for anyone.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7313
https://github.com/broadinstitute/gatk/issues/7314:249,Availability,error,error,249,"I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314
https://github.com/broadinstitute/gatk/issues/7314:647,Availability,error,error,647,"I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314
https://github.com/broadinstitute/gatk/issues/7314:1303,Availability,error,error,1303,"seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM UTC; 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.220 INFO ASEReadCounter - HTSJD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314
https://github.com/broadinstitute/gatk/issues/7314:4993,Availability,down,down,4993,"- Processing 29268134 bp from intervals; 19:13:26.834 WARN IndexUtils - Feature file ""/cga/bass/Chunyang/ref/hg19/1000G_phase1.snps.high_confidence.b37.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 19:13:26.973 INFO ASEReadCounter - Done initializing engine; 19:13:26.977 INFO ProgressMeter - Starting traversal; 19:13:26.977 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 19:13:27.118 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:835092; 19:13:27.118 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:835132; 19:13:27.118 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:835133; ... ; ...; ...; 19:13:28.229 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:29617944; 19:13:28.230 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:29618025; 19:13:28.231 INFO ASEReadCounter - 0 read(s) filtered by: ValidAlignmentStartReadFilter; 0 read(s) filtered by: ValidAlignmentEndReadFilter; 0 read(s) filtered by: HasReadGroupReadFilter; 0 read(s) filtered by: MatchingBasesAndQualsReadFilter; 0 read(s) filtered by: SeqIsStoredReadFilter; 51 read(s) filtered by: NotDuplicateReadFilter; 63 read(s) filtered by: NotSecondaryAlignmentReadFilter; 3 read(s) filtered by: MappedReadFilter; 117 total reads filtered; 19:13:28.231 INFO ProgressMeter - 1:29618022 0.0 110019 5264067.0; 19:13:28.231 INFO ProgressMeter - Traversal complete. Processed 110019 total loci in 0.0 minutes.; 19:13:28.233 INFO ASEReadCounter - Shutting down engine; [June 14, 2021 7:13:28 PM UTC] org.broadinstitute.hellbender.tools.walkers.rnaseq.ASEReadCounter done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2303197184; ; . output.txt:. contig position variantID refAllele altAllele refCount altCount totalCount lowMAPQDepth lowBaseQDepth rawDepth otherBases improperPairs. . Thanks for your help!. Chunyang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314
https://github.com/broadinstitute/gatk/issues/7314:699,Performance,Load,Loading,699,"I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314
https://github.com/broadinstitute/gatk/issues/7314:1034,Security,authenticat,authenticated,1034,"unter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM UTC; 19:13:26.219 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314
https://github.com/broadinstitute/gatk/issues/7314:1389,Security,authenticat,authentication,1389,"-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM UTC; 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.220 INFO ASEReadCounter - HTSJDK Version: 2.23.0; 19:13:26.220 INFO ASEReadCounter - Picard Version: 2.22.8; 19:13:26.220 INFO ASEReadCounter - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314
https://github.com/broadinstitute/gatk/issues/7314:183,Testability,log,log,183,"I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314
https://github.com/broadinstitute/gatk/issues/7314:653,Testability,log,log,653,"I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314
https://github.com/broadinstitute/gatk/issues/7315:571,Availability,error,error,571,"Hi, I want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:595,Availability,ERROR,ERROR,595,"Hi, I want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:845,Availability,ERROR,ERROR,845,want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:889,Availability,ERROR,ERROR,889,want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:984,Availability,ERROR,ERROR,984,want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1078,Availability,ERROR,ERROR,1078,want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1173,Availability,ERROR,ERROR,1173,want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1425,Availability,error,error,1425,"R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""L",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1449,Availability,ERROR,ERROR,1449," like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,N",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1462,Availability,ERROR,ERROR,1462," like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,N",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1613,Availability,ERROR,ERROR,1613," like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,N",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1864,Availability,error,error,1864,"ype Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Des",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1878,Availability,ERROR,ERROR,1878,"not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1921,Availability,error,error,1921,"not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:2135,Availability,ERROR,ERROR,2135," ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:601,Integrability,MESSAGE,MESSAGE,601,"Hi, I want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1468,Integrability,MESSAGE,MESSAGE,1468," like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,N",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:1884,Integrability,MESSAGE,MESSAGE,1884,"not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:3550,Safety,detect,detect,3550,"like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">. Could you tell me how to fix it and why the VCF4.1 format generated by HaplotypeCaller didn't work on the same version of GenotypeGVCFs . ; Thank you very much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7315:3542,Testability,Test,Test,3542,"like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">. Could you tell me how to fix it and why the VCF4.1 format generated by HaplotypeCaller didn't work on the same version of GenotypeGVCFs . ; Thank you very much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315
https://github.com/broadinstitute/gatk/issues/7318:363,Availability,error,error,363,"As discussed at the GATK Office Hours meeting, the --genotype-germline-sites should no longer be labeled as experimental. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status). \--. If not an error, choose a category for your question(REQUIRED): ; ; e) Will Mutect2's \[--genotype-germline-sites\](/hc/en-us/articles/360037593851-Mutect2#--genotype-germline-sites) be in future releases?. I notice that this flag is still set to `EXPERIMENTAL` in the Mutect2 docs. Is there a way I can track the status of this feature? It is important for the functionality of the pipeline I have built (i.e. we want to be able to see all of the germline variants that Mutect wants to call -- doesn't have to perfect per s, but those calls are important), so we want to ensure it stays in the picture because it seems to be working for our needs (even after a lot of pressure testing).<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/149087'>Zendesk ticket #149087</a>)<br>gz#149087</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7318
https://github.com/broadinstitute/gatk/issues/7318:549,Deployability,release,releases,549,"As discussed at the GATK Office Hours meeting, the --genotype-germline-sites should no longer be labeled as experimental. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status). \--. If not an error, choose a category for your question(REQUIRED): ; ; e) Will Mutect2's \[--genotype-germline-sites\](/hc/en-us/articles/360037593851-Mutect2#--genotype-germline-sites) be in future releases?. I notice that this flag is still set to `EXPERIMENTAL` in the Mutect2 docs. Is there a way I can track the status of this feature? It is important for the functionality of the pipeline I have built (i.e. we want to be able to see all of the germline variants that Mutect wants to call -- doesn't have to perfect per s, but those calls are important), so we want to ensure it stays in the picture because it seems to be working for our needs (even after a lot of pressure testing).<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/149087'>Zendesk ticket #149087</a>)<br>gz#149087</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7318
https://github.com/broadinstitute/gatk/issues/7318:738,Deployability,pipeline,pipeline,738,"As discussed at the GATK Office Hours meeting, the --genotype-germline-sites should no longer be labeled as experimental. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status). \--. If not an error, choose a category for your question(REQUIRED): ; ; e) Will Mutect2's \[--genotype-germline-sites\](/hc/en-us/articles/360037593851-Mutect2#--genotype-germline-sites) be in future releases?. I notice that this flag is still set to `EXPERIMENTAL` in the Mutect2 docs. Is there a way I can track the status of this feature? It is important for the functionality of the pipeline I have built (i.e. we want to be able to see all of the germline variants that Mutect wants to call -- doesn't have to perfect per s, but those calls are important), so we want to ensure it stays in the picture because it seems to be working for our needs (even after a lot of pressure testing).<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/149087'>Zendesk ticket #149087</a>)<br>gz#149087</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7318
https://github.com/broadinstitute/gatk/issues/7318:1034,Testability,test,testing,1034,"As discussed at the GATK Office Hours meeting, the --genotype-germline-sites should no longer be labeled as experimental. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status). \--. If not an error, choose a category for your question(REQUIRED): ; ; e) Will Mutect2's \[--genotype-germline-sites\](/hc/en-us/articles/360037593851-Mutect2#--genotype-germline-sites) be in future releases?. I notice that this flag is still set to `EXPERIMENTAL` in the Mutect2 docs. Is there a way I can track the status of this feature? It is important for the functionality of the pipeline I have built (i.e. we want to be able to see all of the germline variants that Mutect wants to call -- doesn't have to perfect per s, but those calls are important), so we want to ensure it stays in the picture because it seems to be working for our needs (even after a lot of pressure testing).<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/149087'>Zendesk ticket #149087</a>)<br>gz#149087</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7318
https://github.com/broadinstitute/gatk/pull/7319:39,Integrability,message,messages,39,create .bigqueryrc to suppress warning messages; change disk size to 150,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7319
https://github.com/broadinstitute/gatk/pull/7320:157,Performance,load,load,157,"- adds a scatter to the `SNPsVariantRecalibrator` call when the number of samples is over a certain threshold; - separates out ""generate tsv/csv files"" and ""load tsv/csv files into BigQuery"" steps of `UploadFilterSetToBQ` into two different tasks, `CreateFilterSetFiles` (scattered) and `UploadFilterSetFilesToBQ` (not). Closes https://github.com/broadinstitute/dsp-spec-ops/issues/326. - [ ] To do before merging: remove change to `.dockstore.yml`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7320
https://github.com/broadinstitute/gatk/issues/7321:30,Availability,recover,recovers,30,Interesting to see whether it recovers real vairants.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7321
https://github.com/broadinstitute/gatk/issues/7321:30,Safety,recover,recovers,30,Interesting to see whether it recovers real vairants.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7321
https://github.com/broadinstitute/gatk/issues/7322:83,Deployability,release,release,83,"Hello,. Do you have an estimate on when you might be releasing the next minor GATK release? I'm hoping to pick up the GenomicsDB update (https://github.com/broadinstitute/gatk/commit/8796404cab594b716d43755f61fd405c92208141). I realize we could build our own, but for one of our projects we release a public dataset with documentation on the tools, and it's nicer to use official releases. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7322
https://github.com/broadinstitute/gatk/issues/7322:129,Deployability,update,update,129,"Hello,. Do you have an estimate on when you might be releasing the next minor GATK release? I'm hoping to pick up the GenomicsDB update (https://github.com/broadinstitute/gatk/commit/8796404cab594b716d43755f61fd405c92208141). I realize we could build our own, but for one of our projects we release a public dataset with documentation on the tools, and it's nicer to use official releases. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7322
https://github.com/broadinstitute/gatk/issues/7322:291,Deployability,release,release,291,"Hello,. Do you have an estimate on when you might be releasing the next minor GATK release? I'm hoping to pick up the GenomicsDB update (https://github.com/broadinstitute/gatk/commit/8796404cab594b716d43755f61fd405c92208141). I realize we could build our own, but for one of our projects we release a public dataset with documentation on the tools, and it's nicer to use official releases. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7322
https://github.com/broadinstitute/gatk/issues/7322:380,Deployability,release,releases,380,"Hello,. Do you have an estimate on when you might be releasing the next minor GATK release? I'm hoping to pick up the GenomicsDB update (https://github.com/broadinstitute/gatk/commit/8796404cab594b716d43755f61fd405c92208141). I realize we could build our own, but for one of our projects we release a public dataset with documentation on the tools, and it's nicer to use official releases. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7322
https://github.com/broadinstitute/gatk/issues/7324:239,Availability,down,down,239,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:294,Availability,down,down,294,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:387,Availability,error,error,387,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:401,Availability,error,error,401,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:1134,Availability,Error,Error,1134,"ing GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-b12; 10:49:12.232 INFO GenomicsDBImport - Start Date/Time: June 18, 2021 10:49:11 AM KST; 10:49:12.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:4064,Availability,down,down,4064," INFO GenomicsDBImport - Initializing engine; 10:49:12.577 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 10:49:12.938 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 10:49:13.163 INFO IntervalArgumentCollection - Processing 51304566 bp from intervals; 10:49:13.163 INFO GenomicsDBImport - Done initializing engine; 10:49:13.164 INFO GenomicsDBImport - Callset Map JSON file will be re-written to /mnt/mone/OMICS/Project/Joint_call/GATK_GenomicDB/test_database/test_overwrite_1/callset.json; 10:49:13.164 INFO GenomicsDBImport - Incrementally importing to workspace - /mnt/mone/OMICS/Project/Joint_call/GATK_GenomicDB/test_database/test_overwrite_1; 10:49:13.164 INFO ProgressMeter - Starting traversal; 10:49:13.164 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:49:13.231 INFO GenomicsDBImport - Shutting down engine; [June 18, 2021 10:49:13 AM KST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2194669568; org.genomicsdb.exception.GenomicsDBException: Duplicate sample name found: 4762. Sample was originally in /mnt/mone/OMICS/Project/Asian_Genome/Korea/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf; at org.genomicsdb.importer.extensions.CallSetMapExtensions.checkDuplicateCallsetsForIncrementalImport(CallSetMapExtensions.java:270); at org.genomicsdb.importer.extensions.CallSetMapExtensions.mergeCallsetsForIncrementalImport(CallSetMapExtensions.java:241); at org.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:252); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:745); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:183,Deployability,update,update,183,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:485,Deployability,update,update,485,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:516,Deployability,update,update,516,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:934,Deployability,update,update-workspace-path,934,"## Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Ser",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:3164,Deployability,update,update-workspace-path,3164,"---------------------------------------------------; 10:49:12.233 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.233 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 10:49:12.233 INFO GenomicsDBImport - Picard Version: 2.22.8; 10:49:12.234 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:12.234 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:12.234 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:49:12.234 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:49:12.234 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:49:12.234 INFO GenomicsDBImport - Inflater: IntelInflater; 10:49:12.234 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:49:12.234 INFO GenomicsDBImport - Requester pays: disabled; 10:49:12.235 INFO GenomicsDBImport - Initializing engine; 10:49:12.577 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 10:49:12.938 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 10:49:13.163 INFO IntervalArgumentCollection - Processing 51304566 bp from intervals; 10:49:13.163 INFO GenomicsDBImport - Done initializing engine; 10:49:13.164 INFO GenomicsDBImport - Callset Map JSON file will be re-written to /mnt/mone/OMICS/Project/Joint_call/GATK_GenomicDB/test_database/test_overwrite_1/callset.json; 10:49:13.164 INFO GenomicsDBImport - Incrementally importing to workspace - /mnt/mone/OMICS/Project/Joint_call/GATK_GenomicDB/test_database/test_overwrite_1; 10:49:13.164 INFO ProgressMeter - Starting traversal; 10:49:13.164 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:49:13.231 INFO GenomicsDBImport - Shutting down engine; [June 18, 2021 10:49:13 AM KST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport don",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:393,Integrability,message,message,393,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:407,Integrability,message,message,407,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:1140,Integrability,message,message,1140,"ing GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-b12; 10:49:12.232 INFO GenomicsDBImport - Start Date/Time: June 18, 2021 10:49:11 AM KST; 10:49:12.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:1010,Performance,optimiz,optimizations,1010,"## Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Ser",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:1189,Performance,Load,Loading,1189,"te genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-b12; 10:49:12.232 INFO GenomicsDBImport - Start Date/Time: June 18, 2021 10:49:11 AM KST; 10:49:12.233 INFO GenomicsDBImport - --------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7324:1488,Safety,detect,detect,1488,"o update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-b12; 10:49:12.232 INFO GenomicsDBImport - Start Date/Time: June 18, 2021 10:49:11 AM KST; 10:49:12.233 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.233 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.233 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 10:49:12.233 INFO GenomicsDBImport - Picard Version: 2.22.8; 10:49:12.234 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324
https://github.com/broadinstitute/gatk/issues/7327:921,Availability,error,error,921,"We have determined that a sites-only VCF causes ASEReadCounter to only output a header. There are warnings in the stack trace but it is not clear that the tool found no genotypes in the file. We can look into adding a check in ASEReadCounter to exit out if the VCF has no genotype fields. The documentation for this tool should also be more specific. This request was created from a contribution made by Chunyang Bao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota excee",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:1294,Availability,down,downsample,1294,"cumentation for this tool should also be more specific. This request was created from a contribution made by Chunyang Bao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:1364,Availability,error,error,1364,"was created from a contribution made by Chunyang Bao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1. 19:13:26",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:2027,Availability,error,error,2027,"ATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1. 19:13:26.218 INFO ASEReadCounter - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/). 19:13:26.219 INFO ASEReadCounter - Executing as [cbao@uger-c009.broadinstitute.org](mailto:cbao@uger-c009.broadinstitute.org) on Linux v3.10.0-1160.15.2.el7.x86\_64 amd64. 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0\_181-b13. 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM UTC. 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:5880,Availability,down,down,5880,"nps.high\_confidence.b37.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file. 19:13:26.973 INFO ASEReadCounter - Done initializing engine. 19:13:26.977 INFO ProgressMeter - Starting traversal. 19:13:26.977 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute. 19:13:27.118 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:835092. 19:13:27.118 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:835132. 19:13:27.118 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:835133. .... ... ... 19:13:28.229 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:29617944. 19:13:28.230 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:29618025. 19:13:28.231 INFO ASEReadCounter - 0 read(s) filtered by: ValidAlignmentStartReadFilter. 0 read(s) filtered by: ValidAlignmentEndReadFilter. 0 read(s) filtered by: HasReadGroupReadFilter. 0 read(s) filtered by: MatchingBasesAndQualsReadFilter. 0 read(s) filtered by: SeqIsStoredReadFilter. 51 read(s) filtered by: NotDuplicateReadFilter. 63 read(s) filtered by: NotSecondaryAlignmentReadFilter. 3 read(s) filtered by: MappedReadFilter. 117 total reads filtered. 19:13:28.231 INFO ProgressMeter - 1:29618022 0.0 110019 5264067.0. 19:13:28.231 INFO ProgressMeter - Traversal complete. Processed 110019 total loci in 0.0 minutes. 19:13:28.233 INFO ASEReadCounter - Shutting down engine. \[June 14, 2021 7:13:28 PM UTC\] org.broadinstitute.hellbender.tools.walkers.rnaseq.ASEReadCounter done. Elapsed time: 0.04 minutes. Runtime.totalMemory()=2303197184. output.txt:. contig position variantID refAllele altAllele refCount altCount totalCount lowMAPQDepth lowBaseQDepth rawDepth otherBases improperPairs. Thanks for your help!. Chunyang<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/165377'>Zendesk ticket #165377</a>)<br>gz#165377</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:1416,Performance,Load,Loading,1416,"ao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1. 19:13:26.218 INFO ASEReadCounter - For support and document",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:1758,Security,authenticat,authenticated,1758,"unter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1. 19:13:26.218 INFO ASEReadCounter - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/). 19:13:26.219 INFO ASEReadCounter - Executing as [cbao@uger-c009.broadinstitute.org](mailto:cbao@uger-c009.broadinstitute.org) on Linux v3.10.0-1160.15.2.el7.x86\_64 amd64. 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:2114,Security,authenticat,authentication,2114,"\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1. 19:13:26.218 INFO ASEReadCounter - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/). 19:13:26.219 INFO ASEReadCounter - Executing as [cbao@uger-c009.broadinstitute.org](mailto:cbao@uger-c009.broadinstitute.org) on Linux v3.10.0-1160.15.2.el7.x86\_64 amd64. 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0\_181-b13. 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM UTC. 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.220 INFO ASEReadCounter - HTSJDK Version: 2.23.0. 19:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:2161,Security,authenticat,authentication,2161,"d.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1. 19:13:26.218 INFO ASEReadCounter - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/). 19:13:26.219 INFO ASEReadCounter - Executing as [cbao@uger-c009.broadinstitute.org](mailto:cbao@uger-c009.broadinstitute.org) on Linux v3.10.0-1160.15.2.el7.x86\_64 amd64. 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0\_181-b13. 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM UTC. 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.220 INFO ASEReadCounter - HTSJDK Version: 2.23.0. 19:13:26.220 INFO ASEReadCounter - Picard V",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:855,Testability,log,log,855,"We have determined that a sites-only VCF causes ASEReadCounter to only output a header. There are warnings in the stack trace but it is not clear that the tool found no genotypes in the file. We can look into adding a check in ASEReadCounter to exit out if the VCF has no genotype fields. The documentation for this tool should also be more specific. This request was created from a contribution made by Chunyang Bao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota excee",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:1370,Testability,log,log,1370,"was created from a contribution made by Chunyang Bao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1. 19:13:26",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/issues/7327:140,Usability,clear,clear,140,"We have determined that a sites-only VCF causes ASEReadCounter to only output a header. There are warnings in the stack trace but it is not clear that the tool found no genotypes in the file. We can look into adding a check in ASEReadCounter to exit out if the VCF has no genotype fields. The documentation for this tool should also be more specific. This request was created from a contribution made by Chunyang Bao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota excee",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327
https://github.com/broadinstitute/gatk/pull/7328:152,Deployability,pipeline,pipeline,152,"This is a tool intended to evaluate the performance of genotyping (not sequencing/variant discovery), for example from a genotyping array or imputation pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7328
https://github.com/broadinstitute/gatk/pull/7328:40,Performance,perform,performance,40,"This is a tool intended to evaluate the performance of genotyping (not sequencing/variant discovery), for example from a genotyping array or imputation pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7328
https://github.com/broadinstitute/gatk/issues/7332:4279,Availability,down,down,4279,":06:29.368 INFO ProgressMeter - Lama-PacBio.Chr08:1676524 1.6 4676000 2883156.9; 17:06:39.369 INFO ProgressMeter - Lama-PacBio.Chr17:545310 1.8 9925000 5549291.3; 17:06:49.558 INFO ProgressMeter - Lama-PacBio.Chr20:3003652 2.0 14424000 7365509.5; 17:06:59.558 INFO ProgressMeter - Lama-PacBio.Chr26:426929 2.1 19191000 9031058.8; 17:07:09.558 INFO ProgressMeter - Lama-PacBio.Chr30:1051399 2.3 24396000 10645527.3; 17:07:19.559 INFO ProgressMeter - Lama-PacBio.Chr34:95733 2.5 29543000 12017410.1; 17:07:23.141 INFO DepthOfCoverage - 1031666 read(s) filtered by: WellformedReadFilter ; 0 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 1031666 total reads filtered; 17:07:23.142 INFO ProgressMeter - Lama-PacBio.Chr34:1982733 2.5 31430935 12482169.5; 17:07:23.142 INFO ProgressMeter - Traversal complete. Processed 31430935 total loci in 2.5 minutes.; 17:07:23.142 INFO DepthOfCoverage - Shutting down engine; [June 29, 2021 5:07:23 PM GMT] org.broadinstitute.hellbender.tools.walkers.coverage.DepthOfCoverage done. Elapsed time: 2.54 minutes.; Runtime.totalMemory()=244318208; java.lang.ArrayIndexOutOfBoundsException: 0; 	at org.broadinstitute.hellbender.tools.walkers.coverage.CoverageOutputWriter.printIntervalTable(CoverageOutputWriter.java:616); 		at org.broadinstitute.hellbender.tools.walkers.coverage.CoverageOutputWriter.writeOutputIntervalStatistics(CoverageOutputWriter.java:364); 	at org.broadinstitute.hellbender.tools.walkers.coverage.DepthOfCoverage.onTraversalSuccess(DepthOfCoverage.java:397); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7332
https://github.com/broadinstitute/gatk/issues/7332:2,Performance,perform,performed,2,"I performed a DepthOfCoverage analysis using the latest version of GATK (4.2.0) within the docker environment. I have provided only the required arguments and files, as shown in the line below:; gatk DepthOfCoverage -R assembly-Pacbio.genome.fasta -O Coverage_Pacbio -I Alignment_sorted_Pacbio.bam -L scaffolds-Pacbio-Chr-sizes.interval_list. The execution came to an end and apparently without a problem:. 17:04:50.881 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 29, 2021 5:04:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:04:51.384 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.385 INFO DepthOfCoverage - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:04:51.385 INFO DepthOfCoverage - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:04:51.385 INFO DepthOfCoverage - Executing as root@d84100edcb97 on Linux v5.4.0-77-generic amd64; 17:04:51.385 INFO DepthOfCoverage - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 17:04:51.386 INFO DepthOfCoverage - Start Date/Time: June 29, 2021 5:04:50 PM GMT; 17:04:51.386 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.386 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Version: 2.24.0; 17:04:51.387 INFO DepthOfCoverage - Picard Version: 2.25.0; 17:04:51.387 INFO DepthOfCoverage - Built for Spark Version: 2.4.5; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:04",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7332
https://github.com/broadinstitute/gatk/issues/7332:447,Performance,Load,Loading,447,"I performed a DepthOfCoverage analysis using the latest version of GATK (4.2.0) within the docker environment. I have provided only the required arguments and files, as shown in the line below:; gatk DepthOfCoverage -R assembly-Pacbio.genome.fasta -O Coverage_Pacbio -I Alignment_sorted_Pacbio.bam -L scaffolds-Pacbio-Chr-sizes.interval_list. The execution came to an end and apparently without a problem:. 17:04:50.881 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 29, 2021 5:04:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:04:51.384 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.385 INFO DepthOfCoverage - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:04:51.385 INFO DepthOfCoverage - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:04:51.385 INFO DepthOfCoverage - Executing as root@d84100edcb97 on Linux v5.4.0-77-generic amd64; 17:04:51.385 INFO DepthOfCoverage - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 17:04:51.386 INFO DepthOfCoverage - Start Date/Time: June 29, 2021 5:04:50 PM GMT; 17:04:51.386 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.386 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Version: 2.24.0; 17:04:51.387 INFO DepthOfCoverage - Picard Version: 2.25.0; 17:04:51.387 INFO DepthOfCoverage - Built for Spark Version: 2.4.5; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:04",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7332
https://github.com/broadinstitute/gatk/issues/7332:702,Safety,detect,detect,702,"I performed a DepthOfCoverage analysis using the latest version of GATK (4.2.0) within the docker environment. I have provided only the required arguments and files, as shown in the line below:; gatk DepthOfCoverage -R assembly-Pacbio.genome.fasta -O Coverage_Pacbio -I Alignment_sorted_Pacbio.bam -L scaffolds-Pacbio-Chr-sizes.interval_list. The execution came to an end and apparently without a problem:. 17:04:50.881 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 29, 2021 5:04:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:04:51.384 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.385 INFO DepthOfCoverage - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:04:51.385 INFO DepthOfCoverage - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:04:51.385 INFO DepthOfCoverage - Executing as root@d84100edcb97 on Linux v5.4.0-77-generic amd64; 17:04:51.385 INFO DepthOfCoverage - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 17:04:51.386 INFO DepthOfCoverage - Start Date/Time: June 29, 2021 5:04:50 PM GMT; 17:04:51.386 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.386 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Version: 2.24.0; 17:04:51.387 INFO DepthOfCoverage - Picard Version: 2.25.0; 17:04:51.387 INFO DepthOfCoverage - Built for Spark Version: 2.4.5; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:04",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7332
https://github.com/broadinstitute/gatk/issues/7334:4168,Availability,down,down,4168,"O ReblockGVCF - Inflater: IntelInflater; 11:25:55.711 INFO ReblockGVCF - GCS max retries/reopens: 20; 11:25:55.711 INFO ReblockGVCF - Requester pays: disabled; 11:25:55.711 WARN ReblockGVCF - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: ReblockGVCF is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:25:55.711 INFO ReblockGVCF - Initializing engine; 11:25:56.290 INFO FeatureManager - Using codec VCFCodec to read file file:///rprojectnb2/kageproj/gatk/gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz; 11:25:56.569 INFO ReblockGVCF - Done initializing engine; 11:25:56.690 INFO ProgressMeter - Starting traversal; 11:25:56.690 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:26:06.694 INFO ProgressMeter - chr1:5066659 0.2 771000 4624612.6; 11:26:16.711 INFO ProgressMeter - chr1:12628456 0.3 1886000 5652065.3; 11:26:26.103 INFO ReblockGVCF - Shutting down engine; [June 30, 2021 11:26:26 AM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ReblockGVCF done. Elapsed time: 0.51 minutes.; Runtime.totalMemory()=3303538688; java.lang.IllegalArgumentException: cannot add a genotype with GQ=-1 because it's not within bounds [0,20); 	at org.broadinstitute.hellbender.utils.variant.writers.HomRefBlock.add(HomRefBlock.java:99); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.createNewBlock(GVCFBlockCombiner.java:168); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.addHomRefSite(GVCFBlockCombiner.java:137); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.submit(GVCFBlockCombiner.java:200); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFWriter.add(GVCFWriter.java:91); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ReblockGVCF.apply(ReblockGVCF.java:229); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334
https://github.com/broadinstitute/gatk/issues/7334:870,Deployability,install,install,870,"----. ## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF . ### Affected version(s); 4.2.0.0. ### Description ; When running ReblockGVCF the following exception occurs:. `java.lang.IllegalArgumentException: cannot add a genotype with GQ=-1 because it's not within bounds [0,20); `. #### Steps to reproduce. Using a gVCF created with 4.2.0.0 HaplotypeCaller... `gatk ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz`. #### Expected behavior; Should run to completion and create reblocked GVCF. #### Actual behavior; ```; Reblocking gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334
https://github.com/broadinstitute/gatk/issues/7334:1162,Deployability,install,install,1162,"urs:. `java.lang.IllegalArgumentException: cannot add a genotype with GQ=-1 because it's not within bounds [0,20); `. #### Steps to reproduce. Using a gVCF created with 4.2.0.0 HaplotypeCaller... `gatk ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz`. #### Expected behavior; Should run to completion and create reblocked GVCF. #### Actual behavior; ```; Reblocking gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:25:55.709 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:25:55.709 INFO ReblockGVCF - Executing as farrell@scc-hadoop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334
https://github.com/broadinstitute/gatk/issues/7334:1555,Deployability,install,install,1555,"cf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz`. #### Expected behavior; Should run to completion and create reblocked GVCF. #### Actual behavior; ```; Reblocking gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:25:55.709 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:25:55.709 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 11:25:55.709 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:25:55.709 INFO ReblockGVCF - Start Date/Time: June 30, 2021 11:25:55 AM EDT; 11:25:55.710 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.710 INFO ReblockGVCF - --------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334
https://github.com/broadinstitute/gatk/issues/7334:5766,Integrability,wrap,wrapAndCopyInto,5766,137); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.submit(GVCFBlockCombiner.java:200); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFWriter.add(GVCFWriter.java:91); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ReblockGVCF.apply(ReblockGVCF.java:229); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334
https://github.com/broadinstitute/gatk/issues/7334:936,Modifiability,variab,variable,936,"----. ## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF . ### Affected version(s); 4.2.0.0. ### Description ; When running ReblockGVCF the following exception occurs:. `java.lang.IllegalArgumentException: cannot add a genotype with GQ=-1 because it's not within bounds [0,20); `. #### Steps to reproduce. Using a gVCF created with 4.2.0.0 HaplotypeCaller... `gatk ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz`. #### Expected behavior; Should run to completion and create reblocked GVCF. #### Actual behavior; ```; Reblocking gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334
https://github.com/broadinstitute/gatk/issues/7334:1485,Performance,Load,Loading,1485,"DWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz`. #### Expected behavior; Should run to completion and create reblocked GVCF. #### Actual behavior; ```; Reblocking gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:25:55.709 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:25:55.709 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 11:25:55.709 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:25:55.709 INFO ReblockGVCF - Start Date/Time: June 30, 2021 11:25:55 AM EDT; 11:25:55.710 INFO ReblockGVCF - -----------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334
https://github.com/broadinstitute/gatk/issues/7334:1773,Safety,detect,detect,1773,"vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:25:55.709 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:25:55.709 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 11:25:55.709 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:25:55.709 INFO ReblockGVCF - Start Date/Time: June 30, 2021 11:25:55 AM EDT; 11:25:55.710 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.710 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.710 INFO ReblockGVCF - HTSJDK Version: 2.24.0; 11:25:55.710 INFO ReblockGVCF - Picard Version: 2.25.0; 11:25:55.710 INFO ReblockGVCF - Built for Spark Version: 2.4.5; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334
https://github.com/broadinstitute/gatk/issues/7338:869,Performance,perform,performance,869,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/issues/7338:1361,Testability,log,logging,1361,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/issues/7338:1375,Testability,Log,LogManager,1375,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/issues/7338:1398,Testability,Log,LogManager,1398,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/issues/7338:1434,Testability,log,logging,1434,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/issues/7338:1448,Testability,Log,LogManager,1448,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/issues/7338:1469,Testability,Log,LogManager,1469,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/issues/7338:1505,Testability,log,logging,1505,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/issues/7338:1519,Testability,Log,LogManager,1519,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/issues/7338:1540,Testability,Log,LogManager,1540,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338
https://github.com/broadinstitute/gatk/pull/7339:15,Deployability,release,release,15,"Generate multi-release jars, to automatically select the implementation version when running under Java 9 or newer. This fixes https://github.com/broadinstitute/gatk/issues/7338",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7339
https://github.com/broadinstitute/gatk/issues/7341:1214,Availability,down,downstream,1214,"This request was created from a contribution made by Matt Johnson on July 05, 2021 21:23 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community\_comment\_4403173344411](https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community_comment_4403173344411). \--. Hello, I am also having this issue. \[This page\](/hc/en-us/articles/360035530752-What-types-of-variants-can-GATK-tools-detect-or-handle-)indicates the following definition of SYMBOLIC:. SYMBOLIC (such as the `<NON-REF>` allele used in GVCFs produced by HaplotypeCaller, the `*` allele used to signify the presence of a [spanning deletion](https://gatk.zendesk.com/hc/en-us/articles/360035531912), or undefined events like a very large allele or one that's fuzzy and not fully modeled; i.e. there's some event going on here but we don't know what exactly). Therefore I would expectSelectVariants --select-type-to-exclude SYMBOLIC to not have any calls containing spanning deletions. However, the output VCFs still do (in gatk4 4.2.0.0). This causes problems for downstream tools like FastaAlternateReferenceMaker:. java.lang.IllegalArgumentException: the input sequence contains invalid base calls like: \*. Is there any way to force GATK to exclude spanning deletions when filtering a VCF?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/168722'>Zendesk ticket #168722</a>)<br>gz#168722</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7341
https://github.com/broadinstitute/gatk/issues/7341:565,Safety,detect,detect-or-handle,565,"This request was created from a contribution made by Matt Johnson on July 05, 2021 21:23 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community\_comment\_4403173344411](https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community_comment_4403173344411). \--. Hello, I am also having this issue. \[This page\](/hc/en-us/articles/360035530752-What-types-of-variants-can-GATK-tools-detect-or-handle-)indicates the following definition of SYMBOLIC:. SYMBOLIC (such as the `<NON-REF>` allele used in GVCFs produced by HaplotypeCaller, the `*` allele used to signify the presence of a [spanning deletion](https://gatk.zendesk.com/hc/en-us/articles/360035531912), or undefined events like a very large allele or one that's fuzzy and not fully modeled; i.e. there's some event going on here but we don't know what exactly). Therefore I would expectSelectVariants --select-type-to-exclude SYMBOLIC to not have any calls containing spanning deletions. However, the output VCFs still do (in gatk4 4.2.0.0). This causes problems for downstream tools like FastaAlternateReferenceMaker:. java.lang.IllegalArgumentException: the input sequence contains invalid base calls like: \*. Is there any way to force GATK to exclude spanning deletions when filtering a VCF?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/168722'>Zendesk ticket #168722</a>)<br>gz#168722</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7341
https://github.com/broadinstitute/gatk/pull/7343:82,Deployability,Update,Updated,82,Added Gencode's GeneTranscriptType as an annotation field in GencodeFunctotation. Updated unit tests after adding new annotation field. resolves #4408,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7343
https://github.com/broadinstitute/gatk/pull/7343:95,Testability,test,tests,95,Added Gencode's GeneTranscriptType as an annotation field in GencodeFunctotation. Updated unit tests after adding new annotation field. resolves #4408,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7343
https://github.com/broadinstitute/gatk/issues/7344:550,Testability,Log,Log,550,"## Feature request. ### Tool(s) or class(es) involved; HaplotypeCaller of GATK v4.2.0.0. ### Description; Hi, I'm using GATK to call variants in Aedes Aegypti samples.; My output is spammed with WARNINGs (see below). I wondered:; 1. If this was an issue, i.e. my samples have bad QC/Coverage? Or is it something expected? In particular, there is one sample that seems to generates much more warnings than the others, so that's why I thought it could be a coverage issue?; 2. Is there a way to deactivate these warnings, or print them only once?. ### Log example (only the first lines, the log is several gigs filled with these warnings); ```; 16:16:40.556 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:18938 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.556 WARN StrandBiasBySample - Annotation will not be calculated at position 1:18938 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.568 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:18946 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.568 WARN StrandBiasBySample - Annotation will not be calculated at position 1:18946 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.726 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:19264 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.726 WARN StrandBiasBySample - Annotation will not be calculated at position 1:19264 and possibly subsequent; genotype for sample TOTO is not called; 16:16:42.644 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:20675 and possibly subsequent; genotype for sample TOTO is not called; 16:16:42.644 WARN StrandBiasBySample - Annotation will not be calculated at position 1:20675 and possibly subsequent; genotype for sample TOTO is not called; 16:16:42.646 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:20685 and possibl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7344
https://github.com/broadinstitute/gatk/issues/7344:589,Testability,log,log,589,"## Feature request. ### Tool(s) or class(es) involved; HaplotypeCaller of GATK v4.2.0.0. ### Description; Hi, I'm using GATK to call variants in Aedes Aegypti samples.; My output is spammed with WARNINGs (see below). I wondered:; 1. If this was an issue, i.e. my samples have bad QC/Coverage? Or is it something expected? In particular, there is one sample that seems to generates much more warnings than the others, so that's why I thought it could be a coverage issue?; 2. Is there a way to deactivate these warnings, or print them only once?. ### Log example (only the first lines, the log is several gigs filled with these warnings); ```; 16:16:40.556 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:18938 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.556 WARN StrandBiasBySample - Annotation will not be calculated at position 1:18938 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.568 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:18946 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.568 WARN StrandBiasBySample - Annotation will not be calculated at position 1:18946 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.726 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:19264 and possibly subsequent; genotype for sample TOTO is not called; 16:16:40.726 WARN StrandBiasBySample - Annotation will not be calculated at position 1:19264 and possibly subsequent; genotype for sample TOTO is not called; 16:16:42.644 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:20675 and possibly subsequent; genotype for sample TOTO is not called; 16:16:42.644 WARN StrandBiasBySample - Annotation will not be calculated at position 1:20675 and possibly subsequent; genotype for sample TOTO is not called; 16:16:42.646 WARN DepthPerSampleHC - Annotation will not be calculated at position 1:20685 and possibl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7344
https://github.com/broadinstitute/gatk/pull/7345:238,Performance,Load,Loading,238,"Added:. - service account support for SitesOnly task, added GATK flag to not output a timestamp info VCF to help Call Caching; - switch to SSD, increased size and moved to pre-emptibles for Annotate task; - service account support for BQ Loading; - service account support for BQ Smoke Test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7345
https://github.com/broadinstitute/gatk/pull/7345:286,Testability,Test,Test,286,"Added:. - service account support for SitesOnly task, added GATK flag to not output a timestamp info VCF to help Call Caching; - switch to SSD, increased size and moved to pre-emptibles for Annotate task; - service account support for BQ Loading; - service account support for BQ Smoke Test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7345
https://github.com/broadinstitute/gatk/issues/7346:66,Testability,test,tests,66,We had a breakage in our java 11 support which went undetected by tests because it only happened when we actually packaged the jar and that isn't done in our java 11 tests.; ; See #7339 #7338. We should add a simple test to make sure something like this doesn't happen again.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7346
https://github.com/broadinstitute/gatk/issues/7346:166,Testability,test,tests,166,We had a breakage in our java 11 support which went undetected by tests because it only happened when we actually packaged the jar and that isn't done in our java 11 tests.; ; See #7339 #7338. We should add a simple test to make sure something like this doesn't happen again.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7346
https://github.com/broadinstitute/gatk/issues/7346:216,Testability,test,test,216,We had a breakage in our java 11 support which went undetected by tests because it only happened when we actually packaged the jar and that isn't done in our java 11 tests.; ; See #7339 #7338. We should add a simple test to make sure something like this doesn't happen again.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7346
https://github.com/broadinstitute/gatk/issues/7346:209,Usability,simpl,simple,209,We had a breakage in our java 11 support which went undetected by tests because it only happened when we actually packaged the jar and that isn't done in our java 11 tests.; ; See #7339 #7338. We should add a simple test to make sure something like this doesn't happen again.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7346
https://github.com/broadinstitute/gatk/pull/7347:71,Security,hash,hash,71,"Otherwise, when json is refreshed, contents of the file are different, hash of the file is different, and call-caching will not register a match, despite the same ""account"" being used. - changed input type from `File` to `String`; - changed the name to make it more obvious/clear. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/327",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7347
https://github.com/broadinstitute/gatk/pull/7347:274,Usability,clear,clear,274,"Otherwise, when json is refreshed, contents of the file are different, hash of the file is different, and call-caching will not register a match, despite the same ""account"" being used. - changed input type from `File` to `String`; - changed the name to make it more obvious/clear. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/327",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7347
https://github.com/broadinstitute/gatk/issues/7348:53,Availability,error,error,53,"This user received an ArrayIndexOutofBoundsException error when running GenotypeGVCFs. The user confirmed that the headers of their vcf files and the their fasta files have matching IDs and contig lengths. The user also tried running ValidateVariants and received the following error: A USER ERROR has occurred: Input MA1.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position 1A:3456221 are not observed at all in the sample genotypes. This request was created from a contribution made by Alon Ziv on July 07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:278,Availability,error,error,278,"This user received an ArrayIndexOutofBoundsException error when running GenotypeGVCFs. The user confirmed that the headers of their vcf files and the their fasta files have matching IDs and contig lengths. The user also tried running ValidateVariants and received the following error: A USER ERROR has occurred: Input MA1.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position 1A:3456221 are not observed at all in the sample genotypes. This request was created from a contribution made by Alon Ziv on July 07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:292,Availability,ERROR,ERROR,292,"This user received an ArrayIndexOutofBoundsException error when running GenotypeGVCFs. The user confirmed that the headers of their vcf files and the their fasta files have matching IDs and contig lengths. The user also tried running ValidateVariants and received the following error: A USER ERROR has occurred: Input MA1.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position 1A:3456221 are not observed at all in the sample genotypes. This request was created from a contribution made by Alon Ziv on July 07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:901,Availability,error,error,901,"This user received an ArrayIndexOutofBoundsException error when running GenotypeGVCFs. The user confirmed that the headers of their vcf files and the their fasta files have matching IDs and contig lengths. The user also tried running ValidateVariants and received the following error: A USER ERROR has occurred: Input MA1.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position 1A:3456221 are not observed at all in the sample genotypes. This request was created from a contribution made by Alon Ziv on July 07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:1555,Availability,error,error,1555,"07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/ ; ; 14:28:22.448 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 07, 2021 2:28:22 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:28:22.617 INFO Genoty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:6115,Availability,down,down,6115,"reedingCoeff - InbreedingCoeff will not be calculated at position 1A:219798 and possibly subsequent; at least 10 samples must have called genotypes ; ; 14:28:47.373 INFO ProgressMeter - 1A:568402 0.4 1000 2590.3 ; ; 14:28:57.413 INFO ProgressMeter - 1A:44165059 0.6 255000 460815.6 ; ; 14:29:07.419 INFO ProgressMeter - 1A:78552884 0.7 435000 604040.8 ; ; 14:29:25.201 INFO ProgressMeter - 1A:137636565 1.0 670000 659113.6 ; ; 14:29:35.211 INFO ProgressMeter - 1A:278089494 1.2 994000 839988.2 ; ; 14:29:45.226 INFO ProgressMeter - 1A:317697103 1.4 1162000 860570.8 ; ; 14:30:01.906 INFO ProgressMeter - 1A:363225043 1.6 1347000 827260.1 ; ; 14:30:12.084 INFO ProgressMeter - 1A:441459399 1.8 1676000 932198.7 ; ; 14:30:22.093 INFO ProgressMeter - 1A:466677934 2.0 1835000 933976.9 ; ; 14:30:38.874 INFO ProgressMeter - 1A:495722203 2.2 1996000 889324.5 ; ; 14:30:48.882 INFO ProgressMeter - 1A:536558193 2.4 2320000 962176.5 ; ; 14:30:49.143 INFO GenotypeGVCFs - Shutting down engine ; ; GENOMICSDB\_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),84.09050524498751,Cpu time(s),59.479603645012425 ; ; \[July 7, 2021 2:30:49 PM IDT\] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 2.45 minutes. ; ; Runtime.totalMemory()=12867076096 ; ; java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:142) ; ; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106) ; ; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129) ; ; at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177) ; ; at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:233) ; ; at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.closeTool(GenotypeGVCFs.java:295) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1064) ; ; at org.broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:2154,Performance,Load,Loading,2154,"f -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/ ; ; 14:28:22.448 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 07, 2021 2:28:22 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:28:22.617 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:28:22.618 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:28:22.618 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:28:22.618 INFO GenotypeGVCFs - Executing as alonzi@khalil1 on Linux v4.19.0-17-amd64 amd64 ; ; 14:28:22.618 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_282-b08 ; ; 14:28:22.618 INFO GenotypeGVCFs - Start Date/Time: July 7, 2021 2:28:22 PM IDT ; ; 14:28:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:2471,Safety,detect,detect,2471,"toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/ ; ; 14:28:22.448 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 07, 2021 2:28:22 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:28:22.617 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:28:22.618 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:28:22.618 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:28:22.618 INFO GenotypeGVCFs - Executing as alonzi@khalil1 on Linux v4.19.0-17-amd64 amd64 ; ; 14:28:22.618 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_282-b08 ; ; 14:28:22.618 INFO GenotypeGVCFs - Start Date/Time: July 7, 2021 2:28:22 PM IDT ; ; 14:28:22.618 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:28:22.618 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:28:22.618 INFO GenotypeGVCFs - HTSJDK Version: 2.24.0 ; ; 14:28:22.618 INFO GenotypeGVCFs - Picard Version: 2.25.0 ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:234,Security,Validat,ValidateVariants,234,"This user received an ArrayIndexOutofBoundsException error when running GenotypeGVCFs. The user confirmed that the headers of their vcf files and the their fasta files have matching IDs and contig lengths. The user also tried running ValidateVariants and received the following error: A USER ERROR has occurred: Input MA1.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position 1A:3456221 are not observed at all in the sample genotypes. This request was created from a contribution made by Alon Ziv on July 07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:341,Security,validat,validation,341,"This user received an ArrayIndexOutofBoundsException error when running GenotypeGVCFs. The user confirmed that the headers of their vcf files and the their fasta files have matching IDs and contig lengths. The user also tried running ValidateVariants and received the following error: A USER ERROR has occurred: Input MA1.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position 1A:3456221 are not observed at all in the sample genotypes. This request was created from a contribution made by Alon Ziv on July 07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/issues/7348:1561,Testability,log,log,1561,"07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/ ; ; 14:28:22.448 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 07, 2021 2:28:22 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:28:22.617 INFO Genoty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348
https://github.com/broadinstitute/gatk/pull/7349:69,Deployability,integrat,integration,69,Added ability for user to override to annotate again. Wrote unit and integration tests for new feature and override ability. resolves #5679,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7349
https://github.com/broadinstitute/gatk/pull/7349:69,Integrability,integrat,integration,69,Added ability for user to override to annotate again. Wrote unit and integration tests for new feature and override ability. resolves #5679,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7349
https://github.com/broadinstitute/gatk/pull/7349:81,Testability,test,tests,81,Added ability for user to override to annotate again. Wrote unit and integration tests for new feature and override ability. resolves #5679,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7349
https://github.com/broadinstitute/gatk/pull/7352:233,Availability,error,error,233,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:1196,Availability,Error,Error,1196,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:1219,Availability,Error,Error,1219,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:59,Deployability,release,release,59,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:103,Security,validat,validation,103,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:125,Security,validat,validation,125,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:143,Security,Validat,Validation,143,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:337,Testability,test,tests,337,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:412,Testability,test,test,412,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:442,Testability,test,testName,442,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/pull/7352:979,Testability,test,test,979,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352
https://github.com/broadinstitute/gatk/issues/7353:339,Modifiability,plugin,plugin,339,Although womtool-65.jar indicates syntax is correct for https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl however there are two external sources that say the syntax is incorrect for [line 1036](https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl#L1036). Both the VS Code WDL plugin by Broad indicates this is incorrect syntax (see image). Also DNAnexus's `dxWDL.jar` also gives a `wdlTools.syntax.SyntaxException: invalid place holder at 1036:40-1036:105 in /home/dnanexus/mutect2.wdl`. gatk/scripts/mutect2_wdl/mutect2.wdl; ![mutect2 master wdl  analysis-workflows 2021-07-16 12-25-27](https://user-images.githubusercontent.com/78239029/125986167-4e8b04a5-c594-42a6-ba1c-ff283949d04a.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7353
https://github.com/broadinstitute/gatk/pull/7355:24,Deployability,Update,Update,24,Assign Ids to samples.; Update the sample info table.; Update import wdl to remove the generation of the sample_info.tsv files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7355
https://github.com/broadinstitute/gatk/pull/7355:55,Deployability,Update,Update,55,Assign Ids to samples.; Update the sample info table.; Update import wdl to remove the generation of the sample_info.tsv files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7355
https://github.com/broadinstitute/gatk/issues/7356:423,Usability,simpl,simply,423,Currently the GATK4 version of SplitNCigarReads softclips the overlapping segments of the reads across the split segments. This is a departure from the original GAKT3 behavior which hardclipped the edges. A few discussions have happened where this has confused users since running HaplotypeCaller/Mutect on the results can often result in confusing indels when they try to align the soft-clipped segments. Currently we can simply tell people to ignore soft-clipped bases in those tools but another solution for users who want to call based on split reads would be to add an option to the tool SplitNCigarReads to call to the hard-clipping machinery instead. We would have to be careful that the mate tags are correctly computed based on the hard-clipping.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7356
https://github.com/broadinstitute/gatk/pull/7358:172,Deployability,release,release,172,Looks like a missing set of parentheses caused the logging output for HaplotypeCaller to become unusably flooded with garbage. @droazen we should really get this in before release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7358
https://github.com/broadinstitute/gatk/pull/7358:51,Testability,log,logging,51,Looks like a missing set of parentheses caused the logging output for HaplotypeCaller to become unusably flooded with garbage. @droazen we should really get this in before release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7358
https://github.com/broadinstitute/gatk/pull/7360:378,Security,validat,validation,378,"Closes https://github.com/broadinstitute/dsp-spec-ops/issues/366 by putting into SQL what is in English in that ticket:; > All variants in the region, chr19:35,740,407-35,740,469, overlap transcripts with multiple genes and those genes are always IGFLR1 and AD000671.2. Do not consider rows that include a consequence of downstream_gene_variant or upstream_gene_variant in this validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7360
https://github.com/broadinstitute/gatk/issues/7362:27809,Availability,down,down,27809,"aracter '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; 11:20:40.460 INFO GenomicsDBImport - Done importing batch 1/1 ; ; 11:20:40.463 INFO ProgressMeter - unmapped 0.0 1 30.2 ; ; 11:20:40.463 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 0.0 minutes. ; ; 11:20:40.463 INFO GenomicsDBImport - Import completed! ; ; 11:20:40.464 INFO GenomicsDBImport - Shutting down engine ; ; \[July 13, 2021 11:20:40 AM EDT\] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.09 minutes. ; ; Runtime.totalMemory()=2418016256 ; ; Tool returned: ; ; true<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/169705'>Zendesk ticket #169705</a>)<br>gz#169705</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7362
https://github.com/broadinstitute/gatk/issues/7362:17117,Performance,Load,Loading,17117,"ro741f416.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/ocd4005001236yy3.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/fetal0015D.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq264f96.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq436f163.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/neuro337f175.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq530f196.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq608f226.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq621f231.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq639f237.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/neuro442f249.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq944f346.Roche-M.mutect2.vcf ; ; 11:20:35.249 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ec3408/GATK-4.2.0.0/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 13, 2021 11:20:35 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:20:35.481 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 11:20:35.482 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 11:20:35.482 INFO GenomicsDBImport - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:20:35.482 INFO GenomicsDBImport - Executing as [ec3408@dev2.igm.cumc.columbia.edu](mailto:ec3408@dev2.igm.cumc.columbia.edu) on Linux v3.10.0-957.27.2.el7.x86\_64 amd64 ; ; 11:20:35.482 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_222-b10 ; ; 11:20:35.482 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7362
https://github.com/broadinstitute/gatk/issues/7362:17415,Safety,detect,detect,17415,"jects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq436f163.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/neuro337f175.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq530f196.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq608f226.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq621f231.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq639f237.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/neuro442f249.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq944f346.Roche-M.mutect2.vcf ; ; 11:20:35.249 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ec3408/GATK-4.2.0.0/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 13, 2021 11:20:35 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:20:35.481 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 11:20:35.482 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 11:20:35.482 INFO GenomicsDBImport - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:20:35.482 INFO GenomicsDBImport - Executing as [ec3408@dev2.igm.cumc.columbia.edu](mailto:ec3408@dev2.igm.cumc.columbia.edu) on Linux v3.10.0-957.27.2.el7.x86\_64 amd64 ; ; 11:20:35.482 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_222-b10 ; ; 11:20:35.482 INFO GenomicsDBImport - Start Date/Time: July 13, 2021 11:20:35 AM EDT ; ; 11:20:35.482 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 11:20:35.482 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 11:20:35.483 INFO G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7362
https://github.com/broadinstitute/gatk/issues/7362:88,Security,Validat,ValidateVariants,88,"This user is receiving an empty output file when running GenomicsDBImport. The user ran ValidateVariants on the input files which was successful. . This request was created from a contribution made by Enrico Cocchi on July 14, 2021 10:31 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403568031515-Mutect2-PoN-GenomicsDBImport-creates-empty-DB](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403568031515-Mutect2-PoN-GenomicsDBImport-creates-empty-DB). \--. I am trying to follow GATK 4.2.0 best-practice guidelines for \[Mutect2 PoN creation\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2). I called variants in my samples as recommended with:. gatk Mutect2 \\ ; ; \-R ${REF} \\ ; ; \-L ${EXOME\_INPUT\_INTERVALS} \\ ; ; \-I ${BAM} \\ ; ; \--sequence-dictionary ${DICT} \\ ; ; \--max-mnp-distance 0 \\ ; ; \-O ${SAMPLE\_NAME}.mutect2.vcf. but I see that the tool is unable to create a proper `GenomicsDB` through the \[GenomicsDBImport\](/hc/en-us/articles/360057439331-GenomicsDBImport)command. Even focusing the analysis on a little interval in which I know I have variants in the Mutect2 generated VCFs, here the `SelectVariants` output from one of the VCF I'll use in the `GenomicsDBImport` command:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT XXX ; ; 1 883625 . A G . . AS\_SB\_TABLE=0,0|12,41;DP=54;ECNT=1;MBQ=0,33;MFRL=0,260;MMQ=60,60;MPOS=31;POPAF=7.30;TLOD=182.40 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:0,53:0.981:53:0,26:0,26:0,0,12,41. \`\`. and here the command to generate the DB:. gatk \ ; . \--java-options ""-Djava.io.tmpdir=/nfs/projects/CNV\_WGS/CHIP-PON-DB/TMP-DIR"" \\ ; ; GenomicsDBImport \\ ; ; \-R $REF \\ ; ; \-L 1:883600-883650 \\ ; ; \--genomicsdb-workspace-path $OUT \\ ; ; \--tmp-dir /nfs/projects/CNV\_WGS/CHIP-PON-DB/TMP-DIR \\ ; ; \-V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/fetal0003D.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/fetal0020D.Roche-M.mutect2.vcf -V ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7362
https://github.com/broadinstitute/gatk/issues/7362:9412,Testability,log,logs,9412,"_WGS/Mutetc2-PON-OUT/Roche-M/diagseq608f226.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq621f231.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq639f237.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/neuro442f249.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq944f346.Roche-M.mutect2.vcf. \`\`. I get an EMPTY GenomicsDB!!!. gatk SelectVariants --java-options ""-Djava.io.tmpdir=/nfs/projects/CNV\_WGS/CHIP-PON-DB/TMP-DIR"" \\ ; ; \-R $REF ; ; \-V gendb://$OUT \\ ; ; \-L 1:883600-883650 \\ ; ; \-O output.chr1\_883600\_883650.vcf. ### OUTPUT: empty VCF (only header). Does anybody had this problem before or have a clue why? I'm losing my mind! I also googled and searched around and found similar issues \[like this\](/hc/en-us/community/posts/4403164207515-Created-panel-of-normals-returns-empty-vcf)but none explained why this should happen with my data!. Here the full logs from the GenomicsDBImport command:. Using GATK jar /home/ec3408/GATK-4.2.0.0/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/nfs/projects/CNV\_WGS/CHIP-PON-DB/TMP-DIR -jar /home/ec3408/GATK-4.2.0.0/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenomicsDBImport -R /nfs/seqscratch09/AZ-IPF/reference/hs37d5.fa -L 1:883600-883650 --genomicsdb-workspace-path /nfs/projects/CNV\_WGS/CHIP-PON-DB/ROCHE-M-n84-PON\_DB --tmp-dir /nfs/projects/CNV\_WGS/CHIP-PON-DB/TMP-DIR -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/fetal0003D.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/fetal0020D.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/fetal0008D.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq48f19.Roche-M.mutect2.vcf -V /nfs/projects",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7362
https://github.com/broadinstitute/gatk/issues/7362:539,Usability,guid,guidelines,539,"This user is receiving an empty output file when running GenomicsDBImport. The user ran ValidateVariants on the input files which was successful. . This request was created from a contribution made by Enrico Cocchi on July 14, 2021 10:31 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403568031515-Mutect2-PoN-GenomicsDBImport-creates-empty-DB](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403568031515-Mutect2-PoN-GenomicsDBImport-creates-empty-DB). \--. I am trying to follow GATK 4.2.0 best-practice guidelines for \[Mutect2 PoN creation\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2). I called variants in my samples as recommended with:. gatk Mutect2 \\ ; ; \-R ${REF} \\ ; ; \-L ${EXOME\_INPUT\_INTERVALS} \\ ; ; \-I ${BAM} \\ ; ; \--sequence-dictionary ${DICT} \\ ; ; \--max-mnp-distance 0 \\ ; ; \-O ${SAMPLE\_NAME}.mutect2.vcf. but I see that the tool is unable to create a proper `GenomicsDB` through the \[GenomicsDBImport\](/hc/en-us/articles/360057439331-GenomicsDBImport)command. Even focusing the analysis on a little interval in which I know I have variants in the Mutect2 generated VCFs, here the `SelectVariants` output from one of the VCF I'll use in the `GenomicsDBImport` command:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT XXX ; ; 1 883625 . A G . . AS\_SB\_TABLE=0,0|12,41;DP=54;ECNT=1;MBQ=0,33;MFRL=0,260;MMQ=60,60;MPOS=31;POPAF=7.30;TLOD=182.40 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:0,53:0.981:53:0,26:0,26:0,0,12,41. \`\`. and here the command to generate the DB:. gatk \ ; . \--java-options ""-Djava.io.tmpdir=/nfs/projects/CNV\_WGS/CHIP-PON-DB/TMP-DIR"" \\ ; ; GenomicsDBImport \\ ; ; \-R $REF \\ ; ; \-L 1:883600-883650 \\ ; ; \--genomicsdb-workspace-path $OUT \\ ; ; \--tmp-dir /nfs/projects/CNV\_WGS/CHIP-PON-DB/TMP-DIR \\ ; ; \-V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/fetal0003D.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/fetal0020D.Roche-M.mutect2.vcf -V ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7362
https://github.com/broadinstitute/gatk/issues/7368:225,Deployability,integrat,integration,225,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:996,Deployability,patch,patchwork,996,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:225,Integrability,integrat,integration,225,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:620,Integrability,inject,injectDefaultVerbosity,620,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:620,Security,inject,injectDefaultVerbosity,620,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:100,Testability,log,logging,100,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:237,Testability,test,tests,237,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:258,Testability,log,log,258,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:285,Testability,test,tests,285,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:295,Testability,assert,assert,295,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:667,Testability,log,logging,667,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:749,Testability,assert,assertions,749,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:824,Testability,log,logging,824,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:892,Testability,assert,assertions,892,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:913,Testability,log,logs,913,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:1020,Testability,log,logging,1020,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:1035,Testability,test,tests,1035,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:53,Usability,clear,clear,53,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/issues/7368:1009,Usability,simpl,simply,1009,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368
https://github.com/broadinstitute/gatk/pull/7374:95,Deployability,pipeline,pipeline,95,"Unlike the other validation rules, this does not test the validity of the VAT, but whether the pipeline completed as we expected it to--so I have added this as the singular test that runs during the pipeline. Validation Rule 2: The number of passing variants in GVS matches the number of variants in the VAT. Please note that we are counting the number of variants in GVS, not the number of sites, which may add a difficulty to this task. Another way to phrase it: ""If I were to make a sites only VCF of GVS and split each passing variant into it's own line, that number should equal the number of unique VIDs in the VAT."". Measure number of unique variants in sites only VCF that is generated. We don't want to count filtered variants so we can't count the GVS table. NOTE:. this pr also has some general cleanup as per discussion with Andrea. ; where would y'all suggest I put the template file for the custom annotations?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7374
https://github.com/broadinstitute/gatk/pull/7374:199,Deployability,pipeline,pipeline,199,"Unlike the other validation rules, this does not test the validity of the VAT, but whether the pipeline completed as we expected it to--so I have added this as the singular test that runs during the pipeline. Validation Rule 2: The number of passing variants in GVS matches the number of variants in the VAT. Please note that we are counting the number of variants in GVS, not the number of sites, which may add a difficulty to this task. Another way to phrase it: ""If I were to make a sites only VCF of GVS and split each passing variant into it's own line, that number should equal the number of unique VIDs in the VAT."". Measure number of unique variants in sites only VCF that is generated. We don't want to count filtered variants so we can't count the GVS table. NOTE:. this pr also has some general cleanup as per discussion with Andrea. ; where would y'all suggest I put the template file for the custom annotations?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7374
https://github.com/broadinstitute/gatk/pull/7374:17,Security,validat,validation,17,"Unlike the other validation rules, this does not test the validity of the VAT, but whether the pipeline completed as we expected it to--so I have added this as the singular test that runs during the pipeline. Validation Rule 2: The number of passing variants in GVS matches the number of variants in the VAT. Please note that we are counting the number of variants in GVS, not the number of sites, which may add a difficulty to this task. Another way to phrase it: ""If I were to make a sites only VCF of GVS and split each passing variant into it's own line, that number should equal the number of unique VIDs in the VAT."". Measure number of unique variants in sites only VCF that is generated. We don't want to count filtered variants so we can't count the GVS table. NOTE:. this pr also has some general cleanup as per discussion with Andrea. ; where would y'all suggest I put the template file for the custom annotations?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7374
https://github.com/broadinstitute/gatk/pull/7374:209,Security,Validat,Validation,209,"Unlike the other validation rules, this does not test the validity of the VAT, but whether the pipeline completed as we expected it to--so I have added this as the singular test that runs during the pipeline. Validation Rule 2: The number of passing variants in GVS matches the number of variants in the VAT. Please note that we are counting the number of variants in GVS, not the number of sites, which may add a difficulty to this task. Another way to phrase it: ""If I were to make a sites only VCF of GVS and split each passing variant into it's own line, that number should equal the number of unique VIDs in the VAT."". Measure number of unique variants in sites only VCF that is generated. We don't want to count filtered variants so we can't count the GVS table. NOTE:. this pr also has some general cleanup as per discussion with Andrea. ; where would y'all suggest I put the template file for the custom annotations?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7374
https://github.com/broadinstitute/gatk/pull/7374:49,Testability,test,test,49,"Unlike the other validation rules, this does not test the validity of the VAT, but whether the pipeline completed as we expected it to--so I have added this as the singular test that runs during the pipeline. Validation Rule 2: The number of passing variants in GVS matches the number of variants in the VAT. Please note that we are counting the number of variants in GVS, not the number of sites, which may add a difficulty to this task. Another way to phrase it: ""If I were to make a sites only VCF of GVS and split each passing variant into it's own line, that number should equal the number of unique VIDs in the VAT."". Measure number of unique variants in sites only VCF that is generated. We don't want to count filtered variants so we can't count the GVS table. NOTE:. this pr also has some general cleanup as per discussion with Andrea. ; where would y'all suggest I put the template file for the custom annotations?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7374
https://github.com/broadinstitute/gatk/pull/7374:173,Testability,test,test,173,"Unlike the other validation rules, this does not test the validity of the VAT, but whether the pipeline completed as we expected it to--so I have added this as the singular test that runs during the pipeline. Validation Rule 2: The number of passing variants in GVS matches the number of variants in the VAT. Please note that we are counting the number of variants in GVS, not the number of sites, which may add a difficulty to this task. Another way to phrase it: ""If I were to make a sites only VCF of GVS and split each passing variant into it's own line, that number should equal the number of unique VIDs in the VAT."". Measure number of unique variants in sites only VCF that is generated. We don't want to count filtered variants so we can't count the GVS table. NOTE:. this pr also has some general cleanup as per discussion with Andrea. ; where would y'all suggest I put the template file for the custom annotations?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7374
https://github.com/broadinstitute/gatk/pull/7375:74,Integrability,message,message,74,* The UserException when failing to open a FeatureReader now includes the message from the underlying TribbleException.; * It was previously hard to understand WHY we had failed to open a reader.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7375
https://github.com/broadinstitute/gatk/issues/7376:382,Availability,reliab,reliably,382,"https://github.com/broadinstitute/gatk/blob/b4cba377e0aff179dbff615783506913e7fe3aa4/src/main/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/xsv/LocatableXsvFuncotationFactory.java#L245-L247. Double-Checked Locking is widely cited and used as an efficient method for implementing lazy initialization in a multithreaded environment.; Unfortunately, it will not work reliably in a platform independent way when implemented in Java, without additional synchronization. Modify the variable supportedFieldNames with volatile to tackle the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376
https://github.com/broadinstitute/gatk/issues/7376:263,Energy Efficiency,efficient,efficient,263,"https://github.com/broadinstitute/gatk/blob/b4cba377e0aff179dbff615783506913e7fe3aa4/src/main/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/xsv/LocatableXsvFuncotationFactory.java#L245-L247. Double-Checked Locking is widely cited and used as an efficient method for implementing lazy initialization in a multithreaded environment.; Unfortunately, it will not work reliably in a platform independent way when implemented in Java, without additional synchronization. Modify the variable supportedFieldNames with volatile to tackle the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376
https://github.com/broadinstitute/gatk/issues/7376:466,Integrability,synchroniz,synchronization,466,"https://github.com/broadinstitute/gatk/blob/b4cba377e0aff179dbff615783506913e7fe3aa4/src/main/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/xsv/LocatableXsvFuncotationFactory.java#L245-L247. Double-Checked Locking is widely cited and used as an efficient method for implementing lazy initialization in a multithreaded environment.; Unfortunately, it will not work reliably in a platform independent way when implemented in Java, without additional synchronization. Modify the variable supportedFieldNames with volatile to tackle the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376
https://github.com/broadinstitute/gatk/issues/7376:494,Modifiability,variab,variable,494,"https://github.com/broadinstitute/gatk/blob/b4cba377e0aff179dbff615783506913e7fe3aa4/src/main/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/xsv/LocatableXsvFuncotationFactory.java#L245-L247. Double-Checked Locking is widely cited and used as an efficient method for implementing lazy initialization in a multithreaded environment.; Unfortunately, it will not work reliably in a platform independent way when implemented in Java, without additional synchronization. Modify the variable supportedFieldNames with volatile to tackle the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376
https://github.com/broadinstitute/gatk/issues/7377:198,Deployability,release,release,198,https://github.com/broadinstitute/gatk/blob/b4cba377e0aff179dbff615783506913e7fe3aa4/src/main/java/org/broadinstitute/hellbender/tools/LocalAssembler.java#L1272. The program can potentially fail to release a system resource.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7377
https://github.com/broadinstitute/gatk/issues/7380:8737,Availability,down,down,8737,"with LOD <= -5.0000.; 18:03:57.263 INFO GaussianMixtureModel - Initializing model with 100 k-means iterations...; 18:04:05.276 INFO VariantRecalibratorEngine - Finished iteration 0.; 18:04:07.160 INFO VariantRecalibratorEngine - Finished iteration 5. Current change in mixture coefficients = 0.47495; 18:04:09.021 INFO VariantRecalibratorEngine - Finished iteration 10. Current change in mixture coefficients = 0.07996; 18:04:10.871 INFO VariantRecalibratorEngine - Finished iteration 15. Current change in mixture coefficients = 0.02188; 18:04:12.690 INFO VariantRecalibratorEngine - Finished iteration 20. Current change in mixture coefficients = 0.00815; 18:04:14.555 INFO VariantRecalibratorEngine - Finished iteration 25. Current change in mixture coefficients = 0.00334; 18:04:15.663 INFO VariantRecalibratorEngine - Convergence after 28 iterations!; 18:04:15.938 INFO VariantRecalibratorEngine - Evaluating full set of 3826009 variants...; 18:04:20.008 INFO VariantRecalibrator - Shutting down engine; [July 28, 2021 6:04:20 PM EDT] org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator done. Elapsed time: 7.70 minutes.; Runtime.totalMemory()=105907224576; java.lang.IllegalStateException: Gaussian mean vector does not have the same size as the list of annotations; at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.makeMeansTable(VariantRecalibrator.java:986); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.writeModelReport(VariantRecalibrator.java:887); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:680); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380
https://github.com/broadinstitute/gatk/issues/7380:454,Deployability,install,install,454,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); GATK 4.2.0.0 . ### Description . When running VariantRecalibrator on a joint-called gVCF with 2000 samples, the following java.lang.IllegalStateException occurs: **Gaussian mean vector does not have the same size as the list of annotations**. ```; 17:56:38.072 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 28, 2021 5:56:38 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:56:38.485 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.487 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:56:38.487 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:56:38.488 INFO VariantRecalibrator - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 17:56:38.488 INFO VariantRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 17:56:38.488 INFO VariantRecalibrator - Start Date/Time: July 28, 2021 5:56:38 PM EDT; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.490 INFO VariantRecalibrator - HTSJDK Version: 2.24.0; 17:56:38.491 INFO VariantRecalibrator - Picard Version: 2.25.0; 17:56:38.491 INFO VariantRecalibrator - Built for Spark Version: 2.4.5; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRIT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380
https://github.com/broadinstitute/gatk/issues/7380:10025,Deployability,install,install,10025,".broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.makeMeansTable(VariantRecalibrator.java:986); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.writeModelReport(VariantRecalibrator.java:887); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:680); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms100g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380
https://github.com/broadinstitute/gatk/issues/7380:10326,Deployability,install,install,10326,"or.onTraversalSuccess(VariantRecalibrator.java:680); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms100g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kag",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380
https://github.com/broadinstitute/gatk/issues/7380:10091,Modifiability,variab,variable,10091,".makeMeansTable(VariantRecalibrator.java:986); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.writeModelReport(VariantRecalibrator.java:887); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:680); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms100g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380
https://github.com/broadinstitute/gatk/issues/7380:10517,Modifiability,polymorphi,polymorphic,10517,"eProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms100g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kageproj/gatk/bundle/Homo_sapiens_assembly38.dbsnp138.vcf.gz; ```. #### Steps to reproduce; gatk --java-options -Xms100g VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/ch",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380
https://github.com/broadinstitute/gatk/issues/7380:11605,Modifiability,polymorphi,polymorphic,11605,"he 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kageproj/gatk/bundle/Homo_sapiens_assembly38.dbsnp138.vcf.gz; ```. #### Steps to reproduce; gatk --java-options -Xms100g VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kageproj/gatk/bundle/Homo_sapiens_assembly38.dbsnp138.vcf.gz. The input VCF was generated with the dragen-gatk HaplotypeCaller with Allele Specific annotations. . #### Expected behavi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380
https://github.com/broadinstitute/gatk/issues/7380:384,Performance,Load,Loading,384,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); GATK 4.2.0.0 . ### Description . When running VariantRecalibrator on a joint-called gVCF with 2000 samples, the following java.lang.IllegalStateException occurs: **Gaussian mean vector does not have the same size as the list of annotations**. ```; 17:56:38.072 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 28, 2021 5:56:38 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:56:38.485 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.487 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:56:38.487 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:56:38.488 INFO VariantRecalibrator - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 17:56:38.488 INFO VariantRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 17:56:38.488 INFO VariantRecalibrator - Start Date/Time: July 28, 2021 5:56:38 PM EDT; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.490 INFO VariantRecalibrator - HTSJDK Version: 2.24.0; 17:56:38.491 INFO VariantRecalibrator - Picard Version: 2.25.0; 17:56:38.491 INFO VariantRecalibrator - Built for Spark Version: 2.4.5; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRIT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380
https://github.com/broadinstitute/gatk/issues/7380:671,Safety,detect,detect,671,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); GATK 4.2.0.0 . ### Description . When running VariantRecalibrator on a joint-called gVCF with 2000 samples, the following java.lang.IllegalStateException occurs: **Gaussian mean vector does not have the same size as the list of annotations**. ```; 17:56:38.072 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 28, 2021 5:56:38 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:56:38.485 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.487 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:56:38.487 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:56:38.488 INFO VariantRecalibrator - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 17:56:38.488 INFO VariantRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 17:56:38.488 INFO VariantRecalibrator - Start Date/Time: July 28, 2021 5:56:38 PM EDT; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.490 INFO VariantRecalibrator - HTSJDK Version: 2.24.0; 17:56:38.491 INFO VariantRecalibrator - Picard Version: 2.25.0; 17:56:38.491 INFO VariantRecalibrator - Built for Spark Version: 2.4.5; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRIT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380
https://github.com/broadinstitute/gatk/pull/7381:156,Usability,simpl,simple,156,"This adds the many gnomad subpopulations as columns into the VAT schema, and grabs most of them directly from the nirvana annotations. The max is done as a simple calculation in the python script and retains the order of subpops (for tie-breaking) that Lee specified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7381
https://github.com/broadinstitute/gatk/pull/7382:716,Availability,down,down,716,"Processing an exome takes ~1 minute, which means most of the time is spent on spinning up a VM, pulling docker images, etc. This is not very cost efficient. This PR allows for a `batch_size` to be set and then each task processes that many samples as a unit. The default is `1` which yields the current behavior, but in exomes I have set it to 20 and seen the cost to ingest drop dramatically. The GitHub PR makes it look like a lot has changed but really the changes are:; - a new parameter; - a new task to turn the Array[File] for the VCFs into set of FOFNs (file-of-file-names) similar to how we split up intervals; - a loop in the actual Create TSV task to loop over the files in the FOFNs. For SA mode we copy down each file, and for non-SA mode we rely on the fact that localization is optional and we read them directly anywy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7382
https://github.com/broadinstitute/gatk/pull/7382:146,Energy Efficiency,efficient,efficient,146,"Processing an exome takes ~1 minute, which means most of the time is spent on spinning up a VM, pulling docker images, etc. This is not very cost efficient. This PR allows for a `batch_size` to be set and then each task processes that many samples as a unit. The default is `1` which yields the current behavior, but in exomes I have set it to 20 and seen the cost to ingest drop dramatically. The GitHub PR makes it look like a lot has changed but really the changes are:; - a new parameter; - a new task to turn the Array[File] for the VCFs into set of FOFNs (file-of-file-names) similar to how we split up intervals; - a loop in the actual Create TSV task to loop over the files in the FOFNs. For SA mode we copy down each file, and for non-SA mode we rely on the fact that localization is optional and we read them directly anywy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7382
https://github.com/broadinstitute/gatk/issues/7383:136,Availability,error,error,136,"This request was created from a contribution made by FranBC on July 23, 2021 18:59 UTC.; This user is receiveing a NullPointerException error when running CollectvariantCallingMetrics and has verified that the vcf headers match the reference. The user uploaded their file to the GATK FTP server as ""CollectVariantCallingMetrics_dbsnp155_FranBC2.tar.gz"". Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360074951711-No-results-of-CollectvariantCallingMetrics#community\_comment\_4404056347547](https://gatk.broadinstitute.org/hc/en-us/community/posts/360074951711-No-results-of-CollectvariantCallingMetrics#community_comment_4404056347547). \--. Dear GATK Team,. I am having a similar issue to Yenan, when using CollectVariantCallingMetrics, was a cause/workaround ever found for this?. I have no problem when using this tool with the GRCh38 dbSNP (build 138) vcf file provided in the resource bundle, however whenever I try a different dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383
https://github.com/broadinstitute/gatk/issues/7383:980,Availability,error,error,980,"This request was created from a contribution made by FranBC on July 23, 2021 18:59 UTC.; This user is receiveing a NullPointerException error when running CollectvariantCallingMetrics and has verified that the vcf headers match the reference. The user uploaded their file to the GATK FTP server as ""CollectVariantCallingMetrics_dbsnp155_FranBC2.tar.gz"". Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360074951711-No-results-of-CollectvariantCallingMetrics#community\_comment\_4404056347547](https://gatk.broadinstitute.org/hc/en-us/community/posts/360074951711-No-results-of-CollectvariantCallingMetrics#community_comment_4404056347547). \--. Dear GATK Team,. I am having a similar issue to Yenan, when using CollectVariantCallingMetrics, was a cause/workaround ever found for this?. I have no problem when using this tool with the GRCh38 dbSNP (build 138) vcf file provided in the resource bundle, however whenever I try a different dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383
https://github.com/broadinstitute/gatk/issues/7383:2185,Deployability,release,release,2185,"dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203). at org.broadinstitute.hellbender.Main.main(Main.java:289). As a bit of a background, I am trying to use the latest dbSNP release (build 155, GRCh38,GCF\_000001405.39) and have tried using GATK version 4.1.9.0 and the latest 4.2.0.0, both having the same problem. To prepare the dbSNP file for use with the best practices workflow, I renamed the NCBI chromosome accession numbers to UCSC style names using bcftools annotate, updated the vcf headers usingUpdateVcfSequenceDictionary, and indexed the file usingIndexFeatureFile. The dbSNP file worked well with both HaplotypeCaller and GenotypeGVCFs, with the rsids overlapping perfectly with those obtained when using the dbSNP resource bundle version. Any help with this would be greatly appreciated!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/171466'>Zendesk ticket #171466</a>)<br>gz#171466</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383
https://github.com/broadinstitute/gatk/issues/7383:2490,Deployability,update,updated,2490,"dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203). at org.broadinstitute.hellbender.Main.main(Main.java:289). As a bit of a background, I am trying to use the latest dbSNP release (build 155, GRCh38,GCF\_000001405.39) and have tried using GATK version 4.1.9.0 and the latest 4.2.0.0, both having the same problem. To prepare the dbSNP file for use with the best practices workflow, I renamed the NCBI chromosome accession numbers to UCSC style names using bcftools annotate, updated the vcf headers usingUpdateVcfSequenceDictionary, and indexed the file usingIndexFeatureFile. The dbSNP file worked well with both HaplotypeCaller and GenotypeGVCFs, with the rsids overlapping perfectly with those obtained when using the dbSNP resource bundle version. Any help with this would be greatly appreciated!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/171466'>Zendesk ticket #171466</a>)<br>gz#171466</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383
https://github.com/broadinstitute/gatk/issues/7383:2520,Deployability,Update,UpdateVcfSequenceDictionary,2520,"dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203). at org.broadinstitute.hellbender.Main.main(Main.java:289). As a bit of a background, I am trying to use the latest dbSNP release (build 155, GRCh38,GCF\_000001405.39) and have tried using GATK version 4.1.9.0 and the latest 4.2.0.0, both having the same problem. To prepare the dbSNP file for use with the best practices workflow, I renamed the NCBI chromosome accession numbers to UCSC style names using bcftools annotate, updated the vcf headers usingUpdateVcfSequenceDictionary, and indexed the file usingIndexFeatureFile. The dbSNP file worked well with both HaplotypeCaller and GenotypeGVCFs, with the rsids overlapping perfectly with those obtained when using the dbSNP resource bundle version. Any help with this would be greatly appreciated!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/171466'>Zendesk ticket #171466</a>)<br>gz#171466</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383
https://github.com/broadinstitute/gatk/issues/7383:1506,Performance,load,loadVcf,1506,"atk.broadinstitute.org/hc/en-us/community/posts/360074951711-No-results-of-CollectvariantCallingMetrics#community_comment_4404056347547). \--. Dear GATK Team,. I am having a similar issue to Yenan, when using CollectVariantCallingMetrics, was a cause/workaround ever found for this?. I have no problem when using this tool with the GRCh38 dbSNP (build 138) vcf file provided in the resource bundle, however whenever I try a different dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203). at org.broadinstitute.hellbender.Main.main(Main.java:289). As a bit of a background, I am trying to use the latest dbSNP release (build 155, GRCh38,GCF\_000001405.39) and have tried using GATK version 4.1.9.0 and the latest 4.2.0.0, both having the same problem. To prepare the dbSNP file for use with the best practices workflow, I renamed the NCBI chromosome accession numbers to UCSC style names using bcftools annotate, updated the vcf headers usin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383
https://github.com/broadinstitute/gatk/issues/7383:2426,Security,access,accession,2426,"dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203). at org.broadinstitute.hellbender.Main.main(Main.java:289). As a bit of a background, I am trying to use the latest dbSNP release (build 155, GRCh38,GCF\_000001405.39) and have tried using GATK version 4.1.9.0 and the latest 4.2.0.0, both having the same problem. To prepare the dbSNP file for use with the best practices workflow, I renamed the NCBI chromosome accession numbers to UCSC style names using bcftools annotate, updated the vcf headers usingUpdateVcfSequenceDictionary, and indexed the file usingIndexFeatureFile. The dbSNP file worked well with both HaplotypeCaller and GenotypeGVCFs, with the rsids overlapping perfectly with those obtained when using the dbSNP resource bundle version. Any help with this would be greatly appreciated!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/171466'>Zendesk ticket #171466</a>)<br>gz#171466</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383
https://github.com/broadinstitute/gatk/issues/7385:1761,Availability,error,errors,1761,"encode.v38lift37.annotation.REORDERED.gtf; 14:34:51.448 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 02, 2021 2:34:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.679 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.684 INFO ProgressMeter - Starting traversal; 14:34:51.684 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:34:51.694 INFO IndexFeatureFile - Shutting down engine; [August 2, 2021 at 2:34:51 PM CEST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=113246208; java.lang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385
https://github.com/broadinstitute/gatk/issues/7385:2071,Availability,error,errors,2071,"ngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.679 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.684 INFO ProgressMeter - Starting traversal; 14:34:51.684 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:34:51.694 INFO IndexFeatureFile - Shutting down engine; [August 2, 2021 at 2:34:51 PM CEST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=113246208; java.lang.IllegalArgumentException: Unexpected value: Ensembl_canonical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1391); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:197); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385
https://github.com/broadinstitute/gatk/issues/7385:2450,Availability,down,down,2450,"ing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.679 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.684 INFO ProgressMeter - Starting traversal; 14:34:51.684 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:34:51.694 INFO IndexFeatureFile - Shutting down engine; [August 2, 2021 at 2:34:51 PM CEST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=113246208; java.lang.IllegalArgumentException: Unexpected value: Ensembl_canonical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1391); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:197); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfTranscriptFeature.<init>(GencodeGtfTranscriptFeature.java:19); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfTranscriptFeature.create(GencodeGtfTranscriptFeature.java:23); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$2.create(GencodeGtfFeature.java:768); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.create(GencodeGtfFeature.java:327); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(Ab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385
https://github.com/broadinstitute/gatk/issues/7385:64,Deployability,update,update,64,"As stated in the title. I tried the new gatk version 4.2.1.0 to update the GENCODE data for Funcotator. Log:; /home/robby/Tools/NGS/gatk-4.2.1.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.448 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 02, 2021 2:34:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385
https://github.com/broadinstitute/gatk/issues/7385:711,Performance,Load,Loading,711,"As stated in the title. I tried the new gatk version 4.2.1.0 to update the GENCODE data for Funcotator. Log:; /home/robby/Tools/NGS/gatk-4.2.1.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.448 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 02, 2021 2:34:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385
https://github.com/broadinstitute/gatk/issues/7385:995,Safety,detect,detect,995,"As stated in the title. I tried the new gatk version 4.2.1.0 to update the GENCODE data for Funcotator. Log:; /home/robby/Tools/NGS/gatk-4.2.1.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.448 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 02, 2021 2:34:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385
https://github.com/broadinstitute/gatk/issues/7385:104,Testability,Log,Log,104,"As stated in the title. I tried the new gatk version 4.2.1.0 to update the GENCODE data for Funcotator. Log:; /home/robby/Tools/NGS/gatk-4.2.1.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.448 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 02, 2021 2:34:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385
https://github.com/broadinstitute/gatk/issues/7385:1575,Testability,test,tested,1575,"encode.v38lift37.annotation.REORDERED.gtf; 14:34:51.448 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 02, 2021 2:34:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.679 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.684 INFO ProgressMeter - Starting traversal; 14:34:51.684 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:34:51.694 INFO IndexFeatureFile - Shutting down engine; [August 2, 2021 at 2:34:51 PM CEST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=113246208; java.lang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385
https://github.com/broadinstitute/gatk/issues/7385:1885,Testability,test,tested,1885,"ngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.679 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.684 INFO ProgressMeter - Starting traversal; 14:34:51.684 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:34:51.694 INFO IndexFeatureFile - Shutting down engine; [August 2, 2021 at 2:34:51 PM CEST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=113246208; java.lang.IllegalArgumentException: Unexpected value: Ensembl_canonical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1391); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:197); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385
https://github.com/broadinstitute/gatk/issues/7392:284,Testability,test,tested,284,"Waiting on @meganshand to confirm, but I think the factor of 2 in the denominator of https://github.com/broadinstitute/gatk/blob/796af9fc584929956fda5016d5ab90afd672b822/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ExcessHet.java#L196 should not be there. I've tested a few cases from Table 1 in Wigginton et al. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1199378/, upon which this implementation is based) to confirm this. I also separately (and hastily/inefficiently) reimplemented the non-recurrence form of the expression in python:. ```; import numpy as np; from scipy.special import gammaln, logsumexp. def factorialln(n):; return gammaln(1. + n). def log_prob(n_AB, n_A, N):; n_B = 2 * N - n_A; n_AA = (n_A - n_AB) / 2; n_BB = (n_B - n_AB) / 2; ; return n_AB * np.log(2) + factorialln(N) + factorialln(n_A) + factorialln(n_B) - \; (factorialln(n_AA) + factorialln(n_AB) + factorialln(n_BB) + factorialln(2 * N)). def p_value_high(n_AB, n_A, N):; return np.exp(logsumexp([log_prob(x, n_A, N) for x in range(n_AB, n_A + 2, 2)])). p_value_high(17, 21, 100) # 0.9304235455950692, agrees with Table 1 in Wigginton et al.; ```. The difference is non-negligible. Here's the concordance for data simulated from the null of HWE (for 200 samples x 2000 variants, with a Beta(0.5, 1) alternate-allele frequency spectrum, using pyro):; ![image](https://user-images.githubusercontent.com/11076296/128088837-0e549734-fff7-4d73-8e0d-289e9c0ed674.png). Incidentally, I only realized something might be up when inspecting the distribution of p-values produced by running ExcessHet on this simulated data; there was a weird spike just below and up to 0.5, which I didn't understand. Note that the null p-value distribution isn't expected to be uniform, see https://www.genetics.org/content/180/3/1609.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7392
https://github.com/broadinstitute/gatk/issues/7392:624,Testability,log,logsumexp,624,"Waiting on @meganshand to confirm, but I think the factor of 2 in the denominator of https://github.com/broadinstitute/gatk/blob/796af9fc584929956fda5016d5ab90afd672b822/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ExcessHet.java#L196 should not be there. I've tested a few cases from Table 1 in Wigginton et al. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1199378/, upon which this implementation is based) to confirm this. I also separately (and hastily/inefficiently) reimplemented the non-recurrence form of the expression in python:. ```; import numpy as np; from scipy.special import gammaln, logsumexp. def factorialln(n):; return gammaln(1. + n). def log_prob(n_AB, n_A, N):; n_B = 2 * N - n_A; n_AA = (n_A - n_AB) / 2; n_BB = (n_B - n_AB) / 2; ; return n_AB * np.log(2) + factorialln(N) + factorialln(n_A) + factorialln(n_B) - \; (factorialln(n_AA) + factorialln(n_AB) + factorialln(n_BB) + factorialln(2 * N)). def p_value_high(n_AB, n_A, N):; return np.exp(logsumexp([log_prob(x, n_A, N) for x in range(n_AB, n_A + 2, 2)])). p_value_high(17, 21, 100) # 0.9304235455950692, agrees with Table 1 in Wigginton et al.; ```. The difference is non-negligible. Here's the concordance for data simulated from the null of HWE (for 200 samples x 2000 variants, with a Beta(0.5, 1) alternate-allele frequency spectrum, using pyro):; ![image](https://user-images.githubusercontent.com/11076296/128088837-0e549734-fff7-4d73-8e0d-289e9c0ed674.png). Incidentally, I only realized something might be up when inspecting the distribution of p-values produced by running ExcessHet on this simulated data; there was a weird spike just below and up to 0.5, which I didn't understand. Note that the null p-value distribution isn't expected to be uniform, see https://www.genetics.org/content/180/3/1609.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7392
https://github.com/broadinstitute/gatk/issues/7392:797,Testability,log,log,797,"Waiting on @meganshand to confirm, but I think the factor of 2 in the denominator of https://github.com/broadinstitute/gatk/blob/796af9fc584929956fda5016d5ab90afd672b822/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ExcessHet.java#L196 should not be there. I've tested a few cases from Table 1 in Wigginton et al. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1199378/, upon which this implementation is based) to confirm this. I also separately (and hastily/inefficiently) reimplemented the non-recurrence form of the expression in python:. ```; import numpy as np; from scipy.special import gammaln, logsumexp. def factorialln(n):; return gammaln(1. + n). def log_prob(n_AB, n_A, N):; n_B = 2 * N - n_A; n_AA = (n_A - n_AB) / 2; n_BB = (n_B - n_AB) / 2; ; return n_AB * np.log(2) + factorialln(N) + factorialln(n_A) + factorialln(n_B) - \; (factorialln(n_AA) + factorialln(n_AB) + factorialln(n_BB) + factorialln(2 * N)). def p_value_high(n_AB, n_A, N):; return np.exp(logsumexp([log_prob(x, n_A, N) for x in range(n_AB, n_A + 2, 2)])). p_value_high(17, 21, 100) # 0.9304235455950692, agrees with Table 1 in Wigginton et al.; ```. The difference is non-negligible. Here's the concordance for data simulated from the null of HWE (for 200 samples x 2000 variants, with a Beta(0.5, 1) alternate-allele frequency spectrum, using pyro):; ![image](https://user-images.githubusercontent.com/11076296/128088837-0e549734-fff7-4d73-8e0d-289e9c0ed674.png). Incidentally, I only realized something might be up when inspecting the distribution of p-values produced by running ExcessHet on this simulated data; there was a weird spike just below and up to 0.5, which I didn't understand. Note that the null p-value distribution isn't expected to be uniform, see https://www.genetics.org/content/180/3/1609.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7392
https://github.com/broadinstitute/gatk/issues/7392:993,Testability,log,logsumexp,993,"Waiting on @meganshand to confirm, but I think the factor of 2 in the denominator of https://github.com/broadinstitute/gatk/blob/796af9fc584929956fda5016d5ab90afd672b822/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ExcessHet.java#L196 should not be there. I've tested a few cases from Table 1 in Wigginton et al. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1199378/, upon which this implementation is based) to confirm this. I also separately (and hastily/inefficiently) reimplemented the non-recurrence form of the expression in python:. ```; import numpy as np; from scipy.special import gammaln, logsumexp. def factorialln(n):; return gammaln(1. + n). def log_prob(n_AB, n_A, N):; n_B = 2 * N - n_A; n_AA = (n_A - n_AB) / 2; n_BB = (n_B - n_AB) / 2; ; return n_AB * np.log(2) + factorialln(N) + factorialln(n_A) + factorialln(n_B) - \; (factorialln(n_AA) + factorialln(n_AB) + factorialln(n_BB) + factorialln(2 * N)). def p_value_high(n_AB, n_A, N):; return np.exp(logsumexp([log_prob(x, n_A, N) for x in range(n_AB, n_A + 2, 2)])). p_value_high(17, 21, 100) # 0.9304235455950692, agrees with Table 1 in Wigginton et al.; ```. The difference is non-negligible. Here's the concordance for data simulated from the null of HWE (for 200 samples x 2000 variants, with a Beta(0.5, 1) alternate-allele frequency spectrum, using pyro):; ![image](https://user-images.githubusercontent.com/11076296/128088837-0e549734-fff7-4d73-8e0d-289e9c0ed674.png). Incidentally, I only realized something might be up when inspecting the distribution of p-values produced by running ExcessHet on this simulated data; there was a weird spike just below and up to 0.5, which I didn't understand. Note that the null p-value distribution isn't expected to be uniform, see https://www.genetics.org/content/180/3/1609.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7392
https://github.com/broadinstitute/gatk/pull/7393:78,Testability,test,testing,78,"This PR adds the option to bypass feature reader for GenomicsDBImport. In our testing, this sees about 10-15% speedup, and uses roughly an order of magnitude less memory in the case where vcfs and genomicsdb workspaces are both on local disk. We don't have extensive benchmarking of how this affects GenomicsDBImport in the cloud, but would be interested in exploring that (in conjunction with some of the recent changes for native cloud support). cc: @droazen @lbergelson @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7393
https://github.com/broadinstitute/gatk/pull/7393:267,Testability,benchmark,benchmarking,267,"This PR adds the option to bypass feature reader for GenomicsDBImport. In our testing, this sees about 10-15% speedup, and uses roughly an order of magnitude less memory in the case where vcfs and genomicsdb workspaces are both on local disk. We don't have extensive benchmarking of how this affects GenomicsDBImport in the cloud, but would be interested in exploring that (in conjunction with some of the recent changes for native cloud support). cc: @droazen @lbergelson @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7393
https://github.com/broadinstitute/gatk/pull/7394:133,Deployability,update,updated,133,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7394:170,Deployability,Update,Updated,170,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7394:207,Deployability,integrat,integration,207,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7394:247,Deployability,update,update,247,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7394:254,Deployability,toggle,toggle,254,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7394:207,Integrability,integrat,integration,207,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7394:94,Testability,test,test,94,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7394:160,Testability,test,tests,160,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7394:219,Testability,test,test,219,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7394:43,Usability,undo,undocumented,43,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394
https://github.com/broadinstitute/gatk/pull/7395:131,Availability,error,errors,131,"Update GvsPrepareCallset step so that, instead of creating and inserting all the data into the `_pet_new` table in one step (which errors out with large callsets), add the data in smaller sections that correspond to each `pet_` in the GVS. Closes https://broadworkbench.atlassian.net/browse/VS-48",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7395
https://github.com/broadinstitute/gatk/pull/7395:0,Deployability,Update,Update,0,"Update GvsPrepareCallset step so that, instead of creating and inserting all the data into the `_pet_new` table in one step (which errors out with large callsets), add the data in smaller sections that correspond to each `pet_` in the GVS. Closes https://broadworkbench.atlassian.net/browse/VS-48",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7395
https://github.com/broadinstitute/gatk/issues/7397:63,Availability,down,download,63,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:1202,Availability,error,error,1202,"n-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:3570,Availability,error,error,3570,"nScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python -c import gatktool. Stdout: ; Stderr: Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'gatktool'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:198); when I checked gatktool python package, it is installed in the python packages by conda. after activate gatk4 , I checked with pip install gatktool, and it says the package already installed. Anyone experienced this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:13,Deployability,install,installed,13,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:139,Deployability,release,releases,139,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:236,Deployability,Install,Install-and-use-Conda-for-,236,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:318,Deployability,install,installation,318,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:957,Deployability,Install,Installing,957,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:1021,Deployability,install,installed,1021,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:3400,Deployability,install,installed,3400,"nScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python -c import gatktool. Stdout: ; Stderr: Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'gatktool'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:198); when I checked gatktool python package, it is installed in the python packages by conda. after activate gatk4 , I checked with pip install gatktool, and it says the package already installed. Anyone experienced this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:3485,Deployability,install,install,3485,"nScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python -c import gatktool. Stdout: ; Stderr: Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'gatktool'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:198); when I checked gatktool python package, it is installed in the python packages by conda. after activate gatk4 , I checked with pip install gatktool, and it says the package already installed. Anyone experienced this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:3535,Deployability,install,installed,3535,"nScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python -c import gatktool. Stdout: ; Stderr: Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'gatktool'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:198); when I checked gatktool python package, it is installed in the python packages by conda. after activate gatk4 , I checked with pip install gatktool, and it says the package already installed. Anyone experienced this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7397:835,Performance,cache,cache-,835,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397
https://github.com/broadinstitute/gatk/issues/7398:37,Availability,echo,echo,37,"Generate a toy fastq dataset:; ```; (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""CNGAAAGAAACTAGAGGGGGCCAGGTGCAGTAGCTCACACCTGTAATCTTAGCACTTTGGGAGGCCAAGGCGGGTGGATCATCTGAGGTCAGGAGTTCAAGACCAGCCTGGCCAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:88,Availability,echo,echo,88,"Generate a toy fastq dataset:; ```; (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""CNGAAAGAAACTAGAGGGGGCCAGGTGCAGTAGCTCACACCTGTAATCTTAGCACTTTGGGAGGCCAAGGCGGGTGGATCATCTGAGGTCAGGAGTTCAAGACCAGCCTGGCCAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:247,Availability,echo,echo,247,"Generate a toy fastq dataset:; ```; (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""CNGAAAGAAACTAGAGGGGGCCAGGTGCAGTAGCTCACACCTGTAATCTTAGCACTTTGGGAGGCCAAGGCGGGTGGATCATCTGAGGTCAGGAGTTCAAGACCAGCCTGGCCAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:257,Availability,echo,echo,257,"Generate a toy fastq dataset:; ```; (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""CNGAAAGAAACTAGAGGGGGCCAGGTGCAGTAGCTCACACCTGTAATCTTAGCACTTTGGGAGGCCAAGGCGGGTGGATCATCTGAGGTCAGGAGTTCAAGACCAGCCTGGCCAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:416,Availability,echo,echo,416,"Generate a toy fastq dataset:; ```; (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""CNGAAAGAAACTAGAGGGGGCCAGGTGCAGTAGCTCACACCTGTAATCTTAGCACTTTGGGAGGCCAAGGCGGGTGGATCATCTGAGGTCAGGAGTTCAAGACCAGCCTGGCCAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:467,Availability,echo,echo,467,"Generate a toy fastq dataset:; ```; (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""CNGAAAGAAACTAGAGGGGGCCAGGTGCAGTAGCTCACACCTGTAATCTTAGCACTTTGGGAGGCCAAGGCGGGTGGATCATCTGAGGTCAGGAGTTCAAGACCAGCCTGGCCAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:626,Availability,echo,echo,626,"Generate a toy fastq dataset:; ```; (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""CNGAAAGAAACTAGAGGGGGCCAGGTGCAGTAGCTCACACCTGTAATCTTAGCACTTTGGGAGGCCAAGGCGGGTGGATCATCTGAGGTCAGGAGTTCAAGACCAGCCTGGCCAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:636,Availability,echo,echo,636,"Generate a toy fastq dataset:; ```; (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""CNGAAAGAAACTAGAGGGGGCCAGGTGCAGTAGCTCACACCTGTAATCTTAGCACTTTGGGAGGCCAAGGCGGGTGGATCATCTGAGGTCAGGAGTTCAAGACCAGCCTGGCCAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:821,Availability,echo,echo,821,"CAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:872,Availability,echo,echo,872,"CAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1031,Availability,echo,echo,1031,"CAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1041,Availability,echo,echo,1041,"CAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1200,Availability,echo,echo,1200,"CAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1251,Availability,echo,echo,1251,"CAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1410,Availability,echo,echo,1410,"CAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1420,Availability,echo,echo,1420,"CAACATGTTGAAACCCCGTCTCTATTAAAAATACAAAA""; echo ""+""; echo ""6#A/AEEEEEEEAEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEE<EEEEEEEEAEEEEEEEEEEEEEAEEEAEEEAEEEEEE/EEEEEEEEE/E/E<AEEEEE/6A/<<<EE/AEE/E""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""AGACACATGGCGTTTATCCAGAATTAGCAAATTTATCTTCAGATATGGGATTTTCCTTCTTTTTTTAAATCTTGAGTCTGGCAGCAATTTGTAAAGGCTCATAAAAATCTGAAGCTTACATTTTTTGTCAAGTTACCGATGCTTGTGTCT""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1746,Availability,down,download,1746,"EEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unmerged.bam; ```. Merge unmapped and aligned BAMs:; ```; gatk-4.2.1.0/gatk \",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1805,Availability,echo,echo,1805,"tq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unmerged.bam; ```. Merge unmapped and aligned BAMs:; ```; gatk-4.2.1.0/gatk \; MergeBamAlignment \; -ALIGNED_BAM aligned.unmerged.bam \; -UNMAPPED_BAM unmap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1936,Availability,echo,echo,1936,"CGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unmerged.bam; ```. Merge unmapped and aligned BAMs:; ```; gatk-4.2.1.0/gatk \; MergeBamAlignment \; -ALIGNED_BAM aligned.unmerged.bam \; -UNMAPPED_BAM unmapped.bam \; -OUTPUT aligned.unsorted.bam \; -SORT_ORDER ""unsorted"" \; -REFERENCE_SEQUENCE \; Homo_sapiens_assembly38.fasta; ```. This produces th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:2960,Availability,error,error,2960,"ta--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unmerged.bam; ```. Merge unmapped and aligned BAMs:; ```; gatk-4.2.1.0/gatk \; MergeBamAlignment \; -ALIGNED_BAM aligned.unmerged.bam \; -UNMAPPED_BAM unmapped.bam \; -OUTPUT aligned.unsorted.bam \; -SORT_ORDER ""unsorted"" \; -REFERENCE_SEQUENCE \; Homo_sapiens_assembly38.fasta; ```. This produces the error:; ```; java.lang.IllegalStateException: Aligned record iterator (NB500989:333:HKYJNAFX2:1:11101:10000:1915) is behind the unmapped reads (NB500989:333:HKYJNAFX2:1:11101:24447:1024); 	at picard.sam.AbstractAlignmentMerger.mergeAlignment(AbstractAlignmentMerger.java:557); 	at picard.sam.SamAlignmentMerger.mergeAlignment(SamAlignmentMerger.java:186); 	at picard.sam.MergeBamAlignment.doWork(MergeBamAlignment.java:368); 	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Despite the fact that option `-SORT_ORDER ""unsorted""` is being used and the two BAM files have the reads in the same order:; ```; $ samtools view unmapped.b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1609,Deployability,Install,Install,1609,"EEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1656,Deployability,install,install,1656,"EEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:1737,Deployability,release,releases,1737,"EEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unmerged.bam; ```. Merge unmapped and aligned BAMs:; ```; gatk-4.2.1.0/gatk \",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/issues/7398:4739,Deployability,pipeline,pipelines,4739,"7:1024); 	at picard.sam.AbstractAlignmentMerger.mergeAlignment(AbstractAlignmentMerger.java:557); 	at picard.sam.SamAlignmentMerger.mergeAlignment(SamAlignmentMerger.java:186); 	at picard.sam.MergeBamAlignment.doWork(MergeBamAlignment.java:368); 	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Despite the fact that option `-SORT_ORDER ""unsorted""` is being used and the two BAM files have the reads in the same order:; ```; $ samtools view unmapped.bam | cut -f1; NB500989:333:HKYJNAFX2:1:11101:24447:1024; NB500989:333:HKYJNAFX2:1:11101:24447:1024; NB500989:333:HKYJNAFX2:1:11101:10000:1915; NB500989:333:HKYJNAFX2:1:11101:10000:1915. $ samtools view aligned.unmerged.bam | cut -f1; NB500989:333:HKYJNAFX2:1:11101:24447:1024; NB500989:333:HKYJNAFX2:1:11101:24447:1024; NB500989:333:HKYJNAFX2:1:11101:10000:1915; NB500989:333:HKYJNAFX2:1:11101:10000:1915; ```; Is there a way to run `MergeBamAlignment` letting the tool know that the order of the reads in the input BAMs is the same?. It seems then that the only workaround is to let `FastqToSam` sort the reads, which seems an unnecessary computational step since eventually the reads will be sorted by coordinate anyway. This problem was observed when running together the WDL pipelines [paired-fastq-to-unmapped-bam.wdl](https://github.com/gatk-workflows/seq-format-conversion/blob/master/paired-fastq-to-unmapped-bam.wdl) and [processing-for-variant-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl) where the former runs `FastqToSam` and the latter runs `SamToFastq` and `MergeBamAlignment`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398
https://github.com/broadinstitute/gatk/pull/7399:199,Deployability,Update,Updated,199,"This pr adds the subpopulation AC/AN/AF calculations.; It does this by taking in the ancestry table and making sublists of each---then passing that list of samples into the SelectVariants GATK tool. Updated Lucid chart here: https://lucid.app/lucidchart/fee376a4-4b72-481e-a239-a027f7f6ab1f/edit?page=CsG3hy3S1zEH#. Design Doc for this work:; https://docs.google.com/document/d/1FnPu_Jkz2O9rElApAQld0v6iBEFGe22dKarVWcwNxGI/edit. misc:; how should I add the VAT validation to the VAT pipeline? Should it run automatically?. Anvil data version of this table: spec-ops-aou:anvil_100_for_testing.vat_aug19. <img width=""1379"" alt=""Screen Shot 2021-08-11 at 5 38 22 PM"" src=""https://user-images.githubusercontent.com/6863459/129606564-bfc20a68-119a-4072-88b4-aeaf011cc965.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7399
https://github.com/broadinstitute/gatk/pull/7399:483,Deployability,pipeline,pipeline,483,"This pr adds the subpopulation AC/AN/AF calculations.; It does this by taking in the ancestry table and making sublists of each---then passing that list of samples into the SelectVariants GATK tool. Updated Lucid chart here: https://lucid.app/lucidchart/fee376a4-4b72-481e-a239-a027f7f6ab1f/edit?page=CsG3hy3S1zEH#. Design Doc for this work:; https://docs.google.com/document/d/1FnPu_Jkz2O9rElApAQld0v6iBEFGe22dKarVWcwNxGI/edit. misc:; how should I add the VAT validation to the VAT pipeline? Should it run automatically?. Anvil data version of this table: spec-ops-aou:anvil_100_for_testing.vat_aug19. <img width=""1379"" alt=""Screen Shot 2021-08-11 at 5 38 22 PM"" src=""https://user-images.githubusercontent.com/6863459/129606564-bfc20a68-119a-4072-88b4-aeaf011cc965.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7399
https://github.com/broadinstitute/gatk/pull/7399:461,Security,validat,validation,461,"This pr adds the subpopulation AC/AN/AF calculations.; It does this by taking in the ancestry table and making sublists of each---then passing that list of samples into the SelectVariants GATK tool. Updated Lucid chart here: https://lucid.app/lucidchart/fee376a4-4b72-481e-a239-a027f7f6ab1f/edit?page=CsG3hy3S1zEH#. Design Doc for this work:; https://docs.google.com/document/d/1FnPu_Jkz2O9rElApAQld0v6iBEFGe22dKarVWcwNxGI/edit. misc:; how should I add the VAT validation to the VAT pipeline? Should it run automatically?. Anvil data version of this table: spec-ops-aou:anvil_100_for_testing.vat_aug19. <img width=""1379"" alt=""Screen Shot 2021-08-11 at 5 38 22 PM"" src=""https://user-images.githubusercontent.com/6863459/129606564-bfc20a68-119a-4072-88b4-aeaf011cc965.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7399
https://github.com/broadinstitute/gatk/issues/7401:848,Security,validat,validate,848,"@kcibul reports that if the CNNScoreVariants python code throws an exception during async batch processing, the GATK tool hangs (specifically, it was happening when GATK was sending a . for a missing annotation, and the python code was trying to interpret that as a number and blowing up). It looks like this happens because `StreamingPythonScriptExecutor::waitForPreviousBatchCompletion` waits for the async write thread `Future` to complete first, before checking the fifo for an `ACK`/`NCK` (which is when the exception would be propagated). If the async write thread is blocked because the fifo is full because the python code isn't retrieving data because an exception was thrown, the java side will hang waiting for the `Future` complete. The solution is to reverse the order of the `waitForPreviousBatchCompletion` checking (ack first, then validate that the async write `Future` completes). There is a [branch]( https://github.com/broadinstitute/gatk/tree/cn_async_python_exception) with a test and a fix for the StreamingPythonExecutor, and a [separate branch](https://github.com/broadinstitute/gatk/tree/cn_cnn_exception) with a test for CNNScoreVariants that also has the executor fix. I need to verify that the CNNSCoreVariants test actually fails without the fix, and then this can be turned into a PR, which I'll do when I return from vacation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7401
https://github.com/broadinstitute/gatk/issues/7401:998,Testability,test,test,998,"@kcibul reports that if the CNNScoreVariants python code throws an exception during async batch processing, the GATK tool hangs (specifically, it was happening when GATK was sending a . for a missing annotation, and the python code was trying to interpret that as a number and blowing up). It looks like this happens because `StreamingPythonScriptExecutor::waitForPreviousBatchCompletion` waits for the async write thread `Future` to complete first, before checking the fifo for an `ACK`/`NCK` (which is when the exception would be propagated). If the async write thread is blocked because the fifo is full because the python code isn't retrieving data because an exception was thrown, the java side will hang waiting for the `Future` complete. The solution is to reverse the order of the `waitForPreviousBatchCompletion` checking (ack first, then validate that the async write `Future` completes). There is a [branch]( https://github.com/broadinstitute/gatk/tree/cn_async_python_exception) with a test and a fix for the StreamingPythonExecutor, and a [separate branch](https://github.com/broadinstitute/gatk/tree/cn_cnn_exception) with a test for CNNScoreVariants that also has the executor fix. I need to verify that the CNNSCoreVariants test actually fails without the fix, and then this can be turned into a PR, which I'll do when I return from vacation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7401
https://github.com/broadinstitute/gatk/issues/7401:1139,Testability,test,test,1139,"@kcibul reports that if the CNNScoreVariants python code throws an exception during async batch processing, the GATK tool hangs (specifically, it was happening when GATK was sending a . for a missing annotation, and the python code was trying to interpret that as a number and blowing up). It looks like this happens because `StreamingPythonScriptExecutor::waitForPreviousBatchCompletion` waits for the async write thread `Future` to complete first, before checking the fifo for an `ACK`/`NCK` (which is when the exception would be propagated). If the async write thread is blocked because the fifo is full because the python code isn't retrieving data because an exception was thrown, the java side will hang waiting for the `Future` complete. The solution is to reverse the order of the `waitForPreviousBatchCompletion` checking (ack first, then validate that the async write `Future` completes). There is a [branch]( https://github.com/broadinstitute/gatk/tree/cn_async_python_exception) with a test and a fix for the StreamingPythonExecutor, and a [separate branch](https://github.com/broadinstitute/gatk/tree/cn_cnn_exception) with a test for CNNScoreVariants that also has the executor fix. I need to verify that the CNNSCoreVariants test actually fails without the fix, and then this can be turned into a PR, which I'll do when I return from vacation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7401
https://github.com/broadinstitute/gatk/issues/7401:1240,Testability,test,test,1240,"@kcibul reports that if the CNNScoreVariants python code throws an exception during async batch processing, the GATK tool hangs (specifically, it was happening when GATK was sending a . for a missing annotation, and the python code was trying to interpret that as a number and blowing up). It looks like this happens because `StreamingPythonScriptExecutor::waitForPreviousBatchCompletion` waits for the async write thread `Future` to complete first, before checking the fifo for an `ACK`/`NCK` (which is when the exception would be propagated). If the async write thread is blocked because the fifo is full because the python code isn't retrieving data because an exception was thrown, the java side will hang waiting for the `Future` complete. The solution is to reverse the order of the `waitForPreviousBatchCompletion` checking (ack first, then validate that the async write `Future` completes). There is a [branch]( https://github.com/broadinstitute/gatk/tree/cn_async_python_exception) with a test and a fix for the StreamingPythonExecutor, and a [separate branch](https://github.com/broadinstitute/gatk/tree/cn_cnn_exception) with a test for CNNScoreVariants that also has the executor fix. I need to verify that the CNNSCoreVariants test actually fails without the fix, and then this can be turned into a PR, which I'll do when I return from vacation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7401
https://github.com/broadinstitute/gatk/issues/7403:103,Testability,test,test-logs,103,"Not sure what's going on here, but thought I'd document. See https://storage.googleapis.com/hellbender-test-logs/build_reports/master_35408.2/tests/test/classes/org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModelIntegrationTest.html#testDragstrModelInferenceFailingOverToDefaults[0](1,%20false)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403
https://github.com/broadinstitute/gatk/issues/7403:142,Testability,test,tests,142,"Not sure what's going on here, but thought I'd document. See https://storage.googleapis.com/hellbender-test-logs/build_reports/master_35408.2/tests/test/classes/org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModelIntegrationTest.html#testDragstrModelInferenceFailingOverToDefaults[0](1,%20false)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403
https://github.com/broadinstitute/gatk/issues/7403:148,Testability,test,test,148,"Not sure what's going on here, but thought I'd document. See https://storage.googleapis.com/hellbender-test-logs/build_reports/master_35408.2/tests/test/classes/org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModelIntegrationTest.html#testDragstrModelInferenceFailingOverToDefaults[0](1,%20false)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403
https://github.com/broadinstitute/gatk/issues/7403:247,Testability,test,testDragstrModelInferenceFailingOverToDefaults,247,"Not sure what's going on here, but thought I'd document. See https://storage.googleapis.com/hellbender-test-logs/build_reports/master_35408.2/tests/test/classes/org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModelIntegrationTest.html#testDragstrModelInferenceFailingOverToDefaults[0](1,%20false)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403
https://github.com/broadinstitute/gatk/issues/7406:43,Availability,error,error,43,"This user is receiving a memory allocation error when running MarkDuplicatesSpark. The user has tried limiting the number of executors and raising and lowering the memory allocations but continues to get the same error. . This request was created from a contribution made by Udi L on August 04, 2021 07:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360058452072-MarkDuplicatesSpark-consumes-enormous-amount-of-RAM#community\_comment\_4404649417755](https://gatk.broadinstitute.org/hc/en-us/community/posts/360058452072-MarkDuplicatesSpark-consumes-enormous-amount-of-RAM#community_comment_4404649417755). \--. Hi,. Did you figure it?. I have the same problem. I am using --java-options ""-Xmx80G""<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/173588'>Zendesk ticket #173588</a>)<br>gz#173588</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7406
https://github.com/broadinstitute/gatk/issues/7406:213,Availability,error,error,213,"This user is receiving a memory allocation error when running MarkDuplicatesSpark. The user has tried limiting the number of executors and raising and lowering the memory allocations but continues to get the same error. . This request was created from a contribution made by Udi L on August 04, 2021 07:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360058452072-MarkDuplicatesSpark-consumes-enormous-amount-of-RAM#community\_comment\_4404649417755](https://gatk.broadinstitute.org/hc/en-us/community/posts/360058452072-MarkDuplicatesSpark-consumes-enormous-amount-of-RAM#community_comment_4404649417755). \--. Hi,. Did you figure it?. I have the same problem. I am using --java-options ""-Xmx80G""<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/173588'>Zendesk ticket #173588</a>)<br>gz#173588</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7406
https://github.com/broadinstitute/gatk/pull/7407:10,Performance,load,loading,10,no longer loading sample info table in this wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7407
https://github.com/broadinstitute/gatk/issues/7408:950,Availability,avail,available,950,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [x] Latest public release version [4.2.1.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When processing a VCF with tumor and matched normal into a MAF, the `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` fields are not populated. #### Steps to reproduce. `gatk --java-options -Xmx2048m Funcotator --data-sources-path /cromwell_root/datasources_dir --ref-version hg38 --output-file-format MAF -R /cromwell_root/getzlab-workflows-reference_files-oa/hg38/gdc/GRCh38.d1.vd1.fa -V` [`C3N-02729.vcf.gz`](https://github.com/broadinstitute/gatk/files/6977700/C3N-02729.vcf.gz) `-O C3N-02729.maf --annotation-default normal_barcode:C3N-02729_N --annotation-default tumor_barcode:C3N-02729_T --annotation-default Center:broadinstitute.org --annotation-default source:Unknown --transcript-selection-mode BEST_EFFECT`. (NOTE: reference files available at `gs://getzlab-workflows-reference_files-oa/hg38/gdc`). #### Expected behavior; `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` fields should be populated as appropriate based on the `GT` field for the matched normal in the VCF. #### Actual behavior; `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` are populated with `__UNKNOWN__`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7408
https://github.com/broadinstitute/gatk/issues/7408:107,Deployability,release,release,107,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [x] Latest public release version [4.2.1.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When processing a VCF with tumor and matched normal into a MAF, the `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` fields are not populated. #### Steps to reproduce. `gatk --java-options -Xmx2048m Funcotator --data-sources-path /cromwell_root/datasources_dir --ref-version hg38 --output-file-format MAF -R /cromwell_root/getzlab-workflows-reference_files-oa/hg38/gdc/GRCh38.d1.vd1.fa -V` [`C3N-02729.vcf.gz`](https://github.com/broadinstitute/gatk/files/6977700/C3N-02729.vcf.gz) `-O C3N-02729.maf --annotation-default normal_barcode:C3N-02729_N --annotation-default tumor_barcode:C3N-02729_T --annotation-default Center:broadinstitute.org --annotation-default source:Unknown --transcript-selection-mode BEST_EFFECT`. (NOTE: reference files available at `gs://getzlab-workflows-reference_files-oa/hg38/gdc`). #### Expected behavior; `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` fields should be populated as appropriate based on the `GT` field for the matched normal in the VCF. #### Actual behavior; `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` are populated with `__UNKNOWN__`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7408
https://github.com/broadinstitute/gatk/issues/7408:176,Testability,test,test,176,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [x] Latest public release version [4.2.1.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When processing a VCF with tumor and matched normal into a MAF, the `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` fields are not populated. #### Steps to reproduce. `gatk --java-options -Xmx2048m Funcotator --data-sources-path /cromwell_root/datasources_dir --ref-version hg38 --output-file-format MAF -R /cromwell_root/getzlab-workflows-reference_files-oa/hg38/gdc/GRCh38.d1.vd1.fa -V` [`C3N-02729.vcf.gz`](https://github.com/broadinstitute/gatk/files/6977700/C3N-02729.vcf.gz) `-O C3N-02729.maf --annotation-default normal_barcode:C3N-02729_N --annotation-default tumor_barcode:C3N-02729_T --annotation-default Center:broadinstitute.org --annotation-default source:Unknown --transcript-selection-mode BEST_EFFECT`. (NOTE: reference files available at `gs://getzlab-workflows-reference_files-oa/hg38/gdc`). #### Expected behavior; `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` fields should be populated as appropriate based on the `GT` field for the matched normal in the VCF. #### Actual behavior; `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` are populated with `__UNKNOWN__`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7408
https://github.com/broadinstitute/gatk/issues/7410:237,Availability,error,errors,237,"## Bug Report. ### GermlineCNVCaller. ### Affected version(s); - [ ] (GATK) v4.2.1.0. ### Description ; Java exception raised when aggregating counts for samples with a name shorter than 3 characters. For example, in the pasted logs, it errors out when processing sample with interval read counts in `94.mkdup.sort.rg.tsv`. And I found removing the sample from the list of files would avoid the error. . ### Logs:. `; 06:49:05.526 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/713.mkdup.sort.rg.tsv (40 / 323); 06:49:07.999 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/252.mkdup.sort.rg.tsv (41 / 323); 06:49:10.433 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/547.mkdup.sort.rg.tsv (42 / 323); 06:49:13.341 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/154.mkdup.sort.rg.tsv (43 / 323); 06:49:15.782 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/651.mkdup.sort.rg.tsv (44 / 323); 06:49:18.251 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/94.mkdup.sort.rg.tsv (45 / 323); 06:49:20.605 INFO GermlineCNVCaller - Shutting down engine; [August 13, 2021 6:49:20 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.27 minutes.; Runtime.totalMemory()=2076049408; java.lang.IllegalArgumentException: Prefix string too short; at java.io.File.createTempFile(File.java:2001); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7410
https://github.com/broadinstitute/gatk/issues/7410:395,Availability,error,error,395,"## Bug Report. ### GermlineCNVCaller. ### Affected version(s); - [ ] (GATK) v4.2.1.0. ### Description ; Java exception raised when aggregating counts for samples with a name shorter than 3 characters. For example, in the pasted logs, it errors out when processing sample with interval read counts in `94.mkdup.sort.rg.tsv`. And I found removing the sample from the list of files would avoid the error. . ### Logs:. `; 06:49:05.526 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/713.mkdup.sort.rg.tsv (40 / 323); 06:49:07.999 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/252.mkdup.sort.rg.tsv (41 / 323); 06:49:10.433 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/547.mkdup.sort.rg.tsv (42 / 323); 06:49:13.341 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/154.mkdup.sort.rg.tsv (43 / 323); 06:49:15.782 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/651.mkdup.sort.rg.tsv (44 / 323); 06:49:18.251 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/94.mkdup.sort.rg.tsv (45 / 323); 06:49:20.605 INFO GermlineCNVCaller - Shutting down engine; [August 13, 2021 6:49:20 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.27 minutes.; Runtime.totalMemory()=2076049408; java.lang.IllegalArgumentException: Prefix string too short; at java.io.File.createTempFile(File.java:2001); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7410
https://github.com/broadinstitute/gatk/issues/7410:1424,Availability,down,down,1424," INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/713.mkdup.sort.rg.tsv (40 / 323); 06:49:07.999 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/252.mkdup.sort.rg.tsv (41 / 323); 06:49:10.433 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/547.mkdup.sort.rg.tsv (42 / 323); 06:49:13.341 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/154.mkdup.sort.rg.tsv (43 / 323); 06:49:15.782 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/651.mkdup.sort.rg.tsv (44 / 323); 06:49:18.251 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/94.mkdup.sort.rg.tsv (45 / 323); 06:49:20.605 INFO GermlineCNVCaller - Shutting down engine; [August 13, 2021 6:49:20 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.27 minutes.; Runtime.totalMemory()=2076049408; java.lang.IllegalArgumentException: Prefix string too short; at java.io.File.createTempFile(File.java:2001); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$1(GermlineCNVCaller.java:434); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.Abs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7410
https://github.com/broadinstitute/gatk/issues/7410:2518,Integrability,wrap,wrapAndCopyInto,2518,"nt_tsv/94.mkdup.sort.rg.tsv (45 / 323); 06:49:20.605 INFO GermlineCNVCaller - Shutting down engine; [August 13, 2021 6:49:20 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.27 minutes.; Runtime.totalMemory()=2076049408; java.lang.IllegalArgumentException: Prefix string too short; at java.io.File.createTempFile(File.java:2001); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$1(GermlineCNVCaller.java:434); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.writeIntervalSubsetReadCountFiles(GermlineCNVCaller.java:433); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:323); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192). `; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7410
https://github.com/broadinstitute/gatk/issues/7410:385,Safety,avoid,avoid,385,"## Bug Report. ### GermlineCNVCaller. ### Affected version(s); - [ ] (GATK) v4.2.1.0. ### Description ; Java exception raised when aggregating counts for samples with a name shorter than 3 characters. For example, in the pasted logs, it errors out when processing sample with interval read counts in `94.mkdup.sort.rg.tsv`. And I found removing the sample from the list of files would avoid the error. . ### Logs:. `; 06:49:05.526 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/713.mkdup.sort.rg.tsv (40 / 323); 06:49:07.999 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/252.mkdup.sort.rg.tsv (41 / 323); 06:49:10.433 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/547.mkdup.sort.rg.tsv (42 / 323); 06:49:13.341 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/154.mkdup.sort.rg.tsv (43 / 323); 06:49:15.782 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/651.mkdup.sort.rg.tsv (44 / 323); 06:49:18.251 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/94.mkdup.sort.rg.tsv (45 / 323); 06:49:20.605 INFO GermlineCNVCaller - Shutting down engine; [August 13, 2021 6:49:20 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.27 minutes.; Runtime.totalMemory()=2076049408; java.lang.IllegalArgumentException: Prefix string too short; at java.io.File.createTempFile(File.java:2001); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7410
https://github.com/broadinstitute/gatk/issues/7410:228,Testability,log,logs,228,"## Bug Report. ### GermlineCNVCaller. ### Affected version(s); - [ ] (GATK) v4.2.1.0. ### Description ; Java exception raised when aggregating counts for samples with a name shorter than 3 characters. For example, in the pasted logs, it errors out when processing sample with interval read counts in `94.mkdup.sort.rg.tsv`. And I found removing the sample from the list of files would avoid the error. . ### Logs:. `; 06:49:05.526 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/713.mkdup.sort.rg.tsv (40 / 323); 06:49:07.999 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/252.mkdup.sort.rg.tsv (41 / 323); 06:49:10.433 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/547.mkdup.sort.rg.tsv (42 / 323); 06:49:13.341 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/154.mkdup.sort.rg.tsv (43 / 323); 06:49:15.782 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/651.mkdup.sort.rg.tsv (44 / 323); 06:49:18.251 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/94.mkdup.sort.rg.tsv (45 / 323); 06:49:20.605 INFO GermlineCNVCaller - Shutting down engine; [August 13, 2021 6:49:20 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.27 minutes.; Runtime.totalMemory()=2076049408; java.lang.IllegalArgumentException: Prefix string too short; at java.io.File.createTempFile(File.java:2001); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7410
https://github.com/broadinstitute/gatk/issues/7410:408,Testability,Log,Logs,408,"## Bug Report. ### GermlineCNVCaller. ### Affected version(s); - [ ] (GATK) v4.2.1.0. ### Description ; Java exception raised when aggregating counts for samples with a name shorter than 3 characters. For example, in the pasted logs, it errors out when processing sample with interval read counts in `94.mkdup.sort.rg.tsv`. And I found removing the sample from the list of files would avoid the error. . ### Logs:. `; 06:49:05.526 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/713.mkdup.sort.rg.tsv (40 / 323); 06:49:07.999 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/252.mkdup.sort.rg.tsv (41 / 323); 06:49:10.433 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/547.mkdup.sort.rg.tsv (42 / 323); 06:49:13.341 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/154.mkdup.sort.rg.tsv (43 / 323); 06:49:15.782 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/651.mkdup.sort.rg.tsv (44 / 323); 06:49:18.251 INFO GermlineCNVCaller - Aggregating read-count file output/gCNV/bulkDNAseq_BGI/F20FTSAPHT0350_MUSyfqR/count_tsv/94.mkdup.sort.rg.tsv (45 / 323); 06:49:20.605 INFO GermlineCNVCaller - Shutting down engine; [August 13, 2021 6:49:20 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.27 minutes.; Runtime.totalMemory()=2076049408; java.lang.IllegalArgumentException: Prefix string too short; at java.io.File.createTempFile(File.java:2001); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFileInDirectory(IOUtils.java:685); at org.broadinstitute.hellbender.utils.io.IOUtils.createTempFile(IOUtils.java:666); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.lambda$writeIntervalSubsetReadCountFiles$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7410
https://github.com/broadinstitute/gatk/pull/7414:93,Deployability,pipeline,pipeline,93,Takes in a WGS bam or cram and outputs VCF of SNP/Indel calls on the mitochondria. Note this pipeline does not perform any realignment and just uses read-pairs that map to chrM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7414
https://github.com/broadinstitute/gatk/pull/7414:111,Performance,perform,perform,111,Takes in a WGS bam or cram and outputs VCF of SNP/Indel calls on the mitochondria. Note this pipeline does not perform any realignment and just uses read-pairs that map to chrM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7414
https://github.com/broadinstitute/gatk/pull/7415:208,Availability,down,download,208,"According to [https://docs.travis-ci.com/user/customizing-the-build#git-clone-depth](url), Travis CI provide a way to shallow clone a repository. This has the obvious benefit of speed, since you only need to download a small number of commits.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7415
https://github.com/broadinstitute/gatk/pull/7416:116,Availability,error,error,116,"add ""id_loaded"" column to schema; add ""partition_id != '__UNPARTITIONED__'"" to query so we don't get int conversion error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7416
https://github.com/broadinstitute/gatk/pull/7418:241,Modifiability,extend,extend,241,"According to [Build times out because no output was received](https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received), we should carefully use travis_wait, as it may make the build unstable and extend the build time. =====================; If there are any inappropriate modifications in this PR, please give me a reply and I will change them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7418
https://github.com/broadinstitute/gatk/pull/7422:141,Security,validat,validated,141,- Added code to populate `Match_Norm_Seq_Allele1` and 2.; - Added two samples to `regressionTestVariantSetHG38.vcf` file.; - Regenerated and validated expected outputs for large tests. Fixes #7408,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7422
https://github.com/broadinstitute/gatk/pull/7422:178,Testability,test,tests,178,- Added code to populate `Match_Norm_Seq_Allele1` and 2.; - Added two samples to `regressionTestVariantSetHG38.vcf` file.; - Regenerated and validated expected outputs for large tests. Fixes #7408,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7422
https://github.com/broadinstitute/gatk/issues/7424:436,Availability,avail,available,436,"Funcotator produces MAF files in accordance with `TCGA MAF Spec v2.4.1` ([TCGA_MAF_SPEC_2_4_1.tar.gz](https://github.com/broadinstitute/gatk/files/7008573/TCGA_MAF_SPEC_2_4_1.tar.gz)). The current public MAF format is [`GDC MAF v1.0.0`](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/). . All our documentation must be updated to reflect this. Worse, the TCGA MAF 2.4.1 Spec is now locked behind a login screen and may not be available at all. When I sent an email about this, I was told it would change soon. That was over 2 years ago.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7424
https://github.com/broadinstitute/gatk/issues/7424:329,Deployability,update,updated,329,"Funcotator produces MAF files in accordance with `TCGA MAF Spec v2.4.1` ([TCGA_MAF_SPEC_2_4_1.tar.gz](https://github.com/broadinstitute/gatk/files/7008573/TCGA_MAF_SPEC_2_4_1.tar.gz)). The current public MAF format is [`GDC MAF v1.0.0`](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/). . All our documentation must be updated to reflect this. Worse, the TCGA MAF 2.4.1 Spec is now locked behind a login screen and may not be available at all. When I sent an email about this, I was told it would change soon. That was over 2 years ago.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7424
https://github.com/broadinstitute/gatk/issues/7424:408,Testability,log,login,408,"Funcotator produces MAF files in accordance with `TCGA MAF Spec v2.4.1` ([TCGA_MAF_SPEC_2_4_1.tar.gz](https://github.com/broadinstitute/gatk/files/7008573/TCGA_MAF_SPEC_2_4_1.tar.gz)). The current public MAF format is [`GDC MAF v1.0.0`](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/). . All our documentation must be updated to reflect this. Worse, the TCGA MAF 2.4.1 Spec is now locked behind a login screen and may not be available at all. When I sent an email about this, I was told it would change soon. That was over 2 years ago.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7424
https://github.com/broadinstitute/gatk/issues/7425:105,Modifiability,plugin,plugin,105,"Let's see if we can get Azure Blob support working in GATK using the existing azure-storage-blob-nio NIO plugin:. https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/storage/azure-storage-blob-nio. https://search.maven.org/artifact/com.azure/azure-storage-blob-nio/12.0.0-beta.8/jar. Currently auth info needs to be manually passed in, unlike with the Google NIO plugin, but I've opened a feature request to get the auth info automatically from the environment:. https://github.com/Azure/azure-sdk-for-java/issues/23653",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7425
https://github.com/broadinstitute/gatk/issues/7425:369,Modifiability,plugin,plugin,369,"Let's see if we can get Azure Blob support working in GATK using the existing azure-storage-blob-nio NIO plugin:. https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/storage/azure-storage-blob-nio. https://search.maven.org/artifact/com.azure/azure-storage-blob-nio/12.0.0-beta.8/jar. Currently auth info needs to be manually passed in, unlike with the Google NIO plugin, but I've opened a feature request to get the auth info automatically from the environment:. https://github.com/Azure/azure-sdk-for-java/issues/23653",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7425
https://github.com/broadinstitute/gatk/pull/7432:78,Safety,detect,detection,78,This is the beta version of the pileup based haplotype generation and variant detection code.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7432
https://github.com/broadinstitute/gatk/issues/7433:464,Availability,mask,mask,464,I use these steps to produce variant calls for haploid data:; HaplotypeCaller in GVCF mode -> CombineGVCFs -> GenotypeGVCFs; and I then create a snpmask file by adding to it any sites I don't want and keep all called SNPs and INDELs inside per sample vcf files. I then try to create a consensus sequence by running in GATK v 4.2.1.0:. ```; gatk FastaAlternateReferenceMaker \; -R myref.fasta \; -V persamplevariants.vcf \; -O new.fasta \; --line-width 80 \; --snp-mask mask.vcf \; --snp-mask-priority ; ```. But I get this error:. ```; java.lang.IllegalArgumentException: Illegal base [ ] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:251); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:402); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.lambda$handlePosition$0(FastaAlternateReferenceMaker.java:176); 	at java.util.Optional.orElseGet(Optional.java:267); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.handlePosition(FastaAlternateReferenceMaker.java:176); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.apply(FastaAlternateReferenceMaker.java:141); 	at org.broadinstitute.hellbender.engine.ReferenceWalker.traverse(ReferenceWalker.java:55); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. By narrowing down on where this happens I find it happens here:. ```; chrom	16798	.	TAGC	*	41.94,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433
https://github.com/broadinstitute/gatk/issues/7433:469,Availability,mask,mask,469,I use these steps to produce variant calls for haploid data:; HaplotypeCaller in GVCF mode -> CombineGVCFs -> GenotypeGVCFs; and I then create a snpmask file by adding to it any sites I don't want and keep all called SNPs and INDELs inside per sample vcf files. I then try to create a consensus sequence by running in GATK v 4.2.1.0:. ```; gatk FastaAlternateReferenceMaker \; -R myref.fasta \; -V persamplevariants.vcf \; -O new.fasta \; --line-width 80 \; --snp-mask mask.vcf \; --snp-mask-priority ; ```. But I get this error:. ```; java.lang.IllegalArgumentException: Illegal base [ ] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:251); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:402); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.lambda$handlePosition$0(FastaAlternateReferenceMaker.java:176); 	at java.util.Optional.orElseGet(Optional.java:267); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.handlePosition(FastaAlternateReferenceMaker.java:176); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.apply(FastaAlternateReferenceMaker.java:141); 	at org.broadinstitute.hellbender.engine.ReferenceWalker.traverse(ReferenceWalker.java:55); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. By narrowing down on where this happens I find it happens here:. ```; chrom	16798	.	TAGC	*	41.94,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433
https://github.com/broadinstitute/gatk/issues/7433:487,Availability,mask,mask-priority,487,I use these steps to produce variant calls for haploid data:; HaplotypeCaller in GVCF mode -> CombineGVCFs -> GenotypeGVCFs; and I then create a snpmask file by adding to it any sites I don't want and keep all called SNPs and INDELs inside per sample vcf files. I then try to create a consensus sequence by running in GATK v 4.2.1.0:. ```; gatk FastaAlternateReferenceMaker \; -R myref.fasta \; -V persamplevariants.vcf \; -O new.fasta \; --line-width 80 \; --snp-mask mask.vcf \; --snp-mask-priority ; ```. But I get this error:. ```; java.lang.IllegalArgumentException: Illegal base [ ] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:251); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:402); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.lambda$handlePosition$0(FastaAlternateReferenceMaker.java:176); 	at java.util.Optional.orElseGet(Optional.java:267); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.handlePosition(FastaAlternateReferenceMaker.java:176); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.apply(FastaAlternateReferenceMaker.java:141); 	at org.broadinstitute.hellbender.engine.ReferenceWalker.traverse(ReferenceWalker.java:55); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. By narrowing down on where this happens I find it happens here:. ```; chrom	16798	.	TAGC	*	41.94,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433
https://github.com/broadinstitute/gatk/issues/7433:523,Availability,error,error,523,I use these steps to produce variant calls for haploid data:; HaplotypeCaller in GVCF mode -> CombineGVCFs -> GenotypeGVCFs; and I then create a snpmask file by adding to it any sites I don't want and keep all called SNPs and INDELs inside per sample vcf files. I then try to create a consensus sequence by running in GATK v 4.2.1.0:. ```; gatk FastaAlternateReferenceMaker \; -R myref.fasta \; -V persamplevariants.vcf \; -O new.fasta \; --line-width 80 \; --snp-mask mask.vcf \; --snp-mask-priority ; ```. But I get this error:. ```; java.lang.IllegalArgumentException: Illegal base [ ] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:251); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:402); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.lambda$handlePosition$0(FastaAlternateReferenceMaker.java:176); 	at java.util.Optional.orElseGet(Optional.java:267); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.handlePosition(FastaAlternateReferenceMaker.java:176); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.apply(FastaAlternateReferenceMaker.java:141); 	at org.broadinstitute.hellbender.engine.ReferenceWalker.traverse(ReferenceWalker.java:55); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. By narrowing down on where this happens I find it happens here:. ```; chrom	16798	.	TAGC	*	41.94,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433
https://github.com/broadinstitute/gatk/issues/7433:1918,Availability,down,down,1918,"67); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.handlePosition(FastaAlternateReferenceMaker.java:176); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.apply(FastaAlternateReferenceMaker.java:141); 	at org.broadinstitute.hellbender.engine.ReferenceWalker.traverse(ReferenceWalker.java:55); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. By narrowing down on where this happens I find it happens here:. ```; chrom	16798	.	TAGC	*	41.94	.	AC=1;AF=1.00;AN=1;BaseQRankSum=-5.240e-01;DP=29;FS=3.663;MQ=36.43;MQRankSum=-1.282e+00;QD=1.40;ReadPosRankSum=0.00;SOR=0.446	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16799	.	A	*	0	LowQual	AC=1;AF=1.00;AN=1;DP=29;FS=3.663;QD=0.00;SOR=0.446	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16800	.	G	*	3727.44	.	AC=1;AF=1.00;AN=1;BaseQRankSum=0.00;DP=29;FS=2.256;MQ=42.17;MQRankSum=1.88;QD=21.42;ReadPosRankSum=-6.100e-02;SOR=0.920	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16801	.	C	*	0	LowQual	AC=1;AF=1.00;AN=1;DP=29;FS=3.663;QD=0.00;SOR=0.446	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16802	.	A	*	0	LowQual	AC=1;AF=1.00;AN=1;DP=29;FS=4.509;QD=0.00;SOR=0.378	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16804	.	CAGA	.	379.83	.	AN=1;BaseQRankSum=0.00;DP=48;FS=0.000;MQ=42.50;MQRankSum=-6.740e-01;QD=29.22;ReadPosRankSum=0.524;SOR=0.836	GT:AD:DP:PL	0:48:48:0; ```. The problem is at position 16800. I have gotten this error again in similar positions w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433
https://github.com/broadinstitute/gatk/issues/7433:2903,Availability,error,error,2903,"ferenceMaker.apply(FastaAlternateReferenceMaker.java:141); 	at org.broadinstitute.hellbender.engine.ReferenceWalker.traverse(ReferenceWalker.java:55); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. By narrowing down on where this happens I find it happens here:. ```; chrom	16798	.	TAGC	*	41.94	.	AC=1;AF=1.00;AN=1;BaseQRankSum=-5.240e-01;DP=29;FS=3.663;MQ=36.43;MQRankSum=-1.282e+00;QD=1.40;ReadPosRankSum=0.00;SOR=0.446	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16799	.	A	*	0	LowQual	AC=1;AF=1.00;AN=1;DP=29;FS=3.663;QD=0.00;SOR=0.446	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16800	.	G	*	3727.44	.	AC=1;AF=1.00;AN=1;BaseQRankSum=0.00;DP=29;FS=2.256;MQ=42.17;MQRankSum=1.88;QD=21.42;ReadPosRankSum=-6.100e-02;SOR=0.920	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16801	.	C	*	0	LowQual	AC=1;AF=1.00;AN=1;DP=29;FS=3.663;QD=0.00;SOR=0.446	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16802	.	A	*	0	LowQual	AC=1;AF=1.00;AN=1;DP=29;FS=4.509;QD=0.00;SOR=0.378	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16804	.	CAGA	.	379.83	.	AN=1;BaseQRankSum=0.00;DP=48;FS=0.000;MQ=42.50;MQRankSum=-6.740e-01;QD=29.22;ReadPosRankSum=0.524;SOR=0.836	GT:AD:DP:PL	0:48:48:0; ```. The problem is at position 16800. I have gotten this error again in similar positions where a deletion has happened and the bases after it are called in contradictory ways or with low quality and I was wondering how I could fix it for all my samples.; Could BaseQRankSum have anything to do with it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433
https://github.com/broadinstitute/gatk/pull/7434:74,Testability,log,log,74,Add ability to use the is_loaded column to get the samples for training.; log job id; break up vet to vet_new queries; [VS-165]; [VS-166]; [VS-167]. [VS-165]: https://broadworkbench.atlassian.net/browse/VS-165; [VS-166]: https://broadworkbench.atlassian.net/browse/VS-166; [VS-167]: https://broadworkbench.atlassian.net/browse/VS-167,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7434
https://github.com/broadinstitute/gatk/pull/7435:126,Security,Validat,Validation,126,Remove the <20 obfuscation for AC; This needs to be removed from the Python (which did the original obfuscation); And the VAT Validation WDL -- which checked on it,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7435
https://github.com/broadinstitute/gatk/issues/7437:1470,Availability,Redundant,Redundant,1470,"denceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:05:54.583 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:05:54.584 INFO GenotypeGVCFs - Start Date/Tim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:7691,Availability,down,down,7691,"00:05:56.230 info NativeGenomicsDB - pid=40375 tid=40376 No valid combination operation found for INFO field SOR - the field will NOT be part of INFO fields in the generated VCF records; 00:05:56.776 INFO IntervalArgumentCollection - Processing 105581 bp from intervals; 00:05:56.847 INFO GenotypeGVCFs - Done initializing engine; 00:05:57.036 INFO ProgressMeter - Starting traversal; 00:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:130); at org.broadinstitute.hellbender.tools.walkers.Genotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:985,Deployability,install,install,985,"## Bug Report. ### Affected tool(s) or class(es); GATK GenotypeGVCFs. ### Affected version(s); GATK 4.2.2.0. ### Description . When running GenotypeGVCFs,; 1. multiple warnings of **No valid combination operation found for INFO field** ; 2. AS_VarDP warnings:; ```; WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - -------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:1673,Deployability,install,install,1673,"; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:05:54.583 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:05:54.584 INFO GenotypeGVCFs - Start Date/Time: August 25, 2021 12:05:54 AM EDT; 00:05:54.584 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.584 INFO GenotypeGVCFs - --------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:9428,Integrability,wrap,wrapAndCopyInto,9428,ls.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:130); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:281); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:423); at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadins,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:1360,Performance,optimiz,optimizations,1360,"nvalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:1603,Performance,Load,Loading,1603,"_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:05:54.583 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:05:54.584 INFO GenotypeGVCFs - Start Date/Time: August 25, 2021 12:05:54 AM EDT; 00:05:54.584 INFO GenotypeGVCFs - -------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:10979,Performance,optimiz,optimizations,10979,"maining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:423); at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. 1. reblock gvcf for 10 samples with 4.2.2.0 from chr16; 2. imported the 10 reblocked gvcfs from chr16 into genomicsdb ; 3. genotypeGVCFs with following ; gatk --java-options ""-Xmx60g"" GenotypeGVCFs \; -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -G StandardAnnotation -G AS_StandardAnnotation\; -V $DB \; -L $INTERVAL\; --use-new-qual-calculator\; --only-output-calls-starting-in-intervals TRUE\; --genomicsdb-shared-posixfs-optimizations TRUE\; --tmp-dir tmp\; -O $VCF. #### Expected behavior; Run GenotypeGVCFs without exceptions and no warnings about INFO fields. #### Actual behavior; Null exception with No Variants output to VCF just header.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:313,Safety,Detect,Detected,313,"## Bug Report. ### Affected tool(s) or class(es); GATK GenotypeGVCFs. ### Affected version(s); GATK 4.2.2.0. ### Description . When running GenotypeGVCFs,; 1. multiple warnings of **No valid combination operation found for INFO field** ; 2. AS_VarDP warnings:; ```; WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - -------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:560,Safety,detect,detected,560,"## Bug Report. ### Affected tool(s) or class(es); GATK GenotypeGVCFs. ### Affected version(s); GATK 4.2.2.0. ### Description . When running GenotypeGVCFs,; 1. multiple warnings of **No valid combination operation found for INFO field** ; 2. AS_VarDP warnings:; ```; WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - -------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:1470,Safety,Redund,Redundant,1470,"denceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:05:54.583 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:05:54.584 INFO GenotypeGVCFs - Start Date/Tim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:1891,Safety,detect,detect,1891,"e_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:05:54.583 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:05:54.584 INFO GenotypeGVCFs - Start Date/Time: August 25, 2021 12:05:54 AM EDT; 00:05:54.584 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.584 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.584 INFO GenotypeGVCFs - HTSJDK Version: 2.24.0; 00:05:54.585 INFO GenotypeGVCFs - Picard Version: 2.25.0; 00:05:54.585 INFO GenotypeGVCFs - Built fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:7261,Safety,Detect,Detected,7261,"cords; 00:05:56.230 info NativeGenomicsDB - pid=40375 tid=40376 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 00:05:56.230 info NativeGenomicsDB - pid=40375 tid=40376 No valid combination operation found for INFO field QD - the field will NOT be part of INFO fields in the generated VCF records; 00:05:56.230 info NativeGenomicsDB - pid=40375 tid=40376 No valid combination operation found for INFO field SOR - the field will NOT be part of INFO fields in the generated VCF records; 00:05:56.776 INFO IntervalArgumentCollection - Processing 105581 bp from intervals; 00:05:56.847 INFO GenotypeGVCFs - Done initializing engine; 00:05:57.036 INFO ProgressMeter - Starting traversal; 00:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVari",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:7521,Safety,detect,detected,7521," info NativeGenomicsDB - pid=40375 tid=40376 No valid combination operation found for INFO field QD - the field will NOT be part of INFO fields in the generated VCF records; 00:05:56.230 info NativeGenomicsDB - pid=40375 tid=40376 No valid combination operation found for INFO field SOR - the field will NOT be part of INFO fields in the generated VCF records; 00:05:56.776 INFO IntervalArgumentCollection - Processing 105581 bp from intervals; 00:05:56.847 INFO GenotypeGVCFs - Done initializing engine; 00:05:57.036 INFO ProgressMeter - Starting traversal; 00:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:8033,Security,Hash,HashMap,8033,"g engine; 00:05:57.036 INFO ProgressMeter - Starting traversal; 00:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:130); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:281); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.Ref",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:8055,Security,Hash,HashMap,8055,":57.036 INFO ProgressMeter - Starting traversal; 00:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:130); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:281); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:8087,Security,Hash,HashMap,8087,"raversal; 00:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:130); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:281); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:8102,Security,Hash,HashMap,8102,"0:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:130); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:281); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/issues/7437:780,Testability,log,log,780,"## Bug Report. ### Affected tool(s) or class(es); GATK GenotypeGVCFs. ### Affected version(s); GATK 4.2.2.0. ### Description . When running GenotypeGVCFs,; 1. multiple warnings of **No valid combination operation found for INFO field** ; 2. AS_VarDP warnings:; ```; WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - -------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437
https://github.com/broadinstitute/gatk/pull/7439:0,Deployability,update,update,0,update for assign ids and changes in import,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7439
https://github.com/broadinstitute/gatk/issues/7441:3,Performance,optimiz,optimization,3,"An optimization introduced in https://github.com/broadinstitute/gatk/pull/5466 was removed in https://github.com/broadinstitute/gatk/pull/6885. The latter exposed Smith-Waterman parameters, allowing them to be changed from their default values and thus to possibly violate conditions assumed by the former. We could restore the optimization if we added explicit checks of these conditions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7441
https://github.com/broadinstitute/gatk/issues/7441:328,Performance,optimiz,optimization,328,"An optimization introduced in https://github.com/broadinstitute/gatk/pull/5466 was removed in https://github.com/broadinstitute/gatk/pull/6885. The latter exposed Smith-Waterman parameters, allowing them to be changed from their default values and thus to possibly violate conditions assumed by the former. We could restore the optimization if we added explicit checks of these conditions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7441
https://github.com/broadinstitute/gatk/issues/7441:155,Security,expose,exposed,155,"An optimization introduced in https://github.com/broadinstitute/gatk/pull/5466 was removed in https://github.com/broadinstitute/gatk/pull/6885. The latter exposed Smith-Waterman parameters, allowing them to be changed from their default values and thus to possibly violate conditions assumed by the former. We could restore the optimization if we added explicit checks of these conditions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7441
https://github.com/broadinstitute/gatk/issues/7442:26,Availability,error,error,26,"This user is receiving an error in their workflow when using GATK4.0.3.0. The error in the particular step can be resolved using a newer GATK version but the user has used the older version for the rest of the workflow and would like a solution that allows them to continue with the older version.; This request was created from a contribution made by HT on August 23, 2021 05:44 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405613731739-GATK4-0-3-0-GenotypeGVCFs-Could-not-open-array-genomicsdbarray](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405613731739-GATK4-0-3-0-GenotypeGVCFs-Could-not-open-array-genomicsdbarray). \--. a) GATK version used: **GATK 4.0.3.0**. b) Exact command used:. **\[Tool\]: GenomicsDBImport**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx85g -Xms85g"" GenomicsDBImport \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \--sample-name-map ${dir\_CombineGVCFs}/S2\_cohort.sample\_map \\ ; ; \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4 \\ ; ; \--TMP\_DIR ${dir\_CombineGVCFs}/temporary \\ ; ; \--intervals ${dir\_CombineGVCFs}/intervals/bed3\_tmp.intervals \\ ; ; \--reader-threads 5 \\ ; ; \--batch-size 50. **\[output\]**:. folders and files in; ====================. \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4; ================================================================. callset.json ; ; genomicsdb\_array ; ; \_\_tiledb\_workspace.tdb ; ; vcfheader.vcf ; ; vidmap.json. **\[Tool\]: GenotypeGVCFs**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx4g"" GenotypeGVCFs \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \-V gendb://${dir\_GenomicsDBImport}/tmp4 \\ ; ; \-O ${dir\_GenotypeVCFs}/tmp4.vcf.gz. c) Entire error log:. Using GATK jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:78,Availability,error,error,78,"This user is receiving an error in their workflow when using GATK4.0.3.0. The error in the particular step can be resolved using a newer GATK version but the user has used the older version for the rest of the workflow and would like a solution that allows them to continue with the older version.; This request was created from a contribution made by HT on August 23, 2021 05:44 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405613731739-GATK4-0-3-0-GenotypeGVCFs-Could-not-open-array-genomicsdbarray](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405613731739-GATK4-0-3-0-GenotypeGVCFs-Could-not-open-array-genomicsdbarray). \--. a) GATK version used: **GATK 4.0.3.0**. b) Exact command used:. **\[Tool\]: GenomicsDBImport**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx85g -Xms85g"" GenomicsDBImport \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \--sample-name-map ${dir\_CombineGVCFs}/S2\_cohort.sample\_map \\ ; ; \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4 \\ ; ; \--TMP\_DIR ${dir\_CombineGVCFs}/temporary \\ ; ; \--intervals ${dir\_CombineGVCFs}/intervals/bed3\_tmp.intervals \\ ; ; \--reader-threads 5 \\ ; ; \--batch-size 50. **\[output\]**:. folders and files in; ====================. \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4; ================================================================. callset.json ; ; genomicsdb\_array ; ; \_\_tiledb\_workspace.tdb ; ; vcfheader.vcf ; ; vidmap.json. **\[Tool\]: GenotypeGVCFs**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx4g"" GenotypeGVCFs \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \-V gendb://${dir\_GenomicsDBImport}/tmp4 \\ ; ; \-O ${dir\_GenotypeVCFs}/tmp4.vcf.gz. c) Entire error log:. Using GATK jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:1856,Availability,error,error,1856,"5g -Xms85g"" GenomicsDBImport \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \--sample-name-map ${dir\_CombineGVCFs}/S2\_cohort.sample\_map \\ ; ; \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4 \\ ; ; \--TMP\_DIR ${dir\_CombineGVCFs}/temporary \\ ; ; \--intervals ${dir\_CombineGVCFs}/intervals/bed3\_tmp.intervals \\ ; ; \--reader-threads 5 \\ ; ; \--batch-size 50. **\[output\]**:. folders and files in; ====================. \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4; ================================================================. callset.json ; ; genomicsdb\_array ; ; \_\_tiledb\_workspace.tdb ; ; vcfheader.vcf ; ; vidmap.json. **\[Tool\]: GenotypeGVCFs**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx4g"" GenotypeGVCFs \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \-V gendb://${dir\_GenomicsDBImport}/tmp4 \\ ; ; \-O ${dir\_GenotypeVCFs}/tmp4.vcf.gz. c) Entire error log:. Using GATK jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false ; ; \-Dsamjdk.use\_async\_io\_write\_samtools=true ; ; \-Dsamjdk.use\_async\_io\_write\_tribble=false ; ; \-Dsamjdk.compression\_level=2 ; ; \-Xmx4g -jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; GenotypeGVCFs -R /home/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta ; ; \-V gendb:///home/WES-VCFQC/S2\_GenomicsDBImport/temporary/tmp4 ; ; \-O /home/WES-VCFQC/S2\_GenomicsDBImport/VCF/tmp4.vcf.gz ; ; 12:52:15.187 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 12:52:16.266 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 12:52:16.267 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.3.0 ; ; 12:52:16.267 INFO GenotypeGVCFs - For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:5935,Availability,ERROR,ERROR,5935,", I experience some issues with GenotypeGVCFs in GATK version 4.0.3.0. It cannot open ""genomicsdb\_array"" although the directory of genomicsdb\_array does exist. I found someone else has reported this issue here: [https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000](https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000), but except for using the latest version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. Another question is related to the fasta file:. I downloaded the reference data in the link of [https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37](https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37), when I noticed that this is an old database, I have already generated GVCF files. It seems like GenotypeGVCFs does not understand the FAI index file. error informaion; ================. \[E::fai\_read\] Could not unde",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:6557,Availability,down,downloaded,6557,"test version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. Another question is related to the fasta file:. I downloaded the reference data in the link of [https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37](https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37), when I noticed that this is an old database, I have already generated GVCF files. It seems like GenotypeGVCFs does not understand the FAI index file. error informaion; ================. \[E::fai\_read\] Could not understand FAI /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai line 1 ; ; \[E::fai\_load3\] Failed to read FASTA index /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai. FAI file; ========. 1 dna:chromosome chromosome:GRCh37:1:1:249250621:1 249250621 52 60 61 ; ; 2 dna:chromosome chromosome:GRCh37:2:1:243199373:1 243199373 253404903 60 61 ; ; 3 dna:chromosome chromosome:GRCh37:3:1:198022430:1 198022430 500657651 60 61 ; ; 4 dna:chromosome chromosome:GRCh37:4:1:191154276:1 191154276 701980507 60 61 ; ; 5 dna:chromosome chromosom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:6903,Availability,error,error,6903,"rkflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. Another question is related to the fasta file:. I downloaded the reference data in the link of [https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37](https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37), when I noticed that this is an old database, I have already generated GVCF files. It seems like GenotypeGVCFs does not understand the FAI index file. error informaion; ================. \[E::fai\_read\] Could not understand FAI /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai line 1 ; ; \[E::fai\_load3\] Failed to read FASTA index /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai. FAI file; ========. 1 dna:chromosome chromosome:GRCh37:1:1:249250621:1 249250621 52 60 61 ; ; 2 dna:chromosome chromosome:GRCh37:2:1:243199373:1 243199373 253404903 60 61 ; ; 3 dna:chromosome chromosome:GRCh37:3:1:198022430:1 198022430 500657651 60 61 ; ; 4 dna:chromosome chromosome:GRCh37:4:1:191154276:1 191154276 701980507 60 61 ; ; 5 dna:chromosome chromosome:GRCh37:5:1:180915260:1 180915260 896320740 60 61. If I use the latest fasta data provided by the GATK team [https://console.cloud.google.com/storage/browser/gcp-public-data--broad-references/hg19/v0;tab=objects?prefix=&forceOnObjectsSortingFiltering=false](https://console.cloud.google.com/storage/browser/gcp-public-data--broad-re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:4208,Deployability,patch,patch,4208,"tart Date/Time: August 23, 2021 12:52:14 PM SGT ; ; 12:52:16.268 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 12:52:16.268 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 12:52:16.268 INFO GenotypeGVCFs - HTSJDK Version: 2.14.3 ; ; 12:52:16.269 INFO GenotypeGVCFs - Picard Version: 2.17.2 ; ; 12:52:16.269 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 12:52:16.269 INFO GenotypeGVCFs - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 12:52:16.269 INFO GenotypeGVCFs - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 12:52:16.269 INFO GenotypeGVCFs - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 12:52:16.269 INFO GenotypeGVCFs - Deflater: IntelDeflater ; ; 12:52:16.269 INFO GenotypeGVCFs - Inflater: IntelInflater ; ; 12:52:16.269 INFO GenotypeGVCFs - GCS max retries/reopens: 20 ; ; 12:52:16.269 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from [https://github.com/droazen/google-cloud-java/tree/dr\\\_all\\\_nio\\\_fixes](https://github.com/droazen/google-cloud-java/tree/dr\_all\_nio\_fixes) ; ; 12:52:16.269 INFO GenotypeGVCFs - Initializing engine ; ; terminate called after throwing an instance of 'VariantQueryProcessorException' ; ; what(): VariantQueryProcessorException : Could not open array genomicsdb\_array at workspace: /home/WES-VCFQC/S2\_GenomicsDBImport/temporary/tmp4. Hi, I used GenomicsDBImport to combined 2000 GVCFs. To speed up, I split the bed file and concatenated multiple intervals into a contig. I also met the file locking problem which can be solved by setting TILEDB\_DISABLE\_FILE\_LOCKING=1 in my Linux system. Currently, I experience some issues with GenotypeGVCFs in GATK version 4.0.3.0. It cannot open ""genomicsdb\_array"" although the directory of genomicsdb\_array does exist. I found someone else has reported this issue here: [https://sites.google.com/a/broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:5208,Deployability,a/b,a/broadinstitute,5208,"6d11bef1c81f885c26b2b56c8616b7a705171e4f from [https://github.com/droazen/google-cloud-java/tree/dr\\\_all\\\_nio\\\_fixes](https://github.com/droazen/google-cloud-java/tree/dr\_all\_nio\_fixes) ; ; 12:52:16.269 INFO GenotypeGVCFs - Initializing engine ; ; terminate called after throwing an instance of 'VariantQueryProcessorException' ; ; what(): VariantQueryProcessorException : Could not open array genomicsdb\_array at workspace: /home/WES-VCFQC/S2\_GenomicsDBImport/temporary/tmp4. Hi, I used GenomicsDBImport to combined 2000 GVCFs. To speed up, I split the bed file and concatenated multiple intervals into a contig. I also met the file locking problem which can be solved by setting TILEDB\_DISABLE\_FILE\_LOCKING=1 in my Linux system. Currently, I experience some issues with GenotypeGVCFs in GATK version 4.0.3.0. It cannot open ""genomicsdb\_array"" although the directory of genomicsdb\_array does exist. I found someone else has reported this issue here: [https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000](https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000), but except for using the latest version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:5395,Deployability,a/b,a/broadinstitute,5395,"_fixes) ; ; 12:52:16.269 INFO GenotypeGVCFs - Initializing engine ; ; terminate called after throwing an instance of 'VariantQueryProcessorException' ; ; what(): VariantQueryProcessorException : Could not open array genomicsdb\_array at workspace: /home/WES-VCFQC/S2\_GenomicsDBImport/temporary/tmp4. Hi, I used GenomicsDBImport to combined 2000 GVCFs. To speed up, I split the bed file and concatenated multiple intervals into a contig. I also met the file locking problem which can be solved by setting TILEDB\_DISABLE\_FILE\_LOCKING=1 in my Linux system. Currently, I experience some issues with GenotypeGVCFs in GATK version 4.0.3.0. It cannot open ""genomicsdb\_array"" although the directory of genomicsdb\_array does exist. I found someone else has reported this issue here: [https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000](https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000), but except for using the latest version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:2481,Performance,Load,Loading,2481,"db\_array ; ; \_\_tiledb\_workspace.tdb ; ; vcfheader.vcf ; ; vidmap.json. **\[Tool\]: GenotypeGVCFs**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx4g"" GenotypeGVCFs \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \-V gendb://${dir\_GenomicsDBImport}/tmp4 \\ ; ; \-O ${dir\_GenotypeVCFs}/tmp4.vcf.gz. c) Entire error log:. Using GATK jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false ; ; \-Dsamjdk.use\_async\_io\_write\_samtools=true ; ; \-Dsamjdk.use\_async\_io\_write\_tribble=false ; ; \-Dsamjdk.compression\_level=2 ; ; \-Xmx4g -jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; GenotypeGVCFs -R /home/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta ; ; \-V gendb:///home/WES-VCFQC/S2\_GenomicsDBImport/temporary/tmp4 ; ; \-O /home/WES-VCFQC/S2\_GenomicsDBImport/VCF/tmp4.vcf.gz ; ; 12:52:15.187 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 12:52:16.266 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 12:52:16.267 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.3.0 ; ; 12:52:16.267 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 12:52:16.267 INFO GenotypeGVCFs - Executing as XX@XX on Linux v2.6.32-754.14.2.el6.x86\_64 amd64 ; ; 12:52:16.267 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0\_91-b14 ; ; 12:52:16.267 INFO GenotypeGVCFs - Start Date/Time: August 23, 2021 12:52:14 PM SGT ; ; 12:52:16.268 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 12:52:16.268 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 12:52:16.268 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:6003,Safety,detect,detected,6003,", I experience some issues with GenotypeGVCFs in GATK version 4.0.3.0. It cannot open ""genomicsdb\_array"" although the directory of genomicsdb\_array does exist. I found someone else has reported this issue here: [https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000](https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000), but except for using the latest version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. Another question is related to the fasta file:. I downloaded the reference data in the link of [https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37](https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37), when I noticed that this is an old database, I have already generated GVCF files. It seems like GenotypeGVCFs does not understand the FAI index file. error informaion; ================. \[E::fai\_read\] Could not unde",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:6501,Safety,risk,risk,6501,"genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000), but except for using the latest version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. Another question is related to the fasta file:. I downloaded the reference data in the link of [https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37](https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37), when I noticed that this is an old database, I have already generated GVCF files. It seems like GenotypeGVCFs does not understand the FAI index file. error informaion; ================. \[E::fai\_read\] Could not understand FAI /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai line 1 ; ; \[E::fai\_load3\] Failed to read FASTA index /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai. FAI file; ========. 1 dna:chromosome chromosome:GRCh37:1:1:249250621:1 249250621 52 60 61 ; ; 2 dna:chromosome chromosome:GRCh37:2:1:243199373:1 243199373 253404903 60 61 ; ; 3 dna:chromosome chromosome:GRCh37:3:1:198022430:1 198022430 500657651 60 61 ; ; 4 dna:chromosom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7442:1862,Testability,log,log,1862,"5g -Xms85g"" GenomicsDBImport \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \--sample-name-map ${dir\_CombineGVCFs}/S2\_cohort.sample\_map \\ ; ; \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4 \\ ; ; \--TMP\_DIR ${dir\_CombineGVCFs}/temporary \\ ; ; \--intervals ${dir\_CombineGVCFs}/intervals/bed3\_tmp.intervals \\ ; ; \--reader-threads 5 \\ ; ; \--batch-size 50. **\[output\]**:. folders and files in; ====================. \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4; ================================================================. callset.json ; ; genomicsdb\_array ; ; \_\_tiledb\_workspace.tdb ; ; vcfheader.vcf ; ; vidmap.json. **\[Tool\]: GenotypeGVCFs**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx4g"" GenotypeGVCFs \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \-V gendb://${dir\_GenomicsDBImport}/tmp4 \\ ; ; \-O ${dir\_GenotypeVCFs}/tmp4.vcf.gz. c) Entire error log:. Using GATK jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false ; ; \-Dsamjdk.use\_async\_io\_write\_samtools=true ; ; \-Dsamjdk.use\_async\_io\_write\_tribble=false ; ; \-Dsamjdk.compression\_level=2 ; ; \-Xmx4g -jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; GenotypeGVCFs -R /home/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta ; ; \-V gendb:///home/WES-VCFQC/S2\_GenomicsDBImport/temporary/tmp4 ; ; \-O /home/WES-VCFQC/S2\_GenomicsDBImport/VCF/tmp4.vcf.gz ; ; 12:52:15.187 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 12:52:16.266 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 12:52:16.267 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.3.0 ; ; 12:52:16.267 INFO GenotypeGVCFs - For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442
https://github.com/broadinstitute/gatk/issues/7444:487,Availability,error,error,487,"This request was created from a contribution made by Yangyxt on August 30, 2021 08:18 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.2.0 ; ; b) Exact command used:. ${gatk} PostprocessGermlineCNVCalls \\. \--model-shard-path ${gCNV\_model\_prefix}-model \\. \--calls-shard-path ${gCNV\_case\_prefix}-calls \\. \--allosomal-contig chrX --allosomal-contig chrY \\. \--contig-ploidy-calls ${ploidy\_case\_prefix}-calls \\. \--sample-index ${sample\_index} \\. \--output-denoised-copy-ratios ${cnv\_dir}/${sampleID}.sample\_${sample\_index}.denoised\_copy\_ration.tsv \\. \--output-genotyped-intervals ${cnv\_dir}/genotyped-intervals-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--output-genotyped-segments ${cnv\_dir}/genotyped-segments-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--sequence-dictionary ${ref\_gen}/ucsc.hg19.dict. c) Entire error log:. 11:04:20.841 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:1261,Availability,error,error,1261,"tional-arguments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.2.0 ; ; b) Exact command used:. ${gatk} PostprocessGermlineCNVCalls \\. \--model-shard-path ${gCNV\_model\_prefix}-model \\. \--calls-shard-path ${gCNV\_case\_prefix}-calls \\. \--allosomal-contig chrX --allosomal-contig chrY \\. \--contig-ploidy-calls ${ploidy\_case\_prefix}-calls \\. \--sample-index ${sample\_index} \\. \--output-denoised-copy-ratios ${cnv\_dir}/${sampleID}.sample\_${sample\_index}.denoised\_copy\_ration.tsv \\. \--output-genotyped-intervals ${cnv\_dir}/genotyped-intervals-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--output-genotyped-segments ${cnv\_dir}/genotyped-segments-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--sequence-dictionary ${ref\_gen}/ucsc.hg19.dict. c) Entire error log:. 11:04:20.841 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Executing as yangyxt@paedyl02 on Linux v3.10.0-1160.11.1.el7.x86\_64 amd64 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Serve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:4533,Availability,down,down,4533,"processGermlineCNVCalls - Requester pays: disabled ; ; 11:04:20.985 INFO PostprocessGermlineCNVCalls - Initializing engine ; ; 11:04:26.627 INFO PostprocessGermlineCNVCalls - Done initializing engine ; ; 11:04:27.492 INFO ProgressMeter - Starting traversal ; ; 11:04:27.492 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute ; ; 11:04:27.493 INFO ProgressMeter - unmapped 0.0 0 NaN ; ; 11:04:27.493 INFO ProgressMeter - Traversal complete. Processed 0 total records in 0.0 minutes. ; ; 11:04:27.493 INFO PostprocessGermlineCNVCalls - Generating intervals VCF file... ; ; 11:04:27.510 INFO PostprocessGermlineCNVCalls - Writing intervals VCF file to /staging/wes/1\_sample\_20210615/CNV\_calling/genotyped-intervals-case-A210066-vs-v7cohort.vcf.gz... ; ; 11:04:27.510 INFO PostprocessGermlineCNVCalls - Analyzing shard 1 / 1... ; ; 11:04:30.169 INFO PostprocessGermlineCNVCalls - Generating segments... ; ; 11:04:37.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:7870,Availability,error,error,7870,"titute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:168) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:139) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:739) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(PostprocessGermlineCNVCalls.java:485) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalSuccess(PostprocessGermlineCNVCalls.java:456) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1089) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289) ; ; Using GATK jar /home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar Po. If not an error, choose a category for your question(REQUIRED): ; ; a)How do I (......)? ; ; b) What does (......) mean? ; ; c) Why do I see (......)? ; ; d) Where do I find (......)? ; ; e) Will (......) be in future releases?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/181533'>Zendesk ticket #181533</a>)<br>gz#181533</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:2278,Deployability,release,release-,2278,"O NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Executing as yangyxt@paedyl02 on Linux v3.10.0-1160.11.1.el7.x86\_64 amd64 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Start Date/Time: August 30, 2021 11:04:20 AM HKT ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 2.24.1 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Picard Version: 2.25.4 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Built for Spark Version: 2.4.5 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false. 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 11:04:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:8078,Deployability,release,releases,8078,"titute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:168) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:139) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:739) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(PostprocessGermlineCNVCalls.java:485) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalSuccess(PostprocessGermlineCNVCalls.java:456) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1089) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289) ; ; Using GATK jar /home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar Po. If not an error, choose a category for your question(REQUIRED): ; ; a)How do I (......)? ; ; b) What does (......) mean? ; ; c) Why do I see (......)? ; ; d) Where do I find (......)? ; ; e) Will (......) be in future releases?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/181533'>Zendesk ticket #181533</a>)<br>gz#181533</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:5176,Modifiability,variab,variable,5176,"s VCF file to /staging/wes/1\_sample\_20210615/CNV\_calling/genotyped-intervals-case-A210066-vs-v7cohort.vcf.gz... ; ; 11:04:27.510 INFO PostprocessGermlineCNVCalls - Analyzing shard 1 / 1... ; ; 11:04:30.169 INFO PostprocessGermlineCNVCalls - Generating segments... ; ; 11:04:37.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36.543 INFO segment\_gcnv\_calls - Instantiating the Viterbi segmentation engine... Stderr: Traceback (most recent call last): ; ; File ""/tmp/segment\_gcnv\_calls.8152704641395924200.py"", line 92, in <module> ; ; args.intervals\_vcf, args.clustered\_vcf) ; ; TypeError: \_\_init\_\_() takes 6 positional arguments but 8 were given. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) ; ; at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:193) ; ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:1313,Performance,Load,Loading,1313,"hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.2.0 ; ; b) Exact command used:. ${gatk} PostprocessGermlineCNVCalls \\. \--model-shard-path ${gCNV\_model\_prefix}-model \\. \--calls-shard-path ${gCNV\_case\_prefix}-calls \\. \--allosomal-contig chrX --allosomal-contig chrY \\. \--contig-ploidy-calls ${ploidy\_case\_prefix}-calls \\. \--sample-index ${sample\_index} \\. \--output-denoised-copy-ratios ${cnv\_dir}/${sampleID}.sample\_${sample\_index}.denoised\_copy\_ration.tsv \\. \--output-genotyped-intervals ${cnv\_dir}/genotyped-intervals-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--output-genotyped-segments ${cnv\_dir}/genotyped-segments-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--sequence-dictionary ${ref\_gen}/ucsc.hg19.dict. c) Entire error log:. 11:04:20.841 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Executing as yangyxt@paedyl02 on Linux v3.10.0-1160.11.1.el7.x86\_64 amd64 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 ; ; 11:04:20.984 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:5228,Performance,optimiz,optimizer,5228,"s VCF file to /staging/wes/1\_sample\_20210615/CNV\_calling/genotyped-intervals-case-A210066-vs-v7cohort.vcf.gz... ; ; 11:04:27.510 INFO PostprocessGermlineCNVCalls - Analyzing shard 1 / 1... ; ; 11:04:30.169 INFO PostprocessGermlineCNVCalls - Generating segments... ; ; 11:04:37.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36.543 INFO segment\_gcnv\_calls - Instantiating the Viterbi segmentation engine... Stderr: Traceback (most recent call last): ; ; File ""/tmp/segment\_gcnv\_calls.8152704641395924200.py"", line 92, in <module> ; ; args.intervals\_vcf, args.clustered\_vcf) ; ; TypeError: \_\_init\_\_() takes 6 positional arguments but 8 were given. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) ; ; at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:193) ; ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:5385,Performance,Load,Loading,5385,"ng shard 1 / 1... ; ; 11:04:30.169 INFO PostprocessGermlineCNVCalls - Generating segments... ; ; 11:04:37.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36.543 INFO segment\_gcnv\_calls - Instantiating the Viterbi segmentation engine... Stderr: Traceback (most recent call last): ; ; File ""/tmp/segment\_gcnv\_calls.8152704641395924200.py"", line 92, in <module> ; ; args.intervals\_vcf, args.clustered\_vcf) ; ; TypeError: \_\_init\_\_() takes 6 positional arguments but 8 were given. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) ; ; at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:193) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:168) ; ; at org.broadinstitute.hellbender.utils.python.PythonScrip",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:5460,Performance,Load,Loading,5460,"7.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36.543 INFO segment\_gcnv\_calls - Instantiating the Viterbi segmentation engine... Stderr: Traceback (most recent call last): ; ; File ""/tmp/segment\_gcnv\_calls.8152704641395924200.py"", line 92, in <module> ; ; args.intervals\_vcf, args.clustered\_vcf) ; ; TypeError: \_\_init\_\_() takes 6 positional arguments but 8 were given. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) ; ; at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:193) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:168) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:139) ; ; at org.broadinstitute.hellbender.tools.copynu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:1608,Safety,detect,detect,1608,"sGermlineCNVCalls \\. \--model-shard-path ${gCNV\_model\_prefix}-model \\. \--calls-shard-path ${gCNV\_case\_prefix}-calls \\. \--allosomal-contig chrX --allosomal-contig chrY \\. \--contig-ploidy-calls ${ploidy\_case\_prefix}-calls \\. \--sample-index ${sample\_index} \\. \--output-denoised-copy-ratios ${cnv\_dir}/${sampleID}.sample\_${sample\_index}.denoised\_copy\_ration.tsv \\. \--output-genotyped-intervals ${cnv\_dir}/genotyped-intervals-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--output-genotyped-segments ${cnv\_dir}/genotyped-segments-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--sequence-dictionary ${ref\_gen}/ucsc.hg19.dict. c) Entire error log:. 11:04:20.841 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Executing as yangyxt@paedyl02 on Linux v3.10.0-1160.11.1.el7.x86\_64 amd64 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Start Date/Time: August 30, 2021 11:04:20 AM HKT ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - -----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7444:1267,Testability,log,log,1267,"tional-arguments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.2.0 ; ; b) Exact command used:. ${gatk} PostprocessGermlineCNVCalls \\. \--model-shard-path ${gCNV\_model\_prefix}-model \\. \--calls-shard-path ${gCNV\_case\_prefix}-calls \\. \--allosomal-contig chrX --allosomal-contig chrY \\. \--contig-ploidy-calls ${ploidy\_case\_prefix}-calls \\. \--sample-index ${sample\_index} \\. \--output-denoised-copy-ratios ${cnv\_dir}/${sampleID}.sample\_${sample\_index}.denoised\_copy\_ration.tsv \\. \--output-genotyped-intervals ${cnv\_dir}/genotyped-intervals-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--output-genotyped-segments ${cnv\_dir}/genotyped-segments-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--sequence-dictionary ${ref\_gen}/ucsc.hg19.dict. c) Entire error log:. 11:04:20.841 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Executing as yangyxt@paedyl02 on Linux v3.10.0-1160.11.1.el7.x86\_64 amd64 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Serve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444
https://github.com/broadinstitute/gatk/issues/7445:2563,Usability,clear,clearly,2563,".vcf.gz. c) Why do I see (......)? But I don't retrieve any variants (zero). I'll explain from the beginning. So I have around 8 bam datasets, 7 alleged mutants and 1 from a wild-type parental strain of the other 7. I ran HaplotypeCaller on each bam file using the arguments -ERC GVCF -ploidy 1 because I don't expect differences in ploidy and they're haploid. Then, I combined all the vcf files with CombineGVCFs; genotyped them with GenotypeGVCFs; and filtered the SNPs using these criteria:. \-filter ""QD < 20.0"" --filter-name ""QD20"" \\ ; ; \-filter ""QUAL < 30.0"" --filter-name ""QUAL30"" \\ ; ; \-filter ""SOR > 3.0"" --filter-name ""SOR3"" \\ ; ; \-filter ""FS > 60.0"" --filter-name ""FS60"" \\ ; ; \-filter ""MQ < 40.0"" --filter-name ""MQ40"" \\. Now, what I want to do is to remove all the variants that are present in the wild-type vcf track; or take the variants that are absent in the wild-type, same difference. And for that, I thought about using. gatk SelectVariants -V $combined.vcf.gz -R $genome --discordance $wildtype.vcf -O $discordant.combined.vcf.gz. Where $combined.vcf.gz is the combined file I got after combining, genotyping, and filtering, $genome is my reference genome, and $wildtype.vcf is the initial vcf file I produced with HaplotypeCaller for the wild-type bam dataset. The thing is I get 0 variants back, and I can see there are discordant variants (variants that are present in one or more of the mutants, but not in the wildtype) using IGV and looking at the combined vcf. I also tried running something similar using the individual files generated by HaplotypeCaller in pairwise comparisons with the wild-type track, and I also get 0 variants back, so I must be definitely doing something wrong. By the way, if I use --concordance instead, I get ALL the variants, even though some are clearly not concordant. Thank you for your help,. Carlos<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/180595'>Zendesk ticket #180595</a>)<br>gz#180595</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7445
https://github.com/broadinstitute/gatk/issues/7447:551,Availability,Down,Downloaded,551,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:1375,Availability,avail,available,1375,te/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:2591,Availability,error,error,2591,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:2739,Availability,avail,available,2739,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:138,Deployability,release,release,138,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:715,Deployability,update,update,715,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:1575,Deployability,update,updates,1575, broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:2060,Deployability,update,updates,2060,"oogle.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration detai",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:2150,Deployability,update,updates,2150,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:2242,Deployability,update,updates,2242,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:2333,Deployability,update,updates,2333,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:3065,Deployability,configurat,configuration,3065,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:3065,Modifiability,config,configuration,3065,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:736,Security,secur,security,736,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:770,Security,secur,security,770,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:880,Security,secur,security,880,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:914,Security,secur,security,914,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:974,Security,secur,security,974,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:1008,Security,secur,security,1008,"s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archiv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:1148,Security,secur,security,1148, Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archiv,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:1182,Security,secur,security,1182,cker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:1454,Security,secur,security,1454,te/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:1488,Security,secur,security,1488,fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:2954,Security,secur,securely,2954,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7447:3014,Security,secur,secure,3014,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447
https://github.com/broadinstitute/gatk/issues/7448:1167,Availability,error,error,1167,"This request was created from a contribution made by Yanis Chrys on August 19, 2021 11:35 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-). \--. Hi, ; ; I am working on haploid bacterial data and I ran into a limitation of the program that I either can't solve or it would be nice to add a funtion for it in the future. I'll explain the issue:. Let's say I have (low coverage) data that I want to turn into an alternate fasta reference where: ; ; REF: A. ALT: AAGT,T,CA. If I want to keep variants where the AD > \[threshold\] I can't do. \-select 'vc.getGenotype(""sample"").getAD.1'. because for my sample it could be that the called ALT is getAD.2 and so far I haven't been able to use anything other than a number as an index to getAD. This would be solved if we could do:. getAD.getGT OR getAD.IndexOfAlleleWithHighestCount. but to my knowledge none of these will work because JEXL will give an error. Maybe extending JEXL java operation to the AD array could fix it? Because even getAD\[0\] gives an error. Do you have a solution to this?. PS. I am sorry if this should have been under General Questions<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/177956'>Zendesk ticket #177956</a>)<br>gz#177956</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7448
https://github.com/broadinstitute/gatk/issues/7448:1273,Availability,error,error,1273,"This request was created from a contribution made by Yanis Chrys on August 19, 2021 11:35 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-). \--. Hi, ; ; I am working on haploid bacterial data and I ran into a limitation of the program that I either can't solve or it would be nice to add a funtion for it in the future. I'll explain the issue:. Let's say I have (low coverage) data that I want to turn into an alternate fasta reference where: ; ; REF: A. ALT: AAGT,T,CA. If I want to keep variants where the AD > \[threshold\] I can't do. \-select 'vc.getGenotype(""sample"").getAD.1'. because for my sample it could be that the called ALT is getAD.2 and so far I haven't been able to use anything other than a number as an index to getAD. This would be solved if we could do:. getAD.getGT OR getAD.IndexOfAlleleWithHighestCount. but to my knowledge none of these will work because JEXL will give an error. Maybe extending JEXL java operation to the AD array could fix it? Because even getAD\[0\] gives an error. Do you have a solution to this?. PS. I am sorry if this should have been under General Questions<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/177956'>Zendesk ticket #177956</a>)<br>gz#177956</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7448
https://github.com/broadinstitute/gatk/issues/7448:1180,Modifiability,extend,extending,1180,"This request was created from a contribution made by Yanis Chrys on August 19, 2021 11:35 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-). \--. Hi, ; ; I am working on haploid bacterial data and I ran into a limitation of the program that I either can't solve or it would be nice to add a funtion for it in the future. I'll explain the issue:. Let's say I have (low coverage) data that I want to turn into an alternate fasta reference where: ; ; REF: A. ALT: AAGT,T,CA. If I want to keep variants where the AD > \[threshold\] I can't do. \-select 'vc.getGenotype(""sample"").getAD.1'. because for my sample it could be that the called ALT is getAD.2 and so far I haven't been able to use anything other than a number as an index to getAD. This would be solved if we could do:. getAD.getGT OR getAD.IndexOfAlleleWithHighestCount. but to my knowledge none of these will work because JEXL will give an error. Maybe extending JEXL java operation to the AD array could fix it? Because even getAD\[0\] gives an error. Do you have a solution to this?. PS. I am sorry if this should have been under General Questions<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/177956'>Zendesk ticket #177956</a>)<br>gz#177956</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7448
https://github.com/broadinstitute/gatk/issues/7453:136,Availability,error,error,136,"I am using GATK DepthOfCoverage tool for some of my samples.; I need to use genome reference GRCh37.; Everything is working fine but an error will occur whenever I run it (attached photos).; I have used all different type of references, including Ensembl, UCSC, NCBI, and GATK source itself but the same error is still there.; Also I know that I need to use a unique database and use that to create all fai, dict, bed file so that for sure the namings are the same in my all types of files. But I don't know how to create .bed file out of a reference genome (i.e. Homo_sapiens.GRCh37.dna.primary_assembly.fa); Would you please guide me what can I do about that?. . ![Screenshot from 2021-09-02 00-11-51](https://user-images.githubusercontent.com/87016284/131868327-660a9a9c-cc93-4c6e-a08c-0a67eddf2f47.png); ![Screenshot from 2021-09-02 00-12-00](https://user-images.githubusercontent.com/87016284/131868369-0a80d306-8a05-4a87-a566-02c3713561c4.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7453
https://github.com/broadinstitute/gatk/issues/7453:304,Availability,error,error,304,"I am using GATK DepthOfCoverage tool for some of my samples.; I need to use genome reference GRCh37.; Everything is working fine but an error will occur whenever I run it (attached photos).; I have used all different type of references, including Ensembl, UCSC, NCBI, and GATK source itself but the same error is still there.; Also I know that I need to use a unique database and use that to create all fai, dict, bed file so that for sure the namings are the same in my all types of files. But I don't know how to create .bed file out of a reference genome (i.e. Homo_sapiens.GRCh37.dna.primary_assembly.fa); Would you please guide me what can I do about that?. . ![Screenshot from 2021-09-02 00-11-51](https://user-images.githubusercontent.com/87016284/131868327-660a9a9c-cc93-4c6e-a08c-0a67eddf2f47.png); ![Screenshot from 2021-09-02 00-12-00](https://user-images.githubusercontent.com/87016284/131868369-0a80d306-8a05-4a87-a566-02c3713561c4.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7453
https://github.com/broadinstitute/gatk/issues/7453:627,Usability,guid,guide,627,"I am using GATK DepthOfCoverage tool for some of my samples.; I need to use genome reference GRCh37.; Everything is working fine but an error will occur whenever I run it (attached photos).; I have used all different type of references, including Ensembl, UCSC, NCBI, and GATK source itself but the same error is still there.; Also I know that I need to use a unique database and use that to create all fai, dict, bed file so that for sure the namings are the same in my all types of files. But I don't know how to create .bed file out of a reference genome (i.e. Homo_sapiens.GRCh37.dna.primary_assembly.fa); Would you please guide me what can I do about that?. . ![Screenshot from 2021-09-02 00-11-51](https://user-images.githubusercontent.com/87016284/131868327-660a9a9c-cc93-4c6e-a08c-0a67eddf2f47.png); ![Screenshot from 2021-09-02 00-12-00](https://user-images.githubusercontent.com/87016284/131868369-0a80d306-8a05-4a87-a566-02c3713561c4.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7453
https://github.com/broadinstitute/gatk/issues/7454:85,Availability,outage,outages,85,"I run HaplotypeCaller twice , the former one was stopped because of unexpected power outages. I check the LOG and found the chromosome where HaplotypeCaller stopped. So i star another HaplotypeCaller(later one) with the ""-L *.intervals"", it begin from the chromosome where former HaplotypeCaller stopped.The ref genome and the parameters were all the same. However, HaplotypeCaller give different results. Note: the ref genome has 26 chromosomes :A01-A13;D01-D13. **_The former LOG:_**. nohup: ignoring input and appending output to nohup.out; 09:04:49.857 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:05:02.971 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.971 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:05:02.971 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:05:02.972 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:05:02.972 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:05:02.973 INFO HaplotypeCaller - Start Date/Time: August 22, 2021 9:04:49 AM CST; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.974 INFO HaplotypeCaller - HTSJDK Version: 2.16.1; 09:05:02.974 INFO HaplotypeCaller - Picard Version: 2.18.13; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:13153,Deployability,release,release,13153,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:79,Energy Efficiency,power,power,79,"I run HaplotypeCaller twice , the former one was stopped because of unexpected power outages. I check the LOG and found the chromosome where HaplotypeCaller stopped. So i star another HaplotypeCaller(later one) with the ""-L *.intervals"", it begin from the chromosome where former HaplotypeCaller stopped.The ref genome and the parameters were all the same. However, HaplotypeCaller give different results. Note: the ref genome has 26 chromosomes :A01-A13;D01-D13. **_The former LOG:_**. nohup: ignoring input and appending output to nohup.out; 09:04:49.857 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:05:02.971 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.971 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:05:02.971 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:05:02.972 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:05:02.972 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:05:02.973 INFO HaplotypeCaller - Start Date/Time: August 22, 2021 9:04:49 AM CST; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.974 INFO HaplotypeCaller - HTSJDK Version: 2.16.1; 09:05:02.974 INFO HaplotypeCaller - Picard Version: 2.18.13; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:586,Performance,Load,Loading,586,"I run HaplotypeCaller twice , the former one was stopped because of unexpected power outages. I check the LOG and found the chromosome where HaplotypeCaller stopped. So i star another HaplotypeCaller(later one) with the ""-L *.intervals"", it begin from the chromosome where former HaplotypeCaller stopped.The ref genome and the parameters were all the same. However, HaplotypeCaller give different results. Note: the ref genome has 26 chromosomes :A01-A13;D01-D13. **_The former LOG:_**. nohup: ignoring input and appending output to nohup.out; 09:04:49.857 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:05:02.971 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.971 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:05:02.971 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:05:02.972 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:05:02.972 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:05:02.973 INFO HaplotypeCaller - Start Date/Time: August 22, 2021 9:04:49 AM CST; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.974 INFO HaplotypeCaller - HTSJDK Version: 2.16.1; 09:05:02.974 INFO HaplotypeCaller - Picard Version: 2.18.13; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:2984,Performance,Load,Loading,2984,"NC_IO_WRITE_FOR_TRIBBLE : false; 09:05:02.975 INFO HaplotypeCaller - Deflater: IntelDeflater; 09:05:02.975 INFO HaplotypeCaller - Inflater: IntelInflater; 09:05:02.975 INFO HaplotypeCaller - GCS max retries/reopens: 20; 09:05:02.975 INFO HaplotypeCaller - Requester pays: disabled; 09:05:02.976 INFO HaplotypeCaller - Initializing engine; 09:05:05.609 INFO HaplotypeCaller - Done initializing engine; 09:05:05.616 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:05:06.026 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 09:05:06.026 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:05:06.055 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:05:06.083 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:05:06.083 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:05:06.411 INFO ProgressMeter - Starting traversal; 09:05:06.412 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:05:16.445 INFO ProgressMeter - A01:27883 0.2 110 657.9; 09:05:27.780 INFO ProgressMeter - A01:45980 0.4 240 673.9; 09:05:38.219 INFO ProgressMeter - A01:64498 0.5 360 679.1; .......; 11:56:31.165 INFO ProgressMeter - A13:109860605 16011.4 6803200 424.9; 11:56:41.437 INFO ProgressMeter - A13:109878042 16011.6 6803330 424.9; 11:56:51.882 INFO ProgressMeter - A13:109913861 160",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:3170,Performance,multi-thread,multi-threaded,3170,":05:02.975 INFO HaplotypeCaller - Requester pays: disabled; 09:05:02.976 INFO HaplotypeCaller - Initializing engine; 09:05:05.609 INFO HaplotypeCaller - Done initializing engine; 09:05:05.616 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:05:06.026 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 09:05:06.026 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:05:06.055 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:05:06.083 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:05:06.083 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:05:06.411 INFO ProgressMeter - Starting traversal; 09:05:06.412 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:05:16.445 INFO ProgressMeter - A01:27883 0.2 110 657.9; 09:05:27.780 INFO ProgressMeter - A01:45980 0.4 240 673.9; 09:05:38.219 INFO ProgressMeter - A01:64498 0.5 360 679.1; .......; 11:56:31.165 INFO ProgressMeter - A13:109860605 16011.4 6803200 424.9; 11:56:41.437 INFO ProgressMeter - A13:109878042 16011.6 6803330 424.9; 11:56:51.882 INFO ProgressMeter - A13:109913861 16011.8 6803540 424.9; 11:57:02.097 INFO ProgressMeter - A13:109924924 16011.9 6803620 424.9; 11:57:14.212 INFO ProgressMeter - A13:109930687 16012.1 6803650 424.9; 11:57:26.561 INFO ProgressMeter - A13:109956963 16012.3 6803",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:6927,Performance,Load,Loading,6927,":03:49.911 INFO ProgressMeter - D01:106344 16018.7 6806170 424.9; 12:04:23.685 INFO ProgressMeter - D01:108187 16019.3 6806180 424.9; 12:04:33.872 INFO ProgressMeter - D01:131768 16019.5 6806320 424.9; 12:04:44.080 INFO ProgressMeter - D01:142033 16019.6 6806390 424.9; 12:05:30.576 INFO ProgressMeter - D01:151103 16020.4 6806460 424.9; 12:05:40.721 INFO ProgressMeter - D01:171985 16020.6 6806600 424.9; 12:05:52.649 INFO ProgressMeter - D01:179627 16020.8 6806650 424.9; 12:06:05.001 INFO ProgressMeter - D01:191227 16021.0 6806730 424.9; 12:06:32.206 INFO ProgressMeter - D01:211153 16021.4 6806880 424.9; 12:06:46.312 INFO ProgressMeter - D01:220323 16021.7 6806940 424.9; 12:06:56.646 INFO ProgressMeter - D01:232399 16021.8 6807020 424.9; 12:07:10.267 INFO ProgressMeter - D01:235919 16022.1 6807050 424.9; 12:07:22.064 INFO ProgressMeter - D01:256207 16022.3 6807200 424.9. **_The later LOG_**. nohup: ignoring input and appending output to nohup.out; 09:13:38.398 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:13:57.227 INFO HaplotypeCaller - ------------------------------------------------------------; 09:13:57.228 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:13:57.228 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:13:57.229 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:13:57.229 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:13:57.230 INFO HaplotypeCaller - Start Date/Time: September 3, 2021 9:13:38 AM CST; 09:13:57.230 INFO HaplotypeCaller - ------------------------------------------------------------; 09:13:57.230 INFO HaplotypeCaller - ------------------------------------------------------------; 09:13:57.232 INFO HaplotypeCaller - HTSJDK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:9414,Performance,Load,Loading,9414,"later; 09:13:57.233 INFO HaplotypeCaller - Inflater: IntelInflater; 09:13:57.233 INFO HaplotypeCaller - GCS max retries/reopens: 20; 09:13:57.233 INFO HaplotypeCaller - Requester pays: disabled; 09:13:57.233 INFO HaplotypeCaller - Initializing engine; 09:13:59.096 INFO IntervalArgumentCollection - Processing 818575866 bp from intervals; 09:13:59.161 INFO HaplotypeCaller - Done initializing engine; 09:13:59.164 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:13:59.598 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 09:14:00.256 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:14:00.284 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:14:00.312 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:14:00.312 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:14:00.595 INFO ProgressMeter - Starting traversal; 09:14:00.596 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:14:12.096 INFO ProgressMeter - D01:2563 0.2 20 104.3; 09:14:24.689 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 09:14:24.690 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 09:14:24.884 INFO ProgressMeter - D01:10031 0.4 60 148.2; 09:14:36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:9600,Performance,multi-thread,multi-threaded,9600,"Caller - Initializing engine; 09:13:59.096 INFO IntervalArgumentCollection - Processing 818575866 bp from intervals; 09:13:59.161 INFO HaplotypeCaller - Done initializing engine; 09:13:59.164 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:13:59.598 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 09:14:00.256 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:14:00.284 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:14:00.312 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:14:00.312 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:14:00.595 INFO ProgressMeter - Starting traversal; 09:14:00.596 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:14:12.096 INFO ProgressMeter - D01:2563 0.2 20 104.3; 09:14:24.689 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 09:14:24.690 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 09:14:24.884 INFO ProgressMeter - D01:10031 0.4 60 148.2; 09:14:36.441 INFO ProgressMeter - D01:19554 0.6 130 217.6; 09:14:51.359 INFO ProgressMeter - D01:21053 0.8 140 165.5; 09:15:02.193 INFO ProgressMeter - D01:34263 1.0 220 214.3; 09:15:13.398 INFO ProgressMeter - D01:56554 1.2 360 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:106,Testability,LOG,LOG,106,"I run HaplotypeCaller twice , the former one was stopped because of unexpected power outages. I check the LOG and found the chromosome where HaplotypeCaller stopped. So i star another HaplotypeCaller(later one) with the ""-L *.intervals"", it begin from the chromosome where former HaplotypeCaller stopped.The ref genome and the parameters were all the same. However, HaplotypeCaller give different results. Note: the ref genome has 26 chromosomes :A01-A13;D01-D13. **_The former LOG:_**. nohup: ignoring input and appending output to nohup.out; 09:04:49.857 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:05:02.971 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.971 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:05:02.971 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:05:02.972 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:05:02.972 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:05:02.973 INFO HaplotypeCaller - Start Date/Time: August 22, 2021 9:04:49 AM CST; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.974 INFO HaplotypeCaller - HTSJDK Version: 2.16.1; 09:05:02.974 INFO HaplotypeCaller - Picard Version: 2.18.13; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:478,Testability,LOG,LOG,478,"I run HaplotypeCaller twice , the former one was stopped because of unexpected power outages. I check the LOG and found the chromosome where HaplotypeCaller stopped. So i star another HaplotypeCaller(later one) with the ""-L *.intervals"", it begin from the chromosome where former HaplotypeCaller stopped.The ref genome and the parameters were all the same. However, HaplotypeCaller give different results. Note: the ref genome has 26 chromosomes :A01-A13;D01-D13. **_The former LOG:_**. nohup: ignoring input and appending output to nohup.out; 09:04:49.857 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:05:02.971 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.971 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:05:02.971 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:05:02.972 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:05:02.972 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:05:02.973 INFO HaplotypeCaller - Start Date/Time: August 22, 2021 9:04:49 AM CST; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.974 INFO HaplotypeCaller - HTSJDK Version: 2.16.1; 09:05:02.974 INFO HaplotypeCaller - Picard Version: 2.18.13; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:13223,Testability,test,test,13223,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/issues/7454:13323,Testability,log,logs,13323,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454
https://github.com/broadinstitute/gatk/pull/7455:8,Deployability,pipeline,pipeline,8,"The VAT pipeline creates 3 tables, 2 are intermediary tables used to create the third. I was tired of deleting them, so I made them temp tables. <img width=""565"" alt=""Screen Shot 2021-09-16 at 10 44 09 AM"" src=""https://user-images.githubusercontent.com/6863459/133633262-a41466a9-2cae-4b69-8c61-28e81d1a8706.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7455
https://github.com/broadinstitute/gatk/pull/7456:407,Security,validat,validation,407,"Keeping the dockstore as is for now because I may want to run this on a few shards from the 30k while it's still in review. This pr adds a fair amount of work to the bcftools task (ExtractAnAcAfFromVCF) and adds a significant number of columns to the schema: the sample count for all of the samples, as well as for each subpopulation. Note that AC_hemi will be added in a follow on pr; Note that additional validation tests will be added in a follow on pr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7456
https://github.com/broadinstitute/gatk/pull/7456:418,Testability,test,tests,418,"Keeping the dockstore as is for now because I may want to run this on a few shards from the 30k while it's still in review. This pr adds a fair amount of work to the bcftools task (ExtractAnAcAfFromVCF) and adds a significant number of columns to the schema: the sample count for all of the samples, as well as for each subpopulation. Note that AC_hemi will be added in a follow on pr; Note that additional validation tests will be added in a follow on pr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7456
https://github.com/broadinstitute/gatk/pull/7457:459,Security,secur,secure-,459,"The manifest file describes the files produced by the GvsExtractCallset task and is uploaded to GCS once all the shards have finished running. The existence of the manifest.txt file can be used to determine if the extraction is complete or not by just using `gsutil` command and not going through Cromwell/Terra. Here's a snippet of what that looks like. ```; interval_number, vcf_file_location, vcf_file_bytes, vcf_index_location, vcf_index_bytes; 0,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz,879403,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz.tbi,1682; 1,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz,871855,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz.tbi,1670; 2,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz,710617,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz.tbi,1629; 3,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz,715565,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz.tbi,1645; ...; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7457
https://github.com/broadinstitute/gatk/pull/7457:598,Security,secur,secure-,598,"The manifest file describes the files produced by the GvsExtractCallset task and is uploaded to GCS once all the shards have finished running. The existence of the manifest.txt file can be used to determine if the extraction is complete or not by just using `gsutil` command and not going through Cromwell/Terra. Here's a snippet of what that looks like. ```; interval_number, vcf_file_location, vcf_file_bytes, vcf_index_location, vcf_index_bytes; 0,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz,879403,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz.tbi,1682; 1,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz,871855,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz.tbi,1670; 2,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz,710617,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz.tbi,1629; 3,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz,715565,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz.tbi,1645; ...; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7457
https://github.com/broadinstitute/gatk/pull/7457:742,Security,secur,secure-,742,"The manifest file describes the files produced by the GvsExtractCallset task and is uploaded to GCS once all the shards have finished running. The existence of the manifest.txt file can be used to determine if the extraction is complete or not by just using `gsutil` command and not going through Cromwell/Terra. Here's a snippet of what that looks like. ```; interval_number, vcf_file_location, vcf_file_bytes, vcf_index_location, vcf_index_bytes; 0,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz,879403,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz.tbi,1682; 1,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz,871855,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz.tbi,1670; 2,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz,710617,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz.tbi,1629; 3,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz,715565,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz.tbi,1645; ...; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7457
https://github.com/broadinstitute/gatk/pull/7457:881,Security,secur,secure-,881,"The manifest file describes the files produced by the GvsExtractCallset task and is uploaded to GCS once all the shards have finished running. The existence of the manifest.txt file can be used to determine if the extraction is complete or not by just using `gsutil` command and not going through Cromwell/Terra. Here's a snippet of what that looks like. ```; interval_number, vcf_file_location, vcf_file_bytes, vcf_index_location, vcf_index_bytes; 0,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz,879403,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz.tbi,1682; 1,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz,871855,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz.tbi,1670; 2,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz,710617,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz.tbi,1629; 3,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz,715565,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz.tbi,1645; ...; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7457
https://github.com/broadinstitute/gatk/pull/7457:1025,Security,secur,secure-,1025,"The manifest file describes the files produced by the GvsExtractCallset task and is uploaded to GCS once all the shards have finished running. The existence of the manifest.txt file can be used to determine if the extraction is complete or not by just using `gsutil` command and not going through Cromwell/Terra. Here's a snippet of what that looks like. ```; interval_number, vcf_file_location, vcf_file_bytes, vcf_index_location, vcf_index_bytes; 0,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz,879403,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz.tbi,1682; 1,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz,871855,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz.tbi,1670; 2,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz,710617,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz.tbi,1629; 3,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz,715565,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz.tbi,1645; ...; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7457
https://github.com/broadinstitute/gatk/pull/7457:1164,Security,secur,secure-,1164,"The manifest file describes the files produced by the GvsExtractCallset task and is uploaded to GCS once all the shards have finished running. The existence of the manifest.txt file can be used to determine if the extraction is complete or not by just using `gsutil` command and not going through Cromwell/Terra. Here's a snippet of what that looks like. ```; interval_number, vcf_file_location, vcf_file_bytes, vcf_index_location, vcf_index_bytes; 0,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz,879403,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz.tbi,1682; 1,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz,871855,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz.tbi,1670; 2,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz,710617,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz.tbi,1629; 3,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz,715565,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz.tbi,1645; ...; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7457
https://github.com/broadinstitute/gatk/pull/7457:1308,Security,secur,secure-,1308,"The manifest file describes the files produced by the GvsExtractCallset task and is uploaded to GCS once all the shards have finished running. The existence of the manifest.txt file can be used to determine if the extraction is complete or not by just using `gsutil` command and not going through Cromwell/Terra. Here's a snippet of what that looks like. ```; interval_number, vcf_file_location, vcf_file_bytes, vcf_index_location, vcf_index_bytes; 0,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz,879403,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz.tbi,1682; 1,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz,871855,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz.tbi,1670; 2,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz,710617,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz.tbi,1629; 3,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz,715565,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz.tbi,1645; ...; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7457
https://github.com/broadinstitute/gatk/pull/7457:1447,Security,secur,secure-,1447,"The manifest file describes the files produced by the GvsExtractCallset task and is uploaded to GCS once all the shards have finished running. The existence of the manifest.txt file can be used to determine if the extraction is complete or not by just using `gsutil` command and not going through Cromwell/Terra. Here's a snippet of what that looks like. ```; interval_number, vcf_file_location, vcf_file_bytes, vcf_index_location, vcf_index_bytes; 0,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz,879403,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_0.vcf.gz.tbi,1682; 1,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz,871855,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_1.vcf.gz.tbi,1670; 2,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz,710617,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_2.vcf.gz.tbi,1629; 3,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz,715565,gs://fc-secure-765db3ba-3e2d-432e-89eb-9efda7430f93/genomic-extractions/66f225d3-35cf-4ff7-a4ec-3d887a86ea8b/vcfs/interval_3.vcf.gz.tbi,1645; ...; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7457
https://github.com/broadinstitute/gatk/issues/7459:240,Deployability,release,release,240,"I am running `gatk GenotypeGVCFs` and want to get output for all genomic sites. I was expecting the output to include parameters like QUAL and mapping quality (MQ) for invariant sites. This is based on a previous study that used an earlier release of GATK, v2.8-1 ([reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4617969/)) that used QUAL and MQ values from that sites (although I am aware that these values are computed differently as for variant sites). However, the version I am using, v4.2.1.0, seems to not produce this output, and I cannot find a relevant option to include it. Am I missing something? is there anyway to get this information?. GATK is run as:. gatk HaplotypeCaller -I sample1.bam -O sample1.vcf -R reference.fa -ploidy 1 -ERC BP_RESOLUTION -stand-call-conf 10.0; ...; gatk CombineGVCFs -R reference.fa -O all_samples.g.vcf --variant sample1.vcf --variant sample2.vcf ...; gatk GenotypeGVCFs -R reference.fa -V all_samples.g.vcf -O all_samples.vcf -ploidy 1 -all-sites. Thank you",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7459
https://github.com/broadinstitute/gatk/issues/7460:160,Availability,error,error,160,"Hello, I was using GATK4 LiftoverVcf to adjust the coordinates of variants in my vcf4.2 file (created by plink1.9) to hg38 reference build. However, I got this error:; The provided VCF file is malformed at approximately line number 218: Insertions/Deletions are not supported when reading 3.x VCF's. Please convert your file to VCF4 using VCFTools, available at http://vcftools.sourceforge.net/index.html. And I tried to use vcf-convert in vcftools to convert my vcf4.2 file to vcf4.0 form, but the problem was not solved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7460
https://github.com/broadinstitute/gatk/issues/7460:349,Availability,avail,available,349,"Hello, I was using GATK4 LiftoverVcf to adjust the coordinates of variants in my vcf4.2 file (created by plink1.9) to hg38 reference build. However, I got this error:; The provided VCF file is malformed at approximately line number 218: Insertions/Deletions are not supported when reading 3.x VCF's. Please convert your file to VCF4 using VCFTools, available at http://vcftools.sourceforge.net/index.html. And I tried to use vcf-convert in vcftools to convert my vcf4.2 file to vcf4.0 form, but the problem was not solved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7460
https://github.com/broadinstitute/gatk/pull/7462:48,Performance,cache,cache,48,"Making GetSampleIds volatile so it doesn't call cache to outdated information (since BQ could have changed, and cromwell wouldn't know it)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7462
https://github.com/broadinstitute/gatk/pull/7463:106,Deployability,update,updates,106,- added GvsAssignIds to .dockstore.yaml; - added logic to GvsAssignIds to prevent bug from empty input; - updates to Quickstart README directions. Closes https://broadworkbench.atlassian.net/browse/VS-183,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7463
https://github.com/broadinstitute/gatk/pull/7463:49,Testability,log,logic,49,- added GvsAssignIds to .dockstore.yaml; - added logic to GvsAssignIds to prevent bug from empty input; - updates to Quickstart README directions. Closes https://broadworkbench.atlassian.net/browse/VS-183,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7463
https://github.com/broadinstitute/gatk/pull/7464:126,Deployability,configurat,configuration,126,gCNV in the CASE mode now fills in all hidden DenoisingModelConfig and CopyNumberCallingConfig arguments from the input model configuration. . This addresses issue #6994,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7464
https://github.com/broadinstitute/gatk/pull/7464:126,Modifiability,config,configuration,126,gCNV in the CASE mode now fills in all hidden DenoisingModelConfig and CopyNumberCallingConfig arguments from the input model configuration. . This addresses issue #6994,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7464
https://github.com/broadinstitute/gatk/issues/7465:420,Availability,error,error-IndexOutOfBoundsException,420,"This issue came up on the forum and Ted Brookings took a look at the stack trace to verify that it is most likely not an issue with the input data. The user also specified that they did not get this issue with GATK 4.2.0.0. This request was created from a contribution made by Quentin Chartreux on September 10, 2021 12:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException). \--. I try to perform joint genotyping with genotypeGVCF.. Before i run haplotypecaller then Genomicsdbiimport and now genotypesGVCF.. I used gatk 4.2.2.0.. I run Genomicsdbiimport by interval so i would like to perform genotypesGVCF by interval.. the command use for genotypesgvcf is :. gatk \\ ; ; \--java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" \\ ; ; GenotypeGVCFs \\ ; ; \-R ${REF\_Genome} \\ ; ; \-V gendb://${vcf\_database\_tmp} \\ ; ; \-O ${TMP\_DIR}/gentaumix\_interval\_${SLURM\_ARRAY\_TASK\_ID}\_raw.vcf.gz \\ ; ; \-D ${DBSNP} \\ ; ; \--sequence-dictionary ${Dict} \\ ; ; \-L ${Interval} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation \\ ; ; \--only-output-calls-starting-in-intervals \\ ; ; \--merge-input-intervals \\ ; ; 2> ${log\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}. And the log (for one interval but it's the same for all):. Using GATK jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:538,Availability,error,error-IndexOutOfBoundsException,538,"This issue came up on the forum and Ted Brookings took a look at the stack trace to verify that it is most likely not an issue with the input data. The user also specified that they did not get this issue with GATK 4.2.0.0. This request was created from a contribution made by Quentin Chartreux on September 10, 2021 12:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException). \--. I try to perform joint genotyping with genotypeGVCF.. Before i run haplotypecaller then Genomicsdbiimport and now genotypesGVCF.. I used gatk 4.2.2.0.. I run Genomicsdbiimport by interval so i would like to perform genotypesGVCF by interval.. the command use for genotypesgvcf is :. gatk \\ ; ; \--java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" \\ ; ; GenotypeGVCFs \\ ; ; \-R ${REF\_Genome} \\ ; ; \-V gendb://${vcf\_database\_tmp} \\ ; ; \-O ${TMP\_DIR}/gentaumix\_interval\_${SLURM\_ARRAY\_TASK\_ID}\_raw.vcf.gz \\ ; ; \-D ${DBSNP} \\ ; ; \--sequence-dictionary ${Dict} \\ ; ; \-L ${Interval} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation \\ ; ; \--only-output-calls-starting-in-intervals \\ ; ; \--merge-input-intervals \\ ; ; 2> ${log\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}. And the log (for one interval but it's the same for all):. Using GATK jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:2646,Availability,Redundant,Redundant,2646,"mtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/projects/gentaumix/Ressources/grch38\_BWA\_2/GCA\_000001405.15\_GRCh38\_no\_alt\_plus\_hs38d1\_analysis\_set.fa -V gendb:///tmp/tmp.6QEyWPGpWs/vcf\_database/Interval\_6 -O /tmp/tmp.6QEyWPGpWs/gentaumix\_interval\_6\_raw.vcf.gz -D /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dbsnp138.vcf --sequence-dictionary /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dict -L /shared/projects/gentaumix/Ressources/interval\_genomicsdbi/temp\_6/interval.interval\_list -G StandardAnnotation -G AS\_StandardAnnotation --merge-input-intervals ; ; 14:17:35.171 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 14:17:35.232 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Sep 10, 2021 2:17:35 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:17:35.492 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.492 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 14:17:35.492 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:17:35.493 INFO GenotypeGVCFs - Executing as quentin67100@cpu-node-9 on Linux v3.10.0-1160.6.1.el7.x86\_64 amd64 ; ; 14:17:35.493 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:6820,Availability,down,down,6820," found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records ; ; 14:17:39.941 info NativeGenomicsDB - pid=12231 tid=12232 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records ; ; 14:17:41.513 INFO FeatureManager - Using codec IntervalListCodec to read file file:///shared/projects/gentaumix/Ressources/interval\_genomicsdbi/temp\_6/interval.interval\_list ; ; 14:17:41.628 INFO IntervalArgumentCollection - Processing 62000000 bp from intervals ; ; 14:17:41.743 INFO GenotypeGVCFs - Done initializing engine ; ; 14:17:41.897 INFO ProgressMeter - Starting traversal ; ; 14:17:41.898 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute ; ; 14:17:52.020 INFO ProgressMeter - chr2:60009645 0.2 9000 53349.1 ; ; 14:18:02.071 INFO ProgressMeter - chr2:60039632 0.3 37000 110048.1 ; ; 14:18:02.683 INFO GenotypeGVCFs - Shutting down engine ; ; GENOMICSDB\_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),5.257768857000008,Cpu time(s),5.221303873999989 ; ; \[10 septembre 2021 14:18:02 CEST\] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.46 minutes. ; ; Runtime.totalMemory()=7034372096 ; ; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0 ; ; at java.util.ArrayList.rangeCheck(ArrayList.java:659) ; ; at java.util.ArrayList.get(ArrayList.java:435) ; ; at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.combineAttributeMap(StrandBiasUtils.java:120) ; ; at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS\_StrandBiasTest.combineRawData(AS\_StrandBiasTest.java:118) ; ; at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:234) ; ; at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:8918,Integrability,wrap,wrapAndCopyInto,8918,iantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142) ; ; at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:130) ; ; at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:281) ; ; at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; ; at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ; at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ; at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ; at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ; at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ; ; at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; ; at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490) ; ; at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:586,Performance,perform,perform,586,"This issue came up on the forum and Ted Brookings took a look at the stack trace to verify that it is most likely not an issue with the input data. The user also specified that they did not get this issue with GATK 4.2.0.0. This request was created from a contribution made by Quentin Chartreux on September 10, 2021 12:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException). \--. I try to perform joint genotyping with genotypeGVCF.. Before i run haplotypecaller then Genomicsdbiimport and now genotypesGVCF.. I used gatk 4.2.2.0.. I run Genomicsdbiimport by interval so i would like to perform genotypesGVCF by interval.. the command use for genotypesgvcf is :. gatk \\ ; ; \--java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" \\ ; ; GenotypeGVCFs \\ ; ; \-R ${REF\_Genome} \\ ; ; \-V gendb://${vcf\_database\_tmp} \\ ; ; \-O ${TMP\_DIR}/gentaumix\_interval\_${SLURM\_ARRAY\_TASK\_ID}\_raw.vcf.gz \\ ; ; \-D ${DBSNP} \\ ; ; \--sequence-dictionary ${Dict} \\ ; ; \-L ${Interval} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation \\ ; ; \--only-output-calls-starting-in-intervals \\ ; ; \--merge-input-intervals \\ ; ; 2> ${log\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}. And the log (for one interval but it's the same for all):. Using GATK jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:787,Performance,perform,perform,787,"This issue came up on the forum and Ted Brookings took a look at the stack trace to verify that it is most likely not an issue with the input data. The user also specified that they did not get this issue with GATK 4.2.0.0. This request was created from a contribution made by Quentin Chartreux on September 10, 2021 12:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException). \--. I try to perform joint genotyping with genotypeGVCF.. Before i run haplotypecaller then Genomicsdbiimport and now genotypesGVCF.. I used gatk 4.2.2.0.. I run Genomicsdbiimport by interval so i would like to perform genotypesGVCF by interval.. the command use for genotypesgvcf is :. gatk \\ ; ; \--java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" \\ ; ; GenotypeGVCFs \\ ; ; \-R ${REF\_Genome} \\ ; ; \-V gendb://${vcf\_database\_tmp} \\ ; ; \-O ${TMP\_DIR}/gentaumix\_interval\_${SLURM\_ARRAY\_TASK\_ID}\_raw.vcf.gz \\ ; ; \-D ${DBSNP} \\ ; ; \--sequence-dictionary ${Dict} \\ ; ; \-L ${Interval} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation \\ ; ; \--only-output-calls-starting-in-intervals \\ ; ; \--merge-input-intervals \\ ; ; 2> ${log\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}. And the log (for one interval but it's the same for all):. Using GATK jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:2782,Performance,Load,Loading,2782,"e -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/projects/gentaumix/Ressources/grch38\_BWA\_2/GCA\_000001405.15\_GRCh38\_no\_alt\_plus\_hs38d1\_analysis\_set.fa -V gendb:///tmp/tmp.6QEyWPGpWs/vcf\_database/Interval\_6 -O /tmp/tmp.6QEyWPGpWs/gentaumix\_interval\_6\_raw.vcf.gz -D /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dbsnp138.vcf --sequence-dictionary /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dict -L /shared/projects/gentaumix/Ressources/interval\_genomicsdbi/temp\_6/interval.interval\_list -G StandardAnnotation -G AS\_StandardAnnotation --merge-input-intervals ; ; 14:17:35.171 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 14:17:35.232 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Sep 10, 2021 2:17:35 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:17:35.492 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.492 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 14:17:35.492 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:17:35.493 INFO GenotypeGVCFs - Executing as quentin67100@cpu-node-9 on Linux v3.10.0-1160.6.1.el7.x86\_64 amd64 ; ; 14:17:35.493 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_282-b08 ; ; 14:17:35.493 INFO GenotypeGVCFs - Start Date",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:11097,Performance,optimiz,optimizations,11097,"g.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289). For information the command used for haplotype caller and genomicsdbiimport are :. srun --ntasks=1 gatk --java-options ""-Xmx${SLURM\_MEM\_PER\_CPU}M"" HaplotypeCaller \\ ; ; \-R ${REF\_Genome} \\ ; ; \-L ${Scattered\_DIR}/temp\_${i}\_of\_${SCATTER\_COUNT}/scattered.interval\_list \\ ; ; \-I ${BAM\_INPUT\_DIR}/${BAM\_INPUT} \\ ; ; \-O ${temp\_gVCF\_OUTPUT\_DIR}/${i}.${GVCF\_OUTPUT} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation -G StandardHCAnnotation \\ ; ; \-GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \\ ; ; \-ERC GVCF \\ ; ; \--pcr-indel-model NONE \\ ; ; 2> ${logs\_HC}/${i}.${GVCF\_OUTPUT}.log &. and. gatk --java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" GenomicsDBImport \\ ; ; \--sample-name-map ${cohort\_map} \\ ; ; \--genomicsdb-workspace-path ${VCF\_database\_DIR\_tmp}/Interval\_${SLURM\_ARRAY\_TASK\_ID} \\ ; ; \--batch-size 74 \\ ; ; \--reader-threads ${SLURM\_CPUS\_PER\_TASK} \\ ; ; \-L ${Interval} \\ ; ; \--tmp-dir ${TMP\_DIR} \\ ; ; \--genomicsdb-shared-posixfs-optimizations \\ ; ; 2> ${logs\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}.log<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/186539'>Zendesk ticket #186539</a>)<br>gz#186539</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:2646,Safety,Redund,Redundant,2646,"mtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/projects/gentaumix/Ressources/grch38\_BWA\_2/GCA\_000001405.15\_GRCh38\_no\_alt\_plus\_hs38d1\_analysis\_set.fa -V gendb:///tmp/tmp.6QEyWPGpWs/vcf\_database/Interval\_6 -O /tmp/tmp.6QEyWPGpWs/gentaumix\_interval\_6\_raw.vcf.gz -D /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dbsnp138.vcf --sequence-dictionary /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dict -L /shared/projects/gentaumix/Ressources/interval\_genomicsdbi/temp\_6/interval.interval\_list -G StandardAnnotation -G AS\_StandardAnnotation --merge-input-intervals ; ; 14:17:35.171 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 14:17:35.232 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Sep 10, 2021 2:17:35 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:17:35.492 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.492 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 14:17:35.492 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:17:35.493 INFO GenotypeGVCFs - Executing as quentin67100@cpu-node-9 on Linux v3.10.0-1160.6.1.el7.x86\_64 amd64 ; ; 14:17:35.493 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:3123,Safety,detect,detect,3123,"endb:///tmp/tmp.6QEyWPGpWs/vcf\_database/Interval\_6 -O /tmp/tmp.6QEyWPGpWs/gentaumix\_interval\_6\_raw.vcf.gz -D /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dbsnp138.vcf --sequence-dictionary /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dict -L /shared/projects/gentaumix/Ressources/interval\_genomicsdbi/temp\_6/interval.interval\_list -G StandardAnnotation -G AS\_StandardAnnotation --merge-input-intervals ; ; 14:17:35.171 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 14:17:35.232 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Sep 10, 2021 2:17:35 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:17:35.492 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.492 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 14:17:35.492 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:17:35.493 INFO GenotypeGVCFs - Executing as quentin67100@cpu-node-9 on Linux v3.10.0-1160.6.1.el7.x86\_64 amd64 ; ; 14:17:35.493 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_282-b08 ; ; 14:17:35.493 INFO GenotypeGVCFs - Start Date/Time: 10 septembre 2021 14:17:35 CEST ; ; 14:17:35.493 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.494 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.495 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1 ; ; 14:17:35.495 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:1386,Testability,log,log,1386,"ux on September 10, 2021 12:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException). \--. I try to perform joint genotyping with genotypeGVCF.. Before i run haplotypecaller then Genomicsdbiimport and now genotypesGVCF.. I used gatk 4.2.2.0.. I run Genomicsdbiimport by interval so i would like to perform genotypesGVCF by interval.. the command use for genotypesgvcf is :. gatk \\ ; ; \--java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" \\ ; ; GenotypeGVCFs \\ ; ; \-R ${REF\_Genome} \\ ; ; \-V gendb://${vcf\_database\_tmp} \\ ; ; \-O ${TMP\_DIR}/gentaumix\_interval\_${SLURM\_ARRAY\_TASK\_ID}\_raw.vcf.gz \\ ; ; \-D ${DBSNP} \\ ; ; \--sequence-dictionary ${Dict} \\ ; ; \-L ${Interval} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation \\ ; ; \--only-output-calls-starting-in-intervals \\ ; ; \--merge-input-intervals \\ ; ; 2> ${log\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}. And the log (for one interval but it's the same for all):. Using GATK jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/projects/gentaumix/Ressources/grch38\_BWA\_2/GCA\_000001405.15\_GRCh38\_no\_alt\_plus\_hs38d1\_analysis\_set.fa -V gendb:///tmp/tmp.6QEyWPGpWs/vcf\_database/Interval\_6 -O /tmp/tmp.6QEyWPGpWs/gentaumix\_interval\_6\_raw.vcf.gz -D /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:1441,Testability,log,log,1441,"atk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException). \--. I try to perform joint genotyping with genotypeGVCF.. Before i run haplotypecaller then Genomicsdbiimport and now genotypesGVCF.. I used gatk 4.2.2.0.. I run Genomicsdbiimport by interval so i would like to perform genotypesGVCF by interval.. the command use for genotypesgvcf is :. gatk \\ ; ; \--java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" \\ ; ; GenotypeGVCFs \\ ; ; \-R ${REF\_Genome} \\ ; ; \-V gendb://${vcf\_database\_tmp} \\ ; ; \-O ${TMP\_DIR}/gentaumix\_interval\_${SLURM\_ARRAY\_TASK\_ID}\_raw.vcf.gz \\ ; ; \-D ${DBSNP} \\ ; ; \--sequence-dictionary ${Dict} \\ ; ; \-L ${Interval} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation \\ ; ; \--only-output-calls-starting-in-intervals \\ ; ; \--merge-input-intervals \\ ; ; 2> ${log\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}. And the log (for one interval but it's the same for all):. Using GATK jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/projects/gentaumix/Ressources/grch38\_BWA\_2/GCA\_000001405.15\_GRCh38\_no\_alt\_plus\_hs38d1\_analysis\_set.fa -V gendb:///tmp/tmp.6QEyWPGpWs/vcf\_database/Interval\_6 -O /tmp/tmp.6QEyWPGpWs/gentaumix\_interval\_6\_raw.vcf.gz -D /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dbsnp138.vcf --sequence-dictionary /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dict -L /shared/projects/gentaumix/Ressourc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:10620,Testability,log,logs,10620,"g.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289). For information the command used for haplotype caller and genomicsdbiimport are :. srun --ntasks=1 gatk --java-options ""-Xmx${SLURM\_MEM\_PER\_CPU}M"" HaplotypeCaller \\ ; ; \-R ${REF\_Genome} \\ ; ; \-L ${Scattered\_DIR}/temp\_${i}\_of\_${SCATTER\_COUNT}/scattered.interval\_list \\ ; ; \-I ${BAM\_INPUT\_DIR}/${BAM\_INPUT} \\ ; ; \-O ${temp\_gVCF\_OUTPUT\_DIR}/${i}.${GVCF\_OUTPUT} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation -G StandardHCAnnotation \\ ; ; \-GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \\ ; ; \-ERC GVCF \\ ; ; \--pcr-indel-model NONE \\ ; ; 2> ${logs\_HC}/${i}.${GVCF\_OUTPUT}.log &. and. gatk --java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" GenomicsDBImport \\ ; ; \--sample-name-map ${cohort\_map} \\ ; ; \--genomicsdb-workspace-path ${VCF\_database\_DIR\_tmp}/Interval\_${SLURM\_ARRAY\_TASK\_ID} \\ ; ; \--batch-size 74 \\ ; ; \--reader-threads ${SLURM\_CPUS\_PER\_TASK} \\ ; ; \-L ${Interval} \\ ; ; \--tmp-dir ${TMP\_DIR} \\ ; ; \--genomicsdb-shared-posixfs-optimizations \\ ; ; 2> ${logs\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}.log<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/186539'>Zendesk ticket #186539</a>)<br>gz#186539</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:10651,Testability,log,log,10651,"g.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289). For information the command used for haplotype caller and genomicsdbiimport are :. srun --ntasks=1 gatk --java-options ""-Xmx${SLURM\_MEM\_PER\_CPU}M"" HaplotypeCaller \\ ; ; \-R ${REF\_Genome} \\ ; ; \-L ${Scattered\_DIR}/temp\_${i}\_of\_${SCATTER\_COUNT}/scattered.interval\_list \\ ; ; \-I ${BAM\_INPUT\_DIR}/${BAM\_INPUT} \\ ; ; \-O ${temp\_gVCF\_OUTPUT\_DIR}/${i}.${GVCF\_OUTPUT} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation -G StandardHCAnnotation \\ ; ; \-GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \\ ; ; \-ERC GVCF \\ ; ; \--pcr-indel-model NONE \\ ; ; 2> ${logs\_HC}/${i}.${GVCF\_OUTPUT}.log &. and. gatk --java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" GenomicsDBImport \\ ; ; \--sample-name-map ${cohort\_map} \\ ; ; \--genomicsdb-workspace-path ${VCF\_database\_DIR\_tmp}/Interval\_${SLURM\_ARRAY\_TASK\_ID} \\ ; ; \--batch-size 74 \\ ; ; \--reader-threads ${SLURM\_CPUS\_PER\_TASK} \\ ; ; \-L ${Interval} \\ ; ; \--tmp-dir ${TMP\_DIR} \\ ; ; \--genomicsdb-shared-posixfs-optimizations \\ ; ; 2> ${logs\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}.log<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/186539'>Zendesk ticket #186539</a>)<br>gz#186539</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:11123,Testability,log,logs,11123,"g.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289). For information the command used for haplotype caller and genomicsdbiimport are :. srun --ntasks=1 gatk --java-options ""-Xmx${SLURM\_MEM\_PER\_CPU}M"" HaplotypeCaller \\ ; ; \-R ${REF\_Genome} \\ ; ; \-L ${Scattered\_DIR}/temp\_${i}\_of\_${SCATTER\_COUNT}/scattered.interval\_list \\ ; ; \-I ${BAM\_INPUT\_DIR}/${BAM\_INPUT} \\ ; ; \-O ${temp\_gVCF\_OUTPUT\_DIR}/${i}.${GVCF\_OUTPUT} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation -G StandardHCAnnotation \\ ; ; \-GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \\ ; ; \-ERC GVCF \\ ; ; \--pcr-indel-model NONE \\ ; ; 2> ${logs\_HC}/${i}.${GVCF\_OUTPUT}.log &. and. gatk --java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" GenomicsDBImport \\ ; ; \--sample-name-map ${cohort\_map} \\ ; ; \--genomicsdb-workspace-path ${VCF\_database\_DIR\_tmp}/Interval\_${SLURM\_ARRAY\_TASK\_ID} \\ ; ; \--batch-size 74 \\ ; ; \--reader-threads ${SLURM\_CPUS\_PER\_TASK} \\ ; ; \-L ${Interval} \\ ; ; \--tmp-dir ${TMP\_DIR} \\ ; ; \--genomicsdb-shared-posixfs-optimizations \\ ; ; 2> ${logs\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}.log<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/186539'>Zendesk ticket #186539</a>)<br>gz#186539</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/issues/7465:11170,Testability,log,log,11170,"g.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289). For information the command used for haplotype caller and genomicsdbiimport are :. srun --ntasks=1 gatk --java-options ""-Xmx${SLURM\_MEM\_PER\_CPU}M"" HaplotypeCaller \\ ; ; \-R ${REF\_Genome} \\ ; ; \-L ${Scattered\_DIR}/temp\_${i}\_of\_${SCATTER\_COUNT}/scattered.interval\_list \\ ; ; \-I ${BAM\_INPUT\_DIR}/${BAM\_INPUT} \\ ; ; \-O ${temp\_gVCF\_OUTPUT\_DIR}/${i}.${GVCF\_OUTPUT} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation -G StandardHCAnnotation \\ ; ; \-GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \\ ; ; \-ERC GVCF \\ ; ; \--pcr-indel-model NONE \\ ; ; 2> ${logs\_HC}/${i}.${GVCF\_OUTPUT}.log &. and. gatk --java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" GenomicsDBImport \\ ; ; \--sample-name-map ${cohort\_map} \\ ; ; \--genomicsdb-workspace-path ${VCF\_database\_DIR\_tmp}/Interval\_${SLURM\_ARRAY\_TASK\_ID} \\ ; ; \--batch-size 74 \\ ; ; \--reader-threads ${SLURM\_CPUS\_PER\_TASK} \\ ; ; \-L ${Interval} \\ ; ; \--tmp-dir ${TMP\_DIR} \\ ; ; \--genomicsdb-shared-posixfs-optimizations \\ ; ; 2> ${logs\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}.log<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/186539'>Zendesk ticket #186539</a>)<br>gz#186539</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465
https://github.com/broadinstitute/gatk/pull/7468:244,Deployability,update,updates,244,"If there are overlapping (e.g. a long SNV overlapping an INDEL) or multi-allelic germline variants, Mutect2 will check only the AF of the first variant/allele when searching for germline sites to exclude during active region detection. This PR updates the logic to iterate through all germline alleles.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7468
https://github.com/broadinstitute/gatk/pull/7468:225,Safety,detect,detection,225,"If there are overlapping (e.g. a long SNV overlapping an INDEL) or multi-allelic germline variants, Mutect2 will check only the AF of the first variant/allele when searching for germline sites to exclude during active region detection. This PR updates the logic to iterate through all germline alleles.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7468
https://github.com/broadinstitute/gatk/pull/7468:256,Testability,log,logic,256,"If there are overlapping (e.g. a long SNV overlapping an INDEL) or multi-allelic germline variants, Mutect2 will check only the AF of the first variant/allele when searching for germline sites to exclude during active region detection. This PR updates the logic to iterate through all germline alleles.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7468
https://github.com/broadinstitute/gatk/pull/7472:31,Deployability,pipeline,pipeline,31,"This pr adds a step to the VAT pipeline which will export each chromosome of the VAT (TODO need to add X, Y, M) into it's own directory in GCS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7472
https://github.com/broadinstitute/gatk/issues/7474:55,Availability,down,down,55,"## Bug Report; 18:47:11.757 INFO Funcotator - Shutting down engine; [September 19, 2021 6:47:11 PM CST] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinx",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474
https://github.com/broadinstitute/gatk/issues/7474:840,Availability,error,error,840,"## Bug Report; 18:47:11.757 INFO Funcotator - Shutting down engine; [September 19, 2021 6:47:11 PM CST] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinx",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474
https://github.com/broadinstitute/gatk/issues/7474:893,Availability,error,error,893,"## Bug Report; 18:47:11.757 INFO Funcotator - Shutting down engine; [September 19, 2021 6:47:11 PM CST] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinx",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474
https://github.com/broadinstitute/gatk/issues/7474:1052,Availability,error,error,1052,"ing down engine; [September 19, 2021 6:47:11 PM CST] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinxi_pkuhpc/lus; tre1/ljx/reference_genomes/funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474
https://github.com/broadinstitute/gatk/issues/7474:1103,Availability,error,error,1103,"ing down engine; [September 19, 2021 6:47:11 PM CST] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinxi_pkuhpc/lus; tre1/ljx/reference_genomes/funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474
https://github.com/broadinstitute/gatk/issues/7474:1113,Availability,down,downloaded,1113,"T] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinxi_pkuhpc/lus; tre1/ljx/reference_genomes/funcotator_dataSources.v1.7.20200521s/ --ref; -version hg38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474
https://github.com/broadinstitute/gatk/issues/7474:1283,Availability,error,error,1283,"T] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinxi_pkuhpc/lus; tre1/ljx/reference_genomes/funcotator_dataSources.v1.7.20200521s/ --ref; -version hg38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474
https://github.com/broadinstitute/gatk/issues/7474:877,Testability,log,logic,877,"## Bug Report; 18:47:11.757 INFO Funcotator - Shutting down engine; [September 19, 2021 6:47:11 PM CST] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinx",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474
https://github.com/broadinstitute/gatk/issues/7474:1087,Testability,log,logic,1087,"ing down engine; [September 19, 2021 6:47:11 PM CST] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinxi_pkuhpc/lus; tre1/ljx/reference_genomes/funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474
https://github.com/broadinstitute/gatk/issues/7479:95,Availability,error,error,95,"This user was getting a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2  GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2)for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is:. gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:253,Availability,error,error,253,"This user was getting a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2  GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2)for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is:. gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:1010,Availability,error,error,1010,"ing a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2  GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2)for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is:. gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processi ng\_faci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:1503,Availability,error,error,1503,"e-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2  GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2)for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is:. gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processi ng\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gat k/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common \_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA \_B07.table ; ; 01:03:32.752 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar: file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compressi on.so ; ; Sep 12, 2021 1:03:32 AM shaded.cloud\_nio.com.goo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:4684,Availability,down,down,4684,"rk Version: 2.4.5 ; ; 01:03:32.955 INFO GetPileupSummaries - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 01:03:32.955 INFO GetPileupSummaries - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SA MTOOLS : false ; ; 01:03:32.955 INFO GetPileupSummaries - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_S AMTOOLS : true ; ; 01:03:32.956 INFO GetPileupSummaries - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_T RIBBLE : false ; ; 01:03:32.956 INFO GetPileupSummaries - Deflater: IntelDeflater ; ; 01:03:32.956 INFO GetPileupSummaries - Inflater: IntelInflater ; ; 01:03:32.956 INFO GetPileupSummaries - GCS max retries/reopens: 20 ; ; 01:03:32.956 INFO GetPileupSummaries - Requester pays: disabled ; ; 01:03:32.956 INFO GetPileupSummaries - Initializing engine ; ; 01:03:33.330 INFO FeatureManager - Using codec VCFCodec to read file file:///ga tk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_commo n\_3.hg19.vcf.gz ; ; 01:03:33.395 INFO GetPileupSummaries - Shutting down engine ; ; \[September 12, 2021 1:03:33 AM GMT\] org.broadinstitute.hellbender.tools.walkers. contamination.GetPileupSummaries done. Elapsed time: 0.01 minutes. ; ; Runtime.totalMemory()=462946304 ; ; java.lang.IllegalArgumentException: Dictionary cannot have size zero ; ; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798) ; ; at org.broadinstitute.hellbender.utils.MRUCachingSAMSequenceDictionary.< init>(MRUCachingSAMSequenceDictionary.java:35) ; ; at org.broadinstitute.hellbender.utils.GenomeLocParser.<init>(GenomeLocP arser.java:78) ; ; at org.broadinstitute.hellbender.utils.GenomeLocParser.<init>(GenomeLocP arser.java:62) ; ; at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArg umentCollection.getTraversalParameters(IntervalArgumentCollection.java:180) ; ; at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArg umentCollection.getIntervals(IntervalArgumentCollection.java:111) ; ; at org.broadinstitute.hellbender.engine.GATKTool.i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:101,Integrability,message,message,101,"This user was getting a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2  GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2)for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is:. gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:259,Integrability,message,message,259,"This user was getting a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2  GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2)for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is:. gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:2331,Performance,Load,Loading,2331,"ng\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processi ng\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gat k/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common \_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA \_B07.table ; ; 01:03:32.752 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar: file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compressi on.so ; ; Sep 12, 2021 1:03:32 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCre dentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 01:03:32.953 INFO GetPileupSummaries - ---------------------------------------- -------------------- ; ; 01:03:32.954 INFO GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.2. 0.0 ; ; 01:03:32.954 INFO GetPileupSummaries - For support and documentation go to http s://software.broadinstitute.org/gatk/ ; ; 01:03:32.954 INFO GetPileupSummaries - Executing as root@a2e87404023d on Linux v5.8.0-1039-azure amd64 ; ; 01:03:32.954 INFO GetPileupSummaries - Java runtime: OpenJDK 64-Bit Server VM v 1.8.0\_242-8u242-b08-0ubuntu3~18.04-b08 ; ; 01:03:32.954 INFO GetPileupSummaries - Start Date/Time: September 12, 2021 1:03 :32 AM GMT ; ; 01:03:32.955 INFO GetPileupSumm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:963,Safety,detect,detecting,963,"This user was getting a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2  GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2)for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is:. gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:2598,Safety,detect,detect,2598,"ur help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processi ng\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gat k/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common \_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA \_B07.table ; ; 01:03:32.752 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar: file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compressi on.so ; ; Sep 12, 2021 1:03:32 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCre dentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 01:03:32.953 INFO GetPileupSummaries - ---------------------------------------- -------------------- ; ; 01:03:32.954 INFO GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.2. 0.0 ; ; 01:03:32.954 INFO GetPileupSummaries - For support and documentation go to http s://software.broadinstitute.org/gatk/ ; ; 01:03:32.954 INFO GetPileupSummaries - Executing as root@a2e87404023d on Linux v5.8.0-1039-azure amd64 ; ; 01:03:32.954 INFO GetPileupSummaries - Java runtime: OpenJDK 64-Bit Server VM v 1.8.0\_242-8u242-b08-0ubuntu3~18.04-b08 ; ; 01:03:32.954 INFO GetPileupSummaries - Start Date/Time: September 12, 2021 1:03 :32 AM GMT ; ; 01:03:32.955 INFO GetPileupSummaries - ---------------------------------------- -------------------- ; ; 01:03:32.955 INFO GetPileupSummaries - ---------------------------------------- -------------------- ; ; 01:03:32.955 INFO GetPileupSummaries - HTSJDK Version: 2.24.0 ; ; 01:03:32.955 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:5008,Security,validat,validateArg,5008," INFO GetPileupSummaries - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_T RIBBLE : false ; ; 01:03:32.956 INFO GetPileupSummaries - Deflater: IntelDeflater ; ; 01:03:32.956 INFO GetPileupSummaries - Inflater: IntelInflater ; ; 01:03:32.956 INFO GetPileupSummaries - GCS max retries/reopens: 20 ; ; 01:03:32.956 INFO GetPileupSummaries - Requester pays: disabled ; ; 01:03:32.956 INFO GetPileupSummaries - Initializing engine ; ; 01:03:33.330 INFO FeatureManager - Using codec VCFCodec to read file file:///ga tk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_commo n\_3.hg19.vcf.gz ; ; 01:03:33.395 INFO GetPileupSummaries - Shutting down engine ; ; \[September 12, 2021 1:03:33 AM GMT\] org.broadinstitute.hellbender.tools.walkers. contamination.GetPileupSummaries done. Elapsed time: 0.01 minutes. ; ; Runtime.totalMemory()=462946304 ; ; java.lang.IllegalArgumentException: Dictionary cannot have size zero ; ; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798) ; ; at org.broadinstitute.hellbender.utils.MRUCachingSAMSequenceDictionary.< init>(MRUCachingSAMSequenceDictionary.java:35) ; ; at org.broadinstitute.hellbender.utils.GenomeLocParser.<init>(GenomeLocP arser.java:78) ; ; at org.broadinstitute.hellbender.utils.GenomeLocParser.<init>(GenomeLocP arser.java:62) ; ; at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArg umentCollection.getTraversalParameters(IntervalArgumentCollection.java:180) ; ; at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArg umentCollection.getIntervals(IntervalArgumentCollection.java:111) ; ; at org.broadinstitute.hellbender.engine.GATKTool.initializeIntervals(GAT KTool.java:514) ; ; at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java :709) ; ; at org.broadinstitute.hellbender.engine.LocusWalker.onStartup(LocusWalke r.java:136) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(Comm andLineProgram.java:138) ; ; at org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/issues/7479:1509,Testability,log,log,1509,"e-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2  GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2)for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is:. gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processi ng\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gat k/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common \_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA \_B07.table ; ; 01:03:32.752 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar: file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compressi on.so ; ; Sep 12, 2021 1:03:32 AM shaded.cloud\_nio.com.goo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479
https://github.com/broadinstitute/gatk/pull/7480:149,Availability,error,error,149,"Closes https://broadworkbench.atlassian.net/browse/VS-159. - move execute_with_retry to utils so that only defined in one place; - add more specific error handling. **NOTE for reviewers**: you probably want to check ""Hide whitespace changes"" under settings because IntelliJ did some ""helpful"" cleanup when I made some code changes to files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7480
https://github.com/broadinstitute/gatk/issues/7481:798,Testability,test,test,798,"InbreedingCoeff uses likelihoods for its calculation _and_ uses GQ0s (regardless of call or no-call), which add equal likelihoods (to homRef and het or all three) because of the flat prior, which is out of HWE. So the more GQ0s there are, the more likely a variant ""looks"" like it's out of Hardy-Weinberg equilibrium. I would rather assign uninformative genotypes to be no-calls and ignore no-call genotypes in the InbreedingCoeff calculation. (ExcessHet has its own idiosyncrasies, which are different because it rounds to integer genotypes counts.). One place to enforce more consistent behavior is in GenotypeUtils.shouldBeCalled, which currently maintains the previous behavior of GT=./.;PL=[0,0,0] genotypes for reblocked GT=0/0;GQ=0 genotypes. This is going to involve overwriting a bunch of test data.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7481
https://github.com/broadinstitute/gatk/issues/7483:2147,Availability,error,error,2147,"rox=|851|0|0;AS_RAW_BaseQRankSum=|||;AS_RAW_MQ=0.00|12289.00|0.00|0.00;AS_RAW_MQRankSum=|||;AS_RAW_ReadPosRankSum=|||;AS_SB_TABLE=0,0|1,4|0,0|0,0;AS_VarDP=0|26|1|0;DP=49;QUALapprox=853;RAW_GT_COUNT=0,0,1;RAW_MQandDP=144241,49;VarDP=27	GT:AD:DP:GQ:PL:SB	1/2:0,26,1,0:27:2:853,2,347,878,0,989,875,282,907,1161:0,0,1,4; ```. When merged into a GenomicsDB, they give an empty element in the allele-specific array fields such as `AS_RAW_MQ` and `AS_SB_TABLE`:. ```; chr7 2377548 . A G,*,<NON_REF> . . AS_QUALapprox=|338|0|0;AS_RAW_BaseQRankSum=|1.000,1||;AS_RAW_MQ=35536.000|25290.000||0.000;AS_RAW_MQRankSum=|-0.800,1||;AS_RAW_ReadPosRankSum=|-2.300,1||;AS_SB_TABLE=3,7|0,8||0,0;AS_VarDP=10|10|0|0;BaseQRankSum=1.006;DP=71;MQRankSum=-0.752;QUALapprox=338;RAW_GT_COUNT=0,1,0;RAW_MQandDP=72828,22;ReadPosRankSum=-2.285;VarDP=20 GT:AD:GQ:PL:SB:D; P ./.:10,10,0,0:99:338,0,387,341,416,762,341,416,762,762:3,7,0,8:20 ./.:0,0,26,0:2:853,875,1161,2,282,347,875,1161,282,1161:0,0,1,4:27; ```. Which leads to a GnarlyGenotyper error:. ```; gatk GnarlyGenotyper -R Homo_sapiens_assembly38.fasta -O gnarlied.vcf.gz --only-output-calls-starting-in-intervals --keep-all-sites -V gendb.gs://mybucket/genomicsdbs/interval_20_outof_50 -L 0020-scattered.interval_list; <...>; java.lang.IllegalStateException: Something went wrong:; 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyperEngine.finalizeGenotype(GnarlyGenotyperEngine.java:147); 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyperEngine.finalizeGenotype(GnarlyGenotyperEngine.java:78); 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyper.apply(GnarlyGenotyper.java:298); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(Re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7483
https://github.com/broadinstitute/gatk/issues/7483:5286,Availability,error,error,5286,s$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); 	at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.NumberFormatException: empty String; 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842); 	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); 	at java.lang.Double.parseDouble(Double.java:538); 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS_RMSMappingQuality.parseRawDataString(AS_RMSMappingQuality.java:172); 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS_RMSMappingQuality.finalizeRawData(AS_RMSMappingQuality.java:196); 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyperEngine.finalizeGenotype(GnarlyGenotyperEngine.java:137); 	... 23 more; ```. GenotypeGVCFs throws a similar error but with the `AS_SB_TABLE` field.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7483
https://github.com/broadinstitute/gatk/issues/7483:3502,Integrability,wrap,wrapAndCopyInto,3502,typerEngine.finalizeGenotype(GnarlyGenotyperEngine.java:147); 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyperEngine.finalizeGenotype(GnarlyGenotyperEngine.java:78); 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyper.apply(GnarlyGenotyper.java:298); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); 	at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7483
https://github.com/broadinstitute/gatk/issues/7484:145,Availability,error,error,145,"I have run :; gatk MarkDuplicates -MAX_FILE_HANDLES 1000 -I ERR036185_sort.bam -O ERR036185_mark.bam -M ERR036185_mark_metrics.txt. but raise an error:OSError: [Errno 8] Exec format error: 'java'; i dont know why this error raise, as I can successfully run the command ""gatk"" without error,but if ""gatk MarkDuplicates"", the error raised again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7484
https://github.com/broadinstitute/gatk/issues/7484:182,Availability,error,error,182,"I have run :; gatk MarkDuplicates -MAX_FILE_HANDLES 1000 -I ERR036185_sort.bam -O ERR036185_mark.bam -M ERR036185_mark_metrics.txt. but raise an error:OSError: [Errno 8] Exec format error: 'java'; i dont know why this error raise, as I can successfully run the command ""gatk"" without error,but if ""gatk MarkDuplicates"", the error raised again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7484
https://github.com/broadinstitute/gatk/issues/7484:219,Availability,error,error,219,"I have run :; gatk MarkDuplicates -MAX_FILE_HANDLES 1000 -I ERR036185_sort.bam -O ERR036185_mark.bam -M ERR036185_mark_metrics.txt. but raise an error:OSError: [Errno 8] Exec format error: 'java'; i dont know why this error raise, as I can successfully run the command ""gatk"" without error,but if ""gatk MarkDuplicates"", the error raised again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7484
https://github.com/broadinstitute/gatk/issues/7484:285,Availability,error,error,285,"I have run :; gatk MarkDuplicates -MAX_FILE_HANDLES 1000 -I ERR036185_sort.bam -O ERR036185_mark.bam -M ERR036185_mark_metrics.txt. but raise an error:OSError: [Errno 8] Exec format error: 'java'; i dont know why this error raise, as I can successfully run the command ""gatk"" without error,but if ""gatk MarkDuplicates"", the error raised again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7484
https://github.com/broadinstitute/gatk/issues/7484:325,Availability,error,error,325,"I have run :; gatk MarkDuplicates -MAX_FILE_HANDLES 1000 -I ERR036185_sort.bam -O ERR036185_mark.bam -M ERR036185_mark_metrics.txt. but raise an error:OSError: [Errno 8] Exec format error: 'java'; i dont know why this error raise, as I can successfully run the command ""gatk"" without error,but if ""gatk MarkDuplicates"", the error raised again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7484
https://github.com/broadinstitute/gatk/issues/7485:654,Availability,Failure,Failures,654,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - any version with basename(basename()) function call. (e.g. latest version on master). ### Description ; ```; ""Failed to evaluate 'output_basename' (reason 1 of 1): Evaluating basename(basename(tumor_reads, "".bam""), "".cram"") failed: Failed to interpret 'CDS-00rz9N.hg38' as a file path input for basename (reason 1 of 1): java.lang.IllegalArgumentException: Could not build the path ""CDS-00rz9N.hg38"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, DRS. Failures: ; HTTP: CDS-00rz9N.hg38 does not have an http or https scheme (IllegalArgumentException); Google Cloud Storage: Path ""CDS-00rz9N.hg38"" does not have a gcs scheme (IllegalArgumentException); DRS: CDS-00rz9N.hg38 does not have a drs scheme. (IllegalArgumentException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; ```; #### Steps to reproduce; Just run the mutect2.wdl on terra it seems create the issue (maybe using a bam filepath with a name in ""gs://[path]/[NAME].hg38.bam"". #### Expected behavior; I think using basename(basename( is not working with the new version of terra, I would expect another solution with an if on the name end or something.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7485
https://github.com/broadinstitute/gatk/issues/7485:996,Modifiability,config,configure,996,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - any version with basename(basename()) function call. (e.g. latest version on master). ### Description ; ```; ""Failed to evaluate 'output_basename' (reason 1 of 1): Evaluating basename(basename(tumor_reads, "".bam""), "".cram"") failed: Failed to interpret 'CDS-00rz9N.hg38' as a file path input for basename (reason 1 of 1): java.lang.IllegalArgumentException: Could not build the path ""CDS-00rz9N.hg38"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, DRS. Failures: ; HTTP: CDS-00rz9N.hg38 does not have an http or https scheme (IllegalArgumentException); Google Cloud Storage: Path ""CDS-00rz9N.hg38"" does not have a gcs scheme (IllegalArgumentException); DRS: CDS-00rz9N.hg38 does not have a drs scheme. (IllegalArgumentException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; ```; #### Steps to reproduce; Just run the mutect2.wdl on terra it seems create the issue (maybe using a bam filepath with a name in ""gs://[path]/[NAME].hg38.bam"". #### Expected behavior; I think using basename(basename( is not working with the new version of terra, I would expect another solution with an if on the name end or something.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7485
https://github.com/broadinstitute/gatk/issues/7487:222,Availability,error,error,222,"Hi, . I'm trying to run ""cnv_germline_cohort_workflow"" from this workspace (https://app.terra.bio/#workspaces/help-gatk/Germline-CNVs-GATK4), and the workflow is keep failing at the ""CollectCounts"" step with the following error; in multiple shards. --------------------------------------------------------------------------------------------------------------------; A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; --------------------------------------------------------------------------------------------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:374,Availability,ERROR,ERROR,374,"Hi, . I'm trying to run ""cnv_germline_cohort_workflow"" from this workspace (https://app.terra.bio/#workspaces/help-gatk/Germline-CNVs-GATK4), and the workflow is keep failing at the ""CollectCounts"" step with the following error; in multiple shards. --------------------------------------------------------------------------------------------------------------------; A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; --------------------------------------------------------------------------------------------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:848,Availability,error,error,848,"Hi, . I'm trying to run ""cnv_germline_cohort_workflow"" from this workspace (https://app.terra.bio/#workspaces/help-gatk/Germline-CNVs-GATK4), and the workflow is keep failing at the ""CollectCounts"" step with the following error; in multiple shards. --------------------------------------------------------------------------------------------------------------------; A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; --------------------------------------------------------------------------------------------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:1025,Availability,down,download,1025," from this workspace (https://app.terra.bio/#workspaces/help-gatk/Germline-CNVs-GATK4), and the workflow is keep failing at the ""CollectCounts"" step with the following error; in multiple shards. --------------------------------------------------------------------------------------------------------------------; A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; --------------------------------------------------------------------------------------------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:1297,Availability,down,download,1297,"------------------------------------------------------------------------------------------------; A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; --------------------------------------------------------------------------------------------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!`. C345.TCGA-A3-3373-11A-01D-1421-08.5.bam and C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; were successfully downloaded, but since these TCGA files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:1416,Availability,Down,Download,1416,"files are not indexed.; Please index all input files:. samtools index /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; --------------------------------------------------------------------------------------------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!`. C345.TCGA-A3-3373-11A-01D-1421-08.5.bam and C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; were successfully downloaded, but since these TCGA files use DRS URI, they were copied to two separate cromwell folders. /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; /cromwell_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:1705,Availability,down,download,1705,"------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!`. C345.TCGA-A3-3373-11A-01D-1421-08.5.bam and C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; were successfully downloaded, but since these TCGA files use DRS URI, they were copied to two separate cromwell folders. /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai. And GATK doesn't seem to recognize BAM index when it is not inside a same folder. ; Could you maybe add symlink for the BAM and BAI files in the WDL script? . Tha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:1985,Availability,down,download,1985,"----------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!`. C345.TCGA-A3-3373-11A-01D-1421-08.5.bam and C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; were successfully downloaded, but since these TCGA files use DRS URI, they were copied to two separate cromwell folders. /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai. And GATK doesn't seem to recognize BAM index when it is not inside a same folder. ; Could you maybe add symlink for the BAM and BAI files in the WDL script? . Thanks,; Seunghun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:2104,Availability,Down,Download,2104,"----------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!`. C345.TCGA-A3-3373-11A-01D-1421-08.5.bam and C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; were successfully downloaded, but since these TCGA files use DRS URI, they were copied to two separate cromwell folders. /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai. And GATK doesn't seem to recognize BAM index when it is not inside a same folder. ; Could you maybe add symlink for the BAM and BAI files in the WDL script? . Thanks,; Seunghun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:2232,Availability,down,downloaded,2232,"----------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!`. C345.TCGA-A3-3373-11A-01D-1421-08.5.bam and C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; were successfully downloaded, but since these TCGA files use DRS URI, they were copied to two separate cromwell folders. /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai. And GATK doesn't seem to recognize BAM index when it is not inside a same folder. ; Could you maybe add symlink for the BAM and BAI files in the WDL script? . Thanks,; Seunghun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/issues/7487:997,Testability,log,log,997,"Hi, . I'm trying to run ""cnv_germline_cohort_workflow"" from this workspace (https://app.terra.bio/#workspaces/help-gatk/Germline-CNVs-GATK4), and the workflow is keep failing at the ""CollectCounts"" step with the following error; in multiple shards. --------------------------------------------------------------------------------------------------------------------; A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; --------------------------------------------------------------------------------------------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487
https://github.com/broadinstitute/gatk/pull/7488:164,Modifiability,config,configuring,164,"Added two new optional flags to `SplitIntervals`, and their corresponding tests. 1. `--prefix` for adding a prefix to the created interval files; 2. `--digits` for configuring the number of digits used to enumerate the interval files. This modifications were requested by a community user (#7157)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7488
https://github.com/broadinstitute/gatk/pull/7488:74,Testability,test,tests,74,"Added two new optional flags to `SplitIntervals`, and their corresponding tests. 1. `--prefix` for adding a prefix to the created interval files; 2. `--digits` for configuring the number of digits used to enumerate the interval files. This modifications were requested by a community user (#7157)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7488
https://github.com/broadinstitute/gatk/issues/7489:2008,Availability,error,errors,2008,"rageImpl.get(StorageImpl.java:238); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:497); at htsjdk.samtools.util.IOUtil.assertPathsAreReadable(IOUtil.java:525); at picard.fingerprint.CrosscheckFingerprints.doWork(CrosscheckFingerprints.java:449); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }; at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:451); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1089); at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7489
https://github.com/broadinstitute/gatk/issues/7489:191,Integrability,message,message,191,"CrosscheckFingerprints cannot access to ""Requester Pays"" buckets, can it be changed to support the ""--gcs-project-for-requester-pays"" option as in the GenomicsDBImport tool?. ```; code: 400; message: Bucket is requester pays bucket but no user project provided.; reason: required; location: null; retryable: false; com.google.cloud.storage.StorageException: Bucket is requester pays bucket but no user project provided.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:439); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:242); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at shaded.cloud_nio.com.google.cloud.RetryHelper.run(RetryHelper.java:76); at shaded.cloud_nio.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:238); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:497); at htsjdk.samtools.util.IOUtil.assertPathsAreReadable(IOUtil.java:525); at picard.fingerprint.CrosscheckFingerprints.doWork(CrosscheckFingerprints.java:449); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7489
https://github.com/broadinstitute/gatk/issues/7489:2046,Integrability,message,message,2046,"rageImpl.get(StorageImpl.java:238); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:497); at htsjdk.samtools.util.IOUtil.assertPathsAreReadable(IOUtil.java:525); at picard.fingerprint.CrosscheckFingerprints.doWork(CrosscheckFingerprints.java:449); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }; at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:451); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1089); at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7489
https://github.com/broadinstitute/gatk/issues/7489:2153,Integrability,message,message,2153,"Provider.java:736); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:497); at htsjdk.samtools.util.IOUtil.assertPathsAreReadable(IOUtil.java:525); at picard.fingerprint.CrosscheckFingerprints.doWork(CrosscheckFingerprints.java:449); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }; at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:451); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1089); at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:549); at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnpars",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7489
https://github.com/broadinstitute/gatk/issues/7489:30,Security,access,access,30,"CrosscheckFingerprints cannot access to ""Requester Pays"" buckets, can it be changed to support the ""--gcs-project-for-requester-pays"" option as in the GenomicsDBImport tool?. ```; code: 400; message: Bucket is requester pays bucket but no user project provided.; reason: required; location: null; retryable: false; com.google.cloud.storage.StorageException: Bucket is requester pays bucket but no user project provided.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:439); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:242); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at shaded.cloud_nio.com.google.cloud.RetryHelper.run(RetryHelper.java:76); at shaded.cloud_nio.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:238); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:497); at htsjdk.samtools.util.IOUtil.assertPathsAreReadable(IOUtil.java:525); at picard.fingerprint.CrosscheckFingerprints.doWork(CrosscheckFingerprints.java:449); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7489
https://github.com/broadinstitute/gatk/issues/7489:1271,Testability,assert,assertFileIsReadable,1271,"tion: null; retryable: false; com.google.cloud.storage.StorageException: Bucket is requester pays bucket but no user project provided.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:439); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:242); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at shaded.cloud_nio.com.google.cloud.RetryHelper.run(RetryHelper.java:76); at shaded.cloud_nio.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:238); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:497); at htsjdk.samtools.util.IOUtil.assertPathsAreReadable(IOUtil.java:525); at picard.fingerprint.CrosscheckFingerprints.doWork(CrosscheckFingerprints.java:449); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }; at shaded.cloud_nio.com.google.api.client.googleapis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7489
https://github.com/broadinstitute/gatk/issues/7489:1341,Testability,assert,assertPathsAreReadable,1341,": Bucket is requester pays bucket but no user project provided.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:439); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:242); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at shaded.cloud_nio.com.google.cloud.RetryHelper.run(RetryHelper.java:76); at shaded.cloud_nio.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:238); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:497); at htsjdk.samtools.util.IOUtil.assertPathsAreReadable(IOUtil.java:525); at picard.fingerprint.CrosscheckFingerprints.doWork(CrosscheckFingerprints.java:449); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }; at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7489
https://github.com/broadinstitute/gatk/issues/7490:131,Availability,down,downstream,131,"## Feature request. ### Tool(s) or class(es) involved. Mutect2. ### Description. Currently the AD tag reports read depth, but most downstream tools expect this flag to report fragment dept. ; Add flag to M2 to modify behavior of AD and AF tags to reflect fragment counts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7490
https://github.com/broadinstitute/gatk/issues/7492:90,Availability,error,error,90,"Hi,. Using GATK mutect2's wdl file on Terra (version 21 on agora) I keep getting the same error:; ""pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket"" . Here is part of the stacktrace : ; ```; 20:59:48.744 INFO Mutect2 - Inflater: IntelInflater; 20:59:48.744 INFO Mutect2 - GCS max retries/reopens: 20; 20:59:48.744 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 20:59:48.744 INFO Mutect2 - Initializing engine; 20:59:54.630 INFO FeatureManager - Using codec VCFCodec to read file gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7492:684,Availability,down,down,684,"Hi,. Using GATK mutect2's wdl file on Terra (version 21 on agora) I keep getting the same error:; ""pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket"" . Here is part of the stacktrace : ; ```; 20:59:48.744 INFO Mutect2 - Inflater: IntelInflater; 20:59:48.744 INFO Mutect2 - GCS max retries/reopens: 20; 20:59:48.744 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 20:59:48.744 INFO Mutect2 - Initializing engine; 20:59:54.630 INFO FeatureManager - Using codec VCFCodec to read file gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7492:1992,Deployability,release,release,1992,"gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz\ ; -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\ ; -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/7a157f4a-7d93-4a3e-aaf4-c41833463f5a/Mutect2/3be8ce8e-1075-4063-bc43-6f61e386c3f5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list\ ; -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ```. But I gave read (both regular and legacy) access to gs://cclebams (this is a requester pays bucket). This was done on GATK 4.2.2 docker. Best,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7492:868,Integrability,message,message,868,"Hi,. Using GATK mutect2's wdl file on Terra (version 21 on agora) I keep getting the same error:; ""pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket"" . Here is part of the stacktrace : ; ```; 20:59:48.744 INFO Mutect2 - Inflater: IntelInflater; 20:59:48.744 INFO Mutect2 - GCS max retries/reopens: 20; 20:59:48.744 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 20:59:48.744 INFO Mutect2 - Initializing engine; 20:59:54.630 INFO FeatureManager - Using codec VCFCodec to read file gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7492:2278,Performance,cache,cacheCopy,2278,"gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz\ ; -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\ ; -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/7a157f4a-7d93-4a3e-aaf4-c41833463f5a/Mutect2/3be8ce8e-1075-4063-bc43-6f61e386c3f5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list\ ; -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ```. But I gave read (both regular and legacy) access to gs://cclebams (this is a requester pays bucket). This was done on GATK 4.2.2 docker. Best,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7492:204,Security,access,access,204,"Hi,. Using GATK mutect2's wdl file on Terra (version 21 on agora) I keep getting the same error:; ""pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket"" . Here is part of the stacktrace : ; ```; 20:59:48.744 INFO Mutect2 - Inflater: IntelInflater; 20:59:48.744 INFO Mutect2 - GCS max retries/reopens: 20; 20:59:48.744 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 20:59:48.744 INFO Mutect2 - Initializing engine; 20:59:54.630 INFO FeatureManager - Using codec VCFCodec to read file gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7492:982,Security,access,access,982,"i,. Using GATK mutect2's wdl file on Terra (version 21 on agora) I keep getting the same error:; ""pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket"" . Here is part of the stacktrace : ; ```; 20:59:48.744 INFO Mutect2 - Inflater: IntelInflater; 20:59:48.744 INFO Mutect2 - GCS max retries/reopens: 20; 20:59:48.744 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 20:59:48.744 INFO Mutect2 - Initializing engine; 20:59:54.630 INFO FeatureManager - Using codec VCFCodec to read file gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7492:1227,Security,access,access,1227," . Here is part of the stacktrace : ; ```; 20:59:48.744 INFO Mutect2 - Inflater: IntelInflater; 20:59:48.744 INFO Mutect2 - GCS max retries/reopens: 20; 20:59:48.744 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 20:59:48.744 INFO Mutect2 - Initializing engine; 20:59:54.630 INFO FeatureManager - Using codec VCFCodec to read file gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz\ ; -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\ ; -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/7a157f4a-7d93-4a3e-aaf4-c41833463f5a/Mutect2/3be8ce8e-1075-4063-bc43-6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7492:2132,Security,secur,secure-,2132,"gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz\ ; -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\ ; -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/7a157f4a-7d93-4a3e-aaf4-c41833463f5a/Mutect2/3be8ce8e-1075-4063-bc43-6f61e386c3f5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list\ ; -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ```. But I gave read (both regular and legacy) access to gs://cclebams (this is a requester pays bucket). This was done on GATK 4.2.2 docker. Best,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7492:2503,Security,access,access,2503,"gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz\ ; -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\ ; -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/7a157f4a-7d93-4a3e-aaf4-c41833463f5a/Mutect2/3be8ce8e-1075-4063-bc43-6f61e386c3f5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list\ ; -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ```. But I gave read (both regular and legacy) access to gs://cclebams (this is a requester pays bucket). This was done on GATK 4.2.2 docker. Best,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492
https://github.com/broadinstitute/gatk/issues/7494:1466,Availability,error,error,1466,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494
https://github.com/broadinstitute/gatk/issues/7494:344,Modifiability,variab,variable,344,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494
https://github.com/broadinstitute/gatk/issues/7494:1004,Performance,cache,cacheCopy,1004,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494
https://github.com/broadinstitute/gatk/issues/7494:858,Security,secur,secure-,858,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494
https://github.com/broadinstitute/gatk/issues/7494:1193,Security,access,access,1193,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494
https://github.com/broadinstitute/gatk/issues/7494:1281,Security,access,access,1281,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494
https://github.com/broadinstitute/gatk/issues/7496:426,Availability,recover,recover-all-dangling-branches,426,"## Bug Report. ### Affected tool(s) or class(es). Mutect2; `; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx130g -jar /gatk/gatk-package-4.1.8.1-local.jar Mutect2 -R /ucsc.hg19.fasta -I my.bam -L /test.bed --f1r2-tar-gz DD.f1r2.tar.gz --force-active --genotype-germline-sites --kmer-size 10 --kmer-size 20 --recover-all-dangling-branches --max-reads-per-alignment-start 0 --native-pair-hmm-threads 33 -O DD.vcf.gz; `. ### Affected version(s); Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar. ### Description ; When bed is created with a reference genome that is not the same as the bam file, an null pointer can occurs. The error is not catched by GATK, and the error is difficult to understand. Here a discussion about it.; https://gatk.broadinstitute.org/hc/en-us/community/posts/360077477391-Haplotype-caller-fails-to-run-GATK-4-1-8-0-and-GATK-4-2-0-0-. The case below occurs when provided bed has been made with the wrong genome reference.; `; 14:25:55.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 07, 2021 2:25:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:25:55.525 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.525 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 14:25:55.525 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:25:55.525 INFO Mutect2 - Executing as toto on Linux v5.4.123-1.el7.elrepo.x86_64 amd64; 14:25:55.525 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7496:746,Availability,error,error,746,"## Bug Report. ### Affected tool(s) or class(es). Mutect2; `; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx130g -jar /gatk/gatk-package-4.1.8.1-local.jar Mutect2 -R /ucsc.hg19.fasta -I my.bam -L /test.bed --f1r2-tar-gz DD.f1r2.tar.gz --force-active --genotype-germline-sites --kmer-size 10 --kmer-size 20 --recover-all-dangling-branches --max-reads-per-alignment-start 0 --native-pair-hmm-threads 33 -O DD.vcf.gz; `. ### Affected version(s); Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar. ### Description ; When bed is created with a reference genome that is not the same as the bam file, an null pointer can occurs. The error is not catched by GATK, and the error is difficult to understand. Here a discussion about it.; https://gatk.broadinstitute.org/hc/en-us/community/posts/360077477391-Haplotype-caller-fails-to-run-GATK-4-1-8-0-and-GATK-4-2-0-0-. The case below occurs when provided bed has been made with the wrong genome reference.; `; 14:25:55.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 07, 2021 2:25:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:25:55.525 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.525 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 14:25:55.525 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:25:55.525 INFO Mutect2 - Executing as toto on Linux v5.4.123-1.el7.elrepo.x86_64 amd64; 14:25:55.525 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7496:784,Availability,error,error,784,"## Bug Report. ### Affected tool(s) or class(es). Mutect2; `; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx130g -jar /gatk/gatk-package-4.1.8.1-local.jar Mutect2 -R /ucsc.hg19.fasta -I my.bam -L /test.bed --f1r2-tar-gz DD.f1r2.tar.gz --force-active --genotype-germline-sites --kmer-size 10 --kmer-size 20 --recover-all-dangling-branches --max-reads-per-alignment-start 0 --native-pair-hmm-threads 33 -O DD.vcf.gz; `. ### Affected version(s); Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar. ### Description ; When bed is created with a reference genome that is not the same as the bam file, an null pointer can occurs. The error is not catched by GATK, and the error is difficult to understand. Here a discussion about it.; https://gatk.broadinstitute.org/hc/en-us/community/posts/360077477391-Haplotype-caller-fails-to-run-GATK-4-1-8-0-and-GATK-4-2-0-0-. The case below occurs when provided bed has been made with the wrong genome reference.; `; 14:25:55.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 07, 2021 2:25:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:25:55.525 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.525 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 14:25:55.525 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:25:55.525 INFO Mutect2 - Executing as toto on Linux v5.4.123-1.el7.elrepo.x86_64 amd64; 14:25:55.525 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7496:3046,Availability,down,down,3046," 14:25:55.526 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.526 INFO Mutect2 - HTSJDK Version: 2.23.0; 14:25:55.526 INFO Mutect2 - Picard Version: 2.22.8; 14:25:55.526 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:25:55.526 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:25:55.527 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:25:55.527 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:25:55.527 INFO Mutect2 - Deflater: IntelDeflater; 14:25:55.527 INFO Mutect2 - Inflater: IntelInflater; 14:25:55.527 INFO Mutect2 - GCS max retries/reopens: 20; 14:25:55.527 INFO Mutect2 - Requester pays: disabled; 14:25:55.527 INFO Mutect2 - Initializing engine; 14:25:55.994 INFO FeatureManager - Using codec BEDCodec to read file file:///test.bed; 14:25:56.086 INFO IntervalArgumentCollection - Processing 3896357 bp from intervals; 14:25:56.115 INFO Mutect2 - Shutting down engine; [October 7, 2021 2:25:56 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2102919168; **java.lang.NullPointerException**; at java.util.ComparableTimSort.countRunAndMakeAscending(ComparableTimSort.java:325); at java.util.ComparableTimSort.sort(ComparableTimSort.java:202); at java.util.Arrays.sort(Arrays.java:1312); at java.util.Arrays.sort(Arrays.java:1506); at java.util.ArrayList.sort(ArrayList.java:1462); at java.util.Collections.sort(Collections.java:143); at org.broadinstitute.hellbender.utils.IntervalUtils.sortAndMergeIntervals(IntervalUtils.java:467); at org.broadinstitute.hellbender.utils.IntervalUtils.getIntervalsWithFlanks(IntervalUtils.java:965); at org.broadinstitute.hellbender.utils.IntervalUtils.getIntervalsWithFlanks(IntervalUtils.java:980); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.(MultiIntervalLocalReadShard.java:59); at org.broadinstitute.hellbender.engine.AssemblyReg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7496:4757,Integrability,message,message,4757,"e.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2102919168; **java.lang.NullPointerException**; at java.util.ComparableTimSort.countRunAndMakeAscending(ComparableTimSort.java:325); at java.util.ComparableTimSort.sort(ComparableTimSort.java:202); at java.util.Arrays.sort(Arrays.java:1312); at java.util.Arrays.sort(Arrays.java:1506); at java.util.ArrayList.sort(ArrayList.java:1462); at java.util.Collections.sort(Collections.java:143); at org.broadinstitute.hellbender.utils.IntervalUtils.sortAndMergeIntervals(IntervalUtils.java:467); at org.broadinstitute.hellbender.utils.IntervalUtils.getIntervalsWithFlanks(IntervalUtils.java:965); at org.broadinstitute.hellbender.utils.IntervalUtils.getIntervalsWithFlanks(IntervalUtils.java:980); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.(MultiIntervalLocalReadShard.java:59); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.makeReadShards(AssemblyRegionWalker.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:84); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203). #### Expected behavior; Exception should be catched by GATK to provide a meaningful message. #### Actual behavior; NullPointerException is thrown by java. ### Tool(s) or class(es) involved; I think this method https://github.com/broadinstitute/gatk/blob/95852a1e70300b932bad153c845003a097abbbe1/src/main/java/org/broadinstitute/hellbender/utils/GenomeLocParser.java#L517 could return null, and is not correctly handled by caller method.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7496:1110,Performance,Load,Loading,1110," -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx130g -jar /gatk/gatk-package-4.1.8.1-local.jar Mutect2 -R /ucsc.hg19.fasta -I my.bam -L /test.bed --f1r2-tar-gz DD.f1r2.tar.gz --force-active --genotype-germline-sites --kmer-size 10 --kmer-size 20 --recover-all-dangling-branches --max-reads-per-alignment-start 0 --native-pair-hmm-threads 33 -O DD.vcf.gz; `. ### Affected version(s); Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar. ### Description ; When bed is created with a reference genome that is not the same as the bam file, an null pointer can occurs. The error is not catched by GATK, and the error is difficult to understand. Here a discussion about it.; https://gatk.broadinstitute.org/hc/en-us/community/posts/360077477391-Haplotype-caller-fails-to-run-GATK-4-1-8-0-and-GATK-4-2-0-0-. The case below occurs when provided bed has been made with the wrong genome reference.; `; 14:25:55.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 07, 2021 2:25:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:25:55.525 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.525 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 14:25:55.525 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:25:55.525 INFO Mutect2 - Executing as toto on Linux v5.4.123-1.el7.elrepo.x86_64 amd64; 14:25:55.525 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.526 INFO Mutect2 - ----------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7496:426,Safety,recover,recover-all-dangling-branches,426,"## Bug Report. ### Affected tool(s) or class(es). Mutect2; `; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx130g -jar /gatk/gatk-package-4.1.8.1-local.jar Mutect2 -R /ucsc.hg19.fasta -I my.bam -L /test.bed --f1r2-tar-gz DD.f1r2.tar.gz --force-active --genotype-germline-sites --kmer-size 10 --kmer-size 20 --recover-all-dangling-branches --max-reads-per-alignment-start 0 --native-pair-hmm-threads 33 -O DD.vcf.gz; `. ### Affected version(s); Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar. ### Description ; When bed is created with a reference genome that is not the same as the bam file, an null pointer can occurs. The error is not catched by GATK, and the error is difficult to understand. Here a discussion about it.; https://gatk.broadinstitute.org/hc/en-us/community/posts/360077477391-Haplotype-caller-fails-to-run-GATK-4-1-8-0-and-GATK-4-2-0-0-. The case below occurs when provided bed has been made with the wrong genome reference.; `; 14:25:55.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 07, 2021 2:25:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:25:55.525 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.525 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 14:25:55.525 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:25:55.525 INFO Mutect2 - Executing as toto on Linux v5.4.123-1.el7.elrepo.x86_64 amd64; 14:25:55.525 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7496:1365,Safety,detect,detect,1365,"-active --genotype-germline-sites --kmer-size 10 --kmer-size 20 --recover-all-dangling-branches --max-reads-per-alignment-start 0 --native-pair-hmm-threads 33 -O DD.vcf.gz; `. ### Affected version(s); Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar. ### Description ; When bed is created with a reference genome that is not the same as the bam file, an null pointer can occurs. The error is not catched by GATK, and the error is difficult to understand. Here a discussion about it.; https://gatk.broadinstitute.org/hc/en-us/community/posts/360077477391-Haplotype-caller-fails-to-run-GATK-4-1-8-0-and-GATK-4-2-0-0-. The case below occurs when provided bed has been made with the wrong genome reference.; `; 14:25:55.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 07, 2021 2:25:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:25:55.525 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.525 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 14:25:55.525 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:25:55.525 INFO Mutect2 - Executing as toto on Linux v5.4.123-1.el7.elrepo.x86_64 amd64; 14:25:55.525 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.526 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.526 INFO Mutect2 - HTSJDK Version: 2.23.0; 14:25:55.526 INFO Mutect2 - Picard Version: 2.22.8; 14:25:55.526 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:25:55.526 INFO Mutect2 - HTSJDK Defaul",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7496:315,Testability,test,test,315,"## Bug Report. ### Affected tool(s) or class(es). Mutect2; `; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx130g -jar /gatk/gatk-package-4.1.8.1-local.jar Mutect2 -R /ucsc.hg19.fasta -I my.bam -L /test.bed --f1r2-tar-gz DD.f1r2.tar.gz --force-active --genotype-germline-sites --kmer-size 10 --kmer-size 20 --recover-all-dangling-branches --max-reads-per-alignment-start 0 --native-pair-hmm-threads 33 -O DD.vcf.gz; `. ### Affected version(s); Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar. ### Description ; When bed is created with a reference genome that is not the same as the bam file, an null pointer can occurs. The error is not catched by GATK, and the error is difficult to understand. Here a discussion about it.; https://gatk.broadinstitute.org/hc/en-us/community/posts/360077477391-Haplotype-caller-fails-to-run-GATK-4-1-8-0-and-GATK-4-2-0-0-. The case below occurs when provided bed has been made with the wrong genome reference.; `; 14:25:55.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 07, 2021 2:25:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:25:55.525 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.525 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 14:25:55.525 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:25:55.525 INFO Mutect2 - Executing as toto on Linux v5.4.123-1.el7.elrepo.x86_64 amd64; 14:25:55.525 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7496:2914,Testability,test,test,2914,".04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.526 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.526 INFO Mutect2 - HTSJDK Version: 2.23.0; 14:25:55.526 INFO Mutect2 - Picard Version: 2.22.8; 14:25:55.526 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:25:55.526 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:25:55.527 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:25:55.527 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:25:55.527 INFO Mutect2 - Deflater: IntelDeflater; 14:25:55.527 INFO Mutect2 - Inflater: IntelInflater; 14:25:55.527 INFO Mutect2 - GCS max retries/reopens: 20; 14:25:55.527 INFO Mutect2 - Requester pays: disabled; 14:25:55.527 INFO Mutect2 - Initializing engine; 14:25:55.994 INFO FeatureManager - Using codec BEDCodec to read file file:///test.bed; 14:25:56.086 INFO IntervalArgumentCollection - Processing 3896357 bp from intervals; 14:25:56.115 INFO Mutect2 - Shutting down engine; [October 7, 2021 2:25:56 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2102919168; **java.lang.NullPointerException**; at java.util.ComparableTimSort.countRunAndMakeAscending(ComparableTimSort.java:325); at java.util.ComparableTimSort.sort(ComparableTimSort.java:202); at java.util.Arrays.sort(Arrays.java:1312); at java.util.Arrays.sort(Arrays.java:1506); at java.util.ArrayList.sort(ArrayList.java:1462); at java.util.Collections.sort(Collections.java:143); at org.broadinstitute.hellbender.utils.IntervalUtils.sortAndMergeIntervals(IntervalUtils.java:467); at org.broadinstitute.hellbender.utils.IntervalUtils.getIntervalsWithFlanks(IntervalUtils.java:965); at org.broadinstitute.hellbender.utils.IntervalUtils.getIntervalsWithFlanks(IntervalU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496
https://github.com/broadinstitute/gatk/issues/7497:344,Performance,perform,performance,344,"When there are many samples in a VCF and decoding the genotypes is an expensive operation, it makes sense to do all the site-level filtering first before fully decoding the record and examining the genotypes. . Unfortunately, `SelectVariants` currently does some of the genotype-level filtering before the site-level filtering, which can cause performance issues with large callsets.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7497
https://github.com/broadinstitute/gatk/pull/7498:1200,Availability,down,downstream,1200,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:270,Deployability,update,updated,270,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:429,Deployability,update,updated,429,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1249,Deployability,integrat,integration,1249,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1476,Deployability,pipeline,pipeline,1476,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1249,Integrability,integrat,integration,1249,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:294,Modifiability,parameteriz,parameterized,294,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:340,Modifiability,enhance,enhanced,340,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:948,Modifiability,parameteriz,parameterize,948,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:308,Performance,load,loading,308,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1426,Performance,perform,perform,1426,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1524,Performance,load,load,1524,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1042,Testability,test,testing,1042,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1228,Testability,Test,Testing,1228,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1261,Testability,test,test,1261,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1307,Testability,test,tests,1307,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/pull/7498:1367,Testability,test,testing,1367,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498
https://github.com/broadinstitute/gatk/issues/7499:20,Availability,error,error,20,"Dear all:; I have a error flow:; INFO 14:49:42,880 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,883 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.8-1-0-gf15c1c3ef, Compiled 2018/02/19 05:43:50; INFO 14:49:42,884 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute; INFO 14:49:42,884 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk; INFO 14:49:42,884 HelpFormatter - [Sat Oct 09 14:49:42 CST 2021] Executing on Linux 2.6.32-696.el6.x86_64 amd64; INFO 14:49:42,884 HelpFormatter - Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12; INFO 14:49:42,889 HelpFormatter - Program Args: -T HaplotypeCaller -R /share/Onc_Soft_DB/database/capsmart/hg19/hg19_20210805/hg19.rm_CRLF2_P2RY8.fix_PRSS1_MUC16.fasta -I /share/Onc_RD_Pipeline/OncDir/zhuangll/210927-commercial-tissue-zhangaiyuan/germline/Z19W06700-F1WA/2.Realign/Z19W06700-F1WA.bam -L /share/Onc_Soft_DB/database/capsmart/bed/gene102.snpindel.capsmart.bed -U -o /share/Onc_RD_Pipeline/OncDir/yanhs/test/GATK/Z19W06700-F1WA.HaplotypeCaller.raw.vcf -stand_call_conf 50 -A RMSMappingQuality -A BaseCounts; INFO 14:49:42,892 HelpFormatter - Executing as yanhs3941@compute-0-76 on Linux 2.6.32-696.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12.; INFO 14:49:42,892 HelpFormatter - Date/Time: 2021/10/09 14:49:42; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,922 NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/Onc_Soft_DB/software/GATK3.8/GenomeAnalysisTK.jar!/com/intel/gkl/native/libgkl_compression.so; INFO 14:49:42,957 GenomeAnalysisEngine - Deflater: IntelDeflater; INFO 14:49:42,958 GenomeAnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499
https://github.com/broadinstitute/gatk/issues/7499:2065,Availability,Down,Downsampling,2065,"A.bam -L /share/Onc_Soft_DB/database/capsmart/bed/gene102.snpindel.capsmart.bed -U -o /share/Onc_RD_Pipeline/OncDir/yanhs/test/GATK/Z19W06700-F1WA.HaplotypeCaller.raw.vcf -stand_call_conf 50 -A RMSMappingQuality -A BaseCounts; INFO 14:49:42,892 HelpFormatter - Executing as yanhs3941@compute-0-76 on Linux 2.6.32-696.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12.; INFO 14:49:42,892 HelpFormatter - Date/Time: 2021/10/09 14:49:42; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,922 NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/Onc_Soft_DB/software/GATK3.8/GenomeAnalysisTK.jar!/com/intel/gkl/native/libgkl_compression.so; INFO 14:49:42,957 GenomeAnalysisEngine - Deflater: IntelDeflater; INFO 14:49:42,958 GenomeAnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine - Strictness is SILENT; INFO 14:49:43,125 GenomeAnalysisEngine - Downsampling Se. the error is :; maxAltAlleles (6), the following will be dropped: TAAC.; WARN 14:59:10,944 HaplotypeCallerGenotypingEngine - location chr12:21623284-21623286: too many alternative alleles found (8) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: C, CA.; INFO 14:59:13,453 ProgressMeter - chr12:21624342 1.0358131E7 9.5 m 55.0 s 49.5% 19.2 m 9.7 m; WARN 14:59:37,613 HaplotypeCallerGenotypingEngine - location chr12:133237753-133237756: too many alternative alleles found (9) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: G, TAAA, GAAAAAAA.; INFO 14:59:43,454 ProgressMeter - chr13:32892450 1.0955099E7 10.0 m 54.0 s 54.0% 18.5 m 8.5 m; INFO 15:00:13,456 ProgressMeter - chr13:32912251 1.0976428E7 10.5 m 57.0 s 55.2% 19.0 m 8.5 m; ##### ERROR --; ##### ERROR stack trace; java.lang.Ill",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499
https://github.com/broadinstitute/gatk/issues/7499:2086,Availability,error,error,2086,"aw.vcf -stand_call_conf 50 -A RMSMappingQuality -A BaseCounts; INFO 14:49:42,892 HelpFormatter - Executing as yanhs3941@compute-0-76 on Linux 2.6.32-696.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12.; INFO 14:49:42,892 HelpFormatter - Date/Time: 2021/10/09 14:49:42; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,922 NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/Onc_Soft_DB/software/GATK3.8/GenomeAnalysisTK.jar!/com/intel/gkl/native/libgkl_compression.so; INFO 14:49:42,957 GenomeAnalysisEngine - Deflater: IntelDeflater; INFO 14:49:42,958 GenomeAnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine - Strictness is SILENT; INFO 14:49:43,125 GenomeAnalysisEngine - Downsampling Se. the error is :; maxAltAlleles (6), the following will be dropped: TAAC.; WARN 14:59:10,944 HaplotypeCallerGenotypingEngine - location chr12:21623284-21623286: too many alternative alleles found (8) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: C, CA.; INFO 14:59:13,453 ProgressMeter - chr12:21624342 1.0358131E7 9.5 m 55.0 s 49.5% 19.2 m 9.7 m; WARN 14:59:37,613 HaplotypeCallerGenotypingEngine - location chr12:133237753-133237756: too many alternative alleles found (9) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: G, TAAA, GAAAAAAA.; INFO 14:59:43,454 ProgressMeter - chr13:32892450 1.0955099E7 10.0 m 54.0 s 54.0% 18.5 m 8.5 m; INFO 15:00:13,456 ProgressMeter - chr13:32912251 1.0976428E7 10.5 m 57.0 s 55.2% 19.0 m 8.5 m; ##### ERROR --; ##### ERROR stack trace; java.lang.IllegalStateException: Never found start -1 or stop -1 given cigar 17I; at org.broadinstitute.gatk.utils.sam.AlignmentUtils.getBasesCoveringRefInterval(AlignmentUtils.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499
https://github.com/broadinstitute/gatk/issues/7499:2905,Availability,ERROR,ERROR,2905,"AnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine - Strictness is SILENT; INFO 14:49:43,125 GenomeAnalysisEngine - Downsampling Se. the error is :; maxAltAlleles (6), the following will be dropped: TAAC.; WARN 14:59:10,944 HaplotypeCallerGenotypingEngine - location chr12:21623284-21623286: too many alternative alleles found (8) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: C, CA.; INFO 14:59:13,453 ProgressMeter - chr12:21624342 1.0358131E7 9.5 m 55.0 s 49.5% 19.2 m 9.7 m; WARN 14:59:37,613 HaplotypeCallerGenotypingEngine - location chr12:133237753-133237756: too many alternative alleles found (9) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: G, TAAA, GAAAAAAA.; INFO 14:59:43,454 ProgressMeter - chr13:32892450 1.0955099E7 10.0 m 54.0 s 54.0% 18.5 m 8.5 m; INFO 15:00:13,456 ProgressMeter - chr13:32912251 1.0976428E7 10.5 m 57.0 s 55.2% 19.0 m 8.5 m; ##### ERROR --; ##### ERROR stack trace; java.lang.IllegalStateException: Never found start -1 or stop -1 given cigar 17I; at org.broadinstitute.gatk.utils.sam.AlignmentUtils.getBasesCoveringRefInterval(AlignmentUtils.java:204); at org.broadinstitute.gatk.utils.sam.AlignmentUtils.countBasesAtPileupPosition(AlignmentUtils.java:1418); at org.broadinstitute.gatk.tools.walkers.annotator.BaseCounts.getBaseCounts(BaseCounts.java:93); at org.broadinstitute.gatk.tools.walkers.annotator.BaseCounts.annotate(BaseCounts.java:78); at org.broadinstitute.gatk.tools.walkers.annotator.VariantAnnotatorEngine.annotateContextForActiveRegion(VariantAnnotatorEngine.java:315); at org.broadinstitute.gatk.tools.walkers.annotator.VariantAnnotatorEngine.annotateContextForActiveRegion(VariantAnnotatorEngine.java:260); at org.broadinstitute.gatk.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.annotateCall(HaplotypeCallerGenotypingEngine.java:328); at org.broadinstitute.gatk.tools.walkers.haplotypecaller.HaplotypeCallerGe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499
https://github.com/broadinstitute/gatk/issues/7499:2921,Availability,ERROR,ERROR,2921,"AnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine - Strictness is SILENT; INFO 14:49:43,125 GenomeAnalysisEngine - Downsampling Se. the error is :; maxAltAlleles (6), the following will be dropped: TAAC.; WARN 14:59:10,944 HaplotypeCallerGenotypingEngine - location chr12:21623284-21623286: too many alternative alleles found (8) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: C, CA.; INFO 14:59:13,453 ProgressMeter - chr12:21624342 1.0358131E7 9.5 m 55.0 s 49.5% 19.2 m 9.7 m; WARN 14:59:37,613 HaplotypeCallerGenotypingEngine - location chr12:133237753-133237756: too many alternative alleles found (9) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: G, TAAA, GAAAAAAA.; INFO 14:59:43,454 ProgressMeter - chr13:32892450 1.0955099E7 10.0 m 54.0 s 54.0% 18.5 m 8.5 m; INFO 15:00:13,456 ProgressMeter - chr13:32912251 1.0976428E7 10.5 m 57.0 s 55.2% 19.0 m 8.5 m; ##### ERROR --; ##### ERROR stack trace; java.lang.IllegalStateException: Never found start -1 or stop -1 given cigar 17I; at org.broadinstitute.gatk.utils.sam.AlignmentUtils.getBasesCoveringRefInterval(AlignmentUtils.java:204); at org.broadinstitute.gatk.utils.sam.AlignmentUtils.countBasesAtPileupPosition(AlignmentUtils.java:1418); at org.broadinstitute.gatk.tools.walkers.annotator.BaseCounts.getBaseCounts(BaseCounts.java:93); at org.broadinstitute.gatk.tools.walkers.annotator.BaseCounts.annotate(BaseCounts.java:78); at org.broadinstitute.gatk.tools.walkers.annotator.VariantAnnotatorEngine.annotateContextForActiveRegion(VariantAnnotatorEngine.java:315); at org.broadinstitute.gatk.tools.walkers.annotator.VariantAnnotatorEngine.annotateContextForActiveRegion(VariantAnnotatorEngine.java:260); at org.broadinstitute.gatk.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.annotateCall(HaplotypeCallerGenotypingEngine.java:328); at org.broadinstitute.gatk.tools.walkers.haplotypecaller.HaplotypeCallerGe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499
https://github.com/broadinstitute/gatk/issues/7499:1683,Performance,Load,Loading,1683," 2021] Executing on Linux 2.6.32-696.el6.x86_64 amd64; INFO 14:49:42,884 HelpFormatter - Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12; INFO 14:49:42,889 HelpFormatter - Program Args: -T HaplotypeCaller -R /share/Onc_Soft_DB/database/capsmart/hg19/hg19_20210805/hg19.rm_CRLF2_P2RY8.fix_PRSS1_MUC16.fasta -I /share/Onc_RD_Pipeline/OncDir/zhuangll/210927-commercial-tissue-zhangaiyuan/germline/Z19W06700-F1WA/2.Realign/Z19W06700-F1WA.bam -L /share/Onc_Soft_DB/database/capsmart/bed/gene102.snpindel.capsmart.bed -U -o /share/Onc_RD_Pipeline/OncDir/yanhs/test/GATK/Z19W06700-F1WA.HaplotypeCaller.raw.vcf -stand_call_conf 50 -A RMSMappingQuality -A BaseCounts; INFO 14:49:42,892 HelpFormatter - Executing as yanhs3941@compute-0-76 on Linux 2.6.32-696.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12.; INFO 14:49:42,892 HelpFormatter - Date/Time: 2021/10/09 14:49:42; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,922 NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/Onc_Soft_DB/software/GATK3.8/GenomeAnalysisTK.jar!/com/intel/gkl/native/libgkl_compression.so; INFO 14:49:42,957 GenomeAnalysisEngine - Deflater: IntelDeflater; INFO 14:49:42,958 GenomeAnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine - Strictness is SILENT; INFO 14:49:43,125 GenomeAnalysisEngine - Downsampling Se. the error is :; maxAltAlleles (6), the following will be dropped: TAAC.; WARN 14:59:10,944 HaplotypeCallerGenotypingEngine - location chr12:21623284-21623286: too many alternative alleles found (8) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: C, CA.; INFO 14:59:13,453 ProgressMeter - chr12:21624342 1.0358131E7 9.5 m 55.0 s 49.5% 19.2 m 9.7 m; WARN 14:59:37,613 HaplotypeCallerGenotypingEngine -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499
https://github.com/broadinstitute/gatk/issues/7499:1075,Testability,test,test,1075," - ------------------------------------------------------------------------------------; INFO 14:49:42,883 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.8-1-0-gf15c1c3ef, Compiled 2018/02/19 05:43:50; INFO 14:49:42,884 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute; INFO 14:49:42,884 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk; INFO 14:49:42,884 HelpFormatter - [Sat Oct 09 14:49:42 CST 2021] Executing on Linux 2.6.32-696.el6.x86_64 amd64; INFO 14:49:42,884 HelpFormatter - Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12; INFO 14:49:42,889 HelpFormatter - Program Args: -T HaplotypeCaller -R /share/Onc_Soft_DB/database/capsmart/hg19/hg19_20210805/hg19.rm_CRLF2_P2RY8.fix_PRSS1_MUC16.fasta -I /share/Onc_RD_Pipeline/OncDir/zhuangll/210927-commercial-tissue-zhangaiyuan/germline/Z19W06700-F1WA/2.Realign/Z19W06700-F1WA.bam -L /share/Onc_Soft_DB/database/capsmart/bed/gene102.snpindel.capsmart.bed -U -o /share/Onc_RD_Pipeline/OncDir/yanhs/test/GATK/Z19W06700-F1WA.HaplotypeCaller.raw.vcf -stand_call_conf 50 -A RMSMappingQuality -A BaseCounts; INFO 14:49:42,892 HelpFormatter - Executing as yanhs3941@compute-0-76 on Linux 2.6.32-696.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12.; INFO 14:49:42,892 HelpFormatter - Date/Time: 2021/10/09 14:49:42; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,922 NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/Onc_Soft_DB/software/GATK3.8/GenomeAnalysisTK.jar!/com/intel/gkl/native/libgkl_compression.so; INFO 14:49:42,957 GenomeAnalysisEngine - Deflater: IntelDeflater; INFO 14:49:42,958 GenomeAnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine - Strictness is SILENT; INFO 14:49:43,125 GenomeAnalysisEngine - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499
https://github.com/broadinstitute/gatk/issues/7500:9,Availability,down,downloaded,9,"Hiya,. I downloaded some VCF files for SNP detection in GATK. However when I tried to use them at the recalibration step it said I needed an index, when I try an run the index feature function it gives me the error: Input file is not in valid block compressed format. The files are .VCF.gz. Is there a way of reformatting please?. Best wishes,; B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7500
https://github.com/broadinstitute/gatk/issues/7500:209,Availability,error,error,209,"Hiya,. I downloaded some VCF files for SNP detection in GATK. However when I tried to use them at the recalibration step it said I needed an index, when I try an run the index feature function it gives me the error: Input file is not in valid block compressed format. The files are .VCF.gz. Is there a way of reformatting please?. Best wishes,; B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7500
https://github.com/broadinstitute/gatk/issues/7500:43,Safety,detect,detection,43,"Hiya,. I downloaded some VCF files for SNP detection in GATK. However when I tried to use them at the recalibration step it said I needed an index, when I try an run the index feature function it gives me the error: Input file is not in valid block compressed format. The files are .VCF.gz. Is there a way of reformatting please?. Best wishes,; B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7500
https://github.com/broadinstitute/gatk/issues/7504:155,Availability,error,error,155,"An issue has come up on the GATK forum with a user running FilterFuncotations. They have two transcripts for the same gene and are getting a duplicate key error. Ted Brookings identified how to solve this issue:; -The exact problem is in AlleleFrequencyUtils.java, line 30.; -The solution is to skip collecting as a map, have getMaxMinorAlleleFreq take a stream and return an optional float, then return false if the float is missing, otherwise value <= maxMaf. Don't ever call allFrequenciesFiltered. This request was created from a contribution made by Azza Ahmed on October 14, 2021 10:53 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df  broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)).. ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:708,Availability,error,error,708,"An issue has come up on the GATK forum with a user running FilterFuncotations. They have two transcripts for the same gene and are getting a duplicate key error. Ted Brookings identified how to solve this issue:; -The exact problem is in AlleleFrequencyUtils.java, line 30.; -The solution is to skip collecting as a map, have getMaxMinorAlleleFreq take a stream and return an optional float, then return false if the float is missing, otherwise value <= maxMaf. Don't ever call allFrequenciesFiltered. This request was created from a contribution made by Azza Ahmed on October 14, 2021 10:53 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df  broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)).. ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:819,Availability,error,error,819,"An issue has come up on the GATK forum with a user running FilterFuncotations. They have two transcripts for the same gene and are getting a duplicate key error. Ted Brookings identified how to solve this issue:; -The exact problem is in AlleleFrequencyUtils.java, line 30.; -The solution is to skip collecting as a map, have getMaxMinorAlleleFreq take a stream and return an optional float, then return false if the float is missing, otherwise value <= maxMaf. Don't ever call allFrequenciesFiltered. This request was created from a contribution made by Azza Ahmed on October 14, 2021 10:53 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df  broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)).. ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:1753,Availability,error,error,1753,"g/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df  broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)).. ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.u",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:939,Deployability,pipeline,pipeline,939,"An issue has come up on the GATK forum with a user running FilterFuncotations. They have two transcripts for the same gene and are getting a duplicate key error. Ted Brookings identified how to solve this issue:; -The exact problem is in AlleleFrequencyUtils.java, line 30.; -The solution is to skip collecting as a map, have getMaxMinorAlleleFreq take a stream and return an optional float, then return false if the float is missing, otherwise value <= maxMaf. Don't ever call allFrequenciesFiltered. This request was created from a contribution made by Azza Ahmed on October 14, 2021 10:53 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df  broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)).. ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:1147,Deployability,pipeline,pipelines,1147,"duplicate key error. Ted Brookings identified how to solve this issue:; -The exact problem is in AlleleFrequencyUtils.java, line 30.; -The solution is to skip collecting as a map, have getMaxMinorAlleleFreq take a stream and return an optional float, then return false if the float is missing, otherwise value <= maxMaf. Don't ever call allFrequenciesFiltered. This request was created from a contribution made by Azza Ahmed on October 14, 2021 10:53 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df  broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)).. ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:1356,Deployability,pipeline,pipelines,1356,"turn false if the float is missing, otherwise value <= maxMaf. Don't ever call allFrequenciesFiltered. This request was created from a contribution made by Azza Ahmed on October 14, 2021 10:53 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df  broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)).. ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.Ha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:2322,Energy Efficiency,Reduce,ReduceOps,2322,"cratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:2353,Energy Efficiency,Reduce,ReduceOps,2353,"p/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:2666,Energy Efficiency,Reduce,ReduceOps,2666,"a-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.eva",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:2676,Energy Efficiency,Reduce,ReduceOp,2676,"a-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.eva",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:2704,Energy Efficiency,Reduce,ReduceOps,2704,"t true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:3566,Energy Efficiency,Reduce,ReduceOps,3566,bstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.checkFilter(FuncotationFilter.java:49) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$0(FilterFuncotations.java:194) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:3576,Energy Efficiency,Reduce,ReduceOp,3576,bstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.checkFilter(FuncotationFilter.java:49) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$0(FilterFuncotations.java:194) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:3604,Energy Efficiency,Reduce,ReduceOps,3604,opyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.checkFilter(FuncotationFilter.java:49) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$0(FilterFuncotations.java:194) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:3759,Energy Efficiency,reduce,reduce,3759,e.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.checkFilter(FuncotationFilter.java:49) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$0(FilterFuncotations.java:194) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$1(FilterFuncotations.java:196) ; at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:4425,Energy Efficiency,Reduce,ReduceOps,4425,va:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.checkFilter(FuncotationFilter.java:49) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$0(FilterFuncotations.java:194) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$1(FilterFuncotations.java:196) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.base/java.util.stream.ForEac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:4435,Energy Efficiency,Reduce,ReduceOp,4435,va:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.checkFilter(FuncotationFilter.java:49) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$0(FilterFuncotations.java:194) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$1(FilterFuncotations.java:196) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.base/java.util.stream.ForEac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:4463,Energy Efficiency,Reduce,ReduceOps,4463,ava.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.checkFilter(FuncotationFilter.java:49) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$0(FilterFuncotations.java:194) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$1(FilterFuncotations.java:196) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.eva,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:1759,Integrability,message,message,1759,"g/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df  broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)).. ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.u",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:2591,Integrability,wrap,wrapAndCopyInto,2591,"cf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(Redu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:3491,Integrability,wrap,wrapAndCopyInto,3491,opyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.checkFilter(FuncotationFilter.java:49) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$0(FilterFuncotations.java:194) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:4350,Integrability,wrap,wrapAndCopyInto,4350,.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.checkFilter(FuncotationFilter.java:49) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$0(FilterFuncotations.java:194) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$1(FilterFuncotations.java:196) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:5261,Integrability,wrap,wrapAndCopyInto,5261,tractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$null$1(FilterFuncotations.java:196) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$getMatchingFilters$2(FilterFuncotations.java:192) ; at java.base/java.util.HashMap$Values.forEach(HashMap.java:976) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.getMatchingFilters(FilterFuncotations.java:191) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.secondPassApply(FilterFuncotations.java:174) ; at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:19) ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$t,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:6931,Integrability,wrap,wrapAndCopyInto,6931,ers(FilterFuncotations.java:191) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.secondPassApply(FilterFuncotations.java:174) ; at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:19) ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177) ; at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) ; at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497) ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75) ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40) ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:2398,Security,Hash,HashMap,2398,"tions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:2438,Security,Hash,HashMap,2438,"on/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.AlleleFrequencyUtils.lambda$buildMaxMafRule$1(AlleleFrequencyUtils.java:30) ; at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.FuncotationFilter.lambda$checkFilter$0(FuncotationFilter.java:48) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:5820,Security,Hash,HashMap,5820,accept(ForEachOps.java:183) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$getMatchingFilters$2(FilterFuncotations.java:192) ; at java.base/java.util.HashMap$Values.forEach(HashMap.java:976) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.getMatchingFilters(FilterFuncotations.java:191) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.secondPassApply(FilterFuncotations.java:174) ; at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:19) ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177) ; at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) ; at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; at java.base/java.util.stream,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7504:5843,Security,Hash,HashMap,5843,s.java:183) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ; at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ; at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.lambda$getMatchingFilters$2(FilterFuncotations.java:192) ; at java.base/java.util.HashMap$Values.forEach(HashMap.java:976) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.getMatchingFilters(FilterFuncotations.java:191) ; at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.secondPassApply(FilterFuncotations.java:174) ; at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:19) ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40) ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77) ; at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ; at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177) ; at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) ; at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; at java.base/java.util.stream.AbstractPipeli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504
https://github.com/broadinstitute/gatk/issues/7512:394,Deployability,integrat,integration,394,"A shortcut in the code returns empty values if there are no reads supporting the reference, but if an alt has no reads then the rank sum test from Apache Math returns NaN. Output for invalid rank sum test Z-scores should return null/empty rather than NaN. (Pipe-delimited raw annotations don't have to follow the VCF spec using `.` for missing.) This will involve updating a lot of exact match integration test results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7512
https://github.com/broadinstitute/gatk/issues/7512:394,Integrability,integrat,integration,394,"A shortcut in the code returns empty values if there are no reads supporting the reference, but if an alt has no reads then the rank sum test from Apache Math returns NaN. Output for invalid rank sum test Z-scores should return null/empty rather than NaN. (Pipe-delimited raw annotations don't have to follow the VCF spec using `.` for missing.) This will involve updating a lot of exact match integration test results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7512
https://github.com/broadinstitute/gatk/issues/7512:137,Testability,test,test,137,"A shortcut in the code returns empty values if there are no reads supporting the reference, but if an alt has no reads then the rank sum test from Apache Math returns NaN. Output for invalid rank sum test Z-scores should return null/empty rather than NaN. (Pipe-delimited raw annotations don't have to follow the VCF spec using `.` for missing.) This will involve updating a lot of exact match integration test results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7512
https://github.com/broadinstitute/gatk/issues/7512:200,Testability,test,test,200,"A shortcut in the code returns empty values if there are no reads supporting the reference, but if an alt has no reads then the rank sum test from Apache Math returns NaN. Output for invalid rank sum test Z-scores should return null/empty rather than NaN. (Pipe-delimited raw annotations don't have to follow the VCF spec using `.` for missing.) This will involve updating a lot of exact match integration test results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7512
https://github.com/broadinstitute/gatk/issues/7512:406,Testability,test,test,406,"A shortcut in the code returns empty values if there are no reads supporting the reference, but if an alt has no reads then the rank sum test from Apache Math returns NaN. Output for invalid rank sum test Z-scores should return null/empty rather than NaN. (Pipe-delimited raw annotations don't have to follow the VCF spec using `.` for missing.) This will involve updating a lot of exact match integration test results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7512
https://github.com/broadinstitute/gatk/issues/7515:498,Availability,Error,Error,498,"Hi GATK developers:. Have 4 pacbio WGS bam files do to Haplotype calling. Each bam file was divided by chromosomes, but 3 parallele jobs failed due to java core dump:; - Syntax I ran was pretty basic, I also tried latest gatk version4.2.2.0, same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:564,Availability,error,error,564,"Hi GATK developers:. Have 4 pacbio WGS bam files do to Haplotype calling. Each bam file was divided by chromosomes, but 3 parallele jobs failed due to java core dump:; - Syntax I ran was pretty basic, I also tried latest gatk version4.2.2.0, same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:1088,Availability,error,error,1088,"but 3 parallele jobs failed due to java core dump:; - Syntax I ran was pretty basic, I also tried latest gatk version4.2.2.0, same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:1437,Availability,error,error,1437," same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug. ```. Could this be a bug of problem with my data?. Thanks, ; Wen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:1864,Availability,error,error,1864," same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug. ```. Could this be a bug of problem with my data?. Thanks, ; Wen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:504,Integrability,message,message,504,"Hi GATK developers:. Have 4 pacbio WGS bam files do to Haplotype calling. Each bam file was divided by chromosomes, but 3 parallele jobs failed due to java core dump:; - Syntax I ran was pretty basic, I also tried latest gatk version4.2.2.0, same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:579,Safety,detect,detected,579,"Hi GATK developers:. Have 4 pacbio WGS bam files do to Haplotype calling. Each bam file was divided by chromosomes, but 3 parallele jobs failed due to java core dump:; - Syntax I ran was pretty basic, I also tried latest gatk version4.2.2.0, same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:1452,Safety,detect,detected,1452," same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug. ```. Could this be a bug of problem with my data?. Thanks, ; Wen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:1001,Testability,test,testcase,1001,"Hi GATK developers:. Have 4 pacbio WGS bam files do to Haplotype calling. Each bam file was divided by chromosomes, but 3 parallele jobs failed due to java core dump:; - Syntax I ran was pretty basic, I also tried latest gatk version4.2.2.0, same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:1159,Testability,log,log,1159," also tried latest gatk version4.2.2.0, same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug. ```. Could this be a bug ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7515:1937,Testability,log,log,1937," same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug. ```. Could this be a bug of problem with my data?. Thanks, ; Wen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515
https://github.com/broadinstitute/gatk/issues/7516:103,Deployability,update,updated,103,The documentation for `LeftAlignAndTrimVariants` indicates that it only works for indels. It should be updated to work for MNPs as well. . This operation would simply remove any common leading bases from all alleles of a `VariantContext` and update the start position by however many bases were removed. It would be implemented in `LeftAlignAndTrimVariants.java:289` replacing the noop for non-indels.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7516
https://github.com/broadinstitute/gatk/issues/7516:242,Deployability,update,update,242,The documentation for `LeftAlignAndTrimVariants` indicates that it only works for indels. It should be updated to work for MNPs as well. . This operation would simply remove any common leading bases from all alleles of a `VariantContext` and update the start position by however many bases were removed. It would be implemented in `LeftAlignAndTrimVariants.java:289` replacing the noop for non-indels.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7516
https://github.com/broadinstitute/gatk/issues/7516:160,Usability,simpl,simply,160,The documentation for `LeftAlignAndTrimVariants` indicates that it only works for indels. It should be updated to work for MNPs as well. . This operation would simply remove any common leading bases from all alleles of a `VariantContext` and update the start position by however many bases were removed. It would be implemented in `LeftAlignAndTrimVariants.java:289` replacing the noop for non-indels.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7516
https://github.com/broadinstitute/gatk/pull/7521:54,Deployability,update,updates,54,"This replaces a secret that requires a pr to fix, and updates the name of one of the others.; Requires 1 more step after this.; * Switch travis variable name from DOCKER_SERVICE_PASS -> DOCKER_SERVICE_TOKEN for clarity; * Replace gcloud encrypted key",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7521
https://github.com/broadinstitute/gatk/pull/7521:144,Modifiability,variab,variable,144,"This replaces a secret that requires a pr to fix, and updates the name of one of the others.; Requires 1 more step after this.; * Switch travis variable name from DOCKER_SERVICE_PASS -> DOCKER_SERVICE_TOKEN for clarity; * Replace gcloud encrypted key",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7521
https://github.com/broadinstitute/gatk/pull/7521:237,Security,encrypt,encrypted,237,"This replaces a secret that requires a pr to fix, and updates the name of one of the others.; Requires 1 more step after this.; * Switch travis variable name from DOCKER_SERVICE_PASS -> DOCKER_SERVICE_TOKEN for clarity; * Replace gcloud encrypted key",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7521
https://github.com/broadinstitute/gatk/pull/7522:52,Deployability,update,updated,52,- added up-to-date docker image for prepare step; - updated documentation to refer to the right past steps,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7522
https://github.com/broadinstitute/gatk/issues/7523:28,Availability,error,errors,28,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:109,Deployability,integrat,integration,109,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:164,Deployability,integrat,integration,164,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:109,Integrability,integrat,integration,109,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:164,Integrability,integrat,integration,164,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:9,Testability,test,test,9,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:99,Testability,test,tests,99,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:121,Testability,test,tests,121,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:176,Testability,test,tests,176,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:470,Testability,test,tests,470,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/issues/7523:585,Testability,test,tests,585,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523
https://github.com/broadinstitute/gatk/pull/7524:12,Deployability,deploy,deploy,12,* add a new deploy key for travis to use to authenticate to github,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7524
https://github.com/broadinstitute/gatk/pull/7524:44,Security,authenticat,authenticate,44,* add a new deploy key for travis to use to authenticate to github,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7524
https://github.com/broadinstitute/gatk/pull/7525:28,Deployability,install,installing,28,* This fixes an issue while installing gcloud on travis due to permissions in the root directories. @ldgauthier This should fix the issue you were seeing where test files weren't being uploaded.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7525
https://github.com/broadinstitute/gatk/pull/7525:160,Testability,test,test,160,* This fixes an issue while installing gcloud on travis due to permissions in the root directories. @ldgauthier This should fix the issue you were seeing where test files weren't being uploaded.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7525
https://github.com/broadinstitute/gatk/issues/7526:266,Deployability,pipeline,pipeline,266,"If you'd prefer i post this kind of question elsewhere, please let me know. My lab creates a large dataset of macaque variant data. We regularly add new samples to a dataset that currently has ~2300 WGS/WXS datasets. We largely follow the GATK short variant calling pipeline. Our gVCF data are aggregated into a GenomicsDb workspace, followed by GenotypeGVCFs. As is, whenever we get new samples, we append them to this growing GenomicsDb workspace, and then re-call all of the genotypes. These steps are getting slower and slower (even when scatter/gathered on a cluster), and I'm concerned it's going to become untenable. Plus it's just really inefficient to constantly re-call 1000s of datasets at 40m genome-wide sites. My question is: do you have any experience with analogous datasets, where you have a large base of ""static"" datasets with regular additions of new data? It would be quite nice to avoid constantly re-genotyping the existing datasets. We could in theory just run GenotypeGVCFs on the incoming data and do a simple merge with the existing data. Are you aware of anyone running a process that looks more like this?. There are some caveats to this: 1) for the incoming batches of data, we could run GenotypeGVCF where we force it to call genotypes from every site that exists in the current dataset. This would promote consistent calling across a common set of sites, 2) after we genotype the incoming batch, we could compare the sites present in that against the sites in the current data. It's likely there would be a handful of novel sites. We could re-run GenotypeGVCFs on the existing data specifically on those new sites (presumably the existing animals are largely WT at those positions), and merge those new sites with the existing data, 3) we then merge the incoming data with the updated core data, which should each have genotypes called at the identical set of sites. Are there any discussions happening about managing/updating large variant datasets like this? Thanks f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7526
https://github.com/broadinstitute/gatk/issues/7526:1809,Deployability,update,updated,1809,"r i post this kind of question elsewhere, please let me know. My lab creates a large dataset of macaque variant data. We regularly add new samples to a dataset that currently has ~2300 WGS/WXS datasets. We largely follow the GATK short variant calling pipeline. Our gVCF data are aggregated into a GenomicsDb workspace, followed by GenotypeGVCFs. As is, whenever we get new samples, we append them to this growing GenomicsDb workspace, and then re-call all of the genotypes. These steps are getting slower and slower (even when scatter/gathered on a cluster), and I'm concerned it's going to become untenable. Plus it's just really inefficient to constantly re-call 1000s of datasets at 40m genome-wide sites. My question is: do you have any experience with analogous datasets, where you have a large base of ""static"" datasets with regular additions of new data? It would be quite nice to avoid constantly re-genotyping the existing datasets. We could in theory just run GenotypeGVCFs on the incoming data and do a simple merge with the existing data. Are you aware of anyone running a process that looks more like this?. There are some caveats to this: 1) for the incoming batches of data, we could run GenotypeGVCF where we force it to call genotypes from every site that exists in the current dataset. This would promote consistent calling across a common set of sites, 2) after we genotype the incoming batch, we could compare the sites present in that against the sites in the current data. It's likely there would be a handful of novel sites. We could re-run GenotypeGVCFs on the existing data specifically on those new sites (presumably the existing animals are largely WT at those positions), and merge those new sites with the existing data, 3) we then merge the incoming data with the updated core data, which should each have genotypes called at the identical set of sites. Are there any discussions happening about managing/updating large variant datasets like this? Thanks for any ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7526
https://github.com/broadinstitute/gatk/issues/7526:903,Safety,avoid,avoid,903,"If you'd prefer i post this kind of question elsewhere, please let me know. My lab creates a large dataset of macaque variant data. We regularly add new samples to a dataset that currently has ~2300 WGS/WXS datasets. We largely follow the GATK short variant calling pipeline. Our gVCF data are aggregated into a GenomicsDb workspace, followed by GenotypeGVCFs. As is, whenever we get new samples, we append them to this growing GenomicsDb workspace, and then re-call all of the genotypes. These steps are getting slower and slower (even when scatter/gathered on a cluster), and I'm concerned it's going to become untenable. Plus it's just really inefficient to constantly re-call 1000s of datasets at 40m genome-wide sites. My question is: do you have any experience with analogous datasets, where you have a large base of ""static"" datasets with regular additions of new data? It would be quite nice to avoid constantly re-genotyping the existing datasets. We could in theory just run GenotypeGVCFs on the incoming data and do a simple merge with the existing data. Are you aware of anyone running a process that looks more like this?. There are some caveats to this: 1) for the incoming batches of data, we could run GenotypeGVCF where we force it to call genotypes from every site that exists in the current dataset. This would promote consistent calling across a common set of sites, 2) after we genotype the incoming batch, we could compare the sites present in that against the sites in the current data. It's likely there would be a handful of novel sites. We could re-run GenotypeGVCFs on the existing data specifically on those new sites (presumably the existing animals are largely WT at those positions), and merge those new sites with the existing data, 3) we then merge the incoming data with the updated core data, which should each have genotypes called at the identical set of sites. Are there any discussions happening about managing/updating large variant datasets like this? Thanks f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7526
https://github.com/broadinstitute/gatk/issues/7526:1029,Usability,simpl,simple,1029,"refer i post this kind of question elsewhere, please let me know. My lab creates a large dataset of macaque variant data. We regularly add new samples to a dataset that currently has ~2300 WGS/WXS datasets. We largely follow the GATK short variant calling pipeline. Our gVCF data are aggregated into a GenomicsDb workspace, followed by GenotypeGVCFs. As is, whenever we get new samples, we append them to this growing GenomicsDb workspace, and then re-call all of the genotypes. These steps are getting slower and slower (even when scatter/gathered on a cluster), and I'm concerned it's going to become untenable. Plus it's just really inefficient to constantly re-call 1000s of datasets at 40m genome-wide sites. My question is: do you have any experience with analogous datasets, where you have a large base of ""static"" datasets with regular additions of new data? It would be quite nice to avoid constantly re-genotyping the existing datasets. We could in theory just run GenotypeGVCFs on the incoming data and do a simple merge with the existing data. Are you aware of anyone running a process that looks more like this?. There are some caveats to this: 1) for the incoming batches of data, we could run GenotypeGVCF where we force it to call genotypes from every site that exists in the current dataset. This would promote consistent calling across a common set of sites, 2) after we genotype the incoming batch, we could compare the sites present in that against the sites in the current data. It's likely there would be a handful of novel sites. We could re-run GenotypeGVCFs on the existing data specifically on those new sites (presumably the existing animals are largely WT at those positions), and merge those new sites with the existing data, 3) we then merge the incoming data with the updated core data, which should each have genotypes called at the identical set of sites. Are there any discussions happening about managing/updating large variant datasets like this? Thanks for any ide",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7526
https://github.com/broadinstitute/gatk/pull/7530:62,Modifiability,refactor,refactors,62,This PR merges the BQ Write API with the ref ranges table and refactors the Import wdl to be single sample.; I pulled out the create tables into it's own wdl. It doesn't make sense to try this for each individual sample. For now it need to be run separately. We are also not setting the is_loaded field in the sample_info table - this needs to be resolved when we decide how we want to track that info. See https://docs.google.com/document/d/1_ox38x7YjSeQx1I-6K_6kB4TTlonaEah2LRGevN9GmM/edit# for details,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7530
https://github.com/broadinstitute/gatk/pull/7531:201,Availability,failure,failures,201,"Additional cleanup on the VAT--specifically focused on the failing shards and optimizing the workflow with them in mind. As a next step, this workflow will be split up into 3 sub-workflows to keep the failures from knocking over the remaining likely-successful shards",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7531
https://github.com/broadinstitute/gatk/pull/7531:78,Performance,optimiz,optimizing,78,"Additional cleanup on the VAT--specifically focused on the failing shards and optimizing the workflow with them in mind. As a next step, this workflow will be split up into 3 sub-workflows to keep the failures from knocking over the remaining likely-successful shards",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7531
https://github.com/broadinstitute/gatk/issues/7532:118,Deployability,release,release,118,"## Bug/Usability Report. ### Affected tool(s) or class(es); Mutect2 WDL. ### Affected version(s); - [x] Latest public release version [4.1.81]; - [x] Latest master branch as of October 28, 2021. ### Description ; The Mutect2 WDL's Funcotate task has an unintuitive setup with regard to setting memory for the Funcotate task. Funcotate task memory is defined [here](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L1108); ![image](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:1515,Deployability,configurat,configuration,1515,"age](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to speci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:1730,Deployability,configurat,configuration,1730,"295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utiliz",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2501,Deployability,configurat,configuration,2501,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2603,Deployability,configurat,configuration,2603,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2892,Deployability,configurat,configuration,2892,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:1131,Energy Efficiency,allocate,allocate,1131," 2021. ### Description ; The Mutect2 WDL's Funcotate task has an unintuitive setup with regard to setting memory for the Funcotate task. Funcotate task memory is defined [here](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L1108); ![image](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:780,Modifiability,variab,variable,780,"## Bug/Usability Report. ### Affected tool(s) or class(es); Mutect2 WDL. ### Affected version(s); - [x] Latest public release version [4.1.81]; - [x] Latest master branch as of October 28, 2021. ### Description ; The Mutect2 WDL's Funcotate task has an unintuitive setup with regard to setting memory for the Funcotate task. Funcotate task memory is defined [here](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L1108); ![image](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:888,Modifiability,config,configurable,888,"## Bug/Usability Report. ### Affected tool(s) or class(es); Mutect2 WDL. ### Affected version(s); - [x] Latest public release version [4.1.81]; - [x] Latest master branch as of October 28, 2021. ### Description ; The Mutect2 WDL's Funcotate task has an unintuitive setup with regard to setting memory for the Funcotate task. Funcotate task memory is defined [here](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L1108); ![image](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:1218,Modifiability,variab,variable,1218," 2021. ### Description ; The Mutect2 WDL's Funcotate task has an unintuitive setup with regard to setting memory for the Funcotate task. Funcotate task memory is defined [here](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L1108); ![image](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:1405,Modifiability,variab,variables,1405,"age](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to speci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:1515,Modifiability,config,configuration,1515,"age](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to speci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:1706,Modifiability,variab,variables,1706,"295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utiliz",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:1730,Modifiability,config,configuration,1730,"295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utiliz",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:1982,Modifiability,variab,variables,1982,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2383,Modifiability,variab,variables,2383,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2501,Modifiability,config,configuration,2501,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2567,Modifiability,variab,variables,2567,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2603,Modifiability,config,configuration,2603,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2747,Modifiability,variab,variables,2747,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2892,Modifiability,config,configuration,2892,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:7,Usability,Usab,Usability,7,"## Bug/Usability Report. ### Affected tool(s) or class(es); Mutect2 WDL. ### Affected version(s); - [x] Latest public release version [4.1.81]; - [x] Latest master branch as of October 28, 2021. ### Description ; The Mutect2 WDL's Funcotate task has an unintuitive setup with regard to setting memory for the Funcotate task. Funcotate task memory is defined [here](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L1108); ![image](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7532:2686,Usability,clear,clearly,2686,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532
https://github.com/broadinstitute/gatk/issues/7536:82,Testability,test,test,82,"It appears that both the ""branch"" and ""PR"" builds are always running the complete test suite in travis. We had added a check in previously to skip the ""branch"" builds to save on costs. Here's an example:; https://github.com/broadinstitute/gatk/pull/7533",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7536
https://github.com/broadinstitute/gatk/issues/7538:243,Availability,ERROR,ERROR,243,"## Bug Report. ### Affected tool(s) or class(es); LeftAlignAndTrimVariants. ### Affected version(s); I've only tested with v4.2.2.0. ### Description ; When running LeftAlignAndTrimVariants the program exited with a message that said:; ""A USER ERROR has occurred: Input files reference and features have incompatible contigs: No overlapping contigs found.; reference contigs = [chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chrM, chr1_KI270706v1_random, chr1_KI270707v1_random, chr1_KI270708v1_random, chr1_KI270709v1_random, chr1_KI270710v1_random, chr1_KI270711v1_random, chr1_KI270712v1_random, chr1_KI270713v1_random, chr1_KI270714v1_random, chr2_KI270715v1_random, chr2_KI270716v1_random, chr3_GL000221v1_random, chr4_GL000008v2_random, chr5_GL000208v1_random, chr9_KI270717v1_random, chr9_KI270718v1_random, chr9_KI270719v1_random, chr9_KI270720v1_random, chr11_KI270721v1_random, chr14_GL000009v2_random, chr14_GL000225v1_random, chr14_KI270722v1_random, chr14_GL000194v1_random, chr14_KI270723v1_random, chr14_KI270724v1_random, chr14_KI270725v1_random, chr14_KI270726v1_random, chr15_KI270727v1_random, chr16_KI270728v1_random, chr17_GL000205v2_random, chr17_KI270729v1_random, chr17_KI270730v1_random, chr22_KI270731v1_random, chr22_KI270732v1_random, chr22_KI270733v1_random, chr22_KI270734v1_random, chr22_KI270735v1_random, chr22_KI270736v1_random, chr22_KI270737v1_random, chr22_KI270738v1_random, chr22_KI270739v1_random, chrY_KI270740v1_random, chrUn_KI270302v1, chrUn_KI270304v1, chrUn_KI270303v1, chrUn_KI270305v1, chrUn_KI270322v1, chrUn_KI270320v1, chrUn_KI270310v1, chrUn_KI270316v1, chrUn_KI270315v1, chrUn_KI270312v1, chrUn_KI270311v1, chrUn_KI270317v1, chrUn_KI270412v1, chrUn_KI270411v1, chrUn_KI270414v1, chrUn_KI270419v1, chrUn_KI270418v1, chrUn_KI270420v1, chrUn_KI270424v1, chrUn_KI270417v1, chrUn_KI270422v1, chrUn_KI270423v1, chrUn_KI270425v1, chrUn_KI270429v1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538
https://github.com/broadinstitute/gatk/issues/7538:4202,Availability,Error,Errors-about-input-files-having-missing-or-incompatible-contigs,4202,"rUn_KI270374v1, chrUn_KI270372v1, chrUn_KI270373v1, chrUn_KI270375v1, chrUn_KI270371v1, chrUn_KI270448v1, chrUn_KI270521v1, chrUn_GL000195v1, chrUn_GL000219v1, chrUn_GL000220v1, chrUn_GL000224v1, chrUn_KI270741v1, chrUn_GL000226v1, chrUn_GL000213v1, chrUn_KI270743v1, chrUn_KI270744v1, chrUn_KI270745v1, chrUn_KI270746v1, chrUn_KI270747v1, chrUn_KI270748v1, chrUn_KI270749v1, chrUn_KI270750v1, chrUn_KI270751v1, chrUn_KI270752v1, chrUn_KI270753v1, chrUn_KI270754v1, chrUn_KI270755v1, chrUn_KI270756v1, chrUn_KI270757v1, chrUn_GL000214v1, chrUn_KI270742v1, chrUn_GL000216v2, chrUn_GL000218v1, chrEBV]; features contigs = [X, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]"". The VCF that I have uses just numbers for chromosomes, whereas the reference genome uses chr1, chr2, etc. Both naming conventions are valid. This is a 4.3 VCF. I have read https://gatk.broadinstitute.org/hc/en-us/articles/360035891131-Errors-about-input-files-having-missing-or-incompatible-contigs and this seems to be the same issue, but I believe there should be a translation that happens, e.g. 1 -> chr1 or the reverse as well. #### Steps to reproduce; Ran the following command using a VCF and reference file that I have:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar LeftAlignAndTrimVariants --max-indel-length 500 --max-leading-bases 2000 --dont-trim-alleles false --verbosity DEBUG --variant <input vcf file> --output /data/<vcf_output> --reference /data/<reference file> --split-multi-allelics true. #### Expected behavior; I would expect GATK to be able to translate 1 -> chr1, 2 -> chr2, etc. since both naming conventions are valid according to the VCF spec http://samtools.github.io/hts-specs/VCFv4.3.pdf. When running the same exact command on a VCF file that uses chr1, chr2, etc. as the naming convention the command runs su",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538
https://github.com/broadinstitute/gatk/issues/7538:5320,Availability,error,error,5320,"KI270448v1, chrUn_KI270521v1, chrUn_GL000195v1, chrUn_GL000219v1, chrUn_GL000220v1, chrUn_GL000224v1, chrUn_KI270741v1, chrUn_GL000226v1, chrUn_GL000213v1, chrUn_KI270743v1, chrUn_KI270744v1, chrUn_KI270745v1, chrUn_KI270746v1, chrUn_KI270747v1, chrUn_KI270748v1, chrUn_KI270749v1, chrUn_KI270750v1, chrUn_KI270751v1, chrUn_KI270752v1, chrUn_KI270753v1, chrUn_KI270754v1, chrUn_KI270755v1, chrUn_KI270756v1, chrUn_KI270757v1, chrUn_GL000214v1, chrUn_KI270742v1, chrUn_GL000216v2, chrUn_GL000218v1, chrEBV]; features contigs = [X, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]"". The VCF that I have uses just numbers for chromosomes, whereas the reference genome uses chr1, chr2, etc. Both naming conventions are valid. This is a 4.3 VCF. I have read https://gatk.broadinstitute.org/hc/en-us/articles/360035891131-Errors-about-input-files-having-missing-or-incompatible-contigs and this seems to be the same issue, but I believe there should be a translation that happens, e.g. 1 -> chr1 or the reverse as well. #### Steps to reproduce; Ran the following command using a VCF and reference file that I have:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar LeftAlignAndTrimVariants --max-indel-length 500 --max-leading-bases 2000 --dont-trim-alleles false --verbosity DEBUG --variant <input vcf file> --output /data/<vcf_output> --reference /data/<reference file> --split-multi-allelics true. #### Expected behavior; I would expect GATK to be able to translate 1 -> chr1, 2 -> chr2, etc. since both naming conventions are valid according to the VCF spec http://samtools.github.io/hts-specs/VCFv4.3.pdf. When running the same exact command on a VCF file that uses chr1, chr2, etc. as the naming convention the command runs successfully. #### Actual behavior; GATK exits and gives error message mentioned in description.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538
https://github.com/broadinstitute/gatk/issues/7538:215,Integrability,message,message,215,"## Bug Report. ### Affected tool(s) or class(es); LeftAlignAndTrimVariants. ### Affected version(s); I've only tested with v4.2.2.0. ### Description ; When running LeftAlignAndTrimVariants the program exited with a message that said:; ""A USER ERROR has occurred: Input files reference and features have incompatible contigs: No overlapping contigs found.; reference contigs = [chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chrM, chr1_KI270706v1_random, chr1_KI270707v1_random, chr1_KI270708v1_random, chr1_KI270709v1_random, chr1_KI270710v1_random, chr1_KI270711v1_random, chr1_KI270712v1_random, chr1_KI270713v1_random, chr1_KI270714v1_random, chr2_KI270715v1_random, chr2_KI270716v1_random, chr3_GL000221v1_random, chr4_GL000008v2_random, chr5_GL000208v1_random, chr9_KI270717v1_random, chr9_KI270718v1_random, chr9_KI270719v1_random, chr9_KI270720v1_random, chr11_KI270721v1_random, chr14_GL000009v2_random, chr14_GL000225v1_random, chr14_KI270722v1_random, chr14_GL000194v1_random, chr14_KI270723v1_random, chr14_KI270724v1_random, chr14_KI270725v1_random, chr14_KI270726v1_random, chr15_KI270727v1_random, chr16_KI270728v1_random, chr17_GL000205v2_random, chr17_KI270729v1_random, chr17_KI270730v1_random, chr22_KI270731v1_random, chr22_KI270732v1_random, chr22_KI270733v1_random, chr22_KI270734v1_random, chr22_KI270735v1_random, chr22_KI270736v1_random, chr22_KI270737v1_random, chr22_KI270738v1_random, chr22_KI270739v1_random, chrY_KI270740v1_random, chrUn_KI270302v1, chrUn_KI270304v1, chrUn_KI270303v1, chrUn_KI270305v1, chrUn_KI270322v1, chrUn_KI270320v1, chrUn_KI270310v1, chrUn_KI270316v1, chrUn_KI270315v1, chrUn_KI270312v1, chrUn_KI270311v1, chrUn_KI270317v1, chrUn_KI270412v1, chrUn_KI270411v1, chrUn_KI270414v1, chrUn_KI270419v1, chrUn_KI270418v1, chrUn_KI270420v1, chrUn_KI270424v1, chrUn_KI270417v1, chrUn_KI270422v1, chrUn_KI270423v1, chrUn_KI270425v1, chrUn_KI270429v1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538
https://github.com/broadinstitute/gatk/issues/7538:5326,Integrability,message,message,5326,"KI270448v1, chrUn_KI270521v1, chrUn_GL000195v1, chrUn_GL000219v1, chrUn_GL000220v1, chrUn_GL000224v1, chrUn_KI270741v1, chrUn_GL000226v1, chrUn_GL000213v1, chrUn_KI270743v1, chrUn_KI270744v1, chrUn_KI270745v1, chrUn_KI270746v1, chrUn_KI270747v1, chrUn_KI270748v1, chrUn_KI270749v1, chrUn_KI270750v1, chrUn_KI270751v1, chrUn_KI270752v1, chrUn_KI270753v1, chrUn_KI270754v1, chrUn_KI270755v1, chrUn_KI270756v1, chrUn_KI270757v1, chrUn_GL000214v1, chrUn_KI270742v1, chrUn_GL000216v2, chrUn_GL000218v1, chrEBV]; features contigs = [X, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]"". The VCF that I have uses just numbers for chromosomes, whereas the reference genome uses chr1, chr2, etc. Both naming conventions are valid. This is a 4.3 VCF. I have read https://gatk.broadinstitute.org/hc/en-us/articles/360035891131-Errors-about-input-files-having-missing-or-incompatible-contigs and this seems to be the same issue, but I believe there should be a translation that happens, e.g. 1 -> chr1 or the reverse as well. #### Steps to reproduce; Ran the following command using a VCF and reference file that I have:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar LeftAlignAndTrimVariants --max-indel-length 500 --max-leading-bases 2000 --dont-trim-alleles false --verbosity DEBUG --variant <input vcf file> --output /data/<vcf_output> --reference /data/<reference file> --split-multi-allelics true. #### Expected behavior; I would expect GATK to be able to translate 1 -> chr1, 2 -> chr2, etc. since both naming conventions are valid according to the VCF spec http://samtools.github.io/hts-specs/VCFv4.3.pdf. When running the same exact command on a VCF file that uses chr1, chr2, etc. as the naming convention the command runs successfully. #### Actual behavior; GATK exits and gives error message mentioned in description.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538
https://github.com/broadinstitute/gatk/issues/7538:111,Testability,test,tested,111,"## Bug Report. ### Affected tool(s) or class(es); LeftAlignAndTrimVariants. ### Affected version(s); I've only tested with v4.2.2.0. ### Description ; When running LeftAlignAndTrimVariants the program exited with a message that said:; ""A USER ERROR has occurred: Input files reference and features have incompatible contigs: No overlapping contigs found.; reference contigs = [chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chrM, chr1_KI270706v1_random, chr1_KI270707v1_random, chr1_KI270708v1_random, chr1_KI270709v1_random, chr1_KI270710v1_random, chr1_KI270711v1_random, chr1_KI270712v1_random, chr1_KI270713v1_random, chr1_KI270714v1_random, chr2_KI270715v1_random, chr2_KI270716v1_random, chr3_GL000221v1_random, chr4_GL000008v2_random, chr5_GL000208v1_random, chr9_KI270717v1_random, chr9_KI270718v1_random, chr9_KI270719v1_random, chr9_KI270720v1_random, chr11_KI270721v1_random, chr14_GL000009v2_random, chr14_GL000225v1_random, chr14_KI270722v1_random, chr14_GL000194v1_random, chr14_KI270723v1_random, chr14_KI270724v1_random, chr14_KI270725v1_random, chr14_KI270726v1_random, chr15_KI270727v1_random, chr16_KI270728v1_random, chr17_GL000205v2_random, chr17_KI270729v1_random, chr17_KI270730v1_random, chr22_KI270731v1_random, chr22_KI270732v1_random, chr22_KI270733v1_random, chr22_KI270734v1_random, chr22_KI270735v1_random, chr22_KI270736v1_random, chr22_KI270737v1_random, chr22_KI270738v1_random, chr22_KI270739v1_random, chrY_KI270740v1_random, chrUn_KI270302v1, chrUn_KI270304v1, chrUn_KI270303v1, chrUn_KI270305v1, chrUn_KI270322v1, chrUn_KI270320v1, chrUn_KI270310v1, chrUn_KI270316v1, chrUn_KI270315v1, chrUn_KI270312v1, chrUn_KI270311v1, chrUn_KI270317v1, chrUn_KI270412v1, chrUn_KI270411v1, chrUn_KI270414v1, chrUn_KI270419v1, chrUn_KI270418v1, chrUn_KI270420v1, chrUn_KI270424v1, chrUn_KI270417v1, chrUn_KI270422v1, chrUn_KI270423v1, chrUn_KI270425v1, chrUn_KI270429v1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538
https://github.com/broadinstitute/gatk/pull/7541:138,Deployability,update,updates,138,"Implements tool for clustering SVs, built on top of the clustering engine code refined recently in #7243. In addition to a few bug fixes, updates also include:. - `PloidyTable` class, which ingests and serves as a simple data class for a tsv of per-sample contig ploidies. This was necessary for inferring genotypes when input vcfs contain non-matching sample and variant records.; - Modified `SVClusterEngine` to render sorted output.; - Improved code for SV record collapsing (see the `CanonicalSVCollapser`), particularly for CNVs. Genotype collapsing now infers allele phasing in certain unambiguous cases, in particular for DUPs and multi-allelic CNVs. Testing for this has been cleaned up and augmented with further cases to validate this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7541
https://github.com/broadinstitute/gatk/pull/7541:731,Security,validat,validate,731,"Implements tool for clustering SVs, built on top of the clustering engine code refined recently in #7243. In addition to a few bug fixes, updates also include:. - `PloidyTable` class, which ingests and serves as a simple data class for a tsv of per-sample contig ploidies. This was necessary for inferring genotypes when input vcfs contain non-matching sample and variant records.; - Modified `SVClusterEngine` to render sorted output.; - Improved code for SV record collapsing (see the `CanonicalSVCollapser`), particularly for CNVs. Genotype collapsing now infers allele phasing in certain unambiguous cases, in particular for DUPs and multi-allelic CNVs. Testing for this has been cleaned up and augmented with further cases to validate this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7541
https://github.com/broadinstitute/gatk/pull/7541:658,Testability,Test,Testing,658,"Implements tool for clustering SVs, built on top of the clustering engine code refined recently in #7243. In addition to a few bug fixes, updates also include:. - `PloidyTable` class, which ingests and serves as a simple data class for a tsv of per-sample contig ploidies. This was necessary for inferring genotypes when input vcfs contain non-matching sample and variant records.; - Modified `SVClusterEngine` to render sorted output.; - Improved code for SV record collapsing (see the `CanonicalSVCollapser`), particularly for CNVs. Genotype collapsing now infers allele phasing in certain unambiguous cases, in particular for DUPs and multi-allelic CNVs. Testing for this has been cleaned up and augmented with further cases to validate this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7541
https://github.com/broadinstitute/gatk/pull/7541:214,Usability,simpl,simple,214,"Implements tool for clustering SVs, built on top of the clustering engine code refined recently in #7243. In addition to a few bug fixes, updates also include:. - `PloidyTable` class, which ingests and serves as a simple data class for a tsv of per-sample contig ploidies. This was necessary for inferring genotypes when input vcfs contain non-matching sample and variant records.; - Modified `SVClusterEngine` to render sorted output.; - Improved code for SV record collapsing (see the `CanonicalSVCollapser`), particularly for CNVs. Genotype collapsing now infers allele phasing in certain unambiguous cases, in particular for DUPs and multi-allelic CNVs. Testing for this has been cleaned up and augmented with further cases to validate this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7541
https://github.com/broadinstitute/gatk/issues/7542:646,Performance,optimiz,optimizations,646,"Hello,. When using GATK GenomicsDbImport, I understand that both the java layer and C layer use memory and therefore we need to allow a buffer for the genomics DB code. We're getting odd behavior from GenotypeGVCFs memory. We run a command similar to:. ```; java -Djava.io.tmpdir=<path> -Xmx384g -Xms384g -Xss2m \; -jar GenomeAnalysisTK4.jar GenotypeGVCFs \; -R <FASTA> \; --variant gendb:///.....gdb -O <path> \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 --max-alternate-alleles 12 \; --force-output-intervals <BED_FILE> \; -L 3:129881515-132852846 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations. ```. We find the job is using ~466GB, not the 384 I'd expect it to be constrained by. Does using a genomics DB as input result in some other non-java code being used? I'm trying to understand why Xmx/Xmx isnt constraining it. . Thanks for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542
https://github.com/broadinstitute/gatk/issues/7543:728,Usability,usab,usable,728,Recently for Mutect a new class of `JumboInfoFieldAnnotations` and `JumboGenotypeAnntations` were introduced into GATK and their names are somewhat misleading and confusing on first pass. I would suggest renaming them to `FragmentAnnotations` or something that more accurately describes what they are using. . Furthermore given the state of the annotation engine and the type system nightmare that lurks beneath the surface it is quite difficult to use these annotations in any context except when the likelihoods have been computed in terms of fragments which can be a non-trivial conversion that shouldn't happen in every case. We should revisit the types for this whole class and find some way to make these annotations more usable outside of mutect.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7543
https://github.com/broadinstitute/gatk/issues/7548:1737,Availability,avail,available,1737,"### Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Feature request. ### Tool(s) or class(es) involved; `SplitIntervals`. ### Description; Just tried changes from #7157 and they work perfectly. However:. - `SplitIntervals` starts the scatter files counting from 0. Would it be possible to allow the user to specify the starting value (e.g. 1 instead of 0)?; - Also noted that if I have a `bed` file with 3 intervals and ask `SplitIntervals` to scatter it into 6 files, it just creates 3 files. I think it would be nice if `SplitIntervals` would check it and complain if more scatters were requested than intervals are available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7548
https://github.com/broadinstitute/gatk/issues/7549:101,Availability,error,error,101,"Hi; I tried to run the recalibration using the gatk-generated vcf file as a known vcf file. I got an error which has been previously described ""The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0"" but without the solution. ; I am wondering if the solution has been found. Anyone has the experience to fix this issue.; Thank ; **This is the batch file** ; java -Xmx16g -jar /scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ApplyBQSR \; -R /scratch/ddo/refgenomenew/New_IDs.fasta \; -I /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam \; --bqsr-recal-file /scratch/ddo/reclibration/gatkmf01_C18-436P.recal_data.table \; -O /scratch/ddo/reclibration/C18-436P.bqsr.maf01.bam . **This is the log file**; -----------------------------------------------------------------------------------------------------; Picked up JAVA_TOOL_OPTIONS: -Xmx2g; 04:59:42.641 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 08, 2021 4:59:43 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 04:59:43.044 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - The Genome Analysis Toolkit (GATK) v4.1.9.0; 04:59:43.045 INFO ApplyBQSR - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:59:43.045 INFO ApplyBQSR - Executing as on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 04:59:43.045 INFO ApplyBQSR - Java runtime: OpenJDK 64-Bit Server VM v13.0.2+8; 04:59:43.045 INFO ApplyBQSR - Start Date/Time: November 8, 2021 at 4:59:42 a.m. PST; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.046 INFO ApplyBQSR - HTSJDK Version: 2.23.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549
https://github.com/broadinstitute/gatk/issues/7549:3184,Availability,down,down,3184,"FOR_SAMTOOLS : false; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 04:59:43.046 INFO ApplyBQSR - Deflater: IntelDeflater; 04:59:43.046 INFO ApplyBQSR - Inflater: IntelInflater; 04:59:43.046 INFO ApplyBQSR - GCS max retries/reopens: 20; 04:59:43.046 INFO ApplyBQSR - Requester pays: disabled; 04:59:43.047 INFO ApplyBQSR - Initializing engine; WARNING: BAM index file /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam.bai is older than BAM /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam; 04:59:43.556 INFO ApplyBQSR - Done initializing engine; 04:59:43.592 WARN ApplyBQSR - This tool has only been well tested on ILLUMINA-based sequencing data. For other data use at your own risk.; 04:59:43.592 INFO ProgressMeter - Starting traversal; 04:59:43.592 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 04:59:45.014 INFO ApplyBQSR - Shutting down engine; [November 8, 2021 at 4:59:45 a.m. PST] org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSR done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=557842432; java.lang.IllegalStateException: **The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0**; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:750); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.keyForReadGroup(ReadGroupCovariate.java:81); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.recordValues(ReadGroupCovariate.java:53); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.StandardCovariateList.recordAllValuesInStorage(StandardCovariateList.java:133); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:546); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:527); 	at org.broadinstitute.hellbender.tr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549
https://github.com/broadinstitute/gatk/issues/7549:4931,Integrability,wrap,wrapAndCopyInto,4931,t org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:546); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:527); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:145); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:27); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); 	at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549
https://github.com/broadinstitute/gatk/issues/7549:905,Performance,Load,Loading,905,"Hi; I tried to run the recalibration using the gatk-generated vcf file as a known vcf file. I got an error which has been previously described ""The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0"" but without the solution. ; I am wondering if the solution has been found. Anyone has the experience to fix this issue.; Thank ; **This is the batch file** ; java -Xmx16g -jar /scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ApplyBQSR \; -R /scratch/ddo/refgenomenew/New_IDs.fasta \; -I /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam \; --bqsr-recal-file /scratch/ddo/reclibration/gatkmf01_C18-436P.recal_data.table \; -O /scratch/ddo/reclibration/C18-436P.bqsr.maf01.bam . **This is the log file**; -----------------------------------------------------------------------------------------------------; Picked up JAVA_TOOL_OPTIONS: -Xmx2g; 04:59:42.641 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 08, 2021 4:59:43 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 04:59:43.044 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - The Genome Analysis Toolkit (GATK) v4.1.9.0; 04:59:43.045 INFO ApplyBQSR - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:59:43.045 INFO ApplyBQSR - Executing as on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 04:59:43.045 INFO ApplyBQSR - Java runtime: OpenJDK 64-Bit Server VM v13.0.2+8; 04:59:43.045 INFO ApplyBQSR - Start Date/Time: November 8, 2021 at 4:59:42 a.m. PST; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.046 INFO ApplyBQSR - HTSJDK Version: 2.23.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549
https://github.com/broadinstitute/gatk/issues/7549:1180,Safety,detect,detect,1180," ReadGroup V300019285_L2_ in RecalTable0"" but without the solution. ; I am wondering if the solution has been found. Anyone has the experience to fix this issue.; Thank ; **This is the batch file** ; java -Xmx16g -jar /scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ApplyBQSR \; -R /scratch/ddo/refgenomenew/New_IDs.fasta \; -I /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam \; --bqsr-recal-file /scratch/ddo/reclibration/gatkmf01_C18-436P.recal_data.table \; -O /scratch/ddo/reclibration/C18-436P.bqsr.maf01.bam . **This is the log file**; -----------------------------------------------------------------------------------------------------; Picked up JAVA_TOOL_OPTIONS: -Xmx2g; 04:59:42.641 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 08, 2021 4:59:43 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 04:59:43.044 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - The Genome Analysis Toolkit (GATK) v4.1.9.0; 04:59:43.045 INFO ApplyBQSR - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:59:43.045 INFO ApplyBQSR - Executing as on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 04:59:43.045 INFO ApplyBQSR - Java runtime: OpenJDK 64-Bit Server VM v13.0.2+8; 04:59:43.045 INFO ApplyBQSR - Start Date/Time: November 8, 2021 at 4:59:42 a.m. PST; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.046 INFO ApplyBQSR - HTSJDK Version: 2.23.0; 04:59:43.046 INFO ApplyBQSR - Picard Version: 2.23.3; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.US",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549
https://github.com/broadinstitute/gatk/issues/7549:2990,Safety,risk,risk,2990,"R - HTSJDK Version: 2.23.0; 04:59:43.046 INFO ApplyBQSR - Picard Version: 2.23.3; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 04:59:43.046 INFO ApplyBQSR - Deflater: IntelDeflater; 04:59:43.046 INFO ApplyBQSR - Inflater: IntelInflater; 04:59:43.046 INFO ApplyBQSR - GCS max retries/reopens: 20; 04:59:43.046 INFO ApplyBQSR - Requester pays: disabled; 04:59:43.047 INFO ApplyBQSR - Initializing engine; WARNING: BAM index file /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam.bai is older than BAM /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam; 04:59:43.556 INFO ApplyBQSR - Done initializing engine; 04:59:43.592 WARN ApplyBQSR - This tool has only been well tested on ILLUMINA-based sequencing data. For other data use at your own risk.; 04:59:43.592 INFO ProgressMeter - Starting traversal; 04:59:43.592 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 04:59:45.014 INFO ApplyBQSR - Shutting down engine; [November 8, 2021 at 4:59:45 a.m. PST] org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSR done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=557842432; java.lang.IllegalStateException: **The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0**; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:750); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.keyForReadGroup(ReadGroupCovariate.java:81); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.recordValues(ReadGroupCovariate.java:53); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.StandardCovariateList.recordAllValuesInStorage(StandardCovariateList.java:133); 	at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549
https://github.com/broadinstitute/gatk/issues/7549:3519,Security,validat,validate,3519,"CS max retries/reopens: 20; 04:59:43.046 INFO ApplyBQSR - Requester pays: disabled; 04:59:43.047 INFO ApplyBQSR - Initializing engine; WARNING: BAM index file /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam.bai is older than BAM /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam; 04:59:43.556 INFO ApplyBQSR - Done initializing engine; 04:59:43.592 WARN ApplyBQSR - This tool has only been well tested on ILLUMINA-based sequencing data. For other data use at your own risk.; 04:59:43.592 INFO ProgressMeter - Starting traversal; 04:59:43.592 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 04:59:45.014 INFO ApplyBQSR - Shutting down engine; [November 8, 2021 at 4:59:45 a.m. PST] org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSR done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=557842432; java.lang.IllegalStateException: **The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0**; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:750); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.keyForReadGroup(ReadGroupCovariate.java:81); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.recordValues(ReadGroupCovariate.java:53); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.StandardCovariateList.recordAllValuesInStorage(StandardCovariateList.java:133); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:546); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:527); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:145); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:27); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipelin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549
https://github.com/broadinstitute/gatk/issues/7549:713,Testability,log,log,713,"Hi; I tried to run the recalibration using the gatk-generated vcf file as a known vcf file. I got an error which has been previously described ""The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0"" but without the solution. ; I am wondering if the solution has been found. Anyone has the experience to fix this issue.; Thank ; **This is the batch file** ; java -Xmx16g -jar /scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ApplyBQSR \; -R /scratch/ddo/refgenomenew/New_IDs.fasta \; -I /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam \; --bqsr-recal-file /scratch/ddo/reclibration/gatkmf01_C18-436P.recal_data.table \; -O /scratch/ddo/reclibration/C18-436P.bqsr.maf01.bam . **This is the log file**; -----------------------------------------------------------------------------------------------------; Picked up JAVA_TOOL_OPTIONS: -Xmx2g; 04:59:42.641 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 08, 2021 4:59:43 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 04:59:43.044 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - The Genome Analysis Toolkit (GATK) v4.1.9.0; 04:59:43.045 INFO ApplyBQSR - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:59:43.045 INFO ApplyBQSR - Executing as on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 04:59:43.045 INFO ApplyBQSR - Java runtime: OpenJDK 64-Bit Server VM v13.0.2+8; 04:59:43.045 INFO ApplyBQSR - Start Date/Time: November 8, 2021 at 4:59:42 a.m. PST; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.046 INFO ApplyBQSR - HTSJDK Version: 2.23.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549
https://github.com/broadinstitute/gatk/issues/7549:2917,Testability,test,tested,2917,"------------------------------------; 04:59:43.046 INFO ApplyBQSR - HTSJDK Version: 2.23.0; 04:59:43.046 INFO ApplyBQSR - Picard Version: 2.23.3; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:59:43.046 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 04:59:43.046 INFO ApplyBQSR - Deflater: IntelDeflater; 04:59:43.046 INFO ApplyBQSR - Inflater: IntelInflater; 04:59:43.046 INFO ApplyBQSR - GCS max retries/reopens: 20; 04:59:43.046 INFO ApplyBQSR - Requester pays: disabled; 04:59:43.047 INFO ApplyBQSR - Initializing engine; WARNING: BAM index file /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam.bai is older than BAM /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam; 04:59:43.556 INFO ApplyBQSR - Done initializing engine; 04:59:43.592 WARN ApplyBQSR - This tool has only been well tested on ILLUMINA-based sequencing data. For other data use at your own risk.; 04:59:43.592 INFO ProgressMeter - Starting traversal; 04:59:43.592 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 04:59:45.014 INFO ApplyBQSR - Shutting down engine; [November 8, 2021 at 4:59:45 a.m. PST] org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSR done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=557842432; java.lang.IllegalStateException: **The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0**; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:750); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.keyForReadGroup(ReadGroupCovariate.java:81); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.recordValues(ReadGroupCovariate.java:53); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.StandardCovariateList.recordAllValuesInStorage(S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549
https://github.com/broadinstitute/gatk/pull/7555:424,Modifiability,refactor,refactoring,424,"@jonn-smith Quick one when you get a chance - this fixes some things I noticed on my GATK branch when testing my new htsjdk `VCFHeader` code. The `Funcotator.checkIfAlreadyAnnotated `code was checking for a header line that was never generated by `Funcotator` AFAICT, and this also ties together the `Funcotator` engine and test code to use the same constants. More could probably be done here but it would require a bigger refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7555
https://github.com/broadinstitute/gatk/pull/7555:102,Testability,test,testing,102,"@jonn-smith Quick one when you get a chance - this fixes some things I noticed on my GATK branch when testing my new htsjdk `VCFHeader` code. The `Funcotator.checkIfAlreadyAnnotated `code was checking for a header line that was never generated by `Funcotator` AFAICT, and this also ties together the `Funcotator` engine and test code to use the same constants. More could probably be done here but it would require a bigger refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7555
https://github.com/broadinstitute/gatk/pull/7555:324,Testability,test,test,324,"@jonn-smith Quick one when you get a chance - this fixes some things I noticed on my GATK branch when testing my new htsjdk `VCFHeader` code. The `Funcotator.checkIfAlreadyAnnotated `code was checking for a header line that was never generated by `Funcotator` AFAICT, and this also ties together the `Funcotator` engine and test code to use the same constants. More could probably be done here but it would require a bigger refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7555
https://github.com/broadinstitute/gatk/issues/7556:987,Availability,down,downstream,987,"A user on the GATK Forum submitted a request to make the INFO field easier to manipulate through creating a table. At the GATK Office Hours meeting 11/8, we discussed the two ideas and favored the first idea to make a new tool, similar to VariantsToTable, that would unpack the INFO field. This request was created from a contribution made by Shahryar Alavi on October 30, 2020 19:54 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360073983291-VariantsToTable-not-extracting-INFO-sub-fields-#community\_comment\_360013343072](https://gatk.broadinstitute.org/hc/en-us/community/posts/360073983291-VariantsToTable-not-extracting-INFO-sub-fields-#community_comment_360013343072). \--. But MAF output is somewhat different from VCF; and I think the VCF output format is better for germline variant annotation. With Funcotator we get an integrated (and minuter) ""variant calling - annotation"" workflow. But the problem is ""vertical bar"" separated INFOs are not easy for downstream text processing. I have two suggestions for the GATK Team:. You may want to develop a new tool (like VariantsToTable) to separate each ""sub-info"" in the FUNCOTATION INFO, and put them into separate columns with corresponding headers when creating the tab-delimited table. Or add a feature to Funcotator to create multiple INFOs with FUNCOTATION prefix in their IDs; e.g. #INFO=<ID=FUNCOTATION\\\_Gencode\\\_34\\\_hugoSymbol,...> ; ; #INFO=<ID=FUNCOTATION\\\_Gencode\\\_34\\\_ncbiBuild,...>. instead of. #INFO=<ID=FUNCOTATION,...,Description=""Funcotation fields are: Gencode\\\_34\\\_hugoSymbol|Gencode\\\_34\\\_ncbiBuild|..."">. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/45403'>Zendesk ticket #45403</a>)<br>gz#45403</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7556
https://github.com/broadinstitute/gatk/issues/7556:854,Deployability,integrat,integrated,854,"A user on the GATK Forum submitted a request to make the INFO field easier to manipulate through creating a table. At the GATK Office Hours meeting 11/8, we discussed the two ideas and favored the first idea to make a new tool, similar to VariantsToTable, that would unpack the INFO field. This request was created from a contribution made by Shahryar Alavi on October 30, 2020 19:54 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360073983291-VariantsToTable-not-extracting-INFO-sub-fields-#community\_comment\_360013343072](https://gatk.broadinstitute.org/hc/en-us/community/posts/360073983291-VariantsToTable-not-extracting-INFO-sub-fields-#community_comment_360013343072). \--. But MAF output is somewhat different from VCF; and I think the VCF output format is better for germline variant annotation. With Funcotator we get an integrated (and minuter) ""variant calling - annotation"" workflow. But the problem is ""vertical bar"" separated INFOs are not easy for downstream text processing. I have two suggestions for the GATK Team:. You may want to develop a new tool (like VariantsToTable) to separate each ""sub-info"" in the FUNCOTATION INFO, and put them into separate columns with corresponding headers when creating the tab-delimited table. Or add a feature to Funcotator to create multiple INFOs with FUNCOTATION prefix in their IDs; e.g. #INFO=<ID=FUNCOTATION\\\_Gencode\\\_34\\\_hugoSymbol,...> ; ; #INFO=<ID=FUNCOTATION\\\_Gencode\\\_34\\\_ncbiBuild,...>. instead of. #INFO=<ID=FUNCOTATION,...,Description=""Funcotation fields are: Gencode\\\_34\\\_hugoSymbol|Gencode\\\_34\\\_ncbiBuild|..."">. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/45403'>Zendesk ticket #45403</a>)<br>gz#45403</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7556
https://github.com/broadinstitute/gatk/issues/7556:854,Integrability,integrat,integrated,854,"A user on the GATK Forum submitted a request to make the INFO field easier to manipulate through creating a table. At the GATK Office Hours meeting 11/8, we discussed the two ideas and favored the first idea to make a new tool, similar to VariantsToTable, that would unpack the INFO field. This request was created from a contribution made by Shahryar Alavi on October 30, 2020 19:54 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360073983291-VariantsToTable-not-extracting-INFO-sub-fields-#community\_comment\_360013343072](https://gatk.broadinstitute.org/hc/en-us/community/posts/360073983291-VariantsToTable-not-extracting-INFO-sub-fields-#community_comment_360013343072). \--. But MAF output is somewhat different from VCF; and I think the VCF output format is better for germline variant annotation. With Funcotator we get an integrated (and minuter) ""variant calling - annotation"" workflow. But the problem is ""vertical bar"" separated INFOs are not easy for downstream text processing. I have two suggestions for the GATK Team:. You may want to develop a new tool (like VariantsToTable) to separate each ""sub-info"" in the FUNCOTATION INFO, and put them into separate columns with corresponding headers when creating the tab-delimited table. Or add a feature to Funcotator to create multiple INFOs with FUNCOTATION prefix in their IDs; e.g. #INFO=<ID=FUNCOTATION\\\_Gencode\\\_34\\\_hugoSymbol,...> ; ; #INFO=<ID=FUNCOTATION\\\_Gencode\\\_34\\\_ncbiBuild,...>. instead of. #INFO=<ID=FUNCOTATION,...,Description=""Funcotation fields are: Gencode\\\_34\\\_hugoSymbol|Gencode\\\_34\\\_ncbiBuild|..."">. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/45403'>Zendesk ticket #45403</a>)<br>gz#45403</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7556
https://github.com/broadinstitute/gatk/issues/7560:1,Deployability,Integrat,IntegrationTestSpec,1,"`IntegrationTestSpec` currently ignores leading/trailing whitespace by default when doing its comparison against expected outputs. This is problematic given that whitespace can include things like field delimiters, leading to bugs like the one fixed in https://github.com/broadinstitute/gatk/pull/7559. We should change the default to *not* ignore leading/trailing whitespace. Tests that have a legitimate reason for ignoring it can then explicitly opt-in by calling `setTrimWhiteSpace()`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7560
https://github.com/broadinstitute/gatk/issues/7560:1,Integrability,Integrat,IntegrationTestSpec,1,"`IntegrationTestSpec` currently ignores leading/trailing whitespace by default when doing its comparison against expected outputs. This is problematic given that whitespace can include things like field delimiters, leading to bugs like the one fixed in https://github.com/broadinstitute/gatk/pull/7559. We should change the default to *not* ignore leading/trailing whitespace. Tests that have a legitimate reason for ignoring it can then explicitly opt-in by calling `setTrimWhiteSpace()`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7560
https://github.com/broadinstitute/gatk/issues/7560:377,Testability,Test,Tests,377,"`IntegrationTestSpec` currently ignores leading/trailing whitespace by default when doing its comparison against expected outputs. This is problematic given that whitespace can include things like field delimiters, leading to bugs like the one fixed in https://github.com/broadinstitute/gatk/pull/7559. We should change the default to *not* ignore leading/trailing whitespace. Tests that have a legitimate reason for ignoring it can then explicitly opt-in by calling `setTrimWhiteSpace()`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7560
https://github.com/broadinstitute/gatk/pull/7561:111,Modifiability,plugin,plugin,111,"[Compiler daemon](https://docs.gradle.org/current/userguide/performance.html#compiler_daemon). The Gradle Java plugin allows you to run the compiler as a separate process by setting `options.fork = true`. This feature can lead to much less garbage collection and make Gradles infrastructure faster. This project has more than 1000 source files. We can consider enabling this feature. =====================; If there are any inappropriate modifications in this PR, please give me a reply and I will change them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7561
https://github.com/broadinstitute/gatk/pull/7561:60,Performance,perform,performance,60,"[Compiler daemon](https://docs.gradle.org/current/userguide/performance.html#compiler_daemon). The Gradle Java plugin allows you to run the compiler as a separate process by setting `options.fork = true`. This feature can lead to much less garbage collection and make Gradles infrastructure faster. This project has more than 1000 source files. We can consider enabling this feature. =====================; If there are any inappropriate modifications in this PR, please give me a reply and I will change them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7561
https://github.com/broadinstitute/gatk/issues/7562:196,Availability,error,error,196,"A GATK user is getting a java.lang.NullPointerException when running Concordance. It seems that their eval VCF has contigs that do not match the truth dataset. It would be helpful to add a better error message for this case. This request was created from a contribution made by Priyadarshini Thirunavukkarasu on November 11, 2021 10:19 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4410110105755-GATK-concordance](https://gatk.broadinstitute.org/hc/en-us/community/posts/4410110105755-GATK-concordance). \--. Hello. I am running GATK/4.2.2.0-foss-2018b-Java-1.8 in the cluster. After running the given below code, I am not able to find the output file (summary file). This is the link, where the code is given \[[https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary\](/hc/en-us/articles/4405451404699-Concordance#--summary)](https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary](/hc/en-us/articles/4405451404699-Concordance#--summary)). Please also find the log file below. Is the summary file required as input file to run the below script? Please advice. gatk Concordance \\ ; ;  -R /scicore/home/cichon/GROUP/memory\_optimization/data/reference/gch38.fa \\ ; ;  -eval /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf \\ ; ;  --truth /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz \\ ; ;  --summary /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/summary.tsv ; 11:26:21.545 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/scicore/soft/apps/GATK/4.2.2.0-foss-2018b-Java-1.8/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 11, 2021 11:26:21 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:26:21.681 INFO Concordance",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562
https://github.com/broadinstitute/gatk/issues/7562:4293,Availability,down,down,4293,"21.683 INFO Concordance - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 11:26:21.683 INFO Concordance - Deflater: IntelDeflater ; ; 11:26:21.684 INFO Concordance - Inflater: IntelInflater ; ; 11:26:21.684 INFO Concordance - GCS max retries/reopens: 20 ; ; 11:26:21.684 INFO Concordance - Requester pays: disabled ; ; 11:26:21.684 INFO Concordance - Initializing engine ; ; 11:26:22.217 INFO FeatureManager - Using codec VCFCodec to read file file:///scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz ; ; 11:26:22.497 INFO FeatureManager - Using codec VCFCodec to read file file:///scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf ; ; 11:26:22.663 INFO Concordance - Done initializing engine ; ; 11:26:22.672 INFO ProgressMeter - Starting traversal ; ; 11:26:22.672 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute ; ; 11:26:22.682 INFO Concordance - Shutting down engine ; ; \[November 11, 2021 11:26:22 AM CET\] org.broadinstitute.hellbender.tools.walkers.validation.Concordance done. Elapsed time: 0.02 minutes. ; ; Runtime.totalMemory()=559939584 ; ; java.lang.NullPointerException ; ; at htsjdk.variant.variantcontext.VariantContextComparator.compare(VariantContextComparator.java:87) ; ; at org.broadinstitute.hellbender.engine.AbstractConcordanceWalker$ConcordanceIterator.next(AbstractConcordanceWalker.java:192) ; ; at org.broadinstitute.hellbender.engine.AbstractConcordanceWalker$ConcordanceIterator.next(AbstractConcordanceWalker.java:174) ; ; at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ; at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ; at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ; at org.broadinstitute.hellbender.engine.AbstractConcordanceWalker.traverse(AbstractConcordanceWalker.java:132) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562
https://github.com/broadinstitute/gatk/issues/7562:202,Integrability,message,message,202,"A GATK user is getting a java.lang.NullPointerException when running Concordance. It seems that their eval VCF has contigs that do not match the truth dataset. It would be helpful to add a better error message for this case. This request was created from a contribution made by Priyadarshini Thirunavukkarasu on November 11, 2021 10:19 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4410110105755-GATK-concordance](https://gatk.broadinstitute.org/hc/en-us/community/posts/4410110105755-GATK-concordance). \--. Hello. I am running GATK/4.2.2.0-foss-2018b-Java-1.8 in the cluster. After running the given below code, I am not able to find the output file (summary file). This is the link, where the code is given \[[https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary\](/hc/en-us/articles/4405451404699-Concordance#--summary)](https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary](/hc/en-us/articles/4405451404699-Concordance#--summary)). Please also find the log file below. Is the summary file required as input file to run the below script? Please advice. gatk Concordance \\ ; ;  -R /scicore/home/cichon/GROUP/memory\_optimization/data/reference/gch38.fa \\ ; ;  -eval /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf \\ ; ;  --truth /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz \\ ; ;  --summary /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/summary.tsv ; 11:26:21.545 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/scicore/soft/apps/GATK/4.2.2.0-foss-2018b-Java-1.8/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 11, 2021 11:26:21 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:26:21.681 INFO Concordance",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562
https://github.com/broadinstitute/gatk/issues/7562:1601,Performance,Load,Loading,1601,". After running the given below code, I am not able to find the output file (summary file). This is the link, where the code is given \[[https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary\](/hc/en-us/articles/4405451404699-Concordance#--summary)](https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary](/hc/en-us/articles/4405451404699-Concordance#--summary)). Please also find the log file below. Is the summary file required as input file to run the below script? Please advice. gatk Concordance \\ ; ;  -R /scicore/home/cichon/GROUP/memory\_optimization/data/reference/gch38.fa \\ ; ;  -eval /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf \\ ; ;  --truth /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz \\ ; ;  --summary /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/summary.tsv ; 11:26:21.545 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/scicore/soft/apps/GATK/4.2.2.0-foss-2018b-Java-1.8/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 11, 2021 11:26:21 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:26:21.681 INFO Concordance - ------------------------------------------------------------ ; ; 11:26:21.682 INFO Concordance - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:26:21.682 INFO Concordance - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:26:21.682 INFO Concordance - Executing as [thirun0000@shi85.cluster.bc2.ch](mailto:thirun0000@shi85.cluster.bc2.ch) on Linux v3.10.0-1062.18.1.el7.x86\_64 amd64 ; ; 11:26:21.682 INFO Concordance - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_212-b03 ; ; 11:26:21.682 INFO Concordance - S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562
https://github.com/broadinstitute/gatk/issues/7562:1912,Safety,detect,detect,1912,"ute.org/hc/en-us/articles/4405451404699-Concordance#--summary](/hc/en-us/articles/4405451404699-Concordance#--summary)). Please also find the log file below. Is the summary file required as input file to run the below script? Please advice. gatk Concordance \\ ; ;  -R /scicore/home/cichon/GROUP/memory\_optimization/data/reference/gch38.fa \\ ; ;  -eval /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf \\ ; ;  --truth /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz \\ ; ;  --summary /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/summary.tsv ; 11:26:21.545 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/scicore/soft/apps/GATK/4.2.2.0-foss-2018b-Java-1.8/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 11, 2021 11:26:21 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:26:21.681 INFO Concordance - ------------------------------------------------------------ ; ; 11:26:21.682 INFO Concordance - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:26:21.682 INFO Concordance - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:26:21.682 INFO Concordance - Executing as [thirun0000@shi85.cluster.bc2.ch](mailto:thirun0000@shi85.cluster.bc2.ch) on Linux v3.10.0-1062.18.1.el7.x86\_64 amd64 ; ; 11:26:21.682 INFO Concordance - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_212-b03 ; ; 11:26:21.682 INFO Concordance - Start Date/Time: November 11, 2021 11:26:21 AM CET ; ; 11:26:21.682 INFO Concordance - ------------------------------------------------------------ ; ; 11:26:21.682 INFO Concordance - ------------------------------------------------------------ ; ; 11:26:21.683 INFO Concordance - HTSJDK Version: 2.24.1 ; ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562
https://github.com/broadinstitute/gatk/issues/7562:4391,Security,validat,validation,4391,"11:26:21.683 INFO Concordance - Deflater: IntelDeflater ; ; 11:26:21.684 INFO Concordance - Inflater: IntelInflater ; ; 11:26:21.684 INFO Concordance - GCS max retries/reopens: 20 ; ; 11:26:21.684 INFO Concordance - Requester pays: disabled ; ; 11:26:21.684 INFO Concordance - Initializing engine ; ; 11:26:22.217 INFO FeatureManager - Using codec VCFCodec to read file file:///scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz ; ; 11:26:22.497 INFO FeatureManager - Using codec VCFCodec to read file file:///scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf ; ; 11:26:22.663 INFO Concordance - Done initializing engine ; ; 11:26:22.672 INFO ProgressMeter - Starting traversal ; ; 11:26:22.672 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute ; ; 11:26:22.682 INFO Concordance - Shutting down engine ; ; \[November 11, 2021 11:26:22 AM CET\] org.broadinstitute.hellbender.tools.walkers.validation.Concordance done. Elapsed time: 0.02 minutes. ; ; Runtime.totalMemory()=559939584 ; ; java.lang.NullPointerException ; ; at htsjdk.variant.variantcontext.VariantContextComparator.compare(VariantContextComparator.java:87) ; ; at org.broadinstitute.hellbender.engine.AbstractConcordanceWalker$ConcordanceIterator.next(AbstractConcordanceWalker.java:192) ; ; at org.broadinstitute.hellbender.engine.AbstractConcordanceWalker$ConcordanceIterator.next(AbstractConcordanceWalker.java:174) ; ; at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ; at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ; at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ; at org.broadinstitute.hellbender.engine.AbstractConcordanceWalker.traverse(AbstractConcordanceWalker.java:132) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562
https://github.com/broadinstitute/gatk/issues/7562:1047,Testability,log,log,1047,"terException when running Concordance. It seems that their eval VCF has contigs that do not match the truth dataset. It would be helpful to add a better error message for this case. This request was created from a contribution made by Priyadarshini Thirunavukkarasu on November 11, 2021 10:19 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4410110105755-GATK-concordance](https://gatk.broadinstitute.org/hc/en-us/community/posts/4410110105755-GATK-concordance). \--. Hello. I am running GATK/4.2.2.0-foss-2018b-Java-1.8 in the cluster. After running the given below code, I am not able to find the output file (summary file). This is the link, where the code is given \[[https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary\](/hc/en-us/articles/4405451404699-Concordance#--summary)](https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary](/hc/en-us/articles/4405451404699-Concordance#--summary)). Please also find the log file below. Is the summary file required as input file to run the below script? Please advice. gatk Concordance \\ ; ;  -R /scicore/home/cichon/GROUP/memory\_optimization/data/reference/gch38.fa \\ ; ;  -eval /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf \\ ; ;  --truth /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz \\ ; ;  --summary /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/summary.tsv ; 11:26:21.545 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/scicore/soft/apps/GATK/4.2.2.0-foss-2018b-Java-1.8/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 11, 2021 11:26:21 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:26:21.681 INFO Concordance - ----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562
https://github.com/broadinstitute/gatk/issues/7564:57,Testability,stub,stubs,57,"Seems like there are some commented overrides and method stubs, but there are also some remaining unimplemented methods. See https://github.com/broadinstitute/gatk/pull/7394#discussion_r701351063.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7564
https://github.com/broadinstitute/gatk/issues/7567:12,Availability,down,downsampling,12,"Our default downsampling settings in HaplotypeCaller / Mutect2 (cap the maximum number of reads that can start at the same position) is uniquely unsuited to amplicon data. We should detect amplicon data on startup, and warn the user to adjust the downsampling settings (as discussed with @davidbenjamin). @ldgauthier Thoughts on this idea?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7567
https://github.com/broadinstitute/gatk/issues/7567:247,Availability,down,downsampling,247,"Our default downsampling settings in HaplotypeCaller / Mutect2 (cap the maximum number of reads that can start at the same position) is uniquely unsuited to amplicon data. We should detect amplicon data on startup, and warn the user to adjust the downsampling settings (as discussed with @davidbenjamin). @ldgauthier Thoughts on this idea?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7567
https://github.com/broadinstitute/gatk/issues/7567:182,Safety,detect,detect,182,"Our default downsampling settings in HaplotypeCaller / Mutect2 (cap the maximum number of reads that can start at the same position) is uniquely unsuited to amplicon data. We should detect amplicon data on startup, and warn the user to adjust the downsampling settings (as discussed with @davidbenjamin). @ldgauthier Thoughts on this idea?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7567
https://github.com/broadinstitute/gatk/pull/7571:191,Availability,down,down,191,"To test I ran this before and after the fix. Before it would sit there after the line . ```; Tool returned:; 0; ```; for ~10 minutes before it would time out and exit. After the fix it shuts down cleanly. ```; gatk --java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 \; CreateVariantIngestFiles \; -V gs://fc-e2f6ffa2-4033-4517-98fc-889bee4cc7a6/5e6b194b-5f69-40f2-a6de-f4f3f80ce05a/ReblockGVCF/702cdbf7-0666-4ee5-b889-91ba0ffa90bd/call-Reblock/HG00405.haplotypeCalls.er.raw.vcf.gz.rb.g.vcf.gz \; -L chr20:1-100000 \; -IG FORTY \; --ignore-above-gq-threshold false \; --project-id broad-dsp-spec-ops \; --dataset-name gvs_qs_v2_kc \; --output-type BQ \; --enable-reference-ranges true \; --enable-pet false \; --enable-vet true \; -SN ERS4367795 \; --gvs-sample-id 99 \; --ref-version 38; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7571
