quality_attribute,keyword,matched_word,sentence,source,filename,author,repo,version,wiki,url
Modifiability,variab,variables,"""""""; This module will benchmark preprocessing operations in Scanpy that run on counts; API documentation: https://scanpy.readthedocs.io/en/stable/api/preprocessing.html; """"""; # setup variables; """"""Setup global variables before each benchmark.""""""; # ASV suite; # TODO: This would fail: assert ""log1p"" not in adata.uns, ""ASV bug?""; # https://github.com/scverse/scanpy/issues/3052",MatchSource.CODE_COMMENT,benchmarks/benchmarks/preprocessing_counts.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/benchmarks/benchmarks/preprocessing_counts.py
Testability,benchmark,benchmark,"""""""; This module will benchmark preprocessing operations in Scanpy that run on counts; API documentation: https://scanpy.readthedocs.io/en/stable/api/preprocessing.html; """"""; # setup variables; """"""Setup global variables before each benchmark.""""""; # ASV suite; # TODO: This would fail: assert ""log1p"" not in adata.uns, ""ASV bug?""; # https://github.com/scverse/scanpy/issues/3052",MatchSource.CODE_COMMENT,benchmarks/benchmarks/preprocessing_counts.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/benchmarks/benchmarks/preprocessing_counts.py
Modifiability,variab,variables,"""""""; This module will benchmark preprocessing operations in Scanpy that run on log-transformed data; API documentation: https://scanpy.readthedocs.io/en/stable/api/preprocessing.html; """"""; # setup variables; """"""Setup global variables before each benchmark.""""""; # ASV suite; # regress_out is very slow for this dataset; """"""Suite for fast preprocessing operations.""""""",MatchSource.CODE_COMMENT,benchmarks/benchmarks/preprocessing_log.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/benchmarks/benchmarks/preprocessing_log.py
Testability,benchmark,benchmark,"""""""; This module will benchmark preprocessing operations in Scanpy that run on log-transformed data; API documentation: https://scanpy.readthedocs.io/en/stable/api/preprocessing.html; """"""; # setup variables; """"""Setup global variables before each benchmark.""""""; # ASV suite; # regress_out is very slow for this dataset; """"""Suite for fast preprocessing operations.""""""",MatchSource.CODE_COMMENT,benchmarks/benchmarks/preprocessing_log.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/benchmarks/benchmarks/preprocessing_log.py
Modifiability,variab,variables,"""""""; This module will benchmark tool operations in Scanpy; API documentation: https://scanpy.readthedocs.io/en/stable/api/tools.html; """"""; # setup variables",MatchSource.CODE_COMMENT,benchmarks/benchmarks/tools.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/benchmarks/benchmarks/tools.py
Testability,benchmark,benchmark,"""""""; This module will benchmark tool operations in Scanpy; API documentation: https://scanpy.readthedocs.io/en/stable/api/tools.html; """"""; # setup variables",MatchSource.CODE_COMMENT,benchmarks/benchmarks/tools.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/benchmarks/benchmarks/tools.py
Deployability,install,install,"#!python3; """"""; Given a requirement, return the minimum version specifier. Example; -------. >>> min_dep(Requirement(""numpy>=1.0"")); ""numpy==1.0""; """"""; # We'll be mutating this; # If we are referring to other optional dependency lists, resolve them; """"""Parse a pyproject.toml file and output a list of minimum dependencies. Output is directly passable to `pip install`.""""""",MatchSource.CODE_COMMENT,ci/scripts/min-deps.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/ci/scripts/min-deps.py
Integrability,depend,dependency,"#!python3; """"""; Given a requirement, return the minimum version specifier. Example; -------. >>> min_dep(Requirement(""numpy>=1.0"")); ""numpy==1.0""; """"""; # We'll be mutating this; # If we are referring to other optional dependency lists, resolve them; """"""Parse a pyproject.toml file and output a list of minimum dependencies. Output is directly passable to `pip install`.""""""",MatchSource.CODE_COMMENT,ci/scripts/min-deps.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/ci/scripts/min-deps.py
Deployability,configurat,configuration,"# noqa; # Don’t use tkinter agg when importing scanpy → … → matplotlib; # noqa; # -- General configuration ------------------------------------------------; # Warn about broken links. This is here for a reason: Do not change.; # Nicer param docs; # https://github.com/executablebooks/MyST-Parser/issues/262; # General information; # Bumping the version updates all docs, so don't do that; # Bibliography settings; # default settings; # needs to be after napoleon; # needs to be before scanpydoc.rtd_github_links; # needs to be before sphinx.ext.linkcode; # Generate the API documentation when building; # autodoc_default_flags = ['members']; # having a separate entry generally helps readability; # function_images; # -- Options for HTML output ----------------------------------------------; # The theme is sphinx-book-theme, with patches for readthedocs-sphinx-search; """"""App setup hook.""""""; # -- Options for other output formats ------------------------------------------; # -- Suppress link warnings ----------------------------------------------------; # Since numpy 2, numpy.bool is the canonical dtype; # Technical issues; # documented as “attribute”; # Will probably be documented; # Currently undocumented; # https://github.com/mwaskom/seaborn/issues/1810; # Won’t be documented; # Will work once scipy 1.8 is released; # Options for plot examples; # Project root; # extlinks config",MatchSource.CODE_COMMENT,docs/conf.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/docs/conf.py
Modifiability,config,configuration,"# noqa; # Don’t use tkinter agg when importing scanpy → … → matplotlib; # noqa; # -- General configuration ------------------------------------------------; # Warn about broken links. This is here for a reason: Do not change.; # Nicer param docs; # https://github.com/executablebooks/MyST-Parser/issues/262; # General information; # Bumping the version updates all docs, so don't do that; # Bibliography settings; # default settings; # needs to be after napoleon; # needs to be before scanpydoc.rtd_github_links; # needs to be before sphinx.ext.linkcode; # Generate the API documentation when building; # autodoc_default_flags = ['members']; # having a separate entry generally helps readability; # function_images; # -- Options for HTML output ----------------------------------------------; # The theme is sphinx-book-theme, with patches for readthedocs-sphinx-search; """"""App setup hook.""""""; # -- Options for other output formats ------------------------------------------; # -- Suppress link warnings ----------------------------------------------------; # Since numpy 2, numpy.bool is the canonical dtype; # Technical issues; # documented as “attribute”; # Will probably be documented; # Currently undocumented; # https://github.com/mwaskom/seaborn/issues/1810; # Won’t be documented; # Will work once scipy 1.8 is released; # Options for plot examples; # Project root; # extlinks config",MatchSource.CODE_COMMENT,docs/conf.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/docs/conf.py
Usability,undo,undocumented,"# noqa; # Don’t use tkinter agg when importing scanpy → … → matplotlib; # noqa; # -- General configuration ------------------------------------------------; # Warn about broken links. This is here for a reason: Do not change.; # Nicer param docs; # https://github.com/executablebooks/MyST-Parser/issues/262; # General information; # Bumping the version updates all docs, so don't do that; # Bibliography settings; # default settings; # needs to be after napoleon; # needs to be before scanpydoc.rtd_github_links; # needs to be before sphinx.ext.linkcode; # Generate the API documentation when building; # autodoc_default_flags = ['members']; # having a separate entry generally helps readability; # function_images; # -- Options for HTML output ----------------------------------------------; # The theme is sphinx-book-theme, with patches for readthedocs-sphinx-search; """"""App setup hook.""""""; # -- Options for other output formats ------------------------------------------; # -- Suppress link warnings ----------------------------------------------------; # Since numpy 2, numpy.bool is the canonical dtype; # Technical issues; # documented as “attribute”; # Will probably be documented; # Currently undocumented; # https://github.com/mwaskom/seaborn/issues/1810; # Won’t be documented; # Will work once scipy 1.8 is released; # Options for plot examples; # Project root; # extlinks config",MatchSource.CODE_COMMENT,docs/conf.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/docs/conf.py
Integrability,inject,inject,"""""""Extension to inject ``html_theme_options[""repository_branch""]``.""""""; # https://github.com/DisnakeDev/disnake/blob/7853da70b13fcd2978c39c0b7efa59b34d298186/docs/conf.py#L192; """"""Current git reference. Uses branch/tag name if found, otherwise uses commit hash""""""; # (if no name found or relative ref, use commit hash instead)",MatchSource.CODE_COMMENT,docs/extensions/git_ref.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/docs/extensions/git_ref.py
Security,inject,inject,"""""""Extension to inject ``html_theme_options[""repository_branch""]``.""""""; # https://github.com/DisnakeDev/disnake/blob/7853da70b13fcd2978c39c0b7efa59b34d298186/docs/conf.py#L192; """"""Current git reference. Uses branch/tag name if found, otherwise uses commit hash""""""; # (if no name found or relative ref, use commit hash instead)",MatchSource.CODE_COMMENT,docs/extensions/git_ref.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/docs/extensions/git_ref.py
Testability,test,tests,"# https://jinja.palletsprojects.com/en/3.0.x/api/#custom-tests",MatchSource.CODE_COMMENT,docs/extensions/has_attr_test.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/docs/extensions/has_attr_test.py
Deployability,patch,patch,"""""""Extension to patch https://github.com/executablebooks/MyST-NB/pull/599.""""""; # TODO once MyST-NB 1.1.1/1.2.0 is out, this can be removed.",MatchSource.CODE_COMMENT,docs/extensions/patch_myst_nb.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/docs/extensions/patch_myst_nb.py
Integrability,depend,dependency,"""""""Logging and Profiling""""""; # This is currently the only documented API; # strip microseconds; # anndata actually shouldn't, but as long as it's in development; # this is not the same as the requirements!; """"""\; Versions that might influence the numerical results.; Matplotlib and Seaborn are excluded from this. Parameters; ----------; file; Optional path for dependency output.; """"""; """"""\; Print versions of imported packages, OS, and jupyter environment. For more options (including rich output) use `session_info.show` directly. Parameters; ----------; file; Optional path for output.; """"""; # Special module present if test coverage being calculated; # https://gitlab.com/joelostblom/session_info/-/issues/10; """"""\; Useful for starting a notebook so you see when you started working. Parameters; ----------; file; Optional path for output.; """"""; """"""\; Log message with specific level and return current time. Parameters; ----------; msg; Message to display.; time; A time in the past. If this is passed, the time difference from then; to now is appended to `msg` as ` (HH:MM:SS)`.; If `msg` contains `{time_passed}`, the time difference is instead; inserted at that position.; deep; If the current verbosity is higher than the log function’s level,; this gets displayed as well; extra; Additional values you can specify in `msg` like `{time_passed}`.; """"""",MatchSource.CODE_COMMENT,src/scanpy/logging.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/logging.py
Testability,test,test,"""""""Logging and Profiling""""""; # This is currently the only documented API; # strip microseconds; # anndata actually shouldn't, but as long as it's in development; # this is not the same as the requirements!; """"""\; Versions that might influence the numerical results.; Matplotlib and Seaborn are excluded from this. Parameters; ----------; file; Optional path for dependency output.; """"""; """"""\; Print versions of imported packages, OS, and jupyter environment. For more options (including rich output) use `session_info.show` directly. Parameters; ----------; file; Optional path for output.; """"""; # Special module present if test coverage being calculated; # https://gitlab.com/joelostblom/session_info/-/issues/10; """"""\; Useful for starting a notebook so you see when you started working. Parameters; ----------; file; Optional path for output.; """"""; """"""\; Log message with specific level and return current time. Parameters; ----------; msg; Message to display.; time; A time in the past. If this is passed, the time difference from then; to now is appended to `msg` as ` (HH:MM:SS)`.; If `msg` contains `{time_passed}`, the time difference is instead; inserted at that position.; deep; If the current verbosity is higher than the log function’s level,; this gets displayed as well; extra; Additional values you can specify in `msg` like `{time_passed}`.; """"""",MatchSource.CODE_COMMENT,src/scanpy/logging.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/logging.py
Availability,down,downloaded,"The SOFT format is documented here; https://www.ncbi.nlm.nih.gov/geo/info/soft.html. Notes; -----; The function is based on a script by Kerby Shedden.; https://dept.stat.lsa.umich.edu/~kshedden/Python-Workshop/gene_expression_comparison.html; """"""; # The header part of the file contains information about the; # samples. Read that information first.; # Next line is the column headers (sample id's); # The column indices that contain gene expression data; # Restrict the column headers to those that we keep; # Get a list of sample labels; # Read the gene expression data as a list of lists, also get the gene; # identifiers; # This is what signals the end of the gene expression data; # section in the file; # Extract the values that correspond to gene expression measures; # and convert the strings to numbers; # Convert the Python list of lists to a Numpy array and transpose to match; # the Scanpy convention of storing samples in rows and variables in colums.; # -------------------------------------------------------------------------------; # Type conversion; # -------------------------------------------------------------------------------; """"""Check whether string is float. See also; --------; https://stackoverflow.com/questions/736043/checking-if-a-string-can-be-converted-to-float-in-python; """"""; """"""Check whether string is integer.""""""; """"""Check whether string is boolean.""""""; """"""Convert string to int, float or bool.""""""; # -------------------------------------------------------------------------------; # Helper functions for reading and writing; # -------------------------------------------------------------------------------; """"""Get files used by processes with name scanpy.""""""; # This catches a race condition where a process ends; # before we can examine its files; # noqa: F401; # Make sure file doesn’t exist half-downloaded; """"""Check whether the file is present, otherwise download.""""""; """"""Check whether the argument is a filename.""""""; # cases for gzipped/bzipped text files",MatchSource.CODE_COMMENT,src/scanpy/readwrite.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/readwrite.py
Deployability,pipeline,pipelines,"`; filtered barcodes if present in the matrix; :attr:`~anndata.AnnData.var`; Any additional metadata present in /matrix/features is read in.; """"""; """"""; Read hdf5 file from Cell Ranger v2 or earlier versions.; """"""; # AnnData works with csr matrices; # 10x stores the transposed data, so we do the transposition right away; # the csc matrix is automatically the transposed csr matrix; # as scanpy expects it, so, no need for a further transpostion; """"""; Read hdf5 file from Cell Ranger v3 or later versions.; """"""; # Read metadata specific to a feature-barcode matrix; # Read metadata specific to a probe-barcode matrix; """"""\; Read 10x-Genomics-formatted visum dataset. In addition to reading regular 10x output,; this looks for the `spatial` folder and loads images,; coordinates and scale factors.; Based on the `Space Ranger output docs`_. See :func:`~scanpy.pl.spatial` for a compatible plotting function. .. _Space Ranger output docs: https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. Parameters; ----------; path; Path to directory for visium datafiles.; genome; Filter expression to genes within this genome.; count_file; Which file in the passed directory to use as the count file. Typically would be one of:; 'filtered_feature_bc_matrix.h5' or 'raw_feature_bc_matrix.h5'.; library_id; Identifier for the visium library. Can be modified when concatenating multiple adata objects.; source_image_path; Path to the high-resolution tissue image. Path will be included in; `.uns[""spatial""][library_id][""metadata""][""source_image_path""]`. Returns; -------; Annotated data matrix, where observations/cells are named by their; barcode and variables/genes by gene name. Stores the following information:. :attr:`~anndata.AnnData.X`; The data matrix is stored; :attr:`~anndata.AnnData.obs_names`; Cell names; :attr:`~anndata.AnnData.var_names`; Gene names for a feature barcode matrix, probe names for a probe bc matrix; :attr:`~anndata.AnnData.var`\\ `['ge",MatchSource.CODE_COMMENT,src/scanpy/readwrite.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/readwrite.py
Modifiability,variab,variables,"t column are automatically; assumed to be row names.; backup_url; Retrieve the file from an URL if not present on disk.; cache; If `False`, read from source, if `True`, read from fast 'h5ad' cache.; cache_compression; See the h5py :ref:`dataset_compression`.; (Default: `settings.cache_compression`); kwargs; Parameters passed to :func:`~anndata.read_loom`. Returns; -------; An :class:`~anndata.AnnData` object; """"""; # allow passing strings; # generate filename and read to dict; """"""\; Read 10x-Genomics-formatted hdf5 file. Parameters; ----------; filename; Path to a 10x hdf5 file.; genome; Filter expression to genes within this genome. For legacy 10x h5; files, this must be provided if the data contains more than one genome.; gex_only; Only keep 'Gene Expression' data and ignore other feature types,; e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'; backup_url; Retrieve the file from an URL if not present on disk. Returns; -------; Annotated data matrix, where observations/cells are named by their; barcode and variables/genes by gene name. Stores the following information:. :attr:`~anndata.AnnData.X`; The data matrix is stored; :attr:`~anndata.AnnData.obs_names`; Cell names; :attr:`~anndata.AnnData.var_names`; Gene names for a feature barcode matrix, probe names for a probe bc matrix; :attr:`~anndata.AnnData.var`\\ `['gene_ids']`; Gene IDs; :attr:`~anndata.AnnData.var`\\ `['feature_types']`; Feature types; :attr:`~anndata.AnnData.obs`\\ `[filtered_barcodes]`; filtered barcodes if present in the matrix; :attr:`~anndata.AnnData.var`; Any additional metadata present in /matrix/features is read in.; """"""; """"""; Read hdf5 file from Cell Ranger v2 or earlier versions.; """"""; # AnnData works with csr matrices; # 10x stores the transposed data, so we do the transposition right away; # the csc matrix is automatically the transposed csr matrix; # as scanpy expects it, so, no need for a further transpostion; """"""; Read hdf5 file from Cell Ranger v3 or later versions.; """"""",MatchSource.CODE_COMMENT,src/scanpy/readwrite.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/readwrite.py
Performance,cache,cache,"""""""Reading and Writing""""""; # .gz and .bz2 suffixes are also allowed for text formats; # these four are all equivalent; """"""Available file formats for reading data. """"""; # --------------------------------------------------------------------------------; # Reading and Writing data files and AnnData objects; # --------------------------------------------------------------------------------; """"""\; Read file and return :class:`~anndata.AnnData` object. To speed up reading, consider passing ``cache=True``, which creates an hdf5; cache file. Parameters; ----------; filename; If the filename has no file extension, it is interpreted as a key for; generating a filename via ``sc.settings.writedir / (filename +; sc.settings.file_format_data)``. This is the same behavior as in; ``sc.read(filename, ...)``.; backed; If ``'r'``, load :class:`~anndata.AnnData` in ``backed`` mode instead; of fully loading it into memory (`memory` mode). If you want to modify; backed attributes of the AnnData object, you need to choose ``'r+'``.; sheet; Name of sheet/table in hdf5 or Excel file.; ext; Extension that indicates the file type. If ``None``, uses extension of; filename.; delimiter; Delimiter that separates data within text file. If ``None``, will split at; arbitrary number of white spaces, which is different from enforcing; splitting at any single white space ``' '``.; first_column_names; Assume the first column stores row names. This is only necessary if; these are not strings: strings in the first column are automatically; assumed to be row names.; backup_url; Retrieve the file from an URL if not present on disk.; cache; If `False`, read from source, if `True`, read from fast 'h5ad' cache.; cache_compression; See the h5py :ref:`dataset_compression`.; (Default: `settings.cache_compression`); kwargs; Parameters passed to :func:`~anndata.read_loom`. Returns; -------; An :class:`~anndata.AnnData` object; """"""; # allow passing strings; # generate filename and read to dict; """"""\; Read 10x-Genomic",MatchSource.CODE_COMMENT,src/scanpy/readwrite.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/readwrite.py
Usability,usab,usable,"a.AnnData.obs_names`; Cell names; :attr:`~anndata.AnnData.var_names`; Gene names for a feature barcode matrix, probe names for a probe bc matrix; :attr:`~anndata.AnnData.var`\\ `['gene_ids']`; Gene IDs; :attr:`~anndata.AnnData.var`\\ `['feature_types']`; Feature types; :attr:`~anndata.AnnData.obs`\\ `[filtered_barcodes]`; filtered barcodes if present in the matrix; :attr:`~anndata.AnnData.var`; Any additional metadata present in /matrix/features is read in.; :attr:`~anndata.AnnData.uns`\\ `['spatial']`; Dict of spaceranger output files with 'library_id' as key; :attr:`~anndata.AnnData.uns`\\ `['spatial'][library_id]['images']`; Dict of images (`'hires'` and `'lowres'`); :attr:`~anndata.AnnData.uns`\\ `['spatial'][library_id]['scalefactors']`; Scale factors for the spots; :attr:`~anndata.AnnData.uns`\\ `['spatial'][library_id]['metadata']`; Files metadata: 'chemistry_description', 'software_version', 'source_image_path'; :attr:`~anndata.AnnData.obsm`\\ `['spatial']`; Spatial spot coordinates, usable as `basis` by :func:`~scanpy.pl.embedding`.; """"""; # check if files exists, continue if images are missing; # read json scalefactors; # read coordinates; # put image path in uns; # get an absolute path; """"""\; Read 10x-Genomics-formatted mtx directory. Parameters; ----------; path; Path to directory for `.mtx` and `.tsv` files,; e.g. './filtered_gene_bc_matrices/hg19/'.; var_names; The variables index.; make_unique; Whether to make the variables index unique by appending '-1',; '-2' etc. or not.; cache; If `False`, read from source, if `True`, read from fast 'h5ad' cache.; cache_compression; See the h5py :ref:`dataset_compression`.; (Default: `settings.cache_compression`); gex_only; Only keep 'Gene Expression' data and ignore other feature types,; e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'; prefix; Any prefix before `matrix.mtx`, `genes.tsv` and `barcodes.tsv`. For instance,; if the files are named `patientA_matrix.mtx`, `patientA_genes.tsv` and; `patientA_",MatchSource.CODE_COMMENT,src/scanpy/readwrite.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/readwrite.py
Availability,error,error,"# Collected from the print_* functions in matplotlib.backends; # Python 3.7+ ensures iteration order; """"""Logging verbosity levels.""""""; # getLevelName(str) returns the int level…; """"""\; Temporarily override verbosity; """"""; # backwards compat; """"""\; Config manager for scanpy.; """"""; """"""Default number of principal components to use.""""""; # logging; # level will be replaced; # rest; """"""bool: See set_figure_params.""""""; """"""Set to true if you want to include pngs in svgs and pdfs.""""""; """"""Print warning when saving a figure with low resolution.""""""; """"""Time when the settings module is first imported.""""""; """"""Variable for timing program parts.""""""; """"""Stores the previous memory usage.""""""; """"""; Verbosity level (default `warning`). Level 0: only show 'error' messages.; Level 1: also show 'warning' messages.; Level 2: also show 'info' messages.; Level 3: also show 'hint' messages.; Level 4: also show very detailed progress for 'debug'ging.; """"""; """"""Global suffix that is appended to figure filenames.""""""; """"""File format for saving AnnData objects. Allowed are 'txt', 'csv' (comma separated value file) for exporting and 'h5ad'; (hdf5) for lossless saving.; """"""; """"""File format for saving figures. For example 'png', 'pdf' or 'svg'. Many other formats work as well (see; `matplotlib.pyplot.savefig`).; """"""; """"""\; Automatically save figures in :attr:`~scanpy._settings.ScanpyConfig.figdir` (default `False`). Do not show plots/figures interactively.; """"""; """"""\; Automatically show figures if `autosave == False` (default `True`). There is no need to call the matplotlib pl.show() in this case.; """"""; """"""\; Directory where the function scanpy.write writes to by default.; """"""; """"""\; Directory for cache files (default `'./cache/'`).; """"""; """"""\; Directory for example :mod:`~scanpy.datasets` (default `'./data/'`).; """"""; """"""\; Directory for saving figures (default `'./figures/'`).; """"""; """"""\; Compression for `sc.read(..., cache=True)` (default `'lzf'`). May be `'lzf'`, `'gzip'`, or `None`.; """"""; """"""\; Maxi",MatchSource.CODE_COMMENT,src/scanpy/_settings.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_settings.py
Integrability,message,messages,"# Collected from the print_* functions in matplotlib.backends; # Python 3.7+ ensures iteration order; """"""Logging verbosity levels.""""""; # getLevelName(str) returns the int level…; """"""\; Temporarily override verbosity; """"""; # backwards compat; """"""\; Config manager for scanpy.; """"""; """"""Default number of principal components to use.""""""; # logging; # level will be replaced; # rest; """"""bool: See set_figure_params.""""""; """"""Set to true if you want to include pngs in svgs and pdfs.""""""; """"""Print warning when saving a figure with low resolution.""""""; """"""Time when the settings module is first imported.""""""; """"""Variable for timing program parts.""""""; """"""Stores the previous memory usage.""""""; """"""; Verbosity level (default `warning`). Level 0: only show 'error' messages.; Level 1: also show 'warning' messages.; Level 2: also show 'info' messages.; Level 3: also show 'hint' messages.; Level 4: also show very detailed progress for 'debug'ging.; """"""; """"""Global suffix that is appended to figure filenames.""""""; """"""File format for saving AnnData objects. Allowed are 'txt', 'csv' (comma separated value file) for exporting and 'h5ad'; (hdf5) for lossless saving.; """"""; """"""File format for saving figures. For example 'png', 'pdf' or 'svg'. Many other formats work as well (see; `matplotlib.pyplot.savefig`).; """"""; """"""\; Automatically save figures in :attr:`~scanpy._settings.ScanpyConfig.figdir` (default `False`). Do not show plots/figures interactively.; """"""; """"""\; Automatically show figures if `autosave == False` (default `True`). There is no need to call the matplotlib pl.show() in this case.; """"""; """"""\; Directory where the function scanpy.write writes to by default.; """"""; """"""\; Directory for cache files (default `'./cache/'`).; """"""; """"""\; Directory for example :mod:`~scanpy.datasets` (default `'./data/'`).; """"""; """"""\; Directory for saving figures (default `'./figures/'`).; """"""; """"""\; Compression for `sc.read(..., cache=True)` (default `'lzf'`). May be `'lzf'`, `'gzip'`, or `None`.; """"""; """"""\; Maxi",MatchSource.CODE_COMMENT,src/scanpy/_settings.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_settings.py
Performance,cache,cache,"Verbosity level (default `warning`). Level 0: only show 'error' messages.; Level 1: also show 'warning' messages.; Level 2: also show 'info' messages.; Level 3: also show 'hint' messages.; Level 4: also show very detailed progress for 'debug'ging.; """"""; """"""Global suffix that is appended to figure filenames.""""""; """"""File format for saving AnnData objects. Allowed are 'txt', 'csv' (comma separated value file) for exporting and 'h5ad'; (hdf5) for lossless saving.; """"""; """"""File format for saving figures. For example 'png', 'pdf' or 'svg'. Many other formats work as well (see; `matplotlib.pyplot.savefig`).; """"""; """"""\; Automatically save figures in :attr:`~scanpy._settings.ScanpyConfig.figdir` (default `False`). Do not show plots/figures interactively.; """"""; """"""\; Automatically show figures if `autosave == False` (default `True`). There is no need to call the matplotlib pl.show() in this case.; """"""; """"""\; Directory where the function scanpy.write writes to by default.; """"""; """"""\; Directory for cache files (default `'./cache/'`).; """"""; """"""\; Directory for example :mod:`~scanpy.datasets` (default `'./data/'`).; """"""; """"""\; Directory for saving figures (default `'./figures/'`).; """"""; """"""\; Compression for `sc.read(..., cache=True)` (default `'lzf'`). May be `'lzf'`, `'gzip'`, or `None`.; """"""; """"""\; Maximum memory usage in Gigabyte. Is currently not well respected…; """"""; """"""\; Default number of jobs/ CPUs to use for parallel computing. Set to `-1` in order to use all available cores.; Not all algorithms support special behavior for numbers < `-1`,; so make sure to leave this setting as >= `-1`.; """"""; """"""\; The file path `logfile` was set to.; """"""; # set via “file object” branch of logfile.setter; """"""\; The open file to write logs to. Set it to a :class:`~pathlib.Path` or :class:`str` to open a new one.; The default `None` corresponds to :obj:`sys.stdout` in jupyter notebooks; and to :obj:`sys.stderr` otherwise. For backwards compatibility, setting it to `''` behaves like setting",MatchSource.CODE_COMMENT,src/scanpy/_settings.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_settings.py
Testability,log,logging,"# Collected from the print_* functions in matplotlib.backends; # Python 3.7+ ensures iteration order; """"""Logging verbosity levels.""""""; # getLevelName(str) returns the int level…; """"""\; Temporarily override verbosity; """"""; # backwards compat; """"""\; Config manager for scanpy.; """"""; """"""Default number of principal components to use.""""""; # logging; # level will be replaced; # rest; """"""bool: See set_figure_params.""""""; """"""Set to true if you want to include pngs in svgs and pdfs.""""""; """"""Print warning when saving a figure with low resolution.""""""; """"""Time when the settings module is first imported.""""""; """"""Variable for timing program parts.""""""; """"""Stores the previous memory usage.""""""; """"""; Verbosity level (default `warning`). Level 0: only show 'error' messages.; Level 1: also show 'warning' messages.; Level 2: also show 'info' messages.; Level 3: also show 'hint' messages.; Level 4: also show very detailed progress for 'debug'ging.; """"""; """"""Global suffix that is appended to figure filenames.""""""; """"""File format for saving AnnData objects. Allowed are 'txt', 'csv' (comma separated value file) for exporting and 'h5ad'; (hdf5) for lossless saving.; """"""; """"""File format for saving figures. For example 'png', 'pdf' or 'svg'. Many other formats work as well (see; `matplotlib.pyplot.savefig`).; """"""; """"""\; Automatically save figures in :attr:`~scanpy._settings.ScanpyConfig.figdir` (default `False`). Do not show plots/figures interactively.; """"""; """"""\; Automatically show figures if `autosave == False` (default `True`). There is no need to call the matplotlib pl.show() in this case.; """"""; """"""\; Directory where the function scanpy.write writes to by default.; """"""; """"""\; Directory for cache files (default `'./cache/'`).; """"""; """"""\; Directory for example :mod:`~scanpy.datasets` (default `'./data/'`).; """"""; """"""\; Directory for saving figures (default `'./figures/'`).; """"""; """"""\; Compression for `sc.read(..., cache=True)` (default `'lzf'`). May be `'lzf'`, `'gzip'`, or `None`.; """"""; """"""\; Maxi",MatchSource.CODE_COMMENT,src/scanpy/_settings.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_settings.py
Availability,avail,available,"e genes. The saved file contains the annotation of cell types (key: `'bulk_labels'`),; UMAP coordinates, louvain clustering and gene rankings based on the; `bulk_labels`. .. [#norm] Back when the dataset was created, :func:`~scanpy.pp.normalize_per_cell` was used instead.; .. _PBMC 68k dataset: https://www.10xgenomics.com/datasets/fresh-68-k-pbm-cs-donor-a-1-standard-1-1-0. Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.pbmc68k_reduced(); AnnData object with n_obs × n_vars = 700 × 765; obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'; """"""; """"""\; 3k PBMCs from 10x Genomics. The data consists in 3k PBMCs from a Healthy Donor and is freely available; from 10x Genomics (file_ from this webpage_). The exact same data is also used in Seurat’s `basic clustering tutorial`_. .. _file: https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz; .. _webpage: https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k; .. _basic clustering tutorial: https://satijalab.org/seurat/articles/pbmc3k_tutorial.html. .. note::; This downloads 5.9 MB of data upon the first call of the function and stores it in; :attr:`~scanpy._settings.ScanpyConfig.datasetdir`\\ `/pbmc3k_raw.h5ad`. The following code was run to produce the file. .. code:: python. adata = sc.read_10x_mtx(; # the directory with the `.mtx` file; './data/filtered_gene_bc_matrices/hg19/',; # use gene symbols for the variable names (variables-axis index); var_names='gene_symbols',; # write a cache file for faster subsequent reading; cache=True,; ). adata.var_names_make_unique() # this is unnecessary if using 'gene_",MatchSource.CODE_COMMENT,src/scanpy/datasets/_datasets.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/datasets/_datasets.py
Deployability,toggle,toggleswitch,"The data has been sent out by Email from the Amit Lab. An R version for; loading the data can be found `here; <https://github.com/theislab/scAnalysisTutorial>`_. Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.paul15(); AnnData object with n_obs × n_vars = 2730 × 3451; obs: 'paul15_clusters'; uns: 'iroot'; """"""; # Coercing to float32 for backwards compatibility; # each row has to correspond to a observation, therefore transpose; # names reflecting the cell type identifications from the paper; # make string annotations categorical (optional); # just keep the first of the two equivalent names per gene; # remove 10 corrupted gene names; # restrict data array to the 3461 informative genes; # usually we'd set the root cell to an arbitrary cell in the MEP cluster; # adata.uns['iroot'] = np.flatnonzero(adata.obs['paul15_clusters'] == '7MEP')[0]; # here, set the root cell as in Haghverdi et al. (2016); # note that other than in Matlab/R, counting starts at 0; """"""\; Simulated toggleswitch. Data obtained simulating a simple toggleswitch :cite:p:`Gardner2000`. Simulate via :func:`~scanpy.tl.sim`. Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.toggleswitch(); UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; utils.warn_names_duplicates(""obs""); AnnData object with n_obs × n_vars = 200 × 2; uns: 'iroot'; """"""; """"""\; Subsampled and processed 68k PBMCs. `PBMC 68k dataset`_ from 10x Genomics. The original PBMC 68k dataset was preprocessed with steps including; :func:`~scanpy.pp.normalize_total`\\ [#norm]_ and :func:`~scanpy.pp.scale`.; It was saved keeping only 724 cells and 221 highly variable genes. The saved file contains the annotation of cell types (key: `'bulk_labels'`),; UMAP coordinates, louvain clustering and gene rankings based on the; `bulk_labels`. .. [#norm] Back when the dataset was created, :func:`~scanpy.pp.norma",MatchSource.CODE_COMMENT,src/scanpy/datasets/_datasets.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/datasets/_datasets.py
Modifiability,variab,variable,"mative genes; # usually we'd set the root cell to an arbitrary cell in the MEP cluster; # adata.uns['iroot'] = np.flatnonzero(adata.obs['paul15_clusters'] == '7MEP')[0]; # here, set the root cell as in Haghverdi et al. (2016); # note that other than in Matlab/R, counting starts at 0; """"""\; Simulated toggleswitch. Data obtained simulating a simple toggleswitch :cite:p:`Gardner2000`. Simulate via :func:`~scanpy.tl.sim`. Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.toggleswitch(); UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; utils.warn_names_duplicates(""obs""); AnnData object with n_obs × n_vars = 200 × 2; uns: 'iroot'; """"""; """"""\; Subsampled and processed 68k PBMCs. `PBMC 68k dataset`_ from 10x Genomics. The original PBMC 68k dataset was preprocessed with steps including; :func:`~scanpy.pp.normalize_total`\\ [#norm]_ and :func:`~scanpy.pp.scale`.; It was saved keeping only 724 cells and 221 highly variable genes. The saved file contains the annotation of cell types (key: `'bulk_labels'`),; UMAP coordinates, louvain clustering and gene rankings based on the; `bulk_labels`. .. [#norm] Back when the dataset was created, :func:`~scanpy.pp.normalize_per_cell` was used instead.; .. _PBMC 68k dataset: https://www.10xgenomics.com/datasets/fresh-68-k-pbm-cs-donor-a-1-standard-1-1-0. Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.pbmc68k_reduced(); AnnData object with n_obs × n_vars = 700 × 765; obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'; """"""; """"""\; 3k PBMCs from 10x Genomics. The data consists in 3k PBMCs from",MatchSource.CODE_COMMENT,src/scanpy/datasets/_datasets.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/datasets/_datasets.py
Performance,load,loading,"anndata.AnnData.obs`\\ `[""exp_groups""]` contains the stages derived by; flow sorting and GFP marker status:; “primitive streak” (`PS`), “neural plate” (`NP`), “head fold (`HF`),; “four somite” blood/GFP⁺ (4SG), and “four somite” endothelial/GFP¯ (`4SFG`). Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.moignard15(); AnnData object with n_obs × n_vars = 3934 × 42; obs: 'exp_groups'; uns: 'iroot', 'exp_groups_colors'; """"""; # filter out 4 genes as in Haghverdi et al. (2016); # retain non-removed genes; # choose root cell for DPT analysis as in Haghverdi et al. (2016); # note that in Matlab/R, counting starts at 1; # annotate with Moignard et al. (2015) experimental cell groups; # annotate each observation/cell; # fix the order and colors of names in ""groups""; """"""\; Development of Myeloid Progenitors :cite:p:`Paul2015`. Non-logarithmized raw data. The data has been sent out by Email from the Amit Lab. An R version for; loading the data can be found `here; <https://github.com/theislab/scAnalysisTutorial>`_. Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.paul15(); AnnData object with n_obs × n_vars = 2730 × 3451; obs: 'paul15_clusters'; uns: 'iroot'; """"""; # Coercing to float32 for backwards compatibility; # each row has to correspond to a observation, therefore transpose; # names reflecting the cell type identifications from the paper; # make string annotations categorical (optional); # just keep the first of the two equivalent names per gene; # remove 10 corrupted gene names; # restrict data array to the 3461 informative genes; # usually we'd set the root cell to an arbitrary cell in the MEP cluster; # adata.uns['iroot'] = np.flatnonzero(adata.obs['paul15_clusters'] == '7MEP')[0]; # here, set the root cell as in Haghverdi et al. (2016); # note that other than in Matlab/R, counting starts at 0; """"""\; Simulated toggleswitch. Data obtained simulating a simple toggles",MatchSource.CODE_COMMENT,src/scanpy/datasets/_datasets.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/datasets/_datasets.py
Testability,log,logarithmized,"r:`~anndata.AnnData.X` contains the normalized dCt values from supp. table 7 of the publication. :attr:`~anndata.AnnData.obs`\\ `[""exp_groups""]` contains the stages derived by; flow sorting and GFP marker status:; “primitive streak” (`PS`), “neural plate” (`NP`), “head fold (`HF`),; “four somite” blood/GFP⁺ (4SG), and “four somite” endothelial/GFP¯ (`4SFG`). Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.moignard15(); AnnData object with n_obs × n_vars = 3934 × 42; obs: 'exp_groups'; uns: 'iroot', 'exp_groups_colors'; """"""; # filter out 4 genes as in Haghverdi et al. (2016); # retain non-removed genes; # choose root cell for DPT analysis as in Haghverdi et al. (2016); # note that in Matlab/R, counting starts at 1; # annotate with Moignard et al. (2015) experimental cell groups; # annotate each observation/cell; # fix the order and colors of names in ""groups""; """"""\; Development of Myeloid Progenitors :cite:p:`Paul2015`. Non-logarithmized raw data. The data has been sent out by Email from the Amit Lab. An R version for; loading the data can be found `here; <https://github.com/theislab/scAnalysisTutorial>`_. Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.paul15(); AnnData object with n_obs × n_vars = 2730 × 3451; obs: 'paul15_clusters'; uns: 'iroot'; """"""; # Coercing to float32 for backwards compatibility; # each row has to correspond to a observation, therefore transpose; # names reflecting the cell type identifications from the paper; # make string annotations categorical (optional); # just keep the first of the two equivalent names per gene; # remove 10 corrupted gene names; # restrict data array to the 3461 informative genes; # usually we'd set the root cell to an arbitrary cell in the MEP cluster; # adata.uns['iroot'] = np.flatnonzero(adata.obs['paul15_clusters'] == '7MEP')[0]; # here, set the root cell as in Haghverdi et al. (2016); # note that other than i",MatchSource.CODE_COMMENT,src/scanpy/datasets/_datasets.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/datasets/_datasets.py
Usability,simpl,simple,"e data can be found `here; <https://github.com/theislab/scAnalysisTutorial>`_. Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.paul15(); AnnData object with n_obs × n_vars = 2730 × 3451; obs: 'paul15_clusters'; uns: 'iroot'; """"""; # Coercing to float32 for backwards compatibility; # each row has to correspond to a observation, therefore transpose; # names reflecting the cell type identifications from the paper; # make string annotations categorical (optional); # just keep the first of the two equivalent names per gene; # remove 10 corrupted gene names; # restrict data array to the 3461 informative genes; # usually we'd set the root cell to an arbitrary cell in the MEP cluster; # adata.uns['iroot'] = np.flatnonzero(adata.obs['paul15_clusters'] == '7MEP')[0]; # here, set the root cell as in Haghverdi et al. (2016); # note that other than in Matlab/R, counting starts at 0; """"""\; Simulated toggleswitch. Data obtained simulating a simple toggleswitch :cite:p:`Gardner2000`. Simulate via :func:`~scanpy.tl.sim`. Returns; -------; Annotated data matrix. Examples; --------; >>> import scanpy as sc; >>> sc.datasets.toggleswitch(); UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; utils.warn_names_duplicates(""obs""); AnnData object with n_obs × n_vars = 200 × 2; uns: 'iroot'; """"""; """"""\; Subsampled and processed 68k PBMCs. `PBMC 68k dataset`_ from 10x Genomics. The original PBMC 68k dataset was preprocessed with steps including; :func:`~scanpy.pp.normalize_total`\\ [#norm]_ and :func:`~scanpy.pp.scale`.; It was saved keeping only 724 cells and 221 highly variable genes. The saved file contains the annotation of cell types (key: `'bulk_labels'`),; UMAP coordinates, louvain clustering and gene rankings based on the; `bulk_labels`. .. [#norm] Back when the dataset was created, :func:`~scanpy.pp.normalize_per_cell` was used instead.; .. _PBMC 68k dataset: https://www.10xgenomics.com",MatchSource.CODE_COMMENT,src/scanpy/datasets/_datasets.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/datasets/_datasets.py
Availability,down,downloaded,"# Note that data is downloaded from gxa/sc/experiment, not experiments; # Check if server up/ dataset exists; # Report failed url; # TODO: Check what other value could be; """"""\; Load a dataset from the EBI Single Cell Expression Atlas. The atlas_ can be browsed online to find the ``accession`` you want.; Downloaded datasets are saved in the directory specified by; :attr:`~scanpy._settings.ScanpyConfig.datasetdir`. .. _atlas: https://www.ebi.ac.uk/gxa/sc/experiments. Params; ------; accession; Dataset accession. Like ``E-GEOD-98816`` or ``E-MTAB-4888``.; This can be found in the url on the datasets page, for example E-GEOD-98816_. .. _E-GEOD-98816: https://www.ebi.ac.uk/gxa/sc/experiments/E-GEOD-98816/results/tsne; filter_boring; Whether boring labels in `.obs` should be automatically removed, such as; labels with a single or :attr:`~anndata.AnnData.n_obs` distinct values. Returns; -------; Annotated data matrix. Example; -------; >>> import scanpy as sc; >>> sc.datasets.ebi_expression_atlas(""E-MTAB-4888"") # doctest: +ELLIPSIS; AnnData object with n_obs × n_vars = 2261 × 23899; obs: 'Sample Characteristic[organism]', 'Sample Characteristic Ontology Term[organism]', ..., 'Factor Value[cell type]', 'Factor Value Ontology Term[cell type]'; """"""; # Dataset couldn't be read for whatever reason; # To be kind to disk space",MatchSource.CODE_COMMENT,src/scanpy/datasets/_ebi_expression_atlas.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/datasets/_ebi_expression_atlas.py
Security,access,accession,"# Note that data is downloaded from gxa/sc/experiment, not experiments; # Check if server up/ dataset exists; # Report failed url; # TODO: Check what other value could be; """"""\; Load a dataset from the EBI Single Cell Expression Atlas. The atlas_ can be browsed online to find the ``accession`` you want.; Downloaded datasets are saved in the directory specified by; :attr:`~scanpy._settings.ScanpyConfig.datasetdir`. .. _atlas: https://www.ebi.ac.uk/gxa/sc/experiments. Params; ------; accession; Dataset accession. Like ``E-GEOD-98816`` or ``E-MTAB-4888``.; This can be found in the url on the datasets page, for example E-GEOD-98816_. .. _E-GEOD-98816: https://www.ebi.ac.uk/gxa/sc/experiments/E-GEOD-98816/results/tsne; filter_boring; Whether boring labels in `.obs` should be automatically removed, such as; labels with a single or :attr:`~anndata.AnnData.n_obs` distinct values. Returns; -------; Annotated data matrix. Example; -------; >>> import scanpy as sc; >>> sc.datasets.ebi_expression_atlas(""E-MTAB-4888"") # doctest: +ELLIPSIS; AnnData object with n_obs × n_vars = 2261 × 23899; obs: 'Sample Characteristic[organism]', 'Sample Characteristic Ontology Term[organism]', ..., 'Factor Value[cell type]', 'Factor Value Ontology Term[cell type]'; """"""; # Dataset couldn't be read for whatever reason; # To be kind to disk space",MatchSource.CODE_COMMENT,src/scanpy/datasets/_ebi_expression_atlas.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/datasets/_ebi_expression_atlas.py
Integrability,wrap,wrapped,"""""""; Filters anndata.OldFormatWarning from being thrown by the wrapped function.; """"""",MatchSource.CODE_COMMENT,src/scanpy/datasets/_utils.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/datasets/_utils.py
Deployability,update,update,"obs), sqrt(n_obs)]`, where `n_obs` is the number of cells in the dataset (default behavior).; * If any scalar `c`, residuals are clipped to the interval `[-c, c]`. Set \; `clip=np.inf` for no clipping.; """"""; """"""\; check_values; If `True`, checks if counts in selected layer are integers as expected by this; function, and return a warning if non-integers are found. Otherwise, proceed; without checking. Setting this to `False` can speed up code for large datasets.; """"""; """"""\; layer; Layer to use as input instead of `X`. If `None`, `X` is used.; """"""; """"""\; subset; Inplace subset to highly-variable genes if `True` otherwise merely indicate; highly variable genes.; """"""; """"""\; n_top_genes; Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'` or; `flavor='pearson_residuals'`.; batch_key; If specified, highly-variable genes are selected within each batch separately; and merged. This simple process avoids the selection of batch-specific genes; and acts as a lightweight batch correction method. Genes are first sorted by; how many batches they are a HVG. If `flavor='pearson_residuals'`, ties are; broken by the median rank (across batches) based on within-batch residual; variance.; chunksize; If `flavor='pearson_residuals'`, this dertermines how many genes are processed at; once while computing the residual variance. Choosing a smaller value will reduce; the required memory.; """"""; """"""\; n_comps; Number of principal components to compute in the PCA step.; random_state; Random seed for setting the initial states for the optimization in the PCA step.; kwargs_pca; Dictionary of further keyword arguments passed on to `scanpy.pp.pca()`.; """"""; """"""\; inplace; If `True`, update `adata` with results. Otherwise, return results. See below for; details of what is returned.; """"""; """"""\; copy; If `True`, the function runs on a copy of the input object and returns the; modified copy. Otherwise, the input object is modified direcly. Not compatible; with `inplace=False`.; """"""",MatchSource.CODE_COMMENT,src/scanpy/experimental/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/_docs.py
Energy Efficiency,reduce,reduce,"obs), sqrt(n_obs)]`, where `n_obs` is the number of cells in the dataset (default behavior).; * If any scalar `c`, residuals are clipped to the interval `[-c, c]`. Set \; `clip=np.inf` for no clipping.; """"""; """"""\; check_values; If `True`, checks if counts in selected layer are integers as expected by this; function, and return a warning if non-integers are found. Otherwise, proceed; without checking. Setting this to `False` can speed up code for large datasets.; """"""; """"""\; layer; Layer to use as input instead of `X`. If `None`, `X` is used.; """"""; """"""\; subset; Inplace subset to highly-variable genes if `True` otherwise merely indicate; highly variable genes.; """"""; """"""\; n_top_genes; Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'` or; `flavor='pearson_residuals'`.; batch_key; If specified, highly-variable genes are selected within each batch separately; and merged. This simple process avoids the selection of batch-specific genes; and acts as a lightweight batch correction method. Genes are first sorted by; how many batches they are a HVG. If `flavor='pearson_residuals'`, ties are; broken by the median rank (across batches) based on within-batch residual; variance.; chunksize; If `flavor='pearson_residuals'`, this dertermines how many genes are processed at; once while computing the residual variance. Choosing a smaller value will reduce; the required memory.; """"""; """"""\; n_comps; Number of principal components to compute in the PCA step.; random_state; Random seed for setting the initial states for the optimization in the PCA step.; kwargs_pca; Dictionary of further keyword arguments passed on to `scanpy.pp.pca()`.; """"""; """"""\; inplace; If `True`, update `adata` with results. Otherwise, return results. See below for; details of what is returned.; """"""; """"""\; copy; If `True`, the function runs on a copy of the input object and returns the; modified copy. Otherwise, the input object is modified direcly. Not compatible; with `inplace=False`.; """"""",MatchSource.CODE_COMMENT,src/scanpy/experimental/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/_docs.py
Modifiability,variab,variable,"ows correspond to cells and columns to genes.; """"""; """"""\; theta; The negative binomial overdispersion parameter `theta` for Pearson residuals.; Higher values correspond to less overdispersion \; (`var = mean + mean^2/theta`), and `theta=np.inf` corresponds to a Poisson model.; clip; Determines if and how residuals are clipped:. * If `None`, residuals are clipped to the interval \; `[-sqrt(n_obs), sqrt(n_obs)]`, where `n_obs` is the number of cells in the dataset (default behavior).; * If any scalar `c`, residuals are clipped to the interval `[-c, c]`. Set \; `clip=np.inf` for no clipping.; """"""; """"""\; check_values; If `True`, checks if counts in selected layer are integers as expected by this; function, and return a warning if non-integers are found. Otherwise, proceed; without checking. Setting this to `False` can speed up code for large datasets.; """"""; """"""\; layer; Layer to use as input instead of `X`. If `None`, `X` is used.; """"""; """"""\; subset; Inplace subset to highly-variable genes if `True` otherwise merely indicate; highly variable genes.; """"""; """"""\; n_top_genes; Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'` or; `flavor='pearson_residuals'`.; batch_key; If specified, highly-variable genes are selected within each batch separately; and merged. This simple process avoids the selection of batch-specific genes; and acts as a lightweight batch correction method. Genes are first sorted by; how many batches they are a HVG. If `flavor='pearson_residuals'`, ties are; broken by the median rank (across batches) based on within-batch residual; variance.; chunksize; If `flavor='pearson_residuals'`, this dertermines how many genes are processed at; once while computing the residual variance. Choosing a smaller value will reduce; the required memory.; """"""; """"""\; n_comps; Number of principal components to compute in the PCA step.; random_state; Random seed for setting the initial states for the optimization in the PCA step.; kwargs_pca; Dictionary",MatchSource.CODE_COMMENT,src/scanpy/experimental/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/_docs.py
Performance,optimiz,optimization,"obs), sqrt(n_obs)]`, where `n_obs` is the number of cells in the dataset (default behavior).; * If any scalar `c`, residuals are clipped to the interval `[-c, c]`. Set \; `clip=np.inf` for no clipping.; """"""; """"""\; check_values; If `True`, checks if counts in selected layer are integers as expected by this; function, and return a warning if non-integers are found. Otherwise, proceed; without checking. Setting this to `False` can speed up code for large datasets.; """"""; """"""\; layer; Layer to use as input instead of `X`. If `None`, `X` is used.; """"""; """"""\; subset; Inplace subset to highly-variable genes if `True` otherwise merely indicate; highly variable genes.; """"""; """"""\; n_top_genes; Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'` or; `flavor='pearson_residuals'`.; batch_key; If specified, highly-variable genes are selected within each batch separately; and merged. This simple process avoids the selection of batch-specific genes; and acts as a lightweight batch correction method. Genes are first sorted by; how many batches they are a HVG. If `flavor='pearson_residuals'`, ties are; broken by the median rank (across batches) based on within-batch residual; variance.; chunksize; If `flavor='pearson_residuals'`, this dertermines how many genes are processed at; once while computing the residual variance. Choosing a smaller value will reduce; the required memory.; """"""; """"""\; n_comps; Number of principal components to compute in the PCA step.; random_state; Random seed for setting the initial states for the optimization in the PCA step.; kwargs_pca; Dictionary of further keyword arguments passed on to `scanpy.pp.pca()`.; """"""; """"""\; inplace; If `True`, update `adata` with results. Otherwise, return results. See below for; details of what is returned.; """"""; """"""\; copy; If `True`, the function runs on a copy of the input object and returns the; modified copy. Otherwise, the input object is modified direcly. Not compatible; with `inplace=False`.; """"""",MatchSource.CODE_COMMENT,src/scanpy/experimental/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/_docs.py
Safety,avoid,avoids,"e clipped to the interval \; `[-sqrt(n_obs), sqrt(n_obs)]`, where `n_obs` is the number of cells in the dataset (default behavior).; * If any scalar `c`, residuals are clipped to the interval `[-c, c]`. Set \; `clip=np.inf` for no clipping.; """"""; """"""\; check_values; If `True`, checks if counts in selected layer are integers as expected by this; function, and return a warning if non-integers are found. Otherwise, proceed; without checking. Setting this to `False` can speed up code for large datasets.; """"""; """"""\; layer; Layer to use as input instead of `X`. If `None`, `X` is used.; """"""; """"""\; subset; Inplace subset to highly-variable genes if `True` otherwise merely indicate; highly variable genes.; """"""; """"""\; n_top_genes; Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'` or; `flavor='pearson_residuals'`.; batch_key; If specified, highly-variable genes are selected within each batch separately; and merged. This simple process avoids the selection of batch-specific genes; and acts as a lightweight batch correction method. Genes are first sorted by; how many batches they are a HVG. If `flavor='pearson_residuals'`, ties are; broken by the median rank (across batches) based on within-batch residual; variance.; chunksize; If `flavor='pearson_residuals'`, this dertermines how many genes are processed at; once while computing the residual variance. Choosing a smaller value will reduce; the required memory.; """"""; """"""\; n_comps; Number of principal components to compute in the PCA step.; random_state; Random seed for setting the initial states for the optimization in the PCA step.; kwargs_pca; Dictionary of further keyword arguments passed on to `scanpy.pp.pca()`.; """"""; """"""\; inplace; If `True`, update `adata` with results. Otherwise, return results. See below for; details of what is returned.; """"""; """"""\; copy; If `True`, the function runs on a copy of the input object and returns the; modified copy. Otherwise, the input object is modified direcly. Not ",MatchSource.CODE_COMMENT,src/scanpy/experimental/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/_docs.py
Usability,simpl,simple,"e clipped to the interval \; `[-sqrt(n_obs), sqrt(n_obs)]`, where `n_obs` is the number of cells in the dataset (default behavior).; * If any scalar `c`, residuals are clipped to the interval `[-c, c]`. Set \; `clip=np.inf` for no clipping.; """"""; """"""\; check_values; If `True`, checks if counts in selected layer are integers as expected by this; function, and return a warning if non-integers are found. Otherwise, proceed; without checking. Setting this to `False` can speed up code for large datasets.; """"""; """"""\; layer; Layer to use as input instead of `X`. If `None`, `X` is used.; """"""; """"""\; subset; Inplace subset to highly-variable genes if `True` otherwise merely indicate; highly variable genes.; """"""; """"""\; n_top_genes; Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'` or; `flavor='pearson_residuals'`.; batch_key; If specified, highly-variable genes are selected within each batch separately; and merged. This simple process avoids the selection of batch-specific genes; and acts as a lightweight batch correction method. Genes are first sorted by; how many batches they are a HVG. If `flavor='pearson_residuals'`, ties are; broken by the median rank (across batches) based on within-batch residual; variance.; chunksize; If `flavor='pearson_residuals'`, this dertermines how many genes are processed at; once while computing the residual variance. Choosing a smaller value will reduce; the required memory.; """"""; """"""\; n_comps; Number of principal components to compute in the PCA step.; random_state; Random seed for setting the initial states for the optimization in the PCA step.; kwargs_pca; Dictionary of further keyword arguments passed on to `scanpy.pp.pca()`.; """"""; """"""\; inplace; If `True`, update `adata` with results. Otherwise, return results. See below for; details of what is returned.; """"""; """"""\; copy; If `True`, the function runs on a copy of the input object and returns the; modified copy. Otherwise, the input object is modified direcly. Not ",MatchSource.CODE_COMMENT,src/scanpy/experimental/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/_docs.py
Deployability,update,updated,"genes within each batch to nan; # Median rank across batches, ignoring batches in which gene was not selected; # Sort genes by how often they selected as hvg within each batch and; # break ties with median rank of residual variance across batches; """"""\; Select highly variable genes using analytic Pearson residuals :cite:p:`Lause2021`. In :cite:t:`Lause2021`, Pearson residuals of a negative binomial offset model are computed; (with overdispersion `theta` shared across genes). By default, overdispersion; `theta=100` is used and residuals are clipped to `sqrt(n_obs)`. Finally, genes; are ranked by residual variance. Expects raw count input. Parameters; ----------; {adata}; {dist_params}; {genes_batch_chunk}; flavor; Choose the flavor for identifying highly variable genes. In this experimental; version, only 'pearson_residuals' is functional.; {check_values}; {layer}; subset; If `True`, subset the data to highly-variable genes after finding them.; Otherwise merely indicate highly variable genes in `adata.var` (see below).; {inplace}. Returns; -------; If `inplace=True`, `adata.var` is updated with the following fields. Otherwise,; returns the same fields as :class:`~pandas.DataFrame`. highly_variable : :class:`bool`; boolean indicator of highly-variable genes.; means : :class:`float`; means per gene.; variances : :class:`float`; variance per gene.; residual_variances : :class:`float`; For `flavor='pearson_residuals'`, residual variance per gene. Averaged in the; case of multiple batches.; highly_variable_rank : :class:`float`; For `flavor='pearson_residuals'`, rank of the gene according to residual.; variance, median rank in the case of multiple batches.; highly_variable_nbatches : :class:`int`; If `batch_key` given, denotes in how many batches genes are detected as HVG.; highly_variable_intersection : :class:`bool`; If `batch_key` given, denotes the genes that are highly variable in all batches. Notes; -----; Experimental version of `sc.pp.highly_variable_genes()`; """"""",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_highly_variable_genes.py
Modifiability,variab,variable,"""""""; This function navigates the sparsity of the CSC (Compressed Sparse Column) matrix,; returning the value at the specified cell location if it exists, or zero otherwise.; """"""; # Check for raw counts; # check theta; # TODO: would ""underdispersion"" with negative theta make sense?; # then only theta=0 were undefined..; # prepare clipping; # Get pearson residuals for each batch separately; # Filter out zero genes; # Prepare clipping; # Add 0 values for genes that were filtered out; # Get rank per gene within each batch; # argsort twice gives ranks, small rank means most variable; # count in how many batches a genes was among the n_top_genes; # set non-top genes within each batch to nan; # Median rank across batches, ignoring batches in which gene was not selected; # Sort genes by how often they selected as hvg within each batch and; # break ties with median rank of residual variance across batches; """"""\; Select highly variable genes using analytic Pearson residuals :cite:p:`Lause2021`. In :cite:t:`Lause2021`, Pearson residuals of a negative binomial offset model are computed; (with overdispersion `theta` shared across genes). By default, overdispersion; `theta=100` is used and residuals are clipped to `sqrt(n_obs)`. Finally, genes; are ranked by residual variance. Expects raw count input. Parameters; ----------; {adata}; {dist_params}; {genes_batch_chunk}; flavor; Choose the flavor for identifying highly variable genes. In this experimental; version, only 'pearson_residuals' is functional.; {check_values}; {layer}; subset; If `True`, subset the data to highly-variable genes after finding them.; Otherwise merely indicate highly variable genes in `adata.var` (see below).; {inplace}. Returns; -------; If `inplace=True`, `adata.var` is updated with the following fields. Otherwise,; returns the same fields as :class:`~pandas.DataFrame`. highly_variable : :class:`bool`; boolean indicator of highly-variable genes.; means : :class:`float`; means per gene.; variances : :class:",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_highly_variable_genes.py
Safety,detect,detected,"genes within each batch to nan; # Median rank across batches, ignoring batches in which gene was not selected; # Sort genes by how often they selected as hvg within each batch and; # break ties with median rank of residual variance across batches; """"""\; Select highly variable genes using analytic Pearson residuals :cite:p:`Lause2021`. In :cite:t:`Lause2021`, Pearson residuals of a negative binomial offset model are computed; (with overdispersion `theta` shared across genes). By default, overdispersion; `theta=100` is used and residuals are clipped to `sqrt(n_obs)`. Finally, genes; are ranked by residual variance. Expects raw count input. Parameters; ----------; {adata}; {dist_params}; {genes_batch_chunk}; flavor; Choose the flavor for identifying highly variable genes. In this experimental; version, only 'pearson_residuals' is functional.; {check_values}; {layer}; subset; If `True`, subset the data to highly-variable genes after finding them.; Otherwise merely indicate highly variable genes in `adata.var` (see below).; {inplace}. Returns; -------; If `inplace=True`, `adata.var` is updated with the following fields. Otherwise,; returns the same fields as :class:`~pandas.DataFrame`. highly_variable : :class:`bool`; boolean indicator of highly-variable genes.; means : :class:`float`; means per gene.; variances : :class:`float`; variance per gene.; residual_variances : :class:`float`; For `flavor='pearson_residuals'`, residual variance per gene. Averaged in the; case of multiple batches.; highly_variable_rank : :class:`float`; For `flavor='pearson_residuals'`, rank of the gene according to residual.; variance, median rank in the case of multiple batches.; highly_variable_nbatches : :class:`int`; If `batch_key` given, denotes in how many batches genes are detected as HVG.; highly_variable_intersection : :class:`bool`; If `batch_key` given, denotes the genes that are highly variable in all batches. Notes; -----; Experimental version of `sc.pp.highly_variable_genes()`; """"""",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_highly_variable_genes.py
Availability,mask,mask,"with the; normalized values in `results_dict['X']`. `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter.; `.uns['pearson_residuals_normalization']['computed_on']`; The name of the layer on which the residuals were computed.; """"""; """"""\; Applies analytic Pearson residual normalization and PCA, based on :cite:t:`Lause2021`. The residuals are based on a negative binomial offset model with overdispersion; `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`,; overdispersion `theta=100` is used, and PCA is run with 50 components. Operates on the subset of highly variable genes in `adata.var['highly_variable']`; by default. Expects raw count input. Params; ------; {adata}; {dist_params}; {pca_chunk}; {mask_var_hvg}; {check_values}; {inplace}. Returns; -------; If `inplace=False`, returns the Pearson residual-based PCA results (as :class:`~anndata.AnnData`; object). If `inplace=True`, updates `adata` with the following fields:. `.uns['pearson_residuals_normalization']['pearson_residuals_df']`; The subset of highly variable genes, normalized by Pearson residuals.; `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter. `.obsm['X_pca']`; PCA representation of data after gene selection (if applicable) and Pearson; residual normalization.; `.varm['PCs']`; The principal components containing the loadings. When `inplace=True` and; `use_highly_variable=True`, this will contain empty rows for the genes not; selected.; `.uns['pca']['variance_ratio']`; Ratio of explained variance.; `.uns['pca']['variance']`; Explained variance, equivalent to the eigenvalues of the covariance matrix.; """"""; # Unify new mask argument and deprecated use_highly_varible argument; # might be None",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_normalization.py
Deployability,update,updated,"# check theta; # TODO: would ""underdispersion"" with negative theta make sense?; # then only theta=0 were undefined..; # prepare clipping; # clip; """"""\; Applies analytic Pearson residual normalization, based on :cite:t:`Lause2021`. The residuals are based on a negative binomial offset model with overdispersion; `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`; and overdispersion `theta=100` is used. Expects raw count input. Params; ------; {adata}; {dist_params}; {check_values}; {layer}; {inplace}; {copy}. Returns; -------; If `inplace=True`, `adata.X` or the selected layer in `adata.layers` is updated; with the normalized values. `adata.uns` is updated with the following fields.; If `inplace=False`, the same fields are returned as dictionary with the; normalized values in `results_dict['X']`. `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter.; `.uns['pearson_residuals_normalization']['computed_on']`; The name of the layer on which the residuals were computed.; """"""; """"""\; Applies analytic Pearson residual normalization and PCA, based on :cite:t:`Lause2021`. The residuals are based on a negative binomial offset model with overdispersion; `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`,; overdispersion `theta=100` is used, and PCA is run with 50 components. Operates on the subset of highly variable genes in `adata.var['highly_variable']`; by default. Expects raw count input. Params; ------; {adata}; {dist_params}; {pca_chunk}; {mask_var_hvg}; {check_values}; {inplace}. Returns; -------; If `inplace=False`, returns the Pearson residual-based PCA results (as :class:`~anndata.AnnData`; object). If `inplace=True`, updates `adata` with the following fields:. `.uns['pearson_residuals_normalization']['pearson_residuals_df']`; The subset of highly variable genes, normali",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_normalization.py
Modifiability,layers,layers,"# check theta; # TODO: would ""underdispersion"" with negative theta make sense?; # then only theta=0 were undefined..; # prepare clipping; # clip; """"""\; Applies analytic Pearson residual normalization, based on :cite:t:`Lause2021`. The residuals are based on a negative binomial offset model with overdispersion; `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`; and overdispersion `theta=100` is used. Expects raw count input. Params; ------; {adata}; {dist_params}; {check_values}; {layer}; {inplace}; {copy}. Returns; -------; If `inplace=True`, `adata.X` or the selected layer in `adata.layers` is updated; with the normalized values. `adata.uns` is updated with the following fields.; If `inplace=False`, the same fields are returned as dictionary with the; normalized values in `results_dict['X']`. `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter.; `.uns['pearson_residuals_normalization']['computed_on']`; The name of the layer on which the residuals were computed.; """"""; """"""\; Applies analytic Pearson residual normalization and PCA, based on :cite:t:`Lause2021`. The residuals are based on a negative binomial offset model with overdispersion; `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`,; overdispersion `theta=100` is used, and PCA is run with 50 components. Operates on the subset of highly variable genes in `adata.var['highly_variable']`; by default. Expects raw count input. Params; ------; {adata}; {dist_params}; {pca_chunk}; {mask_var_hvg}; {check_values}; {inplace}. Returns; -------; If `inplace=False`, returns the Pearson residual-based PCA results (as :class:`~anndata.AnnData`; object). If `inplace=True`, updates `adata` with the following fields:. `.uns['pearson_residuals_normalization']['pearson_residuals_df']`; The subset of highly variable genes, normali",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_normalization.py
Performance,load,loadings,"with the; normalized values in `results_dict['X']`. `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter.; `.uns['pearson_residuals_normalization']['computed_on']`; The name of the layer on which the residuals were computed.; """"""; """"""\; Applies analytic Pearson residual normalization and PCA, based on :cite:t:`Lause2021`. The residuals are based on a negative binomial offset model with overdispersion; `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`,; overdispersion `theta=100` is used, and PCA is run with 50 components. Operates on the subset of highly variable genes in `adata.var['highly_variable']`; by default. Expects raw count input. Params; ------; {adata}; {dist_params}; {pca_chunk}; {mask_var_hvg}; {check_values}; {inplace}. Returns; -------; If `inplace=False`, returns the Pearson residual-based PCA results (as :class:`~anndata.AnnData`; object). If `inplace=True`, updates `adata` with the following fields:. `.uns['pearson_residuals_normalization']['pearson_residuals_df']`; The subset of highly variable genes, normalized by Pearson residuals.; `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter. `.obsm['X_pca']`; PCA representation of data after gene selection (if applicable) and Pearson; residual normalization.; `.varm['PCs']`; The principal components containing the loadings. When `inplace=True` and; `use_highly_variable=True`, this will contain empty rows for the genes not; selected.; `.uns['pca']['variance_ratio']`; Ratio of explained variance.; `.uns['pca']['variance']`; Explained variance, equivalent to the eigenvalues of the covariance matrix.; """"""; # Unify new mask argument and deprecated use_highly_varible argument; # might be None",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_normalization.py
Deployability,pipeline,pipeline,"""""""\; Full pipeline for HVG selection and normalization by analytic Pearson residuals :cite:p:`Lause2021`. Applies gene selection based on Pearson residuals. On the resulting subset,; Pearson residual normalization and PCA are performed. Expects raw count input. Params; ------; {adata}; {dist_params}; {genes_batch_chunk}; {pca_chunk}; {check_values}; {inplace}. Returns; -------; If `inplace=False`, separately returns the gene selection results (as; :class:`~pandas.DataFrame`) and Pearson residual-based PCA results (as; :class:`~anndata.AnnData`). If `inplace=True`, updates `adata` with the; following fields for gene selection results:. `.var['highly_variable']` : bool; boolean indicator of highly-variable genes.; `.var['means']` : float; means per gene.; `.var['variances']` : float; variances per gene.; `.var['residual_variances']` : float; Pearson residual variance per gene. Averaged in the case of multiple; batches.; `.var['highly_variable_rank']` : float; Rank of the gene according to residual variance, median rank in the; case of multiple batches.; `.var['highly_variable_nbatches']` : int; If batch_key is given, this denotes in how many batches genes are; detected as HVG.; `.var['highly_variable_intersection']` : bool; If batch_key is given, this denotes the genes that are highly variable; in all batches. The following fields contain Pearson residual-based PCA results and; normalization settings:. `.uns['pearson_residuals_normalization']['pearson_residuals_df']`; The subset of highly variable genes, normalized by Pearson residuals.; `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter. `.obsm['X_pca']`; PCA representation of data after gene selection and Pearson residual; normalization.; `.varm['PCs']`; The principal components containing the loadings. When `inplace=True` this; will contain empty rows for the genes not s",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_recipes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_recipes.py
Modifiability,variab,variable,"""""""\; Full pipeline for HVG selection and normalization by analytic Pearson residuals :cite:p:`Lause2021`. Applies gene selection based on Pearson residuals. On the resulting subset,; Pearson residual normalization and PCA are performed. Expects raw count input. Params; ------; {adata}; {dist_params}; {genes_batch_chunk}; {pca_chunk}; {check_values}; {inplace}. Returns; -------; If `inplace=False`, separately returns the gene selection results (as; :class:`~pandas.DataFrame`) and Pearson residual-based PCA results (as; :class:`~anndata.AnnData`). If `inplace=True`, updates `adata` with the; following fields for gene selection results:. `.var['highly_variable']` : bool; boolean indicator of highly-variable genes.; `.var['means']` : float; means per gene.; `.var['variances']` : float; variances per gene.; `.var['residual_variances']` : float; Pearson residual variance per gene. Averaged in the case of multiple; batches.; `.var['highly_variable_rank']` : float; Rank of the gene according to residual variance, median rank in the; case of multiple batches.; `.var['highly_variable_nbatches']` : int; If batch_key is given, this denotes in how many batches genes are; detected as HVG.; `.var['highly_variable_intersection']` : bool; If batch_key is given, this denotes the genes that are highly variable; in all batches. The following fields contain Pearson residual-based PCA results and; normalization settings:. `.uns['pearson_residuals_normalization']['pearson_residuals_df']`; The subset of highly variable genes, normalized by Pearson residuals.; `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter. `.obsm['X_pca']`; PCA representation of data after gene selection and Pearson residual; normalization.; `.varm['PCs']`; The principal components containing the loadings. When `inplace=True` this; will contain empty rows for the genes not s",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_recipes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_recipes.py
Performance,perform,performed,"""""""\; Full pipeline for HVG selection and normalization by analytic Pearson residuals :cite:p:`Lause2021`. Applies gene selection based on Pearson residuals. On the resulting subset,; Pearson residual normalization and PCA are performed. Expects raw count input. Params; ------; {adata}; {dist_params}; {genes_batch_chunk}; {pca_chunk}; {check_values}; {inplace}. Returns; -------; If `inplace=False`, separately returns the gene selection results (as; :class:`~pandas.DataFrame`) and Pearson residual-based PCA results (as; :class:`~anndata.AnnData`). If `inplace=True`, updates `adata` with the; following fields for gene selection results:. `.var['highly_variable']` : bool; boolean indicator of highly-variable genes.; `.var['means']` : float; means per gene.; `.var['variances']` : float; variances per gene.; `.var['residual_variances']` : float; Pearson residual variance per gene. Averaged in the case of multiple; batches.; `.var['highly_variable_rank']` : float; Rank of the gene according to residual variance, median rank in the; case of multiple batches.; `.var['highly_variable_nbatches']` : int; If batch_key is given, this denotes in how many batches genes are; detected as HVG.; `.var['highly_variable_intersection']` : bool; If batch_key is given, this denotes the genes that are highly variable; in all batches. The following fields contain Pearson residual-based PCA results and; normalization settings:. `.uns['pearson_residuals_normalization']['pearson_residuals_df']`; The subset of highly variable genes, normalized by Pearson residuals.; `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter. `.obsm['X_pca']`; PCA representation of data after gene selection and Pearson residual; normalization.; `.varm['PCs']`; The principal components containing the loadings. When `inplace=True` this; will contain empty rows for the genes not s",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_recipes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_recipes.py
Safety,detect,detected,"sed on Pearson residuals. On the resulting subset,; Pearson residual normalization and PCA are performed. Expects raw count input. Params; ------; {adata}; {dist_params}; {genes_batch_chunk}; {pca_chunk}; {check_values}; {inplace}. Returns; -------; If `inplace=False`, separately returns the gene selection results (as; :class:`~pandas.DataFrame`) and Pearson residual-based PCA results (as; :class:`~anndata.AnnData`). If `inplace=True`, updates `adata` with the; following fields for gene selection results:. `.var['highly_variable']` : bool; boolean indicator of highly-variable genes.; `.var['means']` : float; means per gene.; `.var['variances']` : float; variances per gene.; `.var['residual_variances']` : float; Pearson residual variance per gene. Averaged in the case of multiple; batches.; `.var['highly_variable_rank']` : float; Rank of the gene according to residual variance, median rank in the; case of multiple batches.; `.var['highly_variable_nbatches']` : int; If batch_key is given, this denotes in how many batches genes are; detected as HVG.; `.var['highly_variable_intersection']` : bool; If batch_key is given, this denotes the genes that are highly variable; in all batches. The following fields contain Pearson residual-based PCA results and; normalization settings:. `.uns['pearson_residuals_normalization']['pearson_residuals_df']`; The subset of highly variable genes, normalized by Pearson residuals.; `.uns['pearson_residuals_normalization']['theta']`; The used value of the overdisperion parameter theta.; `.uns['pearson_residuals_normalization']['clip']`; The used value of the clipping parameter. `.obsm['X_pca']`; PCA representation of data after gene selection and Pearson residual; normalization.; `.varm['PCs']`; The principal components containing the loadings. When `inplace=True` this; will contain empty rows for the genes not selected during HVG selection.; `.uns['pca']['variance_ratio']`; Ratio of explained variance.; `.uns['pca']['variance']`; Explained v",MatchSource.CODE_COMMENT,src/scanpy/experimental/pp/_recipes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/experimental/pp/_recipes.py
Availability,redundant,redundant,"s for backwards compatibility; # Write cell filter; for now, subplots must be generated from within SPRING,; # so cell filter includes all cells.; # Write 2-D coordinates, after adjusting to roughly match SPRING's default d3js force layout parameters; # Write some useful intermediates, if they exist; # Write PAGA data, if present; # --------------------------------------------------------------------------------; # Helper Functions; # --------------------------------------------------------------------------------; # these are sparse matrices; '''SPRING standard: filename = main_spring_dir + ""counts_norm_sparse_genes.hdf5""'''; '''SPRING standard: filename = main_spring_dir + ""counts_norm_sparse_cells.hdf5""'''; '''SPRING standard: filename = main_spring_dir + ""/counts_norm.npz""'''; # retrieve node data; # retrieve edge level data; # save a threshold weight for showing edges so that by default,; # the number of edges shown is 8X the number of nodes; # save another threshold for even saving edges at all, with 100 edges per node; # make node list; # make link list, avoid redundant encoding (graph is undirected); # save data about edge weights; """"""\; Export adata to a UCSC Cell Browser project directory. If `html_dir` is; set, subsequently build the html files from the project directory into; `html_dir`. If `port` is set, start an HTTP server in the background and; serve `html_dir` on `port`. By default, export all gene expression data from `adata.raw`, the; annotations `louvain`, `percent_mito`, `n_genes` and `n_counts` and the top; `nb_marker` cluster markers. All existing files in data_dir are; overwritten, except `cellbrowser.conf`. See `UCSC Cellbrowser <https://github.com/maximilianh/cellBrowser>`__ for; details. Parameters; ----------; adata; Annotated data matrix; data_dir; Path to directory for exported Cell Browser files.; Usually these are the files `exprMatrix.tsv.gz`, `meta.tsv`,; coordinate files like `tsne.coords.tsv`,; and cluster marker gene lists like `m",MatchSource.CODE_COMMENT,src/scanpy/external/exporting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/exporting.py
Deployability,continuous,continuous,"""""""\; Exporting to formats for other software.; """"""; """"""\; Exports to a SPRING project directory :cite:p:`Weinreb2017`. Visualize annotation present in `adata`. By default, export all gene expression data; from `adata.raw` and categorical and continuous annotations present in `adata.obs`. See `SPRING <https://github.com/AllonKleinLab/SPRING>`__ or :cite:t:`Weinreb2017` for details. Parameters; ----------; adata; Annotated data matrix: `adata.uns['neighbors']` needs to; be present.; project_dir; Path to directory for exported SPRING files.; embedding_method; Name of a 2-D embedding in `adata.obsm`; subplot_name; Name of subplot folder to be created at `project_dir+""/""+subplot_name`; cell_groupings; Instead of importing all categorical annotations when `None`,; pass a list of keys for `adata.obs`.; custom_color_tracks; Specify specific `adata.obs` keys for continuous coloring.; total_counts_key; Name of key for total transcript counts in `adata.obs`.; overwrite; When `True`, existing counts matrices in `project_dir` are overwritten. Examples; --------; See this `tutorial <https://github.com/scverse/scanpy_usage/tree/master/171111_SPRING_export>`__.; """"""; # need to get nearest neighbors first; # check that requested 2-D embedding has been generated; # Make project directory and subplot directory (subplot has same name as project); # For now, the subplot is just all cells in adata; # Write counts matrices as hdf5 files and npz if they do not already exist; # or if user requires overwrite.; # To do: check if Alex's h5sparse format will allow fast loading from just; # one file.; # Ideally, all genes will be written from adata.raw; # Keep track of total counts per cell if present; # Write the counts matrices to project directory; # Get categorical and continuous metadata; # Write continuous colors; # Create and write a dictionary of color profiles to be used by the visualizer; # Write categorical data; # Write graph in two formats for backwards compatibility; # Write cell f",MatchSource.CODE_COMMENT,src/scanpy/external/exporting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/exporting.py
Modifiability,variab,variable,"name for the dataset,; like `""pbmc3k""` or `""tabulamuris""`.; embedding_keys; 2-D embeddings in `adata.obsm` to export.; The prefix `X_` or `X_draw_graph_` is not necessary.; Coordinates missing from `adata` are skipped.; By default (or when specifying `'all'` or `None`), these keys are tried:; [`""tsne""`, `""umap""`, `""pagaFa""`, `""pagaFr""`, `""pagaUmap""`, `""phate""`,; `""fa""`, `""fr""`, `""kk""`, `""drl""`, `""rt""`, `""trimap""`].; For these, default display labels are automatically used.; For other values, you can specify a mapping from coordinate name to; display label, e.g. `{""tsne"": ""t-SNE by Scanpy""}`.; annot_keys; Annotations in `adata.obsm` to export.; Can be a mapping from annotation column name to display label.; Specify `None` for all available columns in `.obs`.; skip_matrix; Do not export the matrix.; If you had previously exported this adata into the same `data_dir`,; then there is no need to export the whole matrix again.; This option will make the export a lot faster,; e.g. when only coordinates or meta data were changed.; html_dir; If this variable is set, the export will build html; files from `data_dir` to `html_dir`, creating html/js/json files.; Usually there is one global html output directory for all datasets.; Often, `html_dir` is located under a webserver's (like Apache); htdocs directory or is copied to one.; A directory `html_dir`/`project_name` will be created and; an index.html will be created under `html_dir` for all subdirectories.; Existing files will be overwritten.; If do not to use html_dir,; you can use the command line tool `cbBuild` to build the html directory.; port; If this variable and `html_dir` are set,; Python's built-in web server will be spawned as a daemon in the; background and serve the files under `html_dir`.; To kill the process, call `cellbrowser.cellbrowser.stop()`.; do_debug; Activate debugging output. Examples; --------; See this; `tutorial <https://github.com/scverse/scanpy_usage/tree/master/181126_Cellbrowser_exports>`__.; """"""",MatchSource.CODE_COMMENT,src/scanpy/external/exporting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/exporting.py
Performance,load,loading,"ng_method; Name of a 2-D embedding in `adata.obsm`; subplot_name; Name of subplot folder to be created at `project_dir+""/""+subplot_name`; cell_groupings; Instead of importing all categorical annotations when `None`,; pass a list of keys for `adata.obs`.; custom_color_tracks; Specify specific `adata.obs` keys for continuous coloring.; total_counts_key; Name of key for total transcript counts in `adata.obs`.; overwrite; When `True`, existing counts matrices in `project_dir` are overwritten. Examples; --------; See this `tutorial <https://github.com/scverse/scanpy_usage/tree/master/171111_SPRING_export>`__.; """"""; # need to get nearest neighbors first; # check that requested 2-D embedding has been generated; # Make project directory and subplot directory (subplot has same name as project); # For now, the subplot is just all cells in adata; # Write counts matrices as hdf5 files and npz if they do not already exist; # or if user requires overwrite.; # To do: check if Alex's h5sparse format will allow fast loading from just; # one file.; # Ideally, all genes will be written from adata.raw; # Keep track of total counts per cell if present; # Write the counts matrices to project directory; # Get categorical and continuous metadata; # Write continuous colors; # Create and write a dictionary of color profiles to be used by the visualizer; # Write categorical data; # Write graph in two formats for backwards compatibility; # Write cell filter; for now, subplots must be generated from within SPRING,; # so cell filter includes all cells.; # Write 2-D coordinates, after adjusting to roughly match SPRING's default d3js force layout parameters; # Write some useful intermediates, if they exist; # Write PAGA data, if present; # --------------------------------------------------------------------------------; # Helper Functions; # --------------------------------------------------------------------------------; # these are sparse matrices; '''SPRING standard: filename = main_spring_dir +",MatchSource.CODE_COMMENT,src/scanpy/external/exporting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/exporting.py
Safety,avoid,avoid,"s for backwards compatibility; # Write cell filter; for now, subplots must be generated from within SPRING,; # so cell filter includes all cells.; # Write 2-D coordinates, after adjusting to roughly match SPRING's default d3js force layout parameters; # Write some useful intermediates, if they exist; # Write PAGA data, if present; # --------------------------------------------------------------------------------; # Helper Functions; # --------------------------------------------------------------------------------; # these are sparse matrices; '''SPRING standard: filename = main_spring_dir + ""counts_norm_sparse_genes.hdf5""'''; '''SPRING standard: filename = main_spring_dir + ""counts_norm_sparse_cells.hdf5""'''; '''SPRING standard: filename = main_spring_dir + ""/counts_norm.npz""'''; # retrieve node data; # retrieve edge level data; # save a threshold weight for showing edges so that by default,; # the number of edges shown is 8X the number of nodes; # save another threshold for even saving edges at all, with 100 edges per node; # make node list; # make link list, avoid redundant encoding (graph is undirected); # save data about edge weights; """"""\; Export adata to a UCSC Cell Browser project directory. If `html_dir` is; set, subsequently build the html files from the project directory into; `html_dir`. If `port` is set, start an HTTP server in the background and; serve `html_dir` on `port`. By default, export all gene expression data from `adata.raw`, the; annotations `louvain`, `percent_mito`, `n_genes` and `n_counts` and the top; `nb_marker` cluster markers. All existing files in data_dir are; overwritten, except `cellbrowser.conf`. See `UCSC Cellbrowser <https://github.com/maximilianh/cellBrowser>`__ for; details. Parameters; ----------; adata; Annotated data matrix; data_dir; Path to directory for exported Cell Browser files.; Usually these are the files `exprMatrix.tsv.gz`, `meta.tsv`,; coordinate files like `tsne.coords.tsv`,; and cluster marker gene lists like `m",MatchSource.CODE_COMMENT,src/scanpy/external/exporting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/exporting.py
Availability,avail,available," final list of; neighbours for the cell. Aligns batches in a quick and lightweight manner. For use in the scanpy workflow as an alternative to :func:`~scanpy.pp.neighbors`. .. note::. This is just a wrapper of :func:`bbknn.bbknn`: up to date docstring,; more information and bug reports there. Params; ------; adata; Needs the PCA computed and stored in `adata.obsm[""X_pca""]`.; batch_key; `adata.obs` column name discriminating between your batches.; use_rep; The dimensionality reduction in `.obsm` to use for neighbour detection. Defaults to PCA.; approx; If `True`, use approximate neighbour finding - annoy or PyNNDescent. This results; in a quicker run time for large datasets while also potentially increasing the degree of; batch correction.; use_annoy; Only used when `approx=True`. If `True`, will use annoy for neighbour finding. If; `False`, will use pyNNDescent instead.; metric; What distance metric to use. The options depend on the choice of neighbour algorithm. ""euclidean"", the default, is always available. Annoy supports ""angular"", ""manhattan"" and ""hamming"". PyNNDescent supports metrics listed in `pynndescent.distances.named_distances`; and custom functions, including compiled Numba code. >>> import pynndescent; >>> pynndescent.distances.named_distances.keys() # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE; dict_keys(['euclidean', 'l2', 'sqeuclidean', 'manhattan', 'taxicab', 'l1', 'chebyshev', 'linfinity',; 'linfty', 'linf', 'minkowski', 'seuclidean', 'standardised_euclidean', 'wminkowski', ...]). KDTree supports members of :class:`sklearn.neighbors.KDTree`’s ``valid_metrics`` list, or parameterised; :class:`~sklearn.metrics.DistanceMetric` objects:. >>> import sklearn.neighbors; >>> sklearn.neighbors.KDTree.valid_metrics; ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity']. .. note:: check the relevant documentation for up-to-date lists.; copy; If `True`, return a copy instead of writing to the supplied adata.; neighbors_wi",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_bbknn.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_bbknn.py
Deployability,install,installed,"e this number times the number; of batches. This then serves as the basis for the construction of a symmetrical; matrix of connectivities.; n_pcs; How many dimensions (in case of PCA, principal components) to use in the analysis.; trim; Trim the neighbours of each cell to these many top connectivities. May help with; population independence and improve the tidiness of clustering. The lower the value the; more independent the individual populations, at the cost of more conserved batch effect.; If `None`, sets the parameter value automatically to 10 times `neighbors_within_batch`; times the number of batches. Set to 0 to skip.; annoy_n_trees; Only used with annoy neighbour identification. The number of trees to construct in the; annoy forest. More trees give higher precision when querying, at the cost of increased; run time and resource intensity.; pynndescent_n_neighbors; Only used with pyNNDescent neighbour identification. The number of neighbours to include; in the approximate neighbour graph. More neighbours give higher precision when querying,; at the cost of increased run time and resource intensity.; pynndescent_random_state; Only used with pyNNDescent neighbour identification. The RNG seed to use when creating; the graph.; use_faiss; If `approx=False` and the metric is ""euclidean"", use the faiss package to compute; nearest neighbours if installed. This improves performance at a minor cost to numerical; precision as faiss operates on float32.; set_op_mix_ratio; UMAP connectivity computation parameter, float between 0 and 1, controlling the; blend between a connectivity matrix formed exclusively from mutual nearest neighbour; pairs (0) and a union of all observed neighbour relationships with the mutual pairs; emphasised (1); local_connectivity; UMAP connectivity computation parameter, how many nearest neighbors of each cell; are assumed to be fully connected (and given a connectivity value of 1). Returns; -------; The `adata` with the batch-corrected graph.; """"""",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_bbknn.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_bbknn.py
Integrability,wrap,wrapper,"""""""\; Batch balanced kNN :cite:p:`Polanski2019`. Batch balanced kNN alters the kNN procedure to identify each cell's top neighbours in; each batch separately instead of the entire cell pool with no accounting for batch.; The nearest neighbours for each batch are then merged to create a final list of; neighbours for the cell. Aligns batches in a quick and lightweight manner. For use in the scanpy workflow as an alternative to :func:`~scanpy.pp.neighbors`. .. note::. This is just a wrapper of :func:`bbknn.bbknn`: up to date docstring,; more information and bug reports there. Params; ------; adata; Needs the PCA computed and stored in `adata.obsm[""X_pca""]`.; batch_key; `adata.obs` column name discriminating between your batches.; use_rep; The dimensionality reduction in `.obsm` to use for neighbour detection. Defaults to PCA.; approx; If `True`, use approximate neighbour finding - annoy or PyNNDescent. This results; in a quicker run time for large datasets while also potentially increasing the degree of; batch correction.; use_annoy; Only used when `approx=True`. If `True`, will use annoy for neighbour finding. If; `False`, will use pyNNDescent instead.; metric; What distance metric to use. The options depend on the choice of neighbour algorithm. ""euclidean"", the default, is always available. Annoy supports ""angular"", ""manhattan"" and ""hamming"". PyNNDescent supports metrics listed in `pynndescent.distances.named_distances`; and custom functions, including compiled Numba code. >>> import pynndescent; >>> pynndescent.distances.named_distances.keys() # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE; dict_keys(['euclidean', 'l2', 'sqeuclidean', 'manhattan', 'taxicab', 'l1', 'chebyshev', 'linfinity',; 'linfty', 'linf', 'minkowski', 'seuclidean', 'standardised_euclidean', 'wminkowski', ...]). KDTree supports members of :class:`sklearn.neighbors.KDTree`’s ``valid_metrics`` list, or parameterised; :class:`~sklearn.metrics.DistanceMetric` objects:. >>> import sklearn.neighbors; >>> sk",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_bbknn.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_bbknn.py
Performance,perform,performance,"e this number times the number; of batches. This then serves as the basis for the construction of a symmetrical; matrix of connectivities.; n_pcs; How many dimensions (in case of PCA, principal components) to use in the analysis.; trim; Trim the neighbours of each cell to these many top connectivities. May help with; population independence and improve the tidiness of clustering. The lower the value the; more independent the individual populations, at the cost of more conserved batch effect.; If `None`, sets the parameter value automatically to 10 times `neighbors_within_batch`; times the number of batches. Set to 0 to skip.; annoy_n_trees; Only used with annoy neighbour identification. The number of trees to construct in the; annoy forest. More trees give higher precision when querying, at the cost of increased; run time and resource intensity.; pynndescent_n_neighbors; Only used with pyNNDescent neighbour identification. The number of neighbours to include; in the approximate neighbour graph. More neighbours give higher precision when querying,; at the cost of increased run time and resource intensity.; pynndescent_random_state; Only used with pyNNDescent neighbour identification. The RNG seed to use when creating; the graph.; use_faiss; If `approx=False` and the metric is ""euclidean"", use the faiss package to compute; nearest neighbours if installed. This improves performance at a minor cost to numerical; precision as faiss operates on float32.; set_op_mix_ratio; UMAP connectivity computation parameter, float between 0 and 1, controlling the; blend between a connectivity matrix formed exclusively from mutual nearest neighbour; pairs (0) and a union of all observed neighbour relationships with the mutual pairs; emphasised (1); local_connectivity; UMAP connectivity computation parameter, how many nearest neighbors of each cell; are assumed to be fully connected (and given a connectivity value of 1). Returns; -------; The `adata` with the batch-corrected graph.; """"""",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_bbknn.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_bbknn.py
Safety,detect,detection,"""""""\; Batch balanced kNN :cite:p:`Polanski2019`. Batch balanced kNN alters the kNN procedure to identify each cell's top neighbours in; each batch separately instead of the entire cell pool with no accounting for batch.; The nearest neighbours for each batch are then merged to create a final list of; neighbours for the cell. Aligns batches in a quick and lightweight manner. For use in the scanpy workflow as an alternative to :func:`~scanpy.pp.neighbors`. .. note::. This is just a wrapper of :func:`bbknn.bbknn`: up to date docstring,; more information and bug reports there. Params; ------; adata; Needs the PCA computed and stored in `adata.obsm[""X_pca""]`.; batch_key; `adata.obs` column name discriminating between your batches.; use_rep; The dimensionality reduction in `.obsm` to use for neighbour detection. Defaults to PCA.; approx; If `True`, use approximate neighbour finding - annoy or PyNNDescent. This results; in a quicker run time for large datasets while also potentially increasing the degree of; batch correction.; use_annoy; Only used when `approx=True`. If `True`, will use annoy for neighbour finding. If; `False`, will use pyNNDescent instead.; metric; What distance metric to use. The options depend on the choice of neighbour algorithm. ""euclidean"", the default, is always available. Annoy supports ""angular"", ""manhattan"" and ""hamming"". PyNNDescent supports metrics listed in `pynndescent.distances.named_distances`; and custom functions, including compiled Numba code. >>> import pynndescent; >>> pynndescent.distances.named_distances.keys() # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE; dict_keys(['euclidean', 'l2', 'sqeuclidean', 'manhattan', 'taxicab', 'l1', 'chebyshev', 'linfinity',; 'linfty', 'linf', 'minkowski', 'seuclidean', 'standardised_euclidean', 'wminkowski', ...]). KDTree supports members of :class:`sklearn.neighbors.KDTree`’s ``valid_metrics`` list, or parameterised; :class:`~sklearn.metrics.DistanceMetric` objects:. >>> import sklearn.neighbors; >>> sk",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_bbknn.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_bbknn.py
Integrability,depend,dependant,"# network args; # training args; """"""\; Deep count autoencoder :cite:p:`Eraslan2019`. Fits a count autoencoder to the raw count data given in the anndata object; in order to denoise the data and to capture hidden representation of; cells in low dimensions. Type of the autoencoder and return values are; determined by the parameters. .. note::; More information and bug reports `here <https://github.com/theislab/dca>`__. Parameters; ----------; adata; An anndata file with `.raw` attribute representing raw counts.; mode; `denoise` overwrites `adata.X` with denoised expression values.; In `latent` mode DCA adds `adata.obsm['X_dca']` to given adata; object. This matrix represent latent representation of cells via DCA.; ae_type; Type of the autoencoder. Return values and the architecture is; determined by the type e.g. `nb` does not provide dropout; probabilities. Types that end with ""-conddisp"", assumes that dispersion is mean dependant.; normalize_per_cell; If true, library size normalization is performed using; the `sc.pp.normalize_per_cell` function in Scanpy and saved into adata; object. Mean layer is re-introduces library size differences by; scaling the mean value of each cell in the output layer. See the; manuscript for more details.; scale; If true, the input of the autoencoder is centered using; `sc.pp.scale` function of Scanpy. Note that the output is kept as raw; counts as loss functions are designed for the count data.; log1p; If true, the input of the autoencoder is log transformed with a; pseudocount of one using `sc.pp.log1p` function of Scanpy.; hidden_size; Width of hidden layers.; hidden_dropout; Probability of weight dropout in the autoencoder (per layer if list; or tuple).; batchnorm; If true, batch normalization is performed.; activation; Activation function of hidden layers.; init; Initialization method used to initialize weights.; network_kwds; Additional keyword arguments for the autoencoder.; epochs; Number of total epochs in training.; reduce_lr; R",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_dca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_dca.py
Modifiability,layers,layers," mode DCA adds `adata.obsm['X_dca']` to given adata; object. This matrix represent latent representation of cells via DCA.; ae_type; Type of the autoencoder. Return values and the architecture is; determined by the type e.g. `nb` does not provide dropout; probabilities. Types that end with ""-conddisp"", assumes that dispersion is mean dependant.; normalize_per_cell; If true, library size normalization is performed using; the `sc.pp.normalize_per_cell` function in Scanpy and saved into adata; object. Mean layer is re-introduces library size differences by; scaling the mean value of each cell in the output layer. See the; manuscript for more details.; scale; If true, the input of the autoencoder is centered using; `sc.pp.scale` function of Scanpy. Note that the output is kept as raw; counts as loss functions are designed for the count data.; log1p; If true, the input of the autoencoder is log transformed with a; pseudocount of one using `sc.pp.log1p` function of Scanpy.; hidden_size; Width of hidden layers.; hidden_dropout; Probability of weight dropout in the autoencoder (per layer if list; or tuple).; batchnorm; If true, batch normalization is performed.; activation; Activation function of hidden layers.; init; Initialization method used to initialize weights.; network_kwds; Additional keyword arguments for the autoencoder.; epochs; Number of total epochs in training.; reduce_lr; Reduces learning rate if validation loss does not improve in given number of epochs.; early_stop; Stops training if validation loss does not improve in given number of epochs.; batch_size; Number of samples in the batch used for SGD.; optimizer; Type of optimization method used for training.; random_state; Seed for python, numpy and tensorflow.; threads; Number of threads to use in training. All cores are used by default.; learning_rate; Learning rate to use in the training.; verbose; If true, prints additional information about training and architecture.; training_kwds; Additional keyword a",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_dca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_dca.py
Performance,perform,performed,"# network args; # training args; """"""\; Deep count autoencoder :cite:p:`Eraslan2019`. Fits a count autoencoder to the raw count data given in the anndata object; in order to denoise the data and to capture hidden representation of; cells in low dimensions. Type of the autoencoder and return values are; determined by the parameters. .. note::; More information and bug reports `here <https://github.com/theislab/dca>`__. Parameters; ----------; adata; An anndata file with `.raw` attribute representing raw counts.; mode; `denoise` overwrites `adata.X` with denoised expression values.; In `latent` mode DCA adds `adata.obsm['X_dca']` to given adata; object. This matrix represent latent representation of cells via DCA.; ae_type; Type of the autoencoder. Return values and the architecture is; determined by the type e.g. `nb` does not provide dropout; probabilities. Types that end with ""-conddisp"", assumes that dispersion is mean dependant.; normalize_per_cell; If true, library size normalization is performed using; the `sc.pp.normalize_per_cell` function in Scanpy and saved into adata; object. Mean layer is re-introduces library size differences by; scaling the mean value of each cell in the output layer. See the; manuscript for more details.; scale; If true, the input of the autoencoder is centered using; `sc.pp.scale` function of Scanpy. Note that the output is kept as raw; counts as loss functions are designed for the count data.; log1p; If true, the input of the autoencoder is log transformed with a; pseudocount of one using `sc.pp.log1p` function of Scanpy.; hidden_size; Width of hidden layers.; hidden_dropout; Probability of weight dropout in the autoencoder (per layer if list; or tuple).; batchnorm; If true, batch normalization is performed.; activation; Activation function of hidden layers.; init; Initialization method used to initialize weights.; network_kwds; Additional keyword arguments for the autoencoder.; epochs; Number of total epochs in training.; reduce_lr; R",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_dca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_dca.py
Security,validat,validation,"rmalize_per_cell` function in Scanpy and saved into adata; object. Mean layer is re-introduces library size differences by; scaling the mean value of each cell in the output layer. See the; manuscript for more details.; scale; If true, the input of the autoencoder is centered using; `sc.pp.scale` function of Scanpy. Note that the output is kept as raw; counts as loss functions are designed for the count data.; log1p; If true, the input of the autoencoder is log transformed with a; pseudocount of one using `sc.pp.log1p` function of Scanpy.; hidden_size; Width of hidden layers.; hidden_dropout; Probability of weight dropout in the autoencoder (per layer if list; or tuple).; batchnorm; If true, batch normalization is performed.; activation; Activation function of hidden layers.; init; Initialization method used to initialize weights.; network_kwds; Additional keyword arguments for the autoencoder.; epochs; Number of total epochs in training.; reduce_lr; Reduces learning rate if validation loss does not improve in given number of epochs.; early_stop; Stops training if validation loss does not improve in given number of epochs.; batch_size; Number of samples in the batch used for SGD.; optimizer; Type of optimization method used for training.; random_state; Seed for python, numpy and tensorflow.; threads; Number of threads to use in training. All cores are used by default.; learning_rate; Learning rate to use in the training.; verbose; If true, prints additional information about training and architecture.; training_kwds; Additional keyword arguments for the training process.; return_model; If true, trained autoencoder object is returned. See ""Returns"".; return_info; If true, all additional parameters of DCA are stored in `adata.obsm` such as dropout; probabilities (obsm['X_dca_dropout']) and estimated dispersion values; (obsm['X_dca_dispersion']), in case that autoencoder is of type; zinb or zinb-conddisp.; copy; If true, a copy of anndata is returned. Returns; -------; ",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_dca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_dca.py
Testability,log,log,"ting raw counts.; mode; `denoise` overwrites `adata.X` with denoised expression values.; In `latent` mode DCA adds `adata.obsm['X_dca']` to given adata; object. This matrix represent latent representation of cells via DCA.; ae_type; Type of the autoencoder. Return values and the architecture is; determined by the type e.g. `nb` does not provide dropout; probabilities. Types that end with ""-conddisp"", assumes that dispersion is mean dependant.; normalize_per_cell; If true, library size normalization is performed using; the `sc.pp.normalize_per_cell` function in Scanpy and saved into adata; object. Mean layer is re-introduces library size differences by; scaling the mean value of each cell in the output layer. See the; manuscript for more details.; scale; If true, the input of the autoencoder is centered using; `sc.pp.scale` function of Scanpy. Note that the output is kept as raw; counts as loss functions are designed for the count data.; log1p; If true, the input of the autoencoder is log transformed with a; pseudocount of one using `sc.pp.log1p` function of Scanpy.; hidden_size; Width of hidden layers.; hidden_dropout; Probability of weight dropout in the autoencoder (per layer if list; or tuple).; batchnorm; If true, batch normalization is performed.; activation; Activation function of hidden layers.; init; Initialization method used to initialize weights.; network_kwds; Additional keyword arguments for the autoencoder.; epochs; Number of total epochs in training.; reduce_lr; Reduces learning rate if validation loss does not improve in given number of epochs.; early_stop; Stops training if validation loss does not improve in given number of epochs.; batch_size; Number of samples in the batch used for SGD.; optimizer; Type of optimization method used for training.; random_state; Seed for python, numpy and tensorflow.; threads; Number of threads to use in training. All cores are used by default.; learning_rate; Learning rate to use in the training.; verbose; If true, ",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_dca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_dca.py
Usability,learn,learning,"rmalize_per_cell` function in Scanpy and saved into adata; object. Mean layer is re-introduces library size differences by; scaling the mean value of each cell in the output layer. See the; manuscript for more details.; scale; If true, the input of the autoencoder is centered using; `sc.pp.scale` function of Scanpy. Note that the output is kept as raw; counts as loss functions are designed for the count data.; log1p; If true, the input of the autoencoder is log transformed with a; pseudocount of one using `sc.pp.log1p` function of Scanpy.; hidden_size; Width of hidden layers.; hidden_dropout; Probability of weight dropout in the autoencoder (per layer if list; or tuple).; batchnorm; If true, batch normalization is performed.; activation; Activation function of hidden layers.; init; Initialization method used to initialize weights.; network_kwds; Additional keyword arguments for the autoencoder.; epochs; Number of total epochs in training.; reduce_lr; Reduces learning rate if validation loss does not improve in given number of epochs.; early_stop; Stops training if validation loss does not improve in given number of epochs.; batch_size; Number of samples in the batch used for SGD.; optimizer; Type of optimization method used for training.; random_state; Seed for python, numpy and tensorflow.; threads; Number of threads to use in training. All cores are used by default.; learning_rate; Learning rate to use in the training.; verbose; If true, prints additional information about training and architecture.; training_kwds; Additional keyword arguments for the training process.; return_model; If true, trained autoencoder object is returned. See ""Returns"".; return_info; If true, all additional parameters of DCA are stored in `adata.obsm` such as dropout; probabilities (obsm['X_dca_dropout']) and estimated dispersion values; (obsm['X_dca_dispersion']), in case that autoencoder is of type; zinb or zinb-conddisp.; copy; If true, a copy of anndata is returned. Returns; -------; ",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_dca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_dca.py
Deployability,integrat,integrate,"""""""; Use harmony to integrate cells from different experiments.; """"""; """"""\; Use harmonypy :cite:p:`Korsunsky2019` to integrate different experiments. Harmony :cite:p:`Korsunsky2019` is an algorithm for integrating single-cell; data from multiple experiments. This function uses the python; port of Harmony, ``harmonypy``, to integrate single-cell data; stored in an AnnData object. As Harmony works by adjusting the; principal components, this function should be run after performing; PCA but before computing the neighbor graph, as illustrated in the; example below. Parameters; ----------; adata; The annotated data matrix.; key; The name of the column in ``adata.obs`` that differentiates; among experiments/batches.; basis; The name of the field in ``adata.obsm`` where the PCA table is; stored. Defaults to ``'X_pca'``, which is the default for; ``sc.pp.pca()``.; adjusted_basis; The name of the field in ``adata.obsm`` where the adjusted PCA; table will be stored after running this function. Defaults to; ``X_pca_harmony``.; kwargs; Any additional arguments will be passed to; ``harmonypy.run_harmony()``. Returns; -------; Updates adata with the field ``adata.obsm[obsm_out_field]``,; containing principal components adjusted by Harmony such that; different experiments are integrated. Example; -------; First, load libraries and example dataset, and preprocess. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.recipe_zheng17(adata); >>> sc.pp.pca(adata). We now arbitrarily assign a batch metadata variable to each cell; for the sake of example, but during real usage there would already; be a column in ``adata.obs`` giving the experiment each cell came; from. >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']. Finally, run harmony. Afterwards, there will be a new table in; ``adata.obsm`` containing the adjusted PC's. >>> sce.pp.harmony_integrate(adata, 'batch'); >>> 'X_pca_harmony' in adata.obsm; True; """"""",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_harmony_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_harmony_integrate.py
Integrability,integrat,integrate,"""""""; Use harmony to integrate cells from different experiments.; """"""; """"""\; Use harmonypy :cite:p:`Korsunsky2019` to integrate different experiments. Harmony :cite:p:`Korsunsky2019` is an algorithm for integrating single-cell; data from multiple experiments. This function uses the python; port of Harmony, ``harmonypy``, to integrate single-cell data; stored in an AnnData object. As Harmony works by adjusting the; principal components, this function should be run after performing; PCA but before computing the neighbor graph, as illustrated in the; example below. Parameters; ----------; adata; The annotated data matrix.; key; The name of the column in ``adata.obs`` that differentiates; among experiments/batches.; basis; The name of the field in ``adata.obsm`` where the PCA table is; stored. Defaults to ``'X_pca'``, which is the default for; ``sc.pp.pca()``.; adjusted_basis; The name of the field in ``adata.obsm`` where the adjusted PCA; table will be stored after running this function. Defaults to; ``X_pca_harmony``.; kwargs; Any additional arguments will be passed to; ``harmonypy.run_harmony()``. Returns; -------; Updates adata with the field ``adata.obsm[obsm_out_field]``,; containing principal components adjusted by Harmony such that; different experiments are integrated. Example; -------; First, load libraries and example dataset, and preprocess. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.recipe_zheng17(adata); >>> sc.pp.pca(adata). We now arbitrarily assign a batch metadata variable to each cell; for the sake of example, but during real usage there would already; be a column in ``adata.obs`` giving the experiment each cell came; from. >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']. Finally, run harmony. Afterwards, there will be a new table in; ``adata.obsm`` containing the adjusted PC's. >>> sce.pp.harmony_integrate(adata, 'batch'); >>> 'X_pca_harmony' in adata.obsm; True; """"""",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_harmony_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_harmony_integrate.py
Modifiability,variab,variable,"""""""; Use harmony to integrate cells from different experiments.; """"""; """"""\; Use harmonypy :cite:p:`Korsunsky2019` to integrate different experiments. Harmony :cite:p:`Korsunsky2019` is an algorithm for integrating single-cell; data from multiple experiments. This function uses the python; port of Harmony, ``harmonypy``, to integrate single-cell data; stored in an AnnData object. As Harmony works by adjusting the; principal components, this function should be run after performing; PCA but before computing the neighbor graph, as illustrated in the; example below. Parameters; ----------; adata; The annotated data matrix.; key; The name of the column in ``adata.obs`` that differentiates; among experiments/batches.; basis; The name of the field in ``adata.obsm`` where the PCA table is; stored. Defaults to ``'X_pca'``, which is the default for; ``sc.pp.pca()``.; adjusted_basis; The name of the field in ``adata.obsm`` where the adjusted PCA; table will be stored after running this function. Defaults to; ``X_pca_harmony``.; kwargs; Any additional arguments will be passed to; ``harmonypy.run_harmony()``. Returns; -------; Updates adata with the field ``adata.obsm[obsm_out_field]``,; containing principal components adjusted by Harmony such that; different experiments are integrated. Example; -------; First, load libraries and example dataset, and preprocess. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.recipe_zheng17(adata); >>> sc.pp.pca(adata). We now arbitrarily assign a batch metadata variable to each cell; for the sake of example, but during real usage there would already; be a column in ``adata.obs`` giving the experiment each cell came; from. >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']. Finally, run harmony. Afterwards, there will be a new table in; ``adata.obsm`` containing the adjusted PC's. >>> sce.pp.harmony_integrate(adata, 'batch'); >>> 'X_pca_harmony' in adata.obsm; True; """"""",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_harmony_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_harmony_integrate.py
Performance,perform,performing,"""""""; Use harmony to integrate cells from different experiments.; """"""; """"""\; Use harmonypy :cite:p:`Korsunsky2019` to integrate different experiments. Harmony :cite:p:`Korsunsky2019` is an algorithm for integrating single-cell; data from multiple experiments. This function uses the python; port of Harmony, ``harmonypy``, to integrate single-cell data; stored in an AnnData object. As Harmony works by adjusting the; principal components, this function should be run after performing; PCA but before computing the neighbor graph, as illustrated in the; example below. Parameters; ----------; adata; The annotated data matrix.; key; The name of the column in ``adata.obs`` that differentiates; among experiments/batches.; basis; The name of the field in ``adata.obsm`` where the PCA table is; stored. Defaults to ``'X_pca'``, which is the default for; ``sc.pp.pca()``.; adjusted_basis; The name of the field in ``adata.obsm`` where the adjusted PCA; table will be stored after running this function. Defaults to; ``X_pca_harmony``.; kwargs; Any additional arguments will be passed to; ``harmonypy.run_harmony()``. Returns; -------; Updates adata with the field ``adata.obsm[obsm_out_field]``,; containing principal components adjusted by Harmony such that; different experiments are integrated. Example; -------; First, load libraries and example dataset, and preprocess. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.recipe_zheng17(adata); >>> sc.pp.pca(adata). We now arbitrarily assign a batch metadata variable to each cell; for the sake of example, but during real usage there would already; be a column in ``adata.obs`` giving the experiment each cell came; from. >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']. Finally, run harmony. Afterwards, there will be a new table in; ``adata.obsm`` containing the adjusted PC's. >>> sce.pp.harmony_integrate(adata, 'batch'); >>> 'X_pca_harmony' in adata.obsm; True; """"""",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_harmony_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_harmony_integrate.py
Deployability,update,updates,"""""""; HashSolo script provides a probabilistic cell hashing demultiplexing method; which generates a noise distribution and signal distribution for; each hashing barcode from empirically observed counts. These distributions; are updates from the global signal and noise barcode distributions, which; helps in the setting where not many cells are observed. Signal distributions; for a hashing barcode are estimated from samples where that hashing barcode; has the highest count. Noise distributions for a hashing barcode are estimated; from samples where that hashing barcode is one the k-2 lowest barcodes, where; k is the number of barcodes. A doublet should then have its two highest; barcode counts most likely coming from a signal distribution for those barcodes.; A singlet should have its highest barcode from a signal distribution, and its; second highest barcode from a noise distribution. A negative two highest; barcodes should come from noise distributions. We test each of these; hypotheses in a bayesian fashion, and select the most probable hypothesis.; """"""; """"""Calculate log likelihoods for each hypothesis, negative, singlet, doublet. Parameters; ----------; data; cells by hashing counts matrix; number_of_noise_barcodes; number of barcodes to used to calculated noise distribution. Returns; -------; log_likelihoods_for_each_hypothesis; a 2d np.array log likelihood of each hypothesis; all_indices; counter_to_barcode_combo; """"""; """"""Update parameters of your gaussian; https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf. Parameters; ----------; data; 1-d array of counts; mu_o; global mean for hashing count distribution; std_o; global std for hashing count distribution. Returns; -------; mean; of gaussian; std; of gaussian; """"""; # probabilites for negative, singlet, doublets; # assume log normal; # global signal and noise counts useful for when we have few cells; # barcodes with the highest number of counts are assumed to be a true signal; # barcodes with rank < k are consid",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_hashsolo.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_hashsolo.py
Modifiability,parameteriz,parameterization,"e log likelihoods for each hypothesis, negative, singlet, doublet. Parameters; ----------; data; cells by hashing counts matrix; number_of_noise_barcodes; number of barcodes to used to calculated noise distribution. Returns; -------; log_likelihoods_for_each_hypothesis; a 2d np.array log likelihood of each hypothesis; all_indices; counter_to_barcode_combo; """"""; """"""Update parameters of your gaussian; https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf. Parameters; ----------; data; 1-d array of counts; mu_o; global mean for hashing count distribution; std_o; global std for hashing count distribution. Returns; -------; mean; of gaussian; std; of gaussian; """"""; # probabilites for negative, singlet, doublets; # assume log normal; # global signal and noise counts useful for when we have few cells; # barcodes with the highest number of counts are assumed to be a true signal; # barcodes with rank < k are considered to be noise; # for each barcode get empirical noise and signal distribution parameterization; # get noise and signal counts; # get parameters of distribution, assuming lognormal do update from global values; # for each combination of noise and signal barcode calculate probiltiy of in silico and real cell hypotheses; # calculate probabilties for each hypothesis for each cell; # each cell and each hypothesis probability; """"""; Calculate bayes rule from log likelihoods. Parameters; ----------; data; Anndata object filled only with hashing counts; priors; a list of your prior for each hypothesis; first element is your prior for the negative hypothesis; second element is your prior for the singlet hypothesis; third element is your prior for the doublet hypothesis; We use [0.01, 0.8, 0.19] by default because we assume the barcodes; in your cell hashing matrix are those cells which have passed QC; in the transcriptome space, e.g. UMI counts, pct mito reads, etc.; number_of_noise_barcodes; number of barcodes to used to calculated noise distribution. Returns; -------; A ",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_hashsolo.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_hashsolo.py
Performance,perform,performed,"hypotheses""`; A 2d np.array probability of each hypothesis; `""log_likelihoods_for_each_hypothesis""`; A 2d np.array log likelihood of each hypothesis; """"""; """"""Probabilistic demultiplexing of cell hashing data using HashSolo :cite:p:`Bernstein2020`. .. note::; More information and bug reports `here <https://github.com/calico/solo>`__. Parameters; ----------; adata; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; cell_hashing_columns; `.obs` columns that contain cell hashing counts.; priors; Prior probabilities of each hypothesis, in; the order `[negative, singlet, doublet]`. The default is set to; `[0.01, 0.8, 0.19]` assuming barcode counts are from cells that; have passed QC in the transcriptome space, e.g. UMI counts, pct; mito reads, etc.; pre_existing_clusters; The column in `.obs` containing pre-existing cluster assignments; (e.g. Leiden clusters or cell types, but not batch assignments).; If provided, demultiplexing will be performed separately for each; cluster.; number_of_noise_barcodes; The number of barcodes used to create the noise distribution.; Defaults to `len(cell_hashing_columns) - 2`.; inplace; Whether to update `adata` in-place or return a copy. Returns; -------; A copy of the input `adata` if `inplace=False`, otherwise the input; `adata`. The following fields are added:. `.obs[""most_likely_hypothesis""]`; Index of the most likely hypothesis, where `0` corresponds to negative,; `1` to singlet, and `2` to doublet.; `.obs[""cluster_feature""]`; The cluster assignments used for demultiplexing.; `.obs[""negative_hypothesis_probability""]`; Probability of the negative hypothesis.; `.obs[""singlet_hypothesis_probability""]`; Probability of the singlet hypothesis.; `.obs[""doublet_hypothesis_probability""]`; Probability of the doublet hypothesis.; `.obs[""Classification""]`:; Classification of the cell, one of the barcodes in `cell_hashing_columns`,; `""Negative""`, or `""Doublet""`. Examples; -------; >>> import an",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_hashsolo.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_hashsolo.py
Security,hash,hashing,"""""""; HashSolo script provides a probabilistic cell hashing demultiplexing method; which generates a noise distribution and signal distribution for; each hashing barcode from empirically observed counts. These distributions; are updates from the global signal and noise barcode distributions, which; helps in the setting where not many cells are observed. Signal distributions; for a hashing barcode are estimated from samples where that hashing barcode; has the highest count. Noise distributions for a hashing barcode are estimated; from samples where that hashing barcode is one the k-2 lowest barcodes, where; k is the number of barcodes. A doublet should then have its two highest; barcode counts most likely coming from a signal distribution for those barcodes.; A singlet should have its highest barcode from a signal distribution, and its; second highest barcode from a noise distribution. A negative two highest; barcodes should come from noise distributions. We test each of these; hypotheses in a bayesian fashion, and select the most probable hypothesis.; """"""; """"""Calculate log likelihoods for each hypothesis, negative, singlet, doublet. Parameters; ----------; data; cells by hashing counts matrix; number_of_noise_barcodes; number of barcodes to used to calculated noise distribution. Returns; -------; log_likelihoods_for_each_hypothesis; a 2d np.array log likelihood of each hypothesis; all_indices; counter_to_barcode_combo; """"""; """"""Update parameters of your gaussian; https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf. Parameters; ----------; data; 1-d array of counts; mu_o; global mean for hashing count distribution; std_o; global std for hashing count distribution. Returns; -------; mean; of gaussian; std; of gaussian; """"""; # probabilites for negative, singlet, doublets; # assume log normal; # global signal and noise counts useful for when we have few cells; # barcodes with the highest number of counts are assumed to be a true signal; # barcodes with rank < k are consid",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_hashsolo.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_hashsolo.py
Testability,test,test,"ript provides a probabilistic cell hashing demultiplexing method; which generates a noise distribution and signal distribution for; each hashing barcode from empirically observed counts. These distributions; are updates from the global signal and noise barcode distributions, which; helps in the setting where not many cells are observed. Signal distributions; for a hashing barcode are estimated from samples where that hashing barcode; has the highest count. Noise distributions for a hashing barcode are estimated; from samples where that hashing barcode is one the k-2 lowest barcodes, where; k is the number of barcodes. A doublet should then have its two highest; barcode counts most likely coming from a signal distribution for those barcodes.; A singlet should have its highest barcode from a signal distribution, and its; second highest barcode from a noise distribution. A negative two highest; barcodes should come from noise distributions. We test each of these; hypotheses in a bayesian fashion, and select the most probable hypothesis.; """"""; """"""Calculate log likelihoods for each hypothesis, negative, singlet, doublet. Parameters; ----------; data; cells by hashing counts matrix; number_of_noise_barcodes; number of barcodes to used to calculated noise distribution. Returns; -------; log_likelihoods_for_each_hypothesis; a 2d np.array log likelihood of each hypothesis; all_indices; counter_to_barcode_combo; """"""; """"""Update parameters of your gaussian; https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf. Parameters; ----------; data; 1-d array of counts; mu_o; global mean for hashing count distribution; std_o; global std for hashing count distribution. Returns; -------; mean; of gaussian; std; of gaussian; """"""; # probabilites for negative, singlet, doublets; # assume log normal; # global signal and noise counts useful for when we have few cells; # barcodes with the highest number of counts are assumed to be a true signal; # barcodes with rank < k are considered to be nois",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_hashsolo.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_hashsolo.py
Availability,recover,recover,"""""""\; Denoise high-dimensional data using MAGIC; """"""; """"""\; Markov Affinity-based Graph Imputation of Cells (MAGIC) API :cite:p:`vanDijk2018`. MAGIC is an algorithm for denoising and transcript recover of single cells; applied to single-cell sequencing data. MAGIC builds a graph from the data; and uses diffusion to smooth out noise and recover the data manifold. The algorithm implemented here has changed primarily in two ways; compared to the algorithm described in :cite:t:`vanDijk2018`. Firstly, we use; the adaptive kernel described in :cite:t:`Moon2019` for; improved stability. Secondly, data diffusion is applied; in the PCA space, rather than the data space, for speed and; memory improvements. More information and bug reports; `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit; <https://krishnaswamylab.org/get-help>. Parameters; ----------; adata; An anndata file with `.raw` attribute representing raw counts.; name_list; Denoised genes to return. The default `'all_genes'`/`None`; may require a large amount of memory if the input data is sparse.; Another possibility is `'pca_only'`.; knn; number of nearest neighbors on which to build kernel.; decay; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used.; knn_max; maximum number of nearest neighbors with nonzero connection.; If `None`, will be set to 3 * `knn`.; t; power to which the diffusion operator is powered.; This sets the level of diffusion. If 'auto', t is selected; according to the Procrustes disparity of the diffused data.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; roughly log(n_samples) time. If `None`, no PCA is performed.; solver; Which solver to use. ""exact"" uses the implementation described; in :cite:t:`vanDijk2018`. ""approximate"" uses a faster; implementation that performs imputation in the PCA space and then; projects back to the gene s",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_magic.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_magic.py
Deployability,update,update,"on described; in :cite:t:`vanDijk2018`. ""approximate"" uses a faster; implementation that performs imputation in the PCA space and then; projects back to the gene space. Note, the ""approximate"" solver may; return negative values.; knn_dist; recommended values: 'euclidean', 'cosine', 'precomputed'; Any metric from `scipy.spatial.distance` can be used; distance metric for building kNN graph. If 'precomputed',; `data` should be an n_samples x n_samples distance or; affinity matrix.; random_state; Random seed. Defaults to the global `numpy` random number generator.; n_jobs; Number of threads to use in training. All cores are used by default.; verbose; If `True` or an integer `>= 2`, print status messages.; If `None`, `sc.settings.verbosity` is used.; copy; If true, a copy of anndata is returned. If `None`, `copy` is True if; `genes` is not `'all_genes'` or `'pca_only'`. `copy` may only be False; if `genes` is `'all_genes'` or `'pca_only'`, as the resultant data; will otherwise have different column names from the input data.; kwargs; Additional arguments to `magic.MAGIC`. Returns; -------; If `copy` is True, AnnData object is returned. If `subset_genes` is not `all_genes`, PCA on MAGIC values of cells are; stored in `adata.obsm['X_magic']` and `adata.X` is not modified. The raw counts are stored in `.raw` attribute of AnnData object. Examples; --------; >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.paul15(); >>> sc.pp.normalize_per_cell(adata); >>> sc.pp.sqrt(adata) # or sc.pp.log1p(adata); >>> adata_magic = sce.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], knn=5); >>> adata_magic.shape; (2730, 3); >>> sce.pp.magic(adata, name_list='pca_only', knn=5); >>> adata.obsm['X_magic'].shape; (2730, 100); >>> sce.pp.magic(adata, name_list='all_genes', knn=5); >>> adata.X.shape; (2730, 3451); """"""; # update AnnData instance; # special case – update adata.obsm with smoothed values; # just return X_magic; # replace data with smoothed data",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_magic.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_magic.py
Energy Efficiency,adapt,adaptive,"""""""\; Denoise high-dimensional data using MAGIC; """"""; """"""\; Markov Affinity-based Graph Imputation of Cells (MAGIC) API :cite:p:`vanDijk2018`. MAGIC is an algorithm for denoising and transcript recover of single cells; applied to single-cell sequencing data. MAGIC builds a graph from the data; and uses diffusion to smooth out noise and recover the data manifold. The algorithm implemented here has changed primarily in two ways; compared to the algorithm described in :cite:t:`vanDijk2018`. Firstly, we use; the adaptive kernel described in :cite:t:`Moon2019` for; improved stability. Secondly, data diffusion is applied; in the PCA space, rather than the data space, for speed and; memory improvements. More information and bug reports; `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit; <https://krishnaswamylab.org/get-help>. Parameters; ----------; adata; An anndata file with `.raw` attribute representing raw counts.; name_list; Denoised genes to return. The default `'all_genes'`/`None`; may require a large amount of memory if the input data is sparse.; Another possibility is `'pca_only'`.; knn; number of nearest neighbors on which to build kernel.; decay; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used.; knn_max; maximum number of nearest neighbors with nonzero connection.; If `None`, will be set to 3 * `knn`.; t; power to which the diffusion operator is powered.; This sets the level of diffusion. If 'auto', t is selected; according to the Procrustes disparity of the diffused data.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; roughly log(n_samples) time. If `None`, no PCA is performed.; solver; Which solver to use. ""exact"" uses the implementation described; in :cite:t:`vanDijk2018`. ""approximate"" uses a faster; implementation that performs imputation in the PCA space and then; projects back to the gene s",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_magic.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_magic.py
Integrability,message,messages,"tes disparity of the diffused data.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; roughly log(n_samples) time. If `None`, no PCA is performed.; solver; Which solver to use. ""exact"" uses the implementation described; in :cite:t:`vanDijk2018`. ""approximate"" uses a faster; implementation that performs imputation in the PCA space and then; projects back to the gene space. Note, the ""approximate"" solver may; return negative values.; knn_dist; recommended values: 'euclidean', 'cosine', 'precomputed'; Any metric from `scipy.spatial.distance` can be used; distance metric for building kNN graph. If 'precomputed',; `data` should be an n_samples x n_samples distance or; affinity matrix.; random_state; Random seed. Defaults to the global `numpy` random number generator.; n_jobs; Number of threads to use in training. All cores are used by default.; verbose; If `True` or an integer `>= 2`, print status messages.; If `None`, `sc.settings.verbosity` is used.; copy; If true, a copy of anndata is returned. If `None`, `copy` is True if; `genes` is not `'all_genes'` or `'pca_only'`. `copy` may only be False; if `genes` is `'all_genes'` or `'pca_only'`, as the resultant data; will otherwise have different column names from the input data.; kwargs; Additional arguments to `magic.MAGIC`. Returns; -------; If `copy` is True, AnnData object is returned. If `subset_genes` is not `all_genes`, PCA on MAGIC values of cells are; stored in `adata.obsm['X_magic']` and `adata.X` is not modified. The raw counts are stored in `.raw` attribute of AnnData object. Examples; --------; >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.paul15(); >>> sc.pp.normalize_per_cell(adata); >>> sc.pp.sqrt(adata) # or sc.pp.log1p(adata); >>> adata_magic = sce.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], knn=5); >>> adata_magic.shape; (2730, 3); >>> sce.pp.",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_magic.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_magic.py
Modifiability,adapt,adaptive,"""""""\; Denoise high-dimensional data using MAGIC; """"""; """"""\; Markov Affinity-based Graph Imputation of Cells (MAGIC) API :cite:p:`vanDijk2018`. MAGIC is an algorithm for denoising and transcript recover of single cells; applied to single-cell sequencing data. MAGIC builds a graph from the data; and uses diffusion to smooth out noise and recover the data manifold. The algorithm implemented here has changed primarily in two ways; compared to the algorithm described in :cite:t:`vanDijk2018`. Firstly, we use; the adaptive kernel described in :cite:t:`Moon2019` for; improved stability. Secondly, data diffusion is applied; in the PCA space, rather than the data space, for speed and; memory improvements. More information and bug reports; `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit; <https://krishnaswamylab.org/get-help>. Parameters; ----------; adata; An anndata file with `.raw` attribute representing raw counts.; name_list; Denoised genes to return. The default `'all_genes'`/`None`; may require a large amount of memory if the input data is sparse.; Another possibility is `'pca_only'`.; knn; number of nearest neighbors on which to build kernel.; decay; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used.; knn_max; maximum number of nearest neighbors with nonzero connection.; If `None`, will be set to 3 * `knn`.; t; power to which the diffusion operator is powered.; This sets the level of diffusion. If 'auto', t is selected; according to the Procrustes disparity of the diffused data.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; roughly log(n_samples) time. If `None`, no PCA is performed.; solver; Which solver to use. ""exact"" uses the implementation described; in :cite:t:`vanDijk2018`. ""approximate"" uses a faster; implementation that performs imputation in the PCA space and then; projects back to the gene s",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_magic.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_magic.py
Performance,perform,performed,"com/KrishnaswamyLab/MAGIC>`__. For help, visit; <https://krishnaswamylab.org/get-help>. Parameters; ----------; adata; An anndata file with `.raw` attribute representing raw counts.; name_list; Denoised genes to return. The default `'all_genes'`/`None`; may require a large amount of memory if the input data is sparse.; Another possibility is `'pca_only'`.; knn; number of nearest neighbors on which to build kernel.; decay; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used.; knn_max; maximum number of nearest neighbors with nonzero connection.; If `None`, will be set to 3 * `knn`.; t; power to which the diffusion operator is powered.; This sets the level of diffusion. If 'auto', t is selected; according to the Procrustes disparity of the diffused data.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; roughly log(n_samples) time. If `None`, no PCA is performed.; solver; Which solver to use. ""exact"" uses the implementation described; in :cite:t:`vanDijk2018`. ""approximate"" uses a faster; implementation that performs imputation in the PCA space and then; projects back to the gene space. Note, the ""approximate"" solver may; return negative values.; knn_dist; recommended values: 'euclidean', 'cosine', 'precomputed'; Any metric from `scipy.spatial.distance` can be used; distance metric for building kNN graph. If 'precomputed',; `data` should be an n_samples x n_samples distance or; affinity matrix.; random_state; Random seed. Defaults to the global `numpy` random number generator.; n_jobs; Number of threads to use in training. All cores are used by default.; verbose; If `True` or an integer `>= 2`, print status messages.; If `None`, `sc.settings.verbosity` is used.; copy; If true, a copy of anndata is returned. If `None`, `copy` is True if; `genes` is not `'all_genes'` or `'pca_only'`. `copy` may only be False; if `genes` is `'all_",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_magic.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_magic.py
Safety,recover,recover,"""""""\; Denoise high-dimensional data using MAGIC; """"""; """"""\; Markov Affinity-based Graph Imputation of Cells (MAGIC) API :cite:p:`vanDijk2018`. MAGIC is an algorithm for denoising and transcript recover of single cells; applied to single-cell sequencing data. MAGIC builds a graph from the data; and uses diffusion to smooth out noise and recover the data manifold. The algorithm implemented here has changed primarily in two ways; compared to the algorithm described in :cite:t:`vanDijk2018`. Firstly, we use; the adaptive kernel described in :cite:t:`Moon2019` for; improved stability. Secondly, data diffusion is applied; in the PCA space, rather than the data space, for speed and; memory improvements. More information and bug reports; `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit; <https://krishnaswamylab.org/get-help>. Parameters; ----------; adata; An anndata file with `.raw` attribute representing raw counts.; name_list; Denoised genes to return. The default `'all_genes'`/`None`; may require a large amount of memory if the input data is sparse.; Another possibility is `'pca_only'`.; knn; number of nearest neighbors on which to build kernel.; decay; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used.; knn_max; maximum number of nearest neighbors with nonzero connection.; If `None`, will be set to 3 * `knn`.; t; power to which the diffusion operator is powered.; This sets the level of diffusion. If 'auto', t is selected; according to the Procrustes disparity of the diffused data.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; roughly log(n_samples) time. If `None`, no PCA is performed.; solver; Which solver to use. ""exact"" uses the implementation described; in :cite:t:`vanDijk2018`. ""approximate"" uses a faster; implementation that performs imputation in the PCA space and then; projects back to the gene s",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_magic.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_magic.py
Testability,log,log,"emory improvements. More information and bug reports; `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit; <https://krishnaswamylab.org/get-help>. Parameters; ----------; adata; An anndata file with `.raw` attribute representing raw counts.; name_list; Denoised genes to return. The default `'all_genes'`/`None`; may require a large amount of memory if the input data is sparse.; Another possibility is `'pca_only'`.; knn; number of nearest neighbors on which to build kernel.; decay; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used.; knn_max; maximum number of nearest neighbors with nonzero connection.; If `None`, will be set to 3 * `knn`.; t; power to which the diffusion operator is powered.; This sets the level of diffusion. If 'auto', t is selected; according to the Procrustes disparity of the diffused data.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; roughly log(n_samples) time. If `None`, no PCA is performed.; solver; Which solver to use. ""exact"" uses the implementation described; in :cite:t:`vanDijk2018`. ""approximate"" uses a faster; implementation that performs imputation in the PCA space and then; projects back to the gene space. Note, the ""approximate"" solver may; return negative values.; knn_dist; recommended values: 'euclidean', 'cosine', 'precomputed'; Any metric from `scipy.spatial.distance` can be used; distance metric for building kNN graph. If 'precomputed',; `data` should be an n_samples x n_samples distance or; affinity matrix.; random_state; Random seed. Defaults to the global `numpy` random number generator.; n_jobs; Number of threads to use in training. All cores are used by default.; verbose; If `True` or an integer `>= 2`, print status messages.; If `None`, `sc.settings.verbosity` is used.; copy; If true, a copy of anndata is returned. If `None`, `copy` is True if; `genes` is not `'",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_magic.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_magic.py
Integrability,depend,depending,"pplying `AnnData` objects.; batch_categories; The `batch_categories` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying AnnData objects.; k; Number of mutual nearest neighbors.; sigma; The bandwidth of the Gaussian smoothing kernel used to compute the; correction vectors. Default is 1.; cos_norm_in; Whether cosine normalization should be performed on the input data prior; to calculating distances between cells.; cos_norm_out; Whether cosine normalization should be performed prior to computing corrected expression values.; svd_dim; The number of dimensions to use for summarizing biological substructure; within each batch. If None, biological components will not be removed; from the correction vectors.; var_adj; Whether to adjust variance of the correction vectors. Note this step; takes most computing time.; compute_angle; Whether to compute the angle between each cell’s correction vector and; the biological subspace of the reference batch.; mnn_order; The order in which batches are to be corrected. When set to None, datas; are corrected sequentially.; svd_mode; `'svd'` computes SVD using a non-randomized SVD-via-ID algorithm,; while `'rsvd'` uses a randomized version. `'irlb'` perfores; truncated SVD by implicitly restarted Lanczos bidiagonalization; (forked from https://github.com/airysen/irlbpy).; do_concatenate; Whether to concatenate the corrected matrices or AnnData objects. Default is True.; save_raw; Whether to save the original expression data in the; :attr:`~anndata.AnnData.raw` attribute.; n_jobs; The number of jobs. When set to `None`, automatically uses; :attr:`scanpy._settings.ScanpyConfig.n_jobs`.; kwargs; optional keyword arguments for irlb. Returns; -------; datas; Corrected matrix/matrices or AnnData object/objects, depending on the; input type and `do_concatenate`.; mnn_list; A list containing MNN pairing information as DataFrames in each iteration step.; angle_list; A list containing angles of each batch.; """"""",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_mnn_correct.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_mnn_correct.py
Modifiability,variab,variables,"""""""\; Correct batch effects by matching mutual nearest neighbors :cite:p:`Haghverdi2018` :cite:p:`Kang2018`. This uses the implementation of mnnpy_ :cite:p:`Kang2018`. Depending on `do_concatenate`, returns matrices or `AnnData` objects in the; original order containing corrected expression values or a concatenated; matrix or AnnData object. Be reminded that it is not advised to use the corrected data matrices for; differential expression testing. More information and bug reports `here <mnnpy>`__. .. _mnnpy: https://github.com/chriscainx/mnnpy. Parameters; ----------; datas; Expression matrices or AnnData objects. Matrices should be shaped like; n_obs × n_vars (n_cell × n_gene) and have consistent number of columns.; AnnData objects should have same number of variables.; var_index; The index (list of str) of vars (genes). Necessary when using only a; subset of vars to perform MNN correction, and should be supplied with; `var_subset`. When `datas` are AnnData objects, `var_index` is ignored.; var_subset; The subset of vars (list of str) to be used when performing MNN; correction. Typically, a list of highly variable genes (HVGs).; When set to `None`, uses all vars.; batch_key; The `batch_key` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying `AnnData` objects.; index_unique; The `index_unique` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying `AnnData` objects.; batch_categories; The `batch_categories` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying AnnData objects.; k; Number of mutual nearest neighbors.; sigma; The bandwidth of the Gaussian smoothing kernel used to compute the; correction vectors. Default is 1.; cos_norm_in; Whether cosine normalization should be performed on the input data prior; to calculating distances between cells.; cos_norm_out; Whether cosine normalization should be performed prior to computing corrected expression values.",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_mnn_correct.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_mnn_correct.py
Performance,perform,perform,"""""""\; Correct batch effects by matching mutual nearest neighbors :cite:p:`Haghverdi2018` :cite:p:`Kang2018`. This uses the implementation of mnnpy_ :cite:p:`Kang2018`. Depending on `do_concatenate`, returns matrices or `AnnData` objects in the; original order containing corrected expression values or a concatenated; matrix or AnnData object. Be reminded that it is not advised to use the corrected data matrices for; differential expression testing. More information and bug reports `here <mnnpy>`__. .. _mnnpy: https://github.com/chriscainx/mnnpy. Parameters; ----------; datas; Expression matrices or AnnData objects. Matrices should be shaped like; n_obs × n_vars (n_cell × n_gene) and have consistent number of columns.; AnnData objects should have same number of variables.; var_index; The index (list of str) of vars (genes). Necessary when using only a; subset of vars to perform MNN correction, and should be supplied with; `var_subset`. When `datas` are AnnData objects, `var_index` is ignored.; var_subset; The subset of vars (list of str) to be used when performing MNN; correction. Typically, a list of highly variable genes (HVGs).; When set to `None`, uses all vars.; batch_key; The `batch_key` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying `AnnData` objects.; index_unique; The `index_unique` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying `AnnData` objects.; batch_categories; The `batch_categories` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying AnnData objects.; k; Number of mutual nearest neighbors.; sigma; The bandwidth of the Gaussian smoothing kernel used to compute the; correction vectors. Default is 1.; cos_norm_in; Whether cosine normalization should be performed on the input data prior; to calculating distances between cells.; cos_norm_out; Whether cosine normalization should be performed prior to computing corrected expression values.",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_mnn_correct.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_mnn_correct.py
Testability,test,testing,"""""""\; Correct batch effects by matching mutual nearest neighbors :cite:p:`Haghverdi2018` :cite:p:`Kang2018`. This uses the implementation of mnnpy_ :cite:p:`Kang2018`. Depending on `do_concatenate`, returns matrices or `AnnData` objects in the; original order containing corrected expression values or a concatenated; matrix or AnnData object. Be reminded that it is not advised to use the corrected data matrices for; differential expression testing. More information and bug reports `here <mnnpy>`__. .. _mnnpy: https://github.com/chriscainx/mnnpy. Parameters; ----------; datas; Expression matrices or AnnData objects. Matrices should be shaped like; n_obs × n_vars (n_cell × n_gene) and have consistent number of columns.; AnnData objects should have same number of variables.; var_index; The index (list of str) of vars (genes). Necessary when using only a; subset of vars to perform MNN correction, and should be supplied with; `var_subset`. When `datas` are AnnData objects, `var_index` is ignored.; var_subset; The subset of vars (list of str) to be used when performing MNN; correction. Typically, a list of highly variable genes (HVGs).; When set to `None`, uses all vars.; batch_key; The `batch_key` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying `AnnData` objects.; index_unique; The `index_unique` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying `AnnData` objects.; batch_categories; The `batch_categories` for :meth:`~anndata.AnnData.concatenate`.; Only valid when `do_concatenate` and supplying AnnData objects.; k; Number of mutual nearest neighbors.; sigma; The bandwidth of the Gaussian smoothing kernel used to compute the; correction vectors. Default is 1.; cos_norm_in; Whether cosine normalization should be performed on the input data prior; to calculating distances between cells.; cos_norm_out; Whether cosine normalization should be performed prior to computing corrected expression values.",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_mnn_correct.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_mnn_correct.py
Availability,avail,available," _scanorama: https://github.com/brianhie/scanorama. Parameters; ----------; adata; The annotated data matrix.; key; The name of the column in ``adata.obs`` that differentiates; among experiments/batches. Cells from the same batch must be; contiguously stored in ``adata``.; basis; The name of the field in ``adata.obsm`` where the PCA table is; stored. Defaults to ``'X_pca'``, which is the default for; ``sc.pp.pca()``.; adjusted_basis; The name of the field in ``adata.obsm`` where the integrated; embeddings will be stored after running this function. Defaults; to ``X_scanorama``.; knn; Number of nearest neighbors to use for matching.; sigma; Correction smoothing parameter on Gaussian kernel.; approx; Use approximate nearest neighbors with Python ``annoy``;; greatly speeds up matching runtime.; alpha; Alignment score minimum cutoff.; batch_size; The batch size used in the alignment vector computation. Useful; when integrating very large (>100k samples) datasets. Set to; large value that runs within available memory.; kwargs; Any additional arguments will be passed to; ``scanorama.assemble()``. Returns; -------; Updates adata with the field ``adata.obsm[adjusted_basis]``,; containing Scanorama embeddings such that different experiments; are integrated. Example; -------; First, load libraries and example dataset, and preprocess. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.recipe_zheng17(adata); >>> sc.pp.pca(adata). We now arbitrarily assign a batch metadata variable to each cell; for the sake of example, but during real usage there would already; be a column in ``adata.obs`` giving the experiment each cell came; from. >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']. Finally, run Scanorama. Afterwards, there will be a new table in; ``adata.obsm`` containing the Scanorama embeddings. >>> sce.pp.scanorama_integrate(adata, 'batch', verbose=1); Processing datasets a <=> b; >>> 'X_scanorama' in adata.obsm; True; """""";",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_scanorama_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_scanorama_integrate.py
Deployability,integrat,integrate,"""""""; Use Scanorama to integrate cells from different experiments.; """"""; """"""\; Use Scanorama :cite:p:`Hie2019` to integrate different experiments. Scanorama :cite:p:`Hie2019` is an algorithm for integrating single-cell; data from multiple experiments stored in an AnnData object. This; function should be run after performing PCA but before computing; the neighbor graph, as illustrated in the example below. This uses the implementation of scanorama_ :cite:p:`Hie2019`. .. _scanorama: https://github.com/brianhie/scanorama. Parameters; ----------; adata; The annotated data matrix.; key; The name of the column in ``adata.obs`` that differentiates; among experiments/batches. Cells from the same batch must be; contiguously stored in ``adata``.; basis; The name of the field in ``adata.obsm`` where the PCA table is; stored. Defaults to ``'X_pca'``, which is the default for; ``sc.pp.pca()``.; adjusted_basis; The name of the field in ``adata.obsm`` where the integrated; embeddings will be stored after running this function. Defaults; to ``X_scanorama``.; knn; Number of nearest neighbors to use for matching.; sigma; Correction smoothing parameter on Gaussian kernel.; approx; Use approximate nearest neighbors with Python ``annoy``;; greatly speeds up matching runtime.; alpha; Alignment score minimum cutoff.; batch_size; The batch size used in the alignment vector computation. Useful; when integrating very large (>100k samples) datasets. Set to; large value that runs within available memory.; kwargs; Any additional arguments will be passed to; ``scanorama.assemble()``. Returns; -------; Updates adata with the field ``adata.obsm[adjusted_basis]``,; containing Scanorama embeddings such that different experiments; are integrated. Example; -------; First, load libraries and example dataset, and preprocess. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.recipe_zheng17(adata); >>> sc.pp.pca(adata). We now arbitrarily assign a batch ",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_scanorama_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_scanorama_integrate.py
Integrability,integrat,integrate,"""""""; Use Scanorama to integrate cells from different experiments.; """"""; """"""\; Use Scanorama :cite:p:`Hie2019` to integrate different experiments. Scanorama :cite:p:`Hie2019` is an algorithm for integrating single-cell; data from multiple experiments stored in an AnnData object. This; function should be run after performing PCA but before computing; the neighbor graph, as illustrated in the example below. This uses the implementation of scanorama_ :cite:p:`Hie2019`. .. _scanorama: https://github.com/brianhie/scanorama. Parameters; ----------; adata; The annotated data matrix.; key; The name of the column in ``adata.obs`` that differentiates; among experiments/batches. Cells from the same batch must be; contiguously stored in ``adata``.; basis; The name of the field in ``adata.obsm`` where the PCA table is; stored. Defaults to ``'X_pca'``, which is the default for; ``sc.pp.pca()``.; adjusted_basis; The name of the field in ``adata.obsm`` where the integrated; embeddings will be stored after running this function. Defaults; to ``X_scanorama``.; knn; Number of nearest neighbors to use for matching.; sigma; Correction smoothing parameter on Gaussian kernel.; approx; Use approximate nearest neighbors with Python ``annoy``;; greatly speeds up matching runtime.; alpha; Alignment score minimum cutoff.; batch_size; The batch size used in the alignment vector computation. Useful; when integrating very large (>100k samples) datasets. Set to; large value that runs within available memory.; kwargs; Any additional arguments will be passed to; ``scanorama.assemble()``. Returns; -------; Updates adata with the field ``adata.obsm[adjusted_basis]``,; containing Scanorama embeddings such that different experiments; are integrated. Example; -------; First, load libraries and example dataset, and preprocess. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.recipe_zheng17(adata); >>> sc.pp.pca(adata). We now arbitrarily assign a batch ",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_scanorama_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_scanorama_integrate.py
Modifiability,variab,variable,"/batches. Cells from the same batch must be; contiguously stored in ``adata``.; basis; The name of the field in ``adata.obsm`` where the PCA table is; stored. Defaults to ``'X_pca'``, which is the default for; ``sc.pp.pca()``.; adjusted_basis; The name of the field in ``adata.obsm`` where the integrated; embeddings will be stored after running this function. Defaults; to ``X_scanorama``.; knn; Number of nearest neighbors to use for matching.; sigma; Correction smoothing parameter on Gaussian kernel.; approx; Use approximate nearest neighbors with Python ``annoy``;; greatly speeds up matching runtime.; alpha; Alignment score minimum cutoff.; batch_size; The batch size used in the alignment vector computation. Useful; when integrating very large (>100k samples) datasets. Set to; large value that runs within available memory.; kwargs; Any additional arguments will be passed to; ``scanorama.assemble()``. Returns; -------; Updates adata with the field ``adata.obsm[adjusted_basis]``,; containing Scanorama embeddings such that different experiments; are integrated. Example; -------; First, load libraries and example dataset, and preprocess. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.recipe_zheng17(adata); >>> sc.pp.pca(adata). We now arbitrarily assign a batch metadata variable to each cell; for the sake of example, but during real usage there would already; be a column in ``adata.obs`` giving the experiment each cell came; from. >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']. Finally, run Scanorama. Afterwards, there will be a new table in; ``adata.obsm`` containing the Scanorama embeddings. >>> sce.pp.scanorama_integrate(adata, 'batch', verbose=1); Processing datasets a <=> b; >>> 'X_scanorama' in adata.obsm; True; """"""; # Get batch indices in linear time.; # Contiguous batches important for preserving cell order.; # Preserve name order.; # Separate batches.; # Integrate.; # Assemble in low dimensional space.",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_scanorama_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_scanorama_integrate.py
Performance,perform,performing,"""""""; Use Scanorama to integrate cells from different experiments.; """"""; """"""\; Use Scanorama :cite:p:`Hie2019` to integrate different experiments. Scanorama :cite:p:`Hie2019` is an algorithm for integrating single-cell; data from multiple experiments stored in an AnnData object. This; function should be run after performing PCA but before computing; the neighbor graph, as illustrated in the example below. This uses the implementation of scanorama_ :cite:p:`Hie2019`. .. _scanorama: https://github.com/brianhie/scanorama. Parameters; ----------; adata; The annotated data matrix.; key; The name of the column in ``adata.obs`` that differentiates; among experiments/batches. Cells from the same batch must be; contiguously stored in ``adata``.; basis; The name of the field in ``adata.obsm`` where the PCA table is; stored. Defaults to ``'X_pca'``, which is the default for; ``sc.pp.pca()``.; adjusted_basis; The name of the field in ``adata.obsm`` where the integrated; embeddings will be stored after running this function. Defaults; to ``X_scanorama``.; knn; Number of nearest neighbors to use for matching.; sigma; Correction smoothing parameter on Gaussian kernel.; approx; Use approximate nearest neighbors with Python ``annoy``;; greatly speeds up matching runtime.; alpha; Alignment score minimum cutoff.; batch_size; The batch size used in the alignment vector computation. Useful; when integrating very large (>100k samples) datasets. Set to; large value that runs within available memory.; kwargs; Any additional arguments will be passed to; ``scanorama.assemble()``. Returns; -------; Updates adata with the field ``adata.obsm[adjusted_basis]``,; containing Scanorama embeddings such that different experiments; are integrated. Example; -------; First, load libraries and example dataset, and preprocess. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.recipe_zheng17(adata); >>> sc.pp.pca(adata). We now arbitrarily assign a batch ",MatchSource.CODE_COMMENT,src/scanpy/external/pp/_scanorama_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/pp/_scanorama_integrate.py
Availability,avail,available,"uted in parallel using n_jobs.; copy; Return a copy instead of writing to `adata`. Returns; -------; Depending on `copy`, returns or updates `.obsm`, `.obsp` and `.uns` with the following:. **X_harmony** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); force directed layout; **harmony_aff** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); affinity matrix; **harmony_aff_aug** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); augmented affinity matrix; **harmony_timepoint_var** - `str` (:attr:`~anndata.AnnData.uns`); The name of the variable passed as `tp`; **harmony_timepoint_connections** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `str`); The links between time points. Example; -------. >>> from itertools import product; >>> import pandas as pd; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import scanpy.external as sce. **Load** `AnnData`. A sample with real data is available here_. .. _here: https://github.com/dpeerlab/Harmony/tree/master/data. Random data sets of three time points with two replicates each:. >>> adata_ref = sc.datasets.pbmc3k(); >>> start = [596, 615, 1682, 1663, 1409, 1432]; >>> adata = AnnData.concatenate(; ... *(adata_ref[i : i + 1000] for i in start),; ... join=""outer"",; ... batch_key=""sample"",; ... batch_categories=[f""sa{i}_Rep{j}"" for i, j in product((1, 2, 3), (1, 2))],; ... ); >>> time_points = adata.obs[""sample""].str.split(""_"", expand=True)[0]; >>> adata.obs[""time_points""] = pd.Categorical(; ... time_points, categories=['sa1', 'sa2', 'sa3']; ... ). Normalize and filter for highly expressed genes. >>> sc.pp.normalize_total(adata, target_sum=10000); >>> sc.pp.log1p(adata); >>> sc.pp.highly_variable_genes(adata, n_top_genes=1000, subset=True). Run harmony_timeseries. >>> sce.tl.harmony_timeseries(adata, tp=""time_points"", n_components=500). Plot time points:. >>> sce.pl.harmony_timeseries(adata). For further demonstrat",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_harmony_timeseries.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_harmony_timeseries.py
Deployability,update,updates,"ps://github.com/dpeerlab/Palantir. .. note::; More information and bug reports `here; <https://github.com/dpeerlab/Harmony>`__. Parameters; ----------; adata; Annotated data matrix of shape n_obs `×` n_vars. Rows correspond to; cells and columns to genes. Rows represent two or more time points,; where replicates of the same time point are consecutive in order.; tp; key name of observation annotation `.obs` representing time points. Time; points should be categorical of `dtype=category`. The unique categories for; the categorical will be used as the time points to construct the timepoint; connections.; n_neighbors; Number of nearest neighbors for graph construction.; n_components; Minimum number of principal components to use. Specify `None` to use; pre-computed components. The higher the value the better to capture 85% of the; variance.; n_jobs; Nearest Neighbors will be computed in parallel using n_jobs.; copy; Return a copy instead of writing to `adata`. Returns; -------; Depending on `copy`, returns or updates `.obsm`, `.obsp` and `.uns` with the following:. **X_harmony** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); force directed layout; **harmony_aff** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); affinity matrix; **harmony_aff_aug** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); augmented affinity matrix; **harmony_timepoint_var** - `str` (:attr:`~anndata.AnnData.uns`); The name of the variable passed as `tp`; **harmony_timepoint_connections** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `str`); The links between time points. Example; -------. >>> from itertools import product; >>> import pandas as pd; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import scanpy.external as sce. **Load** `AnnData`. A sample with real data is available here_. .. _here: https://github.com/dpeerlab/Harmony/tree/master/data. Random data sets of three ",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_harmony_timeseries.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_harmony_timeseries.py
Modifiability,variab,variable,"e used as the time points to construct the timepoint; connections.; n_neighbors; Number of nearest neighbors for graph construction.; n_components; Minimum number of principal components to use. Specify `None` to use; pre-computed components. The higher the value the better to capture 85% of the; variance.; n_jobs; Nearest Neighbors will be computed in parallel using n_jobs.; copy; Return a copy instead of writing to `adata`. Returns; -------; Depending on `copy`, returns or updates `.obsm`, `.obsp` and `.uns` with the following:. **X_harmony** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); force directed layout; **harmony_aff** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); affinity matrix; **harmony_aff_aug** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); augmented affinity matrix; **harmony_timepoint_var** - `str` (:attr:`~anndata.AnnData.uns`); The name of the variable passed as `tp`; **harmony_timepoint_connections** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `str`); The links between time points. Example; -------. >>> from itertools import product; >>> import pandas as pd; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import scanpy.external as sce. **Load** `AnnData`. A sample with real data is available here_. .. _here: https://github.com/dpeerlab/Harmony/tree/master/data. Random data sets of three time points with two replicates each:. >>> adata_ref = sc.datasets.pbmc3k(); >>> start = [596, 615, 1682, 1663, 1409, 1432]; >>> adata = AnnData.concatenate(; ... *(adata_ref[i : i + 1000] for i in start),; ... join=""outer"",; ... batch_key=""sample"",; ... batch_categories=[f""sa{i}_Rep{j}"" for i, j in product((1, 2, 3), (1, 2))],; ... ); >>> time_points = adata.obs[""sample""].str.split(""_"", expand=True)[0]; >>> adata.obs[""time_points""] = pd.Categorical(; ... time_points, categories=['sa1', 'sa2', 'sa3']; ... ). Normalize and filter for hi",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_harmony_timeseries.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_harmony_timeseries.py
Safety,detect,detection,"""""""\; Harmony time series for data visualization with augmented affinity matrix at; discrete time points; """"""; """"""\; Harmony time series for data visualization with augmented affinity matrix; at discrete time points :cite:p:`Nowotschin2019`. Harmony time series is a framework for data visualization, trajectory; detection and interpretation for scRNA-seq data measured at discrete; time points. Harmony constructs an augmented affinity matrix by augmenting; the kNN graph affinity matrix with mutually nearest neighbors between; successive time points. This augmented affinity matrix forms the basis for; generated a force directed layout for visualization and also serves as input; for computing the diffusion operator which can be used for trajectory; detection using Palantir_. .. _Palantir: https://github.com/dpeerlab/Palantir. .. note::; More information and bug reports `here; <https://github.com/dpeerlab/Harmony>`__. Parameters; ----------; adata; Annotated data matrix of shape n_obs `×` n_vars. Rows correspond to; cells and columns to genes. Rows represent two or more time points,; where replicates of the same time point are consecutive in order.; tp; key name of observation annotation `.obs` representing time points. Time; points should be categorical of `dtype=category`. The unique categories for; the categorical will be used as the time points to construct the timepoint; connections.; n_neighbors; Number of nearest neighbors for graph construction.; n_components; Minimum number of principal components to use. Specify `None` to use; pre-computed components. The higher the value the better to capture 85% of the; variance.; n_jobs; Nearest Neighbors will be computed in parallel using n_jobs.; copy; Return a copy instead of writing to `adata`. Returns; -------; Depending on `copy`, returns or updates `.obsm`, `.obsp` and `.uns` with the following:. **X_harmony** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); force directed layout; **harmony_aff",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_harmony_timeseries.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_harmony_timeseries.py
Usability,guid,guide,"nnData.obsp`, dtype `float`); affinity matrix; **harmony_aff_aug** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); augmented affinity matrix; **harmony_timepoint_var** - `str` (:attr:`~anndata.AnnData.uns`); The name of the variable passed as `tp`; **harmony_timepoint_connections** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `str`); The links between time points. Example; -------. >>> from itertools import product; >>> import pandas as pd; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import scanpy.external as sce. **Load** `AnnData`. A sample with real data is available here_. .. _here: https://github.com/dpeerlab/Harmony/tree/master/data. Random data sets of three time points with two replicates each:. >>> adata_ref = sc.datasets.pbmc3k(); >>> start = [596, 615, 1682, 1663, 1409, 1432]; >>> adata = AnnData.concatenate(; ... *(adata_ref[i : i + 1000] for i in start),; ... join=""outer"",; ... batch_key=""sample"",; ... batch_categories=[f""sa{i}_Rep{j}"" for i, j in product((1, 2, 3), (1, 2))],; ... ); >>> time_points = adata.obs[""sample""].str.split(""_"", expand=True)[0]; >>> adata.obs[""time_points""] = pd.Categorical(; ... time_points, categories=['sa1', 'sa2', 'sa3']; ... ). Normalize and filter for highly expressed genes. >>> sc.pp.normalize_total(adata, target_sum=10000); >>> sc.pp.log1p(adata); >>> sc.pp.highly_variable_genes(adata, n_top_genes=1000, subset=True). Run harmony_timeseries. >>> sce.tl.harmony_timeseries(adata, tp=""time_points"", n_components=500). Plot time points:. >>> sce.pl.harmony_timeseries(adata). For further demonstration of Harmony visualizations please follow the notebook; `Harmony_sample_notebook.ipynb; <https://github.com/dpeerlab/Harmony/blob/master/notebooks/; Harmony_sample_notebook.ipynb>`_.; It provides a comprehensive guide to draw *gene expression trends*,; amongst other things.; """"""; # compute the augmented and non-augmented affinity matrices; # Force directed layouts",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_harmony_timeseries.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_harmony_timeseries.py
Availability,avail,available,"ulti-scale data matrix,. - X_palantir_diff_comp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); Array of Diffusion components.; - palantir_EigenValues - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `float`); Array of corresponding eigen values.; - palantir_diff_op - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); The diffusion operator matrix. **Multi scale space results**,; used to build tsne on diffusion components, and to compute branch probabilities; and waypoints,. - X_palantir_multiscale - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); Multi scale data matrix. **MAGIC imputation**,; used for plotting gene expression on tsne, and gene expression trends,. - palantir_imp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.layers`, dtype `float`); Imputed data matrix (MAGIC imputation). Example; -------; >>> import scanpy.external as sce; >>> import scanpy as sc. A sample data is available `here <https://github.com/dpeerlab/Palantir/tree/master/data>`_. **Load sample data**. >>> adata = sc.read_csv(filename=""Palantir/data/marrow_sample_scseq_counts.csv.gz""). *Cleanup and normalize*. >>> sc.pp.filter_cells(adata, min_counts=1000); >>> sc.pp.filter_genes(adata, min_counts=10); >>> sc.pp.normalize_per_cell(adata); >>> sc.pp.log1p(adata). **Data preprocessing**. Palantir builds diffusion maps using one of two optional inputs:. *Principal component analysis*. >>> sc.pp.pca(adata, n_comps=300). or,. *Nearist neighbors graph*. >>> sc.pp.neighbors(adata, knn=30). *Diffusion maps*. Palantir determines the diffusion maps of the data as an estimate of the low; dimensional phenotypic manifold of the data. >>> sce.tl.palantir(adata, n_components=5, knn=30). if pre-computed distances are to be used,. >>> sce.tl.palantir(; ... adata,; ... n_components=5,; ... knn=30,; ... use_adjacency_matrix=True,; ... distances_key=""distances"",; ... ). **Visualizing Palantir results**. *tSNE visua",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_palantir.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_palantir.py
Deployability,update,updates," RNA-seq. .. note::; More information and bug reports `here <https://github.com/dpeerlab/Palantir>`__. Parameters; ----------; adata; An AnnData object.; n_components; Number of diffusion components.; knn; Number of nearest neighbors for graph construction.; alpha; Normalization parameter for the diffusion operator.; use_adjacency_matrix; Use adaptive anisotropic adjacency matrix, instead of PCA projections; (default) to compute diffusion components.; distances_key; With `use_adjacency_matrix=True`, use the indicated distances key for `.obsp`.; If `None`, `'distances'`.; n_eigs; Number of eigen vectors to use. If `None` specified, the number of eigen; vectors will be determined using eigen gap. Passed to; `palantir.utils.determine_multiscale_space`.; impute_data; Impute data using MAGIC.; n_steps; Number of steps in the diffusion operator. Passed to; `palantir.utils.run_magic_imputation`.; copy; Return a copy instead of writing to `adata`. Returns; -------; Depending on `copy`, returns or updates `adata` with the following fields:. **Diffusion maps**,; used for magic imputation, and to generate multi-scale data matrix,. - X_palantir_diff_comp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); Array of Diffusion components.; - palantir_EigenValues - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `float`); Array of corresponding eigen values.; - palantir_diff_op - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); The diffusion operator matrix. **Multi scale space results**,; used to build tsne on diffusion components, and to compute branch probabilities; and waypoints,. - X_palantir_multiscale - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); Multi scale data matrix. **MAGIC imputation**,; used for plotting gene expression on tsne, and gene expression trends,. - palantir_imp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.layers`, dtype `float`); Imputed data matrix (MAGIC im",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_palantir.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_palantir.py
Energy Efficiency,adapt,adaptive,"""""""\; Run Diffusion maps using the adaptive anisotropic kernel; """"""; """"""\; Run Diffusion maps using the adaptive anisotropic kernel :cite:p:`Setty2019`. Palantir is an algorithm to align cells along differentiation trajectories.; Palantir models differentiation as a stochastic process where stem cells; differentiate to terminally differentiated cells by a series of steps through; a low dimensional phenotypic manifold. Palantir effectively captures the; continuity in cell states and the stochasticity in cell fate determination.; Palantir has been designed to work with multidimensional single cell data; from diverse technologies such as Mass cytometry and single cell RNA-seq. .. note::; More information and bug reports `here <https://github.com/dpeerlab/Palantir>`__. Parameters; ----------; adata; An AnnData object.; n_components; Number of diffusion components.; knn; Number of nearest neighbors for graph construction.; alpha; Normalization parameter for the diffusion operator.; use_adjacency_matrix; Use adaptive anisotropic adjacency matrix, instead of PCA projections; (default) to compute diffusion components.; distances_key; With `use_adjacency_matrix=True`, use the indicated distances key for `.obsp`.; If `None`, `'distances'`.; n_eigs; Number of eigen vectors to use. If `None` specified, the number of eigen; vectors will be determined using eigen gap. Passed to; `palantir.utils.determine_multiscale_space`.; impute_data; Impute data using MAGIC.; n_steps; Number of steps in the diffusion operator. Passed to; `palantir.utils.run_magic_imputation`.; copy; Return a copy instead of writing to `adata`. Returns; -------; Depending on `copy`, returns or updates `adata` with the following fields:. **Diffusion maps**,; used for magic imputation, and to generate multi-scale data matrix,. - X_palantir_diff_comp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); Array of Diffusion components.; - palantir_EigenValues - :class:`~numpy.ndarray` (:attr:`~ann",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_palantir.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_palantir.py
Integrability,wrap,wraps,"MPO', 'GATA1', 'IRF8']; ... ). **Running Palantir**. Palantir can be run by specifying an approximate early cell. While Palantir; automatically determines the terminal states, they can also be specified using the; `termine_states` parameter. >>> start_cell = 'Run5_164698952452459'; >>> pr_res = sce.tl.palantir_results(; ... adata,; ... early_cell=start_cell,; ... ms_data='X_palantir_multiscale',; ... num_waypoints=500,; ... ). .. note::; A `start_cell` must be defined for every data set. The start cell for; this dataset was chosen based on high expression of CD34. At this point the returned Palantir object `pr_res` can be used for all downstream; analysis and plotting. Please consult this notebook; `Palantir_sample_notebook.ipynb; <https://github.com/dpeerlab/Palantir/blob/master/notebooks/Palantir_sample_notebook.ipynb>`_.; It provides a comprehensive guide to draw *gene expression trends*, amongst other; things.; """"""; # Diffusion maps; # Determine the multi scale space of the data; # MAGIC imputation; """"""\; **Running Palantir**. A convenience function that wraps `palantir.core.run_palantir` to compute branch; probabilities and waypoints. Parameters; ----------; adata; An AnnData object.; early_cell; Start cell for pseudotime construction.; ms_data; Palantir multi scale data matrix,; terminal_states; List of user defined terminal states; knn; Number of nearest neighbors for graph construction.; num_waypoints; Number of waypoints to sample.; n_jobs; Number of jobs for parallel processing.; scale_components; Transform features by scaling each feature to a given range. Consult the; documentation for `sklearn.preprocessing.minmax_scale`.; use_early_cell_as_start; Use `early_cell` as `start_cell`, instead of determining it from the boundary; cells closest to the defined `early_cell`.; max_iterations; Maximum number of iterations for pseudotime convergence. Returns; -------; PResults object with pseudotime, entropy, branch probabilities and waypoints.; """"""; # noqa: F401",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_palantir.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_palantir.py
Modifiability,adapt,adaptive,"""""""\; Run Diffusion maps using the adaptive anisotropic kernel; """"""; """"""\; Run Diffusion maps using the adaptive anisotropic kernel :cite:p:`Setty2019`. Palantir is an algorithm to align cells along differentiation trajectories.; Palantir models differentiation as a stochastic process where stem cells; differentiate to terminally differentiated cells by a series of steps through; a low dimensional phenotypic manifold. Palantir effectively captures the; continuity in cell states and the stochasticity in cell fate determination.; Palantir has been designed to work with multidimensional single cell data; from diverse technologies such as Mass cytometry and single cell RNA-seq. .. note::; More information and bug reports `here <https://github.com/dpeerlab/Palantir>`__. Parameters; ----------; adata; An AnnData object.; n_components; Number of diffusion components.; knn; Number of nearest neighbors for graph construction.; alpha; Normalization parameter for the diffusion operator.; use_adjacency_matrix; Use adaptive anisotropic adjacency matrix, instead of PCA projections; (default) to compute diffusion components.; distances_key; With `use_adjacency_matrix=True`, use the indicated distances key for `.obsp`.; If `None`, `'distances'`.; n_eigs; Number of eigen vectors to use. If `None` specified, the number of eigen; vectors will be determined using eigen gap. Passed to; `palantir.utils.determine_multiscale_space`.; impute_data; Impute data using MAGIC.; n_steps; Number of steps in the diffusion operator. Passed to; `palantir.utils.run_magic_imputation`.; copy; Return a copy instead of writing to `adata`. Returns; -------; Depending on `copy`, returns or updates `adata` with the following fields:. **Diffusion maps**,; used for magic imputation, and to generate multi-scale data matrix,. - X_palantir_diff_comp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`); Array of Diffusion components.; - palantir_EigenValues - :class:`~numpy.ndarray` (:attr:`~ann",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_palantir.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_palantir.py
Usability,guid,guide,".tsne(; ... adata,; ... gene_symbols=['CD34', 'MPO', 'GATA1', 'IRF8'],; ... layer='palantir_imp',; ... color=['CD34', 'MPO', 'GATA1', 'IRF8']; ... ). **Running Palantir**. Palantir can be run by specifying an approximate early cell. While Palantir; automatically determines the terminal states, they can also be specified using the; `termine_states` parameter. >>> start_cell = 'Run5_164698952452459'; >>> pr_res = sce.tl.palantir_results(; ... adata,; ... early_cell=start_cell,; ... ms_data='X_palantir_multiscale',; ... num_waypoints=500,; ... ). .. note::; A `start_cell` must be defined for every data set. The start cell for; this dataset was chosen based on high expression of CD34. At this point the returned Palantir object `pr_res` can be used for all downstream; analysis and plotting. Please consult this notebook; `Palantir_sample_notebook.ipynb; <https://github.com/dpeerlab/Palantir/blob/master/notebooks/Palantir_sample_notebook.ipynb>`_.; It provides a comprehensive guide to draw *gene expression trends*, amongst other; things.; """"""; # Diffusion maps; # Determine the multi scale space of the data; # MAGIC imputation; """"""\; **Running Palantir**. A convenience function that wraps `palantir.core.run_palantir` to compute branch; probabilities and waypoints. Parameters; ----------; adata; An AnnData object.; early_cell; Start cell for pseudotime construction.; ms_data; Palantir multi scale data matrix,; terminal_states; List of user defined terminal states; knn; Number of nearest neighbors for graph construction.; num_waypoints; Number of waypoints to sample.; n_jobs; Number of jobs for parallel processing.; scale_components; Transform features by scaling each feature to a given range. Consult the; documentation for `sklearn.preprocessing.minmax_scale`.; use_early_cell_as_start; Use `early_cell` as `start_cell`, instead of determining it from the boundary; cells closest to the defined `early_cell`.; max_iterations; Maximum number of iterations for pseudotime convergen",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_palantir.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_palantir.py
Deployability,update,updates,"r; gamma; Informational distance constant between -1 and 1.; `gamma=1` gives the PHATE log potential, `gamma=0` gives; a square root potential.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; log(n_samples) time.; knn_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for building kNN graph; mds_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for MDS; mds; Selects which MDS algorithm is used for dimensionality reduction.; n_jobs; The number of jobs to use for the computation.; If `None`, `sc.settings.n_jobs` is used.; If -1 all CPUs are used. If 1 is given, no parallel computing code is; used at all, which is useful for debugging.; For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for; n_jobs = -2, all CPUs but one are used; random_state; Random seed. Defaults to the global `numpy` random number generator; verbose; If `True` or an `int`/`Verbosity` ≥ 2/`hint`, print status messages.; If `None`, `sc.settings.verbosity` is used.; copy; Return a copy instead of writing to `adata`.; kwargs; Additional arguments to `phate.PHATE`. Returns; -------; Depending on `copy`, returns or updates `adata` with the following fields. **X_phate** : `np.ndarray`, (`adata.obs`, shape=[n_samples, n_components], dtype `float`); PHATE coordinates of data. Examples; --------; >>> from anndata import AnnData; >>> import scanpy.external as sce; >>> import phate; >>> tree_data, tree_clusters = phate.tree.gen_dla(; ... n_dim=100,; ... n_branch=20,; ... branch_length=100,; ... ); >>> tree_data.shape; (2000, 100); >>> adata = AnnData(tree_data); >>> sce.tl.phate(adata, k=5, a=20, t=150); >>> adata.obsm['X_phate'].shape; (2000, 2); >>> sce.pl.phate(adata); """"""; # update AnnData instance; # annotate samples with PHATE coordinates",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phate.py
Energy Efficiency,power,power,"""""""\; Embed high-dimensional data using PHATE; """"""; """"""\; PHATE :cite:p:`Moon2019`. Potential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE); embeds high dimensional single-cell data into two or three dimensions for; visualization of biological progressions. For more information and access to the object-oriented interface, read the; `PHATE documentation <https://phate.readthedocs.io/>`__. For; tutorials, bug reports, and R/MATLAB implementations, visit the `PHATE; GitHub page <https://github.com/KrishnaswamyLab/PHATE/>`__. For help; using PHATE, go `here <https://krishnaswamylab.org/get-help>`__. Parameters; ----------; adata; Annotated data matrix.; n_components; number of dimensions in which the data will be embedded; k; number of nearest neighbors on which to build kernel; a; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used; n_landmark; number of landmarks to use in fast PHATE; t; power to which the diffusion operator is powered; sets the level of diffusion. If 'auto', t is selected; according to the knee point in the Von Neumann Entropy of; the diffusion operator; gamma; Informational distance constant between -1 and 1.; `gamma=1` gives the PHATE log potential, `gamma=0` gives; a square root potential.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; log(n_samples) time.; knn_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for building kNN graph; mds_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for MDS; mds; Selects which MDS algorithm is used for dimensionality reduction.; n_jobs; The number of jobs to use for the computation.; If `None`, `sc.settings.n_jobs` is used.; If -1 all CPUs are used. If 1 is given, no parallel computing code is; used at all,",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phate.py
Integrability,interface,interface,"""""""\; Embed high-dimensional data using PHATE; """"""; """"""\; PHATE :cite:p:`Moon2019`. Potential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE); embeds high dimensional single-cell data into two or three dimensions for; visualization of biological progressions. For more information and access to the object-oriented interface, read the; `PHATE documentation <https://phate.readthedocs.io/>`__. For; tutorials, bug reports, and R/MATLAB implementations, visit the `PHATE; GitHub page <https://github.com/KrishnaswamyLab/PHATE/>`__. For help; using PHATE, go `here <https://krishnaswamylab.org/get-help>`__. Parameters; ----------; adata; Annotated data matrix.; n_components; number of dimensions in which the data will be embedded; k; number of nearest neighbors on which to build kernel; a; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used; n_landmark; number of landmarks to use in fast PHATE; t; power to which the diffusion operator is powered; sets the level of diffusion. If 'auto', t is selected; according to the knee point in the Von Neumann Entropy of; the diffusion operator; gamma; Informational distance constant between -1 and 1.; `gamma=1` gives the PHATE log potential, `gamma=0` gives; a square root potential.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; log(n_samples) time.; knn_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for building kNN graph; mds_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for MDS; mds; Selects which MDS algorithm is used for dimensionality reduction.; n_jobs; The number of jobs to use for the computation.; If `None`, `sc.settings.n_jobs` is used.; If -1 all CPUs are used. If 1 is given, no parallel computing code is; used at all,",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phate.py
Security,access,access,"""""""\; Embed high-dimensional data using PHATE; """"""; """"""\; PHATE :cite:p:`Moon2019`. Potential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE); embeds high dimensional single-cell data into two or three dimensions for; visualization of biological progressions. For more information and access to the object-oriented interface, read the; `PHATE documentation <https://phate.readthedocs.io/>`__. For; tutorials, bug reports, and R/MATLAB implementations, visit the `PHATE; GitHub page <https://github.com/KrishnaswamyLab/PHATE/>`__. For help; using PHATE, go `here <https://krishnaswamylab.org/get-help>`__. Parameters; ----------; adata; Annotated data matrix.; n_components; number of dimensions in which the data will be embedded; k; number of nearest neighbors on which to build kernel; a; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used; n_landmark; number of landmarks to use in fast PHATE; t; power to which the diffusion operator is powered; sets the level of diffusion. If 'auto', t is selected; according to the knee point in the Von Neumann Entropy of; the diffusion operator; gamma; Informational distance constant between -1 and 1.; `gamma=1` gives the PHATE log potential, `gamma=0` gives; a square root potential.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; log(n_samples) time.; knn_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for building kNN graph; mds_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for MDS; mds; Selects which MDS algorithm is used for dimensionality reduction.; n_jobs; The number of jobs to use for the computation.; If `None`, `sc.settings.n_jobs` is used.; If -1 all CPUs are used. If 1 is given, no parallel computing code is; used at all,",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phate.py
Testability,log,log," for; visualization of biological progressions. For more information and access to the object-oriented interface, read the; `PHATE documentation <https://phate.readthedocs.io/>`__. For; tutorials, bug reports, and R/MATLAB implementations, visit the `PHATE; GitHub page <https://github.com/KrishnaswamyLab/PHATE/>`__. For help; using PHATE, go `here <https://krishnaswamylab.org/get-help>`__. Parameters; ----------; adata; Annotated data matrix.; n_components; number of dimensions in which the data will be embedded; k; number of nearest neighbors on which to build kernel; a; sets decay rate of kernel tails.; If None, alpha decaying kernel is not used; n_landmark; number of landmarks to use in fast PHATE; t; power to which the diffusion operator is powered; sets the level of diffusion. If 'auto', t is selected; according to the knee point in the Von Neumann Entropy of; the diffusion operator; gamma; Informational distance constant between -1 and 1.; `gamma=1` gives the PHATE log potential, `gamma=0` gives; a square root potential.; n_pca; Number of principal components to use for calculating; neighborhoods. For extremely large datasets, using; n_pca < 20 allows neighborhoods to be calculated in; log(n_samples) time.; knn_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for building kNN graph; mds_dist; recommended values: 'euclidean' and 'cosine'; Any metric from `scipy.spatial.distance` can be used; distance metric for MDS; mds; Selects which MDS algorithm is used for dimensionality reduction.; n_jobs; The number of jobs to use for the computation.; If `None`, `sc.settings.n_jobs` is used.; If -1 all CPUs are used. If 1 is given, no parallel computing code is; used at all, which is useful for debugging.; For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for; n_jobs = -2, all CPUs but one are used; random_state; Random seed. Defaults to the global `numpy` random number generator; verbose; I",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phate.py
Availability,avail,available,"the cluster labels.; jaccard; If `True`, use Jaccard metric between k-neighborhoods to build graph. If; `False`, use a Gaussian kernel.; primary_metric; Distance metric to define nearest neighbors. Note that performance will be; slower for correlation and cosine.; n_jobs; Nearest Neighbors and Jaccard coefficients will be computed in parallel using; n_jobs. If 1 is given, no parallelism is used. If set to -1, all CPUs are used.; For n_jobs below -1, `n_cpus + 1 + n_jobs` are used.; q_tol; Tolerance, i.e. precision, for monitoring modularity optimization.; louvain_time_limit; Maximum number of seconds to run modularity optimization. If exceeded the best; result so far is returned.; nn_method; Whether to use brute force or kdtree for nearest neighbor search.; For very large high-dimensional data sets, brute force, with parallel; computation, performs faster than kdtree.; partition_type; Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the; available options, consult the documentation for; :func:`~leidenalg.find_partition`.; resolution_parameter; A parameter value controlling the coarseness of the clustering in Leiden. Higher; values lead to more clusters. Set to `None` if overriding `partition_type` to; one that does not accept a `resolution_parameter`.; n_iterations; Number of iterations to run the Leiden algorithm. If the number of iterations is; negative, the Leiden algorithm is run until an iteration in which there was no; improvement.; use_weights; Use vertices in the Leiden computation.; seed; Leiden initialization of the optimization.; copy; Return a copy or write to `adata`.; kargs; Additional arguments passed to :func:`~leidenalg.find_partition` and the; constructor of the `partition_type`. Returns; -------; Depending on `copy`, returns or updates `adata` with the following fields:. **communities** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obs`, dtype `int`); integer array of community assignments for each row in data. **graph** - ",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phenograph.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phenograph.py
Deployability,update,updates,"nsional data sets, brute force, with parallel; computation, performs faster than kdtree.; partition_type; Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the; available options, consult the documentation for; :func:`~leidenalg.find_partition`.; resolution_parameter; A parameter value controlling the coarseness of the clustering in Leiden. Higher; values lead to more clusters. Set to `None` if overriding `partition_type` to; one that does not accept a `resolution_parameter`.; n_iterations; Number of iterations to run the Leiden algorithm. If the number of iterations is; negative, the Leiden algorithm is run until an iteration in which there was no; improvement.; use_weights; Use vertices in the Leiden computation.; seed; Leiden initialization of the optimization.; copy; Return a copy or write to `adata`.; kargs; Additional arguments passed to :func:`~leidenalg.find_partition` and the; constructor of the `partition_type`. Returns; -------; Depending on `copy`, returns or updates `adata` with the following fields:. **communities** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obs`, dtype `int`); integer array of community assignments for each row in data. **graph** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`); the graph that was used for clustering. **Q** - `float` (:attr:`~anndata.AnnData.uns`, dtype `float`); the modularity score for communities on graph. Example; -------; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import scanpy.external as sce; >>> import numpy as np; >>> import pandas as pd. With annotated data as input:. >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.normalize_per_cell(adata). Then do PCA:. >>> sc.pp.pca(adata, n_comps=100). Compute phenograph clusters:. **Louvain** community detection. >>> sce.tl.phenograph(adata, clustering_algo=""louvain"", k=30). **Leiden** community detection. >>> sce.tl.phenograph(adata, clustering_algo=""leiden"", k=30). Return only `Graph` object. >>>",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phenograph.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phenograph.py
Energy Efficiency,monitor,monitoring,"symmetric (`'directed'`) graph.; The graph construction process produces a directed graph, which is symmetrized; by one of two methods (see `prune` below).; prune; `prune=False`, symmetrize by taking the average between the graph and its; transpose. `prune=True`, symmetrize by taking the product between the graph; and its transpose.; min_cluster_size; Cells that end up in a cluster smaller than min_cluster_size are considered; outliers and are assigned to -1 in the cluster labels.; jaccard; If `True`, use Jaccard metric between k-neighborhoods to build graph. If; `False`, use a Gaussian kernel.; primary_metric; Distance metric to define nearest neighbors. Note that performance will be; slower for correlation and cosine.; n_jobs; Nearest Neighbors and Jaccard coefficients will be computed in parallel using; n_jobs. If 1 is given, no parallelism is used. If set to -1, all CPUs are used.; For n_jobs below -1, `n_cpus + 1 + n_jobs` are used.; q_tol; Tolerance, i.e. precision, for monitoring modularity optimization.; louvain_time_limit; Maximum number of seconds to run modularity optimization. If exceeded the best; result so far is returned.; nn_method; Whether to use brute force or kdtree for nearest neighbor search.; For very large high-dimensional data sets, brute force, with parallel; computation, performs faster than kdtree.; partition_type; Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the; available options, consult the documentation for; :func:`~leidenalg.find_partition`.; resolution_parameter; A parameter value controlling the coarseness of the clustering in Leiden. Higher; values lead to more clusters. Set to `None` if overriding `partition_type` to; one that does not accept a `resolution_parameter`.; n_iterations; Number of iterations to run the Leiden algorithm. If the number of iterations is; negative, the Leiden algorithm is run until an iteration in which there was no; improvement.; use_weights; Use vertices in the Leiden computation.;",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phenograph.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phenograph.py
Performance,perform,performance,"rray, n-by-d array of n cells in d dimensions. if sparse matrix,; n-by-n adjacency matrix.; clustering_algo; Choose between `'Louvain'` or `'Leiden'` algorithm for clustering.; k; Number of nearest neighbors to use in first step of graph construction.; directed; Whether to use a symmetric (default) or asymmetric (`'directed'`) graph.; The graph construction process produces a directed graph, which is symmetrized; by one of two methods (see `prune` below).; prune; `prune=False`, symmetrize by taking the average between the graph and its; transpose. `prune=True`, symmetrize by taking the product between the graph; and its transpose.; min_cluster_size; Cells that end up in a cluster smaller than min_cluster_size are considered; outliers and are assigned to -1 in the cluster labels.; jaccard; If `True`, use Jaccard metric between k-neighborhoods to build graph. If; `False`, use a Gaussian kernel.; primary_metric; Distance metric to define nearest neighbors. Note that performance will be; slower for correlation and cosine.; n_jobs; Nearest Neighbors and Jaccard coefficients will be computed in parallel using; n_jobs. If 1 is given, no parallelism is used. If set to -1, all CPUs are used.; For n_jobs below -1, `n_cpus + 1 + n_jobs` are used.; q_tol; Tolerance, i.e. precision, for monitoring modularity optimization.; louvain_time_limit; Maximum number of seconds to run modularity optimization. If exceeded the best; result so far is returned.; nn_method; Whether to use brute force or kdtree for nearest neighbor search.; For very large high-dimensional data sets, brute force, with parallel; computation, performs faster than kdtree.; partition_type; Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the; available options, consult the documentation for; :func:`~leidenalg.find_partition`.; resolution_parameter; A parameter value controlling the coarseness of the clustering in Leiden. Higher; values lead to more clusters. Set to `None` if overriding `partition_",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phenograph.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phenograph.py
Safety,detect,detection,"""""""\; Perform clustering using PhenoGraph; """"""; """"""\; PhenoGraph clustering :cite:p:`Levine2015`. **PhenoGraph** is a clustering method designed for high-dimensional single-cell; data. It works by creating a graph (""network"") representing phenotypic similarities; between cells and then identifying communities in this graph. It supports both; Louvain_ and Leiden_ algorithms for community detection. .. _Louvain: https://louvain-igraph.readthedocs.io/en/latest/. .. _Leiden: https://leidenalg.readthedocs.io/en/latest/reference.html. .. note::; More information and bug reports `here; <https://github.com/dpeerlab/PhenoGraph>`__. Parameters; ----------; data; AnnData, or Array of data to cluster, or sparse matrix of k-nearest neighbor; graph. If ndarray, n-by-d array of n cells in d dimensions. if sparse matrix,; n-by-n adjacency matrix.; clustering_algo; Choose between `'Louvain'` or `'Leiden'` algorithm for clustering.; k; Number of nearest neighbors to use in first step of graph construction.; directed; Whether to use a symmetric (default) or asymmetric (`'directed'`) graph.; The graph construction process produces a directed graph, which is symmetrized; by one of two methods (see `prune` below).; prune; `prune=False`, symmetrize by taking the average between the graph and its; transpose. `prune=True`, symmetrize by taking the product between the graph; and its transpose.; min_cluster_size; Cells that end up in a cluster smaller than min_cluster_size are considered; outliers and are assigned to -1 in the cluster labels.; jaccard; If `True`, use Jaccard metric between k-neighborhoods to build graph. If; `False`, use a Gaussian kernel.; primary_metric; Distance metric to define nearest neighbors. Note that performance will be; slower for correlation and cosine.; n_jobs; Nearest Neighbors and Jaccard coefficients will be computed in parallel using; n_jobs. If 1 is given, no parallelism is used. If set to -1, all CPUs are used.; For n_jobs below -1, `n_cpus + 1 + n_jobs` are",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_phenograph.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_phenograph.py
Safety,predict,predicted,"nts and an annotation of known phases. This reproduces the approach of :cite:t:`Scialdone2015` in the implementation of; :cite:t:`Fechtner2018`. More information and bug reports `here; <https://github.com/rfechtner/pypairs>`__. Parameters; ----------; adata; The annotated data matrix.; annotation; Mapping from category to genes, e.g. `{'phase': [Gene1, ...]}`.; Defaults to ``data.vars['category']``.; fraction; Fraction of cells per category where marker criteria must be satisfied.; filter_genes; Genes for sampling the reference set. Defaults to all genes.; filter_samples; Cells for sampling the reference set. Defaults to all samples. Returns; -------; A dict mapping from category to lists of marker pairs, e.g.:; `{'Category_1': [(Gene_1, Gene_2), ...], ...}`. Examples; --------; >>> from scanpy.external.tl import sandbag; >>> from pypairs import datasets; >>> adata = datasets.leng15(); >>> marker_pairs = sandbag(adata, fraction=0.5); """"""; """"""\; Assigns scores and predicted class to observations :cite:p:`Scialdone2015` :cite:p:`Fechtner2018`. Calculates scores for each observation and each phase and assigns prediction; based on marker pairs indentified by :func:`~scanpy.external.tl.sandbag`. This reproduces the approach of :cite:t:`Scialdone2015` in the implementation of; :cite:t:`Fechtner2018`. Parameters; ----------; adata; The annotated data matrix.; marker_pairs; Mapping of categories to lists of marker pairs.; See :func:`~scanpy.external.tl.sandbag` output.; iterations; An integer scalar specifying the number of; iterations for random sampling to obtain a cycle score.; min_iter; An integer scalar specifying the minimum number of iterations; for score estimation.; min_pairs; An integer scalar specifying the minimum number of pairs; for score estimation. Returns; -------; A :class:`~pandas.DataFrame` with samples as index and categories as columns; with scores for each category for each sample and a additional column with; the name of the max scoring category for e",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_pypairs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_pypairs.py
Availability,avail,available,"e kNN adjacency; matrix output by SAM. If built-in scanpy dimensionality reduction; methods are to be used using the SAM-output AnnData, users; should recompute the neighbors using `.obs['X_pca']` with; `scanpy.pp.neighbors`.; `.obsm['X_pca']`; The principal components output by SAM.; `.obsm['X_umap']`; The UMAP projection output by SAM.; `.layers['X_disp']`; The expression matrix used for nearest-neighbor averaging.; `.layers['X_knn_avg']`; The nearest-neighbor-averaged expression data used for computing the; spatial dispersions of genes. Example; -------; >>> import scanpy.external as sce; >>> import scanpy as sc. *** Running SAM ***. Assuming we are given an AnnData object called `adata`, we can run the SAM; algorithm as follows:. >>> sam_obj = sce.tl.sam(adata,inplace=True). The input AnnData object should contain unstandardized, non-negative; expression values. Preferably, the data should be log-normalized and no; genes should be filtered out. Please see the documentation for a description of all available parameters. For more detailed tutorials, please visit the original Github repository:; https://github.com/atarashansky/self-assembling-manifold/tree/master/tutorial. *** Plotting ***. To visualize the output, we can use:. >>> sce.pl.sam(adata,projection='X_umap'). `sce.pl.sam` accepts all keyword arguments used in the; `matplotlib.pyplot.scatter` function. *** SAMGUI ***. SAM comes with the SAMGUI module, a graphical-user interface written with; `Plotly` and `ipythonwidgets` for interactively exploring and annotating; the scRNAseq data and running SAM. Dependencies can be installed with Anaconda by following the instructions in; the self-assembling-manifold Github README:; https://github.com/atarashansky/self-assembling-manifold. In a Jupyter notebook, execute the following to launch the interface:. >>> from samalg.gui import SAMGUI; >>> sam_gui = SAMGUI(sam_obj) # sam_obj is your SAM object; >>> sam_gui.SamPlot. This can also be enabled in Jupyer Lab by foll",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_sam.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_sam.py
Deployability,install,installed,"ality reduction; methods are to be used using the SAM-output AnnData, users; should recompute the neighbors using `.obs['X_pca']` with; `scanpy.pp.neighbors`.; `.obsm['X_pca']`; The principal components output by SAM.; `.obsm['X_umap']`; The UMAP projection output by SAM.; `.layers['X_disp']`; The expression matrix used for nearest-neighbor averaging.; `.layers['X_knn_avg']`; The nearest-neighbor-averaged expression data used for computing the; spatial dispersions of genes. Example; -------; >>> import scanpy.external as sce; >>> import scanpy as sc. *** Running SAM ***. Assuming we are given an AnnData object called `adata`, we can run the SAM; algorithm as follows:. >>> sam_obj = sce.tl.sam(adata,inplace=True). The input AnnData object should contain unstandardized, non-negative; expression values. Preferably, the data should be log-normalized and no; genes should be filtered out. Please see the documentation for a description of all available parameters. For more detailed tutorials, please visit the original Github repository:; https://github.com/atarashansky/self-assembling-manifold/tree/master/tutorial. *** Plotting ***. To visualize the output, we can use:. >>> sce.pl.sam(adata,projection='X_umap'). `sce.pl.sam` accepts all keyword arguments used in the; `matplotlib.pyplot.scatter` function. *** SAMGUI ***. SAM comes with the SAMGUI module, a graphical-user interface written with; `Plotly` and `ipythonwidgets` for interactively exploring and annotating; the scRNAseq data and running SAM. Dependencies can be installed with Anaconda by following the instructions in; the self-assembling-manifold Github README:; https://github.com/atarashansky/self-assembling-manifold. In a Jupyter notebook, execute the following to launch the interface:. >>> from samalg.gui import SAMGUI; >>> sam_gui = SAMGUI(sam_obj) # sam_obj is your SAM object; >>> sam_gui.SamPlot. This can also be enabled in Jupyer Lab by following the instructions in the; self-assembling-manifold README. """"""",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_sam.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_sam.py
Integrability,interface,interface,"ality reduction; methods are to be used using the SAM-output AnnData, users; should recompute the neighbors using `.obs['X_pca']` with; `scanpy.pp.neighbors`.; `.obsm['X_pca']`; The principal components output by SAM.; `.obsm['X_umap']`; The UMAP projection output by SAM.; `.layers['X_disp']`; The expression matrix used for nearest-neighbor averaging.; `.layers['X_knn_avg']`; The nearest-neighbor-averaged expression data used for computing the; spatial dispersions of genes. Example; -------; >>> import scanpy.external as sce; >>> import scanpy as sc. *** Running SAM ***. Assuming we are given an AnnData object called `adata`, we can run the SAM; algorithm as follows:. >>> sam_obj = sce.tl.sam(adata,inplace=True). The input AnnData object should contain unstandardized, non-negative; expression values. Preferably, the data should be log-normalized and no; genes should be filtered out. Please see the documentation for a description of all available parameters. For more detailed tutorials, please visit the original Github repository:; https://github.com/atarashansky/self-assembling-manifold/tree/master/tutorial. *** Plotting ***. To visualize the output, we can use:. >>> sce.pl.sam(adata,projection='X_umap'). `sce.pl.sam` accepts all keyword arguments used in the; `matplotlib.pyplot.scatter` function. *** SAMGUI ***. SAM comes with the SAMGUI module, a graphical-user interface written with; `Plotly` and `ipythonwidgets` for interactively exploring and annotating; the scRNAseq data and running SAM. Dependencies can be installed with Anaconda by following the instructions in; the self-assembling-manifold Github README:; https://github.com/atarashansky/self-assembling-manifold. In a Jupyter notebook, execute the following to launch the interface:. >>> from samalg.gui import SAMGUI; >>> sam_gui = SAMGUI(sam_obj) # sam_obj is your SAM object; >>> sam_gui.SamPlot. This can also be enabled in Jupyer Lab by following the instructions in the; self-assembling-manifold README. """"""",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_sam.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_sam.py
Modifiability,variab,variable,"""""""\; Run the Self-Assembling Manifold algorithm; """"""; """"""\; Self-Assembling Manifolds single-cell RNA sequencing analysis tool :cite:p:`Tarashansky2019`. SAM iteratively rescales the input gene expression matrix to emphasize; genes that are spatially variable along the intrinsic manifold of the data.; It outputs the gene weights, nearest neighbor matrix, and a 2D projection. The AnnData input should contain unstandardized, non-negative values.; Preferably, the data should be log-normalized and no genes should be filtered out. Parameters; ----------. k; The number of nearest neighbors to identify for each cell. distance; The distance metric to use when identifying nearest neighbors.; Can be any of the distance metrics supported by; :func:`~scipy.spatial.distance.pdist`. max_iter; The maximum number of iterations SAM will run. projection; If 'tsne', generates a t-SNE embedding. If 'umap', generates a UMAP; embedding. If 'None', no embedding will be generated. standardization; If 'Normalizer', use sklearn.preprocessing.Normalizer, which; normalizes expression data prior to PCA such that each cell has; unit L2 norm. If 'StandardScaler', use; sklearn.preprocessing.StandardScaler, which normalizes expression; data prior to PCA such that each gene has zero mean and unit; variance. Otherwise, do not normalize the expression data. We; recommend using 'StandardScaler' for large datasets with many; expected cell types and 'Normalizer' otherwise. If 'None', no; transformation is applied. num_norm_avg; The top 'num_norm_avg' dispersions are averaged to determine the; normalization factor when calculating the weights. This prevents; genes with large spatial dispersions from skewing the distribution; of weights. weight_pcs; If True, scale the principal components by their eigenvalues. In; datasets with many expected cell types, setting this to False might; improve the resolution as these cell types might be encoded by lower-; variance principal components. sparse_pca; If True, use",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_sam.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_sam.py
Testability,log,log-normalized,"""""""\; Run the Self-Assembling Manifold algorithm; """"""; """"""\; Self-Assembling Manifolds single-cell RNA sequencing analysis tool :cite:p:`Tarashansky2019`. SAM iteratively rescales the input gene expression matrix to emphasize; genes that are spatially variable along the intrinsic manifold of the data.; It outputs the gene weights, nearest neighbor matrix, and a 2D projection. The AnnData input should contain unstandardized, non-negative values.; Preferably, the data should be log-normalized and no genes should be filtered out. Parameters; ----------. k; The number of nearest neighbors to identify for each cell. distance; The distance metric to use when identifying nearest neighbors.; Can be any of the distance metrics supported by; :func:`~scipy.spatial.distance.pdist`. max_iter; The maximum number of iterations SAM will run. projection; If 'tsne', generates a t-SNE embedding. If 'umap', generates a UMAP; embedding. If 'None', no embedding will be generated. standardization; If 'Normalizer', use sklearn.preprocessing.Normalizer, which; normalizes expression data prior to PCA such that each cell has; unit L2 norm. If 'StandardScaler', use; sklearn.preprocessing.StandardScaler, which normalizes expression; data prior to PCA such that each gene has zero mean and unit; variance. Otherwise, do not normalize the expression data. We; recommend using 'StandardScaler' for large datasets with many; expected cell types and 'Normalizer' otherwise. If 'None', no; transformation is applied. num_norm_avg; The top 'num_norm_avg' dispersions are averaged to determine the; normalization factor when calculating the weights. This prevents; genes with large spatial dispersions from skewing the distribution; of weights. weight_pcs; If True, scale the principal components by their eigenvalues. In; datasets with many expected cell types, setting this to False might; improve the resolution as these cell types might be encoded by lower-; variance principal components. sparse_pca; If True, use",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_sam.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_sam.py
Deployability,update,updates,"; Embed high-dimensional data using TriMap; """"""; """"""\; TriMap: Large-scale Dimensionality Reduction Using Triplets :cite:p:`Amid2019`. TriMap is a dimensionality reduction method that uses triplet constraints; to form a low-dimensional embedding of a set of points. The triplet; constraints are of the form ""point i is closer to point j than point k"".; The triplets are sampled from the high-dimensional representation of the; points and a weighting scheme is used to reflect the importance of each; triplet. TriMap provides a significantly better global view of the data than the; other dimensionality reduction methods such t-SNE, LargeVis, and UMAP.; The global structure includes relative distances of the clusters, multiple; scales in the data, and the existence of possible outliers. We define a; global score to quantify the quality of an embedding in reflecting the; global structure of the data. Parameters; ----------; adata; Annotated data matrix.; n_components; Number of dimensions of the embedding.; n_inliers; Number of inlier points for triplet constraints.; n_outliers; Number of outlier points for triplet constraints.; n_random; Number of random triplet constraints per point.; metric; Distance measure: 'angular', 'euclidean', 'hamming', 'manhattan'.; weight_adj; Adjusting the weights using a non-linear transformation.; lr; Learning rate.; n_iters; Number of iterations.; verbose; If `True`, print the progress report.; If `None`, `sc.settings.verbosity` is used.; copy; Return a copy instead of writing to `adata`. Returns; -------; Depending on `copy`, returns or updates `adata` with the following fields. **X_trimap** : :class:`~numpy.ndarray`, (:attr:`~anndata.AnnData.obsm`, shape=(n_samples, n_components), dtype `float`); TriMap coordinates of data. Example; -------. >>> import scanpy as sc; >>> import scanpy.external as sce; >>> pbmc = sc.datasets.pbmc68k_reduced(); >>> pbmc = sce.tl.trimap(pbmc, copy=True); >>> sce.pl.trimap(pbmc, color=['bulk_labels'], s=10); """"""",MatchSource.CODE_COMMENT,src/scanpy/external/tl/_trimap.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/external/tl/_trimap.py
Availability,mask,mask,"pression values from `adata.raw`. Returns; -------; A dataframe with `adata.obs_names` as index, and values specified by `keys`; and `obsm_keys`. Examples; --------; Getting value for plotting:. >>> import scanpy as sc; >>> pbmc = sc.datasets.pbmc68k_reduced(); >>> plotdf = sc.get.obs_df(; ... pbmc,; ... keys=[""CD8B"", ""n_genes""],; ... obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]; ... ); >>> plotdf.columns; Index(['CD8B', 'n_genes', 'X_umap-0', 'X_umap-1'], dtype='object'); >>> plotdf.plot.scatter(""X_umap-0"", ""X_umap-1"", c=""CD8B"") # doctest: +SKIP; <Axes: xlabel='X_umap-0', ylabel='X_umap-1'>. Calculating mean expression for marker genes by cluster:. >>> pbmc = sc.datasets.pbmc68k_reduced(); >>> marker_genes = ['CD79A', 'MS4A1', 'CD8A', 'CD8B', 'LYZ']; >>> genedf = sc.get.obs_df(; ... pbmc,; ... keys=[""louvain"", *marker_genes]; ... ); >>> grouped = genedf.groupby(""louvain"", observed=True); >>> mean, var = grouped.mean(), grouped.var(); """"""; # Make df; # add var values; # add obs values; # reorder columns to given order (including duplicates keys if present); """"""\; Return values for observations in adata. Params; ------; adata; AnnData object to get values from.; keys; Keys from either `.obs_names`, or `.var.columns`.; varm_keys; Tuple of `(key from varm, column index of varm[key])`.; layer; Layer of `adata` to use as expression values. Returns; -------; A dataframe with `adata.var_names` as index, and values specified by `keys`; and `varm_keys`.; """"""; # Argument handling; # initialize df; # add obs values; # reorder columns to given order; """"""; Choose array aligned with obs annotation.; """"""; # https://github.com/scverse/scanpy/issues/1546; """"""; Set value for observation rep.; """"""; # Could also be a series, but should be one or the other; """"""; Validate mask argument; Params; ------; data; Annotated data matrix or numpy array.; mask; The mask. Either an appropriatley sized boolean array, or name of a column which will be used to mask.; dim; The dimension being masked.; """"""",MatchSource.CODE_COMMENT,src/scanpy/get/get.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/get/get.py
Security,access,accessing,"""""""This module contains helper functions for accessing data.""""""; # --------------------------------------------------------------------------------; # Plotting data helpers; # --------------------------------------------------------------------------------; # TODO: implement diffxpy method, make singledispatch; """"""\; :func:`scanpy.tl.rank_genes_groups` results in the form of a; :class:`~pandas.DataFrame`. Params; ------; adata; Object to get results from.; group; Which group (as in :func:`scanpy.tl.rank_genes_groups`'s `groupby`; argument) to return results from. Can be a list. All groups are; returned if groups is `None`.; key; Key differential expression groups were stored under.; pval_cutoff; Return only adjusted p-values below the cutoff.; log2fc_min; Minimum logfc to return.; log2fc_max; Maximum logfc to return.; gene_symbols; Column name in `.var` DataFrame that stores gene symbols. Specifying; this will add that column to the returned dataframe. Example; -------; >>> import scanpy as sc; >>> pbmc = sc.datasets.pbmc68k_reduced(); >>> sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", use_raw=True); >>> dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""); """"""; # remove group column for backward compat if len(group) == 1; """"""Common logic for checking indices for obs_df and var_df.""""""; # check that adata.obs does not contain duplicated columns; # if duplicated columns names are present, they will; # be further duplicated when selecting them.; # use only unique keys, otherwise duplicated keys will; # further duplicate when reordering the keys later in the function; # while var_names must be unique, adata.var[gene_symbols] does not; # It's still ambiguous to refer to a duplicated entry though.; # TODO: This should be made easier on the anndata side; # for backed AnnData is important that the indices are ordered; """"""\; Return values for observations in adata. Params; ------; adata; AnnData object to get values from.; keys; Keys from either `.var_names`, `.var[gene_symb",MatchSource.CODE_COMMENT,src/scanpy/get/get.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/get/get.py
Testability,log,logfc,"""""""This module contains helper functions for accessing data.""""""; # --------------------------------------------------------------------------------; # Plotting data helpers; # --------------------------------------------------------------------------------; # TODO: implement diffxpy method, make singledispatch; """"""\; :func:`scanpy.tl.rank_genes_groups` results in the form of a; :class:`~pandas.DataFrame`. Params; ------; adata; Object to get results from.; group; Which group (as in :func:`scanpy.tl.rank_genes_groups`'s `groupby`; argument) to return results from. Can be a list. All groups are; returned if groups is `None`.; key; Key differential expression groups were stored under.; pval_cutoff; Return only adjusted p-values below the cutoff.; log2fc_min; Minimum logfc to return.; log2fc_max; Maximum logfc to return.; gene_symbols; Column name in `.var` DataFrame that stores gene symbols. Specifying; this will add that column to the returned dataframe. Example; -------; >>> import scanpy as sc; >>> pbmc = sc.datasets.pbmc68k_reduced(); >>> sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", use_raw=True); >>> dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""); """"""; # remove group column for backward compat if len(group) == 1; """"""Common logic for checking indices for obs_df and var_df.""""""; # check that adata.obs does not contain duplicated columns; # if duplicated columns names are present, they will; # be further duplicated when selecting them.; # use only unique keys, otherwise duplicated keys will; # further duplicate when reordering the keys later in the function; # while var_names must be unique, adata.var[gene_symbols] does not; # It's still ambiguous to refer to a duplicated entry though.; # TODO: This should be made easier on the anndata side; # for backed AnnData is important that the indices are ordered; """"""\; Return values for observations in adata. Params; ------; adata; AnnData object to get values from.; keys; Keys from either `.var_names`, `.var[gene_symb",MatchSource.CODE_COMMENT,src/scanpy/get/get.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/get/get.py
Availability,mask,mask,"# Used with get_args; """"""\; Functionality for generic grouping and aggregating. There is currently support for count_nonzero, sum, mean, and variance. **Implementation**. Moments are computed using weighted sum aggregation of data by some feature; via multiplication by a sparse coordinate matrix A. Runtime is effectively computation of the product `A @ X`, i.e. the count of (non-zero); entries in X with multiplicity the number of group memberships for that entry.; This is `O(data)` for partitions (each observation belonging to exactly one group),; independent of the number of groups. Params; ------; groupby; :class:`~pandas.Categorical` containing values for grouping by.; data; Data matrix for aggregation.; mask; Mask to be used for aggregation.; """"""; """"""\; Count the number of observations in each group. Returns; -------; Array of counts.; """"""; # pattern = self.data._with_data(np.broadcast_to(1, len(self.data.data))); # return self.indicator_matrix @ pattern; """"""\; Compute the sum per feature per group of observations. Returns; -------; Array of sum.; """"""; """"""\; Compute the mean per feature per group of observations. Returns; -------; Array of mean.; """"""; """"""\; Compute the count, as well as mean and variance per feature, per group of observations. The formula `Var(X) = E(X^2) - E(X)^2` suffers loss of precision when the variance is a; very small fraction of the squared mean. In particular, when X is constant, the formula may; nonetheless be non-zero. By default, our implementation resets the variance to exactly zero; when the computed variance, relative to the squared mean, nears limit of precision of the; floating-point significand. Params; ------; dof; Degrees of freedom for variance. Returns; -------; Object with `count`, `mean`, and `var` attributes.; """"""; # sparse matrices do not support ** for elementwise power.; # TODO: Why these values exactly? Because they are high relative to the datatype?; # (unchanged from original code: https://github.com/scverse/anndata",MatchSource.CODE_COMMENT,src/scanpy/get/_aggregated.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/get/_aggregated.py
Energy Efficiency,power,power,"Returns; -------; Array of counts.; """"""; # pattern = self.data._with_data(np.broadcast_to(1, len(self.data.data))); # return self.indicator_matrix @ pattern; """"""\; Compute the sum per feature per group of observations. Returns; -------; Array of sum.; """"""; """"""\; Compute the mean per feature per group of observations. Returns; -------; Array of mean.; """"""; """"""\; Compute the count, as well as mean and variance per feature, per group of observations. The formula `Var(X) = E(X^2) - E(X)^2` suffers loss of precision when the variance is a; very small fraction of the squared mean. In particular, when X is constant, the formula may; nonetheless be non-zero. By default, our implementation resets the variance to exactly zero; when the computed variance, relative to the squared mean, nears limit of precision of the; floating-point significand. Params; ------; dof; Degrees of freedom for variance. Returns; -------; Object with `count`, `mean`, and `var` attributes.; """"""; # sparse matrices do not support ** for elementwise power.; # TODO: Why these values exactly? Because they are high relative to the datatype?; # (unchanged from original code: https://github.com/scverse/anndata/pull/564); # detects loss of precision in mean_sq - sq_mean, which suggests variance is 0; """"""\; Generate elementwise power of a matrix. Needed for non-square sparse matrices because they do not support `**` so the `.power` function is used. Params; ------; X; Matrix whose power is to be raised.; power; Integer power value. Returns; -------; Matrix whose power has been raised.; """"""; """"""\; Aggregate data matrix based on some categorical grouping. This function is useful for pseudobulking as well as plotting. Aggregation to perform is specified by `func`, which can be a single metric or a; list of metrics. Each metric is computed over the group and results in a new layer; in the output `AnnData` object. If none of `layer`, `obsm`, or `varm` are passed in, `X` will be used for aggregation data. Params; ----",MatchSource.CODE_COMMENT,src/scanpy/get/_aggregated.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/get/_aggregated.py
Modifiability,layers,layers," and results in a new layer; in the output `AnnData` object. If none of `layer`, `obsm`, or `varm` are passed in, `X` will be used for aggregation data. Params; ------; adata; :class:`~anndata.AnnData` to be aggregated.; by; Key of the column to be grouped-by.; func; How to aggregate.; axis; Axis on which to find group by column.; mask; Boolean mask (or key to column containing mask) to apply along the axis.; dof; Degrees of freedom for variance. Defaults to 1.; layer; If not None, key for aggregation data.; obsm; If not None, key for aggregation data.; varm; If not None, key for aggregation data. Returns; -------; Aggregated :class:`~anndata.AnnData`. Examples; --------. Calculating mean expression and number of nonzero entries per cluster:. >>> import scanpy as sc, pandas as pd; >>> pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); >>> pbmc.shape; (2638, 13714); >>> aggregated = sc.get.aggregate(pbmc, by=""louvain"", func=[""mean"", ""count_nonzero""]); >>> aggregated; AnnData object with n_obs × n_vars = 8 × 13714; obs: 'louvain'; var: 'n_cells'; layers: 'mean', 'count_nonzero'. We can group over multiple columns:. >>> pbmc.obs[""percent_mito_binned""] = pd.cut(pbmc.obs[""percent_mito""], bins=5); >>> sc.get.aggregate(pbmc, by=[""louvain"", ""percent_mito_binned""], func=[""mean"", ""count_nonzero""]); AnnData object with n_obs × n_vars = 40 × 13714; obs: 'louvain', 'percent_mito_binned'; var: 'n_cells'; layers: 'mean', 'count_nonzero'. Note that this filters out any combination of groups that wasn't present in the original data.; """"""; # i.e., all of `varm`, `obsm`, `layers` are None so we use `X` which must be transposed; # Actual computation; # Define new var dataframe; # Check if there could be labels; # Create them otherwise; # It's all coming together; # sum is calculated separately from the rest; # here and below for count, if var is present, these can be calculate alongside var; """"""; Returns both the result categories and a dataframe labelling each row; """"""; # It's like ",MatchSource.CODE_COMMENT,src/scanpy/get/_aggregated.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/get/_aggregated.py
Performance,perform,perform,"riance, relative to the squared mean, nears limit of precision of the; floating-point significand. Params; ------; dof; Degrees of freedom for variance. Returns; -------; Object with `count`, `mean`, and `var` attributes.; """"""; # sparse matrices do not support ** for elementwise power.; # TODO: Why these values exactly? Because they are high relative to the datatype?; # (unchanged from original code: https://github.com/scverse/anndata/pull/564); # detects loss of precision in mean_sq - sq_mean, which suggests variance is 0; """"""\; Generate elementwise power of a matrix. Needed for non-square sparse matrices because they do not support `**` so the `.power` function is used. Params; ------; X; Matrix whose power is to be raised.; power; Integer power value. Returns; -------; Matrix whose power has been raised.; """"""; """"""\; Aggregate data matrix based on some categorical grouping. This function is useful for pseudobulking as well as plotting. Aggregation to perform is specified by `func`, which can be a single metric or a; list of metrics. Each metric is computed over the group and results in a new layer; in the output `AnnData` object. If none of `layer`, `obsm`, or `varm` are passed in, `X` will be used for aggregation data. Params; ------; adata; :class:`~anndata.AnnData` to be aggregated.; by; Key of the column to be grouped-by.; func; How to aggregate.; axis; Axis on which to find group by column.; mask; Boolean mask (or key to column containing mask) to apply along the axis.; dof; Degrees of freedom for variance. Defaults to 1.; layer; If not None, key for aggregation data.; obsm; If not None, key for aggregation data.; varm; If not None, key for aggregation data. Returns; -------; Aggregated :class:`~anndata.AnnData`. Examples; --------. Calculating mean expression and number of nonzero entries per cluster:. >>> import scanpy as sc, pandas as pd; >>> pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); >>> pbmc.shape; (2638, 13714); >>> aggregated = sc.get.aggrega",MatchSource.CODE_COMMENT,src/scanpy/get/_aggregated.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/get/_aggregated.py
Safety,detect,detects,"f sum.; """"""; """"""\; Compute the mean per feature per group of observations. Returns; -------; Array of mean.; """"""; """"""\; Compute the count, as well as mean and variance per feature, per group of observations. The formula `Var(X) = E(X^2) - E(X)^2` suffers loss of precision when the variance is a; very small fraction of the squared mean. In particular, when X is constant, the formula may; nonetheless be non-zero. By default, our implementation resets the variance to exactly zero; when the computed variance, relative to the squared mean, nears limit of precision of the; floating-point significand. Params; ------; dof; Degrees of freedom for variance. Returns; -------; Object with `count`, `mean`, and `var` attributes.; """"""; # sparse matrices do not support ** for elementwise power.; # TODO: Why these values exactly? Because they are high relative to the datatype?; # (unchanged from original code: https://github.com/scverse/anndata/pull/564); # detects loss of precision in mean_sq - sq_mean, which suggests variance is 0; """"""\; Generate elementwise power of a matrix. Needed for non-square sparse matrices because they do not support `**` so the `.power` function is used. Params; ------; X; Matrix whose power is to be raised.; power; Integer power value. Returns; -------; Matrix whose power has been raised.; """"""; """"""\; Aggregate data matrix based on some categorical grouping. This function is useful for pseudobulking as well as plotting. Aggregation to perform is specified by `func`, which can be a single metric or a; list of metrics. Each metric is computed over the group and results in a new layer; in the output `AnnData` object. If none of `layer`, `obsm`, or `varm` are passed in, `X` will be used for aggregation data. Params; ------; adata; :class:`~anndata.AnnData` to be aggregated.; by; Key of the column to be grouped-by.; func; How to aggregate.; axis; Axis on which to find group by column.; mask; Boolean mask (or key to column containing mask) to apply along the ax",MatchSource.CODE_COMMENT,src/scanpy/get/_aggregated.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/get/_aggregated.py
Performance,perform,performance,"# Fix for anndata<0.7; ###############################################################################; # Calculation; ###############################################################################; # Some notes on the implementation:; # * This could be phrased as tensor multiplication. However that does not get; # parallelized, which boosts performance almost linearly with cores.; # * Due to the umap setting the default threading backend, a parallel numba; # function that calls another parallel numba function can get stuck. This; # ends up meaning code re-use will be limited until umap 0.4.; # See: https://github.com/lmcinnes/umap/issues/306; # * There can be a fair amount of numerical instability here (big reductions),; # so data is cast to float64. Removing these casts/ conversion will cause the; # tests to fail.; # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~; # Inner functions (per element C); # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~; # For calling gearys_c on collections.; # TODO: These are faster if we can compile them in parallel mode. However,; # `workqueue` does not allow nested functions to be parallelized.; # Additionally, there are currently problems with numba's compiler around; # parallelization of this code:; # https://github.com/numba/numba/issues/6774#issuecomment-788789663; # noqa: PLR0917; # Expanded from 2 * W * ((x_k - x_k_bar) ** 2).sum(), but uses sparsity; # to skip some calculations; # fmt: off; # fmt: on; # noqa: PLR0917; ###############################################################################; # Interface; ###############################################################################",MatchSource.CODE_COMMENT,src/scanpy/metrics/_gearys_c.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/metrics/_gearys_c.py
Testability,test,tests,"# Fix for anndata<0.7; ###############################################################################; # Calculation; ###############################################################################; # Some notes on the implementation:; # * This could be phrased as tensor multiplication. However that does not get; # parallelized, which boosts performance almost linearly with cores.; # * Due to the umap setting the default threading backend, a parallel numba; # function that calls another parallel numba function can get stuck. This; # ends up meaning code re-use will be limited until umap 0.4.; # See: https://github.com/lmcinnes/umap/issues/306; # * There can be a fair amount of numerical instability here (big reductions),; # so data is cast to float64. Removing these casts/ conversion will cause the; # tests to fail.; # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~; # Inner functions (per element C); # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~; # For calling gearys_c on collections.; # TODO: These are faster if we can compile them in parallel mode. However,; # `workqueue` does not allow nested functions to be parallelized.; # Additionally, there are currently problems with numba's compiler around; # parallelization of this code:; # https://github.com/numba/numba/issues/6774#issuecomment-788789663; # noqa: PLR0917; # Expanded from 2 * W * ((x_k - x_k_bar) ** 2).sum(), but uses sparsity; # to skip some calculations; # fmt: off; # fmt: on; # noqa: PLR0917; ###############################################################################; # Interface; ###############################################################################",MatchSource.CODE_COMMENT,src/scanpy/metrics/_gearys_c.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/metrics/_gearys_c.py
Safety,detect,detected,"# some algorithms have some messed up reordering.; """"""\; Create a sparse matrix from a pair of indices and distances. If keep_self=False, it verifies that the first column is the cell itself,; then removes it from the explicitly stored zeroes. Duplicates in the data are kept as explicitly stored zeroes.; """"""; # instead of calling .eliminate_zeros() on our sparse matrix,; # we manually handle the nearest neighbor being the cell itself.; # This allows us to use _ind_dist_shortcut even when the data has duplicates.; # copy the data, otherwise strange behavior here; """"""\; Get indices and distances from a sparse matrix. Makes sure that for both of the returned matrices:; 1. the first column corresponds to the cell itself as nearest neighbor.; 2. the number of neighbors (`.shape[1]`) is restricted to `n_neighbors`.; """"""; # handle RAPIDS style indices_distances lacking the self-column; # If using the shortcut or adding the self column resulted in too many neighbors,; # restrict the output matrices to the correct size; # 'true' and 'spurious' zeros; # account for the fact that there might be more than n_neighbors; # due to an approximate search; # [the point itself was not detected as its own neighbor during the search]; """"""Shortcut for scipy or RAPIDS style distance matrices.""""""; # Check if each row has the correct number of entries",MatchSource.CODE_COMMENT,src/scanpy/neighbors/_common.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/_common.py
Availability,mask,mask,"""""""; Derive gaussian connectivities between data points from their distances. Parameters; ----------; distances; The input matrix of distances between data points.; n_neighbors; The number of nearest neighbors to consider.; knn; Specify if the distances have been restricted to k nearest neighbors.; """"""; # init distances; # exclude the first point, the 0th neighbor; # choose sigma, the heuristic here doesn't seem to make much of a difference,; # but is used to reproduce the figures of Haghverdi et al. (2016); # as the distances are not sorted; # we have decay within the n_neighbors first neighbors; # the last item is already in its sorted position through argpartition; # we have decay beyond the n_neighbors neighbors; # compute the symmetric weight matrix; # make the weight matrix sparse; # restrict number of neighbors to ~k; # build a symmetric mask; # set all entries that are not nearest neighbors to zero; # need to copy the distance matrix here; what follows is inplace; """"""\; This is from umap.fuzzy_simplicial_set :cite:p:`McInnes2018`. Given a set of data X, a neighborhood size, and a measure of distance; compute the fuzzy simplicial set (here represented as a fuzzy graph in; the form of a sparse matrix) associated to the data. This is done by; locally approximating geodesic distance at each point, creating a fuzzy; simplicial set for each such point, and then combining all the local; fuzzy simplicial sets into a global one via a fuzzy union.; """"""; # umap 0.5.0",MatchSource.CODE_COMMENT,src/scanpy/neighbors/_connectivity.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/_connectivity.py
Usability,simpl,simplicial,"""""""; Derive gaussian connectivities between data points from their distances. Parameters; ----------; distances; The input matrix of distances between data points.; n_neighbors; The number of nearest neighbors to consider.; knn; Specify if the distances have been restricted to k nearest neighbors.; """"""; # init distances; # exclude the first point, the 0th neighbor; # choose sigma, the heuristic here doesn't seem to make much of a difference,; # but is used to reproduce the figures of Haghverdi et al. (2016); # as the distances are not sorted; # we have decay within the n_neighbors first neighbors; # the last item is already in its sorted position through argpartition; # we have decay beyond the n_neighbors neighbors; # compute the symmetric weight matrix; # make the weight matrix sparse; # restrict number of neighbors to ~k; # build a symmetric mask; # set all entries that are not nearest neighbors to zero; # need to copy the distance matrix here; what follows is inplace; """"""\; This is from umap.fuzzy_simplicial_set :cite:p:`McInnes2018`. Given a set of data X, a neighborhood size, and a measure of distance; compute the fuzzy simplicial set (here represented as a fuzzy graph in; the form of a sparse matrix) associated to the data. This is done by; locally approximating geodesic distance at each point, creating a fuzzy; simplicial set for each such point, and then combining all the local; fuzzy simplicial sets into a global one via a fuzzy union.; """"""; # umap 0.5.0",MatchSource.CODE_COMMENT,src/scanpy/neighbors/_connectivity.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/_connectivity.py
Availability,avail,available,"r instance to an instance class. If `transformer` is `None` and there are few data points,; `transformer` will be set to a brute force; :class:`~sklearn.neighbors.KNeighborsTransformer`. If `transformer` is `None` and there are many data points,; `transformer` will be set like `umap` does (i.e. to a; ~`pynndescent.PyNNDescentTransformer` with custom `n_trees` and `n_iter`).; """"""; # legacy logic; # Coerce `method` to 'gauss' or 'umap'; # Validate `knn`; # “knn=False” seems to be only intended for method “gauss”; # Coerce `transformer` to an instance; # only obey n_neighbors arg if knn set; # needs dict; # no random_state; # Use defaults from UMAP’s `nearest_neighbors` function; # else `transformer` is probably an instance; """"""\; Compute transition matrix. Parameters; ----------; density_normalize; The density rescaling of Coifman and Lafon (2006): Then only the; geometry of the data matters, not the sampled density. Returns; -------; Makes attributes `.transitions_sym` and `.transitions` available.; """"""; # density normalization as of Coifman et al. (2005); # ensures that kernel matrix is independent of sampling density; # q[i] is an estimate for the sampling density at point i; # it's also the degree of the underlying graph; # z[i] is the square root of the row sum of K; """"""\; Compute eigen decomposition of transition matrix. Parameters; ----------; n_comps; Number of eigenvalues/vectors to be computed, set `n_comps = 0` if; you need all eigenvectors.; sym; Instead of computing the eigendecomposition of the assymetric; transition matrix, computed the eigendecomposition of the symmetric; Ktilde matrix.; random_state; A numpy random seed. Returns; -------; Writes the following attributes. eigen_values : :class:`~numpy.ndarray`; Eigenvalues of transition matrix.; eigen_basis : :class:`~numpy.ndarray`; Matrix of eigenvectors (stored in columns). `.eigen_basis` is; projection of data matrix on right eigenvectors, that is, the; projection on the diffusion components. these ",MatchSource.CODE_COMMENT,src/scanpy/neighbors/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/__init__.py
Deployability,update,update,"# default number of diffusion components; # Backwards compat, constants should be defined in only one place.; """"""Keyword arguments passed to a _KnownTransformer. IMPORTANT: when changing the parameters set here,; update the “*ignored*” part in the parameter docs!; """"""; """"""\; Computes the nearest neighbors distance matrix and a neighborhood graph of observations :cite:p:`McInnes2018`. The neighbor search efficiency of this heavily relies on UMAP :cite:p:`McInnes2018`,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='gauss'`,; connectivities are computed according to :cite:t:`Coifman2005`, in the adaption of; :cite:t:`Haghverdi2016`. Parameters; ----------; adata; Annotated data matrix.; n_neighbors; The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. *ignored if ``transformer`` is an instance.*; {n_pcs}; {use_rep}; knn; If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor.; method; Use 'umap' :cite:p:`McInnes2018` or 'gauss' (Gauss kernel following :cite:t:`Coifman2005`; with adaptive width :cite:t:`Haghverdi2016`) for computing connectivities.; transformer; Approximate kNN search implementation following the API of; :class:`~sklearn.neighbors.KNeighborsTransformer`.; See :doc:`/how-to/knn-transformers` for more details.; Also accepts the following known options:. `None` (the default); Behavior ",MatchSource.CODE_COMMENT,src/scanpy/neighbors/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/__init__.py
Energy Efficiency,adapt,adaption,"# default number of diffusion components; # Backwards compat, constants should be defined in only one place.; """"""Keyword arguments passed to a _KnownTransformer. IMPORTANT: when changing the parameters set here,; update the “*ignored*” part in the parameter docs!; """"""; """"""\; Computes the nearest neighbors distance matrix and a neighborhood graph of observations :cite:p:`McInnes2018`. The neighbor search efficiency of this heavily relies on UMAP :cite:p:`McInnes2018`,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='gauss'`,; connectivities are computed according to :cite:t:`Coifman2005`, in the adaption of; :cite:t:`Haghverdi2016`. Parameters; ----------; adata; Annotated data matrix.; n_neighbors; The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. *ignored if ``transformer`` is an instance.*; {n_pcs}; {use_rep}; knn; If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor.; method; Use 'umap' :cite:p:`McInnes2018` or 'gauss' (Gauss kernel following :cite:t:`Coifman2005`; with adaptive width :cite:t:`Haghverdi2016`) for computing connectivities.; transformer; Approximate kNN search implementation following the API of; :class:`~sklearn.neighbors.KNeighborsTransformer`.; See :doc:`/how-to/knn-transformers` for more details.; Also accepts the following known options:. `None` (the default); Behavior ",MatchSource.CODE_COMMENT,src/scanpy/neighbors/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/__init__.py
Integrability,depend,depends,"s result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. *ignored if ``transformer`` is an instance.*; {n_pcs}; {use_rep}; knn; If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor.; method; Use 'umap' :cite:p:`McInnes2018` or 'gauss' (Gauss kernel following :cite:t:`Coifman2005`; with adaptive width :cite:t:`Haghverdi2016`) for computing connectivities.; transformer; Approximate kNN search implementation following the API of; :class:`~sklearn.neighbors.KNeighborsTransformer`.; See :doc:`/how-to/knn-transformers` for more details.; Also accepts the following known options:. `None` (the default); Behavior depends on data size.; For small data, we will calculate exact kNN, otherwise we use; :class:`~pynndescent.pynndescent_.PyNNDescentTransformer`; `'pynndescent'`; :class:`~pynndescent.pynndescent_.PyNNDescentTransformer`; `'rapids'`; A transformer based on :class:`cuml.neighbors.NearestNeighbors`. .. deprecated:: 1.10.0; Use :func:`rapids_singlecell.pp.neighbors` instead.; metric; A known metric’s name or a callable that returns a distance. *ignored if ``transformer`` is an instance.*; metric_kwds; Options for the metric. *ignored if ``transformer`` is an instance.*; random_state; A numpy random seed. *ignored if ``transformer`` is an instance.*; key_added; If not specified, the neighbors data is stored in `.uns['neighbors']`,; distances and connectivities are stored in `.obsp['distances']` and; `.obsp['connectivities']` respectively.; If specified, the neighbors data is added to .uns[key_added],; distances are stored in `.obsp[key_added+'_distances']` and; connectivities in `.obsp",MatchSource.CODE_COMMENT,src/scanpy/neighbors/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/__init__.py
Modifiability,adapt,adaption,"# default number of diffusion components; # Backwards compat, constants should be defined in only one place.; """"""Keyword arguments passed to a _KnownTransformer. IMPORTANT: when changing the parameters set here,; update the “*ignored*” part in the parameter docs!; """"""; """"""\; Computes the nearest neighbors distance matrix and a neighborhood graph of observations :cite:p:`McInnes2018`. The neighbor search efficiency of this heavily relies on UMAP :cite:p:`McInnes2018`,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='gauss'`,; connectivities are computed according to :cite:t:`Coifman2005`, in the adaption of; :cite:t:`Haghverdi2016`. Parameters; ----------; adata; Annotated data matrix.; n_neighbors; The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. *ignored if ``transformer`` is an instance.*; {n_pcs}; {use_rep}; knn; If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor.; method; Use 'umap' :cite:p:`McInnes2018` or 'gauss' (Gauss kernel following :cite:t:`Coifman2005`; with adaptive width :cite:t:`Haghverdi2016`) for computing connectivities.; transformer; Approximate kNN search implementation following the API of; :class:`~sklearn.neighbors.KNeighborsTransformer`.; See :doc:`/how-to/knn-transformers` for more details.; Also accepts the following known options:. `None` (the default); Behavior ",MatchSource.CODE_COMMENT,src/scanpy/neighbors/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/__init__.py
Performance,cache,cached,"e transition matrix via::. self.transitions_sym = self.Z / self.transitions * self.Z. where ``self.Z`` is the diagonal matrix storing the normalization of the; underlying kernel matrix.; """"""; """"""Eigen values of transition matrix.""""""; """"""Eigen basis of transition matrix.""""""; """"""DPT distances. This is yields :cite:p:`Haghverdi2016`, Eq. 15 from the supplement with the; extensions of :cite:p:`Wolf2019`, supplement on random-walk based distance; measures.; """"""; """"""Generate igraph from connectiviies.""""""; """"""\; Compute distances and connectivities of neighbors. Parameters; ----------; n_neighbors; Use this number of nearest neighbors.; {n_pcs}; {use_rep}; knn; Restrict result to `n_neighbors` nearest neighbors.; method; See :func:`scanpy.pp.neighbors`.; If `None`, skip calculating connectivities. Returns; -------; Writes sparse graph attributes `.distances` and,; if `method` is not `None`, `.connectivities`.; """"""; # very small datasets; # default keyword arguments when `transformer` is not an instance; # most use _params, not _kwds; # do not use the cached rp_forest; # self._distances is a sparse matrix with a diag of 1, fix that; # remove too far away entries in self._distances; # convert to dense; # very cautious here; # TODO catch the correct exception; """"""Return effective `method` and transformer. `method` will be coerced to `'gauss'` or `'umap'`.; `transformer` is coerced from a str or instance to an instance class. If `transformer` is `None` and there are few data points,; `transformer` will be set to a brute force; :class:`~sklearn.neighbors.KNeighborsTransformer`. If `transformer` is `None` and there are many data points,; `transformer` will be set like `umap` does (i.e. to a; ~`pynndescent.PyNNDescentTransformer` with custom `n_trees` and `n_iter`).; """"""; # legacy logic; # Coerce `method` to 'gauss' or 'umap'; # Validate `knn`; # “knn=False” seems to be only intended for method “gauss”; # Coerce `transformer` to an instance; # only obey n_neighbors arg if knn set",MatchSource.CODE_COMMENT,src/scanpy/neighbors/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/__init__.py
Security,access,access,"dtype `float`); Distance matrix of the nearest neighbors search. Each row (cell) has `n_neighbors`-1 non-zero entries. These are the distances to their `n_neighbors`-1 nearest neighbors (excluding the cell itself).; `adata.obsp['connectivities' | key_added+'_connectivities']` : :class:`scipy.sparse._csr.csr_matrix` (dtype `float`); Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities.; `adata.uns['neighbors' | key_added]` : :class:`dict`; neighbors parameters. Examples; --------; >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> # Basic usage; >>> sc.pp.neighbors(adata, 20, metric='cosine'); >>> # Provide your own transformer for more control and flexibility; >>> from sklearn.neighbors import KNeighborsTransformer; >>> transformer = KNeighborsTransformer(n_neighbors=10, metric='manhattan', algorithm='kd_tree'); >>> sc.pp.neighbors(adata, transformer=transformer); >>> # now you can e.g. access the index: `transformer._tree`. See also; --------; :doc:`/how-to/knn-transformers`; """"""; # we shouldn't need this here...; """"""Emulate a matrix where elements are calculated on the fly.""""""; # restrict the array to a subset; # map the index back to the global index; """"""Generate a view restricted to a subset of indices.""""""; """"""\; Data represented as graph of nearest neighbors. Represent a data matrix as a graph of nearest neighbor relations (edges); among data points (nodes). Parameters; ----------; adata; Annotated data object.; n_dcs; Number of diffusion components to use.; neighbors_key; Where to look in `.uns` and `.obsp` for neighbors data; """"""; # use the graph in adata; # estimating n_neighbors; """"""Distances between data points (sparse matrix).""""""; """"""Connectivities between data points (sparse matrix).""""""; """"""Transition matrix (sparse matrix). Is conjugate to the symmetrized transition matrix via::. self.transitions = self.Z * self.transitions_sym / self.Z. where ``self.Z`` is the diagonal",MatchSource.CODE_COMMENT,src/scanpy/neighbors/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/__init__.py
Testability,test,tested,"is here...; """"""Emulate a matrix where elements are calculated on the fly.""""""; # restrict the array to a subset; # map the index back to the global index; """"""Generate a view restricted to a subset of indices.""""""; """"""\; Data represented as graph of nearest neighbors. Represent a data matrix as a graph of nearest neighbor relations (edges); among data points (nodes). Parameters; ----------; adata; Annotated data object.; n_dcs; Number of diffusion components to use.; neighbors_key; Where to look in `.uns` and `.obsp` for neighbors data; """"""; # use the graph in adata; # estimating n_neighbors; """"""Distances between data points (sparse matrix).""""""; """"""Connectivities between data points (sparse matrix).""""""; """"""Transition matrix (sparse matrix). Is conjugate to the symmetrized transition matrix via::. self.transitions = self.Z * self.transitions_sym / self.Z. where ``self.Z`` is the diagonal matrix storing the normalization of the; underlying kernel matrix. Notes; -----; This has not been tested, in contrast to `transitions_sym`.; """"""; """"""Symmetrized transition matrix (sparse matrix). Is conjugate to the transition matrix via::. self.transitions_sym = self.Z / self.transitions * self.Z. where ``self.Z`` is the diagonal matrix storing the normalization of the; underlying kernel matrix.; """"""; """"""Eigen values of transition matrix.""""""; """"""Eigen basis of transition matrix.""""""; """"""DPT distances. This is yields :cite:p:`Haghverdi2016`, Eq. 15 from the supplement with the; extensions of :cite:p:`Wolf2019`, supplement on random-walk based distance; measures.; """"""; """"""Generate igraph from connectiviies.""""""; """"""\; Compute distances and connectivities of neighbors. Parameters; ----------; n_neighbors; Use this number of nearest neighbors.; {n_pcs}; {use_rep}; knn; Restrict result to `n_neighbors` nearest neighbors.; method; See :func:`scanpy.pp.neighbors`.; If `None`, skip calculating connectivities. Returns; -------; Writes sparse graph attributes `.distances` and,; if `method` is not",MatchSource.CODE_COMMENT,src/scanpy/neighbors/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/__init__.py
Usability,simpl,simply,"as of Coifman et al. (2005); # ensures that kernel matrix is independent of sampling density; # q[i] is an estimate for the sampling density at point i; # it's also the degree of the underlying graph; # z[i] is the square root of the row sum of K; """"""\; Compute eigen decomposition of transition matrix. Parameters; ----------; n_comps; Number of eigenvalues/vectors to be computed, set `n_comps = 0` if; you need all eigenvectors.; sym; Instead of computing the eigendecomposition of the assymetric; transition matrix, computed the eigendecomposition of the symmetric; Ktilde matrix.; random_state; A numpy random seed. Returns; -------; Writes the following attributes. eigen_values : :class:`~numpy.ndarray`; Eigenvalues of transition matrix.; eigen_basis : :class:`~numpy.ndarray`; Matrix of eigenvectors (stored in columns). `.eigen_basis` is; projection of data matrix on right eigenvectors, that is, the; projection on the diffusion components. these are simply the; components of the right eigenvectors and can directly be used for; plotting.; """"""; # compute the spectrum; # ncv = max(2 * n_comps + 1, int(np.sqrt(matrix.shape[0]))); # it pays off to increase the stability with a bit more precision; # Setting the random initial vector; # set iroot directly; # set iroot via xroot; # see whether we can set self.iroot using the full data matrix; # account for float32 precision; # thanks to Marius Lange for pointing Alex to this:; # we will likely remove the contributions from the stationary state below when making; # backwards compat breaking changes, they originate from an early implementation in 2015; # they never seem to have deteriorated results, but also other distance measures (see e.g.; # PAGA paper) don't have it, which makes sense; """"""Return pseudotime with respect to root point.""""""; """"""Determine the index of the root cell. Given an expression vector, find the observation index that is closest; to this vector. Parameters; ----------; xroot; Vector that marks the root cel",MatchSource.CODE_COMMENT,src/scanpy/neighbors/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/neighbors/__init__.py
Deployability,update,update,"""""""Color palettes in addition to matplotlib's palettes.""""""; # Colorblindness adjusted vega_10; # See https://github.com/scverse/scanpy/issues/387; # green; # purple; # kakhi; # default matplotlib 2.0 palette; # see 'category20' on https://github.com/vega/vega/wiki/Scales#scale-range-literals; # reorderd, some removed, some added; # dark without grey:; # light without grey:; # manual additions:; # kakhi shifted by missing grey; # TODO: also replace pale colors if necessary; # https://graphicdesign.stackexchange.com/questions/3682/where-can-i-find-a-large-palette-set-of-contrasting-colors-for-coloring-many-d; # update 1; # orig reference https://research.wu.ac.at/en/publications/escaping-rgbland-selecting-colors-for-statistical-graphics-26; # these last ones were added:; # from https://godsnotwheregodsnot.blogspot.com/2012/09/color-distribution-methodology.html; # ""#000000"", # remove the black, as often, we have black colored annotation; # type: plt.Figure, plt.Axes; # Turn off all ticks & spines",MatchSource.CODE_COMMENT,src/scanpy/plotting/palettes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/palettes.py
Energy Efficiency,green,green,"""""""Color palettes in addition to matplotlib's palettes.""""""; # Colorblindness adjusted vega_10; # See https://github.com/scverse/scanpy/issues/387; # green; # purple; # kakhi; # default matplotlib 2.0 palette; # see 'category20' on https://github.com/vega/vega/wiki/Scales#scale-range-literals; # reorderd, some removed, some added; # dark without grey:; # light without grey:; # manual additions:; # kakhi shifted by missing grey; # TODO: also replace pale colors if necessary; # https://graphicdesign.stackexchange.com/questions/3682/where-can-i-find-a-large-palette-set-of-contrasting-colors-for-coloring-many-d; # update 1; # orig reference https://research.wu.ac.at/en/publications/escaping-rgbland-selecting-colors-for-statistical-graphics-26; # these last ones were added:; # from https://godsnotwheregodsnot.blogspot.com/2012/09/color-distribution-methodology.html; # ""#000000"", # remove the black, as often, we have black colored annotation; # type: plt.Figure, plt.Axes; # Turn off all ticks & spines",MatchSource.CODE_COMMENT,src/scanpy/plotting/palettes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/palettes.py
Availability,redundant,redundant,"k_labels', dendrogram=True). Using var_names as dict:. .. plot::; :context: close-figs. markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; sc.pl.tracksplot(adata, markers, groupby='bulk_labels', dendrogram=True). .. currentmodule:: scanpy. See also; --------; pl.rank_genes_groups_tracksplot: to plot marker genes identified using the :func:`~scanpy.tl.rank_genes_groups` function.; """"""; # TODO: fix this line; # get categories colors:; # compute dendrogram if needed and reorder; # rows and columns to match leaves order.; # reorder obs_tidy; # obtain the start and end of each category and make; # a list of ranges that will be used to plot a different; # color; # +2 because of dendrogram on top and categories at bottom; # this is because of the dendrogram; # remove the xticks labels except for the last processed plot.; # Because the plots share the x axis it is redundant and less compact; # to plot the axis for each plot; # the ax to plot the groupby categories is split to add a small space; # between the rest of the plot and the categories; # add lines to plot; """"""\; Plots a dendrogram of the categories defined in `groupby`. See :func:`~scanpy.tl.dendrogram`. Parameters; ----------; adata; Annotated data matrix.; groupby; Categorical data column used to create the dendrogram; dendrogram_key; Key under with the dendrogram information was stored.; By default the dendrogram information is stored under; `.uns[f'dendrogram_{{groupby}}']`.; orientation; Origin of the tree. Will grow into the opposite direction.; remove_labels; Don’t draw labels. Used e.g. by :func:`scanpy.pl.matrixplot`; to annotate matrix columns/rows.; {show_save_ax}. Returns; -------; :class:`matplotlib.axes.Axes`. Examples; --------; .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.tl.dendrogram(adata, 'bulk_labels'); sc.pl.dendrogram(adata, 'bulk_labels'). .. currentmodule:: scanpy. """"""; """"""\; Plots the correlation matrix computed as part ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_anndata.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_anndata.py
Deployability,continuous,continuous,",; or a hex color specification, e.g.,; `'ann1'`, `'#fe57a1'`, or `['ann1', 'ann2']`.; use_raw; Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.; layers; Use the `layers` attribute of `adata` if present: specify the layer for; `x`, `y` and `color`. If `layers` is a string, then it is expanded to; `(layers, layers, layers)`.; basis; String that denotes a plotting tool that computed coordinates.; {scatter_temp}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; """"""; # store .uns annotations that were added to the new adata object; """"""See docstring of scatter.""""""; # Process layers; # color can be a obs column name or a matplotlib color specification; # ignore the '0th' diffusion component; # correct the component vector for use in labeling etc.; # generate the colors; # by default, assume continuous or flat color; # test whether we have categorial or continuous annotation; # coloring according to gene expression; # a flat color; # loop over all categorical annotation and plot it; # actually plot the groups; # draw a frame around the scatter; """"""\; Plot rankings. See, for example, how this is used in pl.pca_loadings. Parameters; ----------; adata; The data.; attr; The attribute of AnnData that contains the score.; keys; The scores to look up an array from the attribute of adata. Returns; -------; Returns matplotlib gridspec with access to the axes.; """"""; """"""\; Violin plot. Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`. Parameters; ----------; adata; Annotated data matrix.; keys; Keys for accessing variables of `.var_names` or fields of `.obs`.; groupby; The key of the observation grouping to consider.; log; Plot on logarithmic axis.; use_raw; Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.; stripplot; Add a stripplot on top of the violin plot.; See :func:`~seaborn.stripplot`.; jitter; Add jitter to the stripplot (only when stripplot",MatchSource.CODE_COMMENT,src/scanpy/plotting/_anndata.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_anndata.py
Energy Efficiency,adapt,adapting,"----. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.pl.violin(adata, keys='S_score'). Plot by category. Rotate x-axis labels so that they do not overlap. .. plot::; :context: close-figs. sc.pl.violin(adata, keys='S_score', groupby='bulk_labels', rotation=90). Set order of categories to be plotted or select specific categories to be plotted. .. plot::; :context: close-figs. groupby_order = ['CD34+', 'CD19+ B']; sc.pl.violin(adata, keys='S_score', groupby='bulk_labels', rotation=90,; order=groupby_order). Plot multiple keys. .. plot::; :context: close-figs. sc.pl.violin(adata, keys=['S_score', 'G2M_score'], groupby='bulk_labels',; rotation=90). For large datasets consider omitting the overlaid scatter plot. .. plot::; :context: close-figs. sc.pl.violin(adata, keys='S_score', stripplot=False). .. currentmodule:: scanpy. See also; --------; pl.stacked_violin; """"""; # Slow import, only import if called; # remove duplicates, preserving the order; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; # set by default the violin plot cut=0 to limit the extend; # of the violin plot (see stacked_violin code) for more info.; """"""\; Hierarchically-clustered heatmap. Wraps :func:`seaborn.clustermap` for :class:`~anndata.AnnData`. Parameters; ----------; adata; Annotated data matrix.; obs_keys; Categorical annotation to plot with a different color map.; Currently, only a single key is supported.; use_raw; Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.; {show_save_ax}; **kwds; Keyword arguments passed to :func:`~seaborn.clustermap`. Returns; -------; If `show` is `False`, a :class:`~seaborn.matrix.ClusterGrid` object; (see :func:`~seaborn.clustermap`). Examples; --------. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.krumsiek11(); sc.pl.clustermap(adata). .. plot::; :context: close-figs. sc.pl.clustermap(adata, obs_keys='cell_ty",MatchSource.CODE_COMMENT,src/scanpy/plotting/_anndata.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_anndata.py
Integrability,depend,depending,"eed to reorder the the groupby; observations based on the dendrogram results. The function checks if a dendrogram has already been precomputed.; If not, `sc.tl.dendrogram` is run with default parameters. The results found in `.uns[dendrogram_key]` are used to reorder; `var_group_labels` and `var_group_positions`. Returns; -------; dictionary with keys:; 'categories_idx_ordered', 'var_group_names_idx_ordered',; 'var_group_labels', and 'var_group_positions'; """"""; # order of groupby categories; # reorder var_groups (if any); # the `dendrogram_key` can be a bool an NoneType or the name of the; # dendrogram key. By default the name of the dendrogram key is 'dendrogram'; """"""\; Plots a dendrogram on the given ax using the precomputed dendrogram; information stored in `.uns[dendrogram_key]`; """"""; """"""\; transforms the dendrogram coordinates to a given new position.; The xlabel_pos and orig_ticks should be of the same; length. This is mostly done for the heatmap case, where the position of the; dendrogram leaves needs to be adjusted depending on the category size. Parameters; ----------; pos_list; list of dendrogram positions that should be translated; new_ticks; sorted list of goal tick positions (e.g. [0,1,2,3] ); old_ticks; sorted list of original tick positions (e.g. [5, 15, 25, 35]),; This list is usually the default position used by; `scipy.cluster.hierarchy.dendrogram`. Returns; -------; translated list of positions. Examples; --------; >>> translate_pos(; ... [5, 15, 20, 21],; ... [0, 1, 2, 3 ],; ... [5, 15, 25, 35],; ... ); [0, 1, 1.5, 1.6]; """"""; # of given coordinates.; # assume that the list is a numpy array; # find smaller and bigger indices; # check that ticks has the same length as orig_ticks; """"""\; Plots categories as colored blocks. If orientation is 'left', the categories; are plotted vertically, otherwise they are plotted horizontally. Parameters; ----------; groupby_ax; obs_tidy; colors; Sequence of valid color names to use for each category.; orientation; ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_anndata.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_anndata.py
Modifiability,variab,variables,"""""""Plotting functions for AnnData.""""""; # TODO: is that all?; # 17 positionals are enough for backwards compatibility; """"""\; Scatter plot along observations or variables axes. Color the plot using annotations of observations (`.obs`), variables; (`.var`) or expression of genes (`.var_names`). Parameters; ----------; adata; Annotated data matrix.; x; x coordinate.; y; y coordinate.; color; Keys for annotations of observations/cells or variables/genes,; or a hex color specification, e.g.,; `'ann1'`, `'#fe57a1'`, or `['ann1', 'ann2']`.; use_raw; Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.; layers; Use the `layers` attribute of `adata` if present: specify the layer for; `x`, `y` and `color`. If `layers` is a string, then it is expanded to; `(layers, layers, layers)`.; basis; String that denotes a plotting tool that computed coordinates.; {scatter_temp}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; """"""; # store .uns annotations that were added to the new adata object; """"""See docstring of scatter.""""""; # Process layers; # color can be a obs column name or a matplotlib color specification; # ignore the '0th' diffusion component; # correct the component vector for use in labeling etc.; # generate the colors; # by default, assume continuous or flat color; # test whether we have categorial or continuous annotation; # coloring according to gene expression; # a flat color; # loop over all categorical annotation and plot it; # actually plot the groups; # draw a frame around the scatter; """"""\; Plot rankings. See, for example, how this is used in pl.pca_loadings. Parameters; ----------; adata; The data.; attr; The attribute of AnnData that contains the score.; keys; The scores to look up an array from the attribute of adata. Returns; -------; Returns matplotlib gridspec with access to the axes.; """"""; """"""\; Violin plot. Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`. Parame",MatchSource.CODE_COMMENT,src/scanpy/plotting/_anndata.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_anndata.py
Safety,redund,redundant,"k_labels', dendrogram=True). Using var_names as dict:. .. plot::; :context: close-figs. markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; sc.pl.tracksplot(adata, markers, groupby='bulk_labels', dendrogram=True). .. currentmodule:: scanpy. See also; --------; pl.rank_genes_groups_tracksplot: to plot marker genes identified using the :func:`~scanpy.tl.rank_genes_groups` function.; """"""; # TODO: fix this line; # get categories colors:; # compute dendrogram if needed and reorder; # rows and columns to match leaves order.; # reorder obs_tidy; # obtain the start and end of each category and make; # a list of ranges that will be used to plot a different; # color; # +2 because of dendrogram on top and categories at bottom; # this is because of the dendrogram; # remove the xticks labels except for the last processed plot.; # Because the plots share the x axis it is redundant and less compact; # to plot the axis for each plot; # the ax to plot the groupby categories is split to add a small space; # between the rest of the plot and the categories; # add lines to plot; """"""\; Plots a dendrogram of the categories defined in `groupby`. See :func:`~scanpy.tl.dendrogram`. Parameters; ----------; adata; Annotated data matrix.; groupby; Categorical data column used to create the dendrogram; dendrogram_key; Key under with the dendrogram information was stored.; By default the dendrogram information is stored under; `.uns[f'dendrogram_{{groupby}}']`.; orientation; Origin of the tree. Will grow into the opposite direction.; remove_labels; Don’t draw labels. Used e.g. by :func:`scanpy.pl.matrixplot`; to annotate matrix columns/rows.; {show_save_ax}. Returns; -------; :class:`matplotlib.axes.Axes`. Examples; --------; .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.tl.dendrogram(adata, 'bulk_labels'); sc.pl.dendrogram(adata, 'bulk_labels'). .. currentmodule:: scanpy. """"""; """"""\; Plots the correlation matrix computed as part ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_anndata.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_anndata.py
Security,access,access,"d coordinates.; {scatter_temp}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; """"""; # store .uns annotations that were added to the new adata object; """"""See docstring of scatter.""""""; # Process layers; # color can be a obs column name or a matplotlib color specification; # ignore the '0th' diffusion component; # correct the component vector for use in labeling etc.; # generate the colors; # by default, assume continuous or flat color; # test whether we have categorial or continuous annotation; # coloring according to gene expression; # a flat color; # loop over all categorical annotation and plot it; # actually plot the groups; # draw a frame around the scatter; """"""\; Plot rankings. See, for example, how this is used in pl.pca_loadings. Parameters; ----------; adata; The data.; attr; The attribute of AnnData that contains the score.; keys; The scores to look up an array from the attribute of adata. Returns; -------; Returns matplotlib gridspec with access to the axes.; """"""; """"""\; Violin plot. Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`. Parameters; ----------; adata; Annotated data matrix.; keys; Keys for accessing variables of `.var_names` or fields of `.obs`.; groupby; The key of the observation grouping to consider.; log; Plot on logarithmic axis.; use_raw; Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.; stripplot; Add a stripplot on top of the violin plot.; See :func:`~seaborn.stripplot`.; jitter; Add jitter to the stripplot (only when stripplot is True); See :func:`~seaborn.stripplot`.; size; Size of the jitter points.; layer; Name of the AnnData object layer that wants to be plotted. By; default adata.raw.X is plotted. If `use_raw=False` is set,; then `adata.X` is plotted. If `layer` is set to a valid layer name,; then the layer is plotted. `layer` takes precedence over `use_raw`.; scale; The method used to scale the width of each violin.; If 'width' (t",MatchSource.CODE_COMMENT,src/scanpy/plotting/_anndata.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_anndata.py
Testability,test,test,",; or a hex color specification, e.g.,; `'ann1'`, `'#fe57a1'`, or `['ann1', 'ann2']`.; use_raw; Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.; layers; Use the `layers` attribute of `adata` if present: specify the layer for; `x`, `y` and `color`. If `layers` is a string, then it is expanded to; `(layers, layers, layers)`.; basis; String that denotes a plotting tool that computed coordinates.; {scatter_temp}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; """"""; # store .uns annotations that were added to the new adata object; """"""See docstring of scatter.""""""; # Process layers; # color can be a obs column name or a matplotlib color specification; # ignore the '0th' diffusion component; # correct the component vector for use in labeling etc.; # generate the colors; # by default, assume continuous or flat color; # test whether we have categorial or continuous annotation; # coloring according to gene expression; # a flat color; # loop over all categorical annotation and plot it; # actually plot the groups; # draw a frame around the scatter; """"""\; Plot rankings. See, for example, how this is used in pl.pca_loadings. Parameters; ----------; adata; The data.; attr; The attribute of AnnData that contains the score.; keys; The scores to look up an array from the attribute of adata. Returns; -------; Returns matplotlib gridspec with access to the axes.; """"""; """"""\; Violin plot. Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`. Parameters; ----------; adata; Annotated data matrix.; keys; Keys for accessing variables of `.var_names` or fields of `.obs`.; groupby; The key of the observation grouping to consider.; log; Plot on logarithmic axis.; use_raw; Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.; stripplot; Add a stripplot on top of the violin plot.; See :func:`~seaborn.stripplot`.; jitter; Add jitter to the stripplot (only when stripplot",MatchSource.CODE_COMMENT,src/scanpy/plotting/_anndata.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_anndata.py
Availability,avail,available,"idx_ordered',; 'var_group_labels' and 'var_group_positions'; """"""; """"""used to clean up warning message""""""; # order of groupby categories; # reorder var_groups (if any); """"""\; Draws brackets that represent groups of genes on the give axis.; For best results, this axis is located on top of an image whose; x axis contains gene names. The gene_groups_ax should share the x axis with the main ax. Eg: gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax). Parameters; ----------; gene_groups_ax; In this axis the gene marks are drawn; group_positions; Each item in the list, should contain the start and end position that the; bracket should cover.; Eg. [(0, 4), (5, 8)] means that there are two brackets, one for the var_names (eg genes); in positions 0-4 and other for positions 5-8; group_labels; List of group labels; left_adjustment; adjustment to plot the bracket start slightly before or after the first gene position.; If the value is negative the start is moved before.; right_adjustment; adjustment to plot the bracket end slightly before or after the last gene position; If the value is negative the start is moved before.; rotation; rotation degrees for the labels. If not given, small labels (<4 characters) are not; rotated, otherwise, they are rotated 90 degrees; orientation; location of the brackets. Either `top` or `right`; """"""; # get the 'brackets' coordinates as lists of start and end positions; # verts and codes are used by PathPatch to make the brackets; # rotate labels if any of them is longer than 4 characters; # lower-left; # upper-left; # upper-right; # lower-right; # upper-left; # upper-right; # lower-right; # lower-left; # cut label to fit available space; # remove y ticks; # remove x ticks and labels; """"""; checks if var_names is a dict. Is this is the cases, then set the; correct values for var_group_labels and var_group_positions. updates var_names, var_group_labels, var_group_positions; """"""; # use list() in case var_list is a numpy array or pandas series",MatchSource.CODE_COMMENT,src/scanpy/plotting/_baseplot_class.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_baseplot_class.py
Deployability,update,updates," then x are; the `groupby` categories and y the `var_names`. Parameters; ----------; swap_axes; Boolean to turn on (True) or off (False) 'swap_axes'. Default True. Returns; -------; Returns `self` for method chaining. """"""; # dendrogram can only be computed between groupby categories; # to correctly plot the dendrogram the categories need to be ordered; # according to the dendrogram ordering.; # hide totals; """"""\; Set visual style parameters. Parameters; ----------; cmap; colormap. Returns; -------; Returns `self` for method chaining.; """"""; # turn of legends by setting width to 0; """"""; Makes the bar plot for totals; """"""; # add numbers to the top of the bars; # for k in total_barplot_ax.spines.keys():; # total_barplot_ax.spines[k].set_visible(False); # add numbers to the right of the bars; """"""; Plots a horizontal colorbar given the ax an normalize values. Parameters; ----------; color_legend_ax; normalize. Returns; -------; `None`, updates color_legend_ax; """"""; # to maintain the fixed height size of the legends, a; # spacer of variable height is added at top and bottom.; # The structure for the legends is:; # first row: variable space to keep the first rows of the same size; # second row: size legend; # to be consistent with the heatmap plot, is better to; # invert the order of the y-axis, such that the first group is on; # top; # +1 for labels; # if the number of categories is small use; # a larger height, otherwise the legends do not fit; # define a layout of 1 rows x 2 columns; # first ax is for the main figure.; # second ax is to plot legends; # add some space in case 'brackets' want to be plotted on top of the image; # gridspec is the same but rows and columns are swapped; # for the figure title use the ax that contains; # all the main graphical elements (main plot, dendrogram etc); # otherwise the title may overlay with the figure.; # also, this puts the title centered on the main figure and not; # centered between the main figure and the legends; # the main plot",MatchSource.CODE_COMMENT,src/scanpy/plotting/_baseplot_class.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_baseplot_class.py
Energy Efficiency,adapt,adapt,"""""""BasePlot for dotplot, matrixplot and stacked_violin""""""; """"""\; title; Title for the figure; colorbar_title; Title for the color bar. New line character (\\n) can be used.; cmap; String denoting matplotlib color map.; standard_scale; Whether or not to standardize the given dimension between 0 and 1, meaning for; each variable or group, subtract the minimum and divide each by its maximum.; swap_axes; By default, the x axis contains `var_names` (e.g. genes) and the y axis; the `groupby` categories. By setting `swap_axes` then x are the; `groupby` categories and y the `var_names`.; return_fig; Returns :class:`DotPlot` object. Useful for fine-tuning; the plot. Takes precedence over `show=False`.; """"""; """"""\; Generic class for the visualization of AnnData categories and; selected `var` (features or genes). Takes care of the visual location of a main plot, additional plots; in the margins (e.g. dendrogram, margin totals) and legends. Also; understand how to adapt the visual parameter if the plot is rotated. Classed based on BasePlot implement their own _mainplot() method. The BasePlot works by method chaining. For example:; BasePlot(adata, ...).legend(title='legend').style(cmap='binary').show(); """"""; # gridspec parameter. Sets the space between mainplot, dendrogram and legend; # maximum number of categories allowed to be plotted; # set default values for legend; # set style defaults; # style default parameters; # minimum height required for legends to plot properly; # after .render() is called the fig value is assigned and ax_dict; # contains a dictionary of the axes used in the plot; """"""; Plots a transposed image. By default, the x axis contains `var_names` (e.g. genes) and the y; axis the `groupby` categories. By setting `swap_axes` then x are; the `groupby` categories and y the `var_names`. Parameters; ----------; swap_axes; Boolean to turn on (True) or off (False) 'swap_axes'. Default True. Returns; -------; Returns `self` for method chaining. """"""; # dendrogram can onl",MatchSource.CODE_COMMENT,src/scanpy/plotting/_baseplot_class.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_baseplot_class.py
Integrability,message,message,"ing of the legends.; kwargs; Passed to :func:`matplotlib.pyplot.savefig`. See also; --------; `render()`: Renders the plot but does not call :func:`matplotlib.pyplot.show`; `show()`: Renders and shows the plot. Examples; -------; >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> markers = [""C1QA"", ""PSAP"", ""CD79A"", ""CD79B"", ""CST3"", ""LYZ""]; >>> sc.pl._baseplot_class.BasePlot(; ... adata, markers, groupby=""bulk_labels""; ... ).savefig(""plot.pdf""); """"""; """"""\; Function used by plotting functions that need to reorder the the groupby; observations based on the dendrogram results. The function checks if a dendrogram has already been precomputed.; If not, `sc.tl.dendrogram` is run with default parameters. The results found in `.uns[dendrogram_key]` are used to reorder; `var_group_labels` and `var_group_positions`. Returns; -------; `None`, internally updates; 'categories_idx_ordered', 'var_group_names_idx_ordered',; 'var_group_labels' and 'var_group_positions'; """"""; """"""used to clean up warning message""""""; # order of groupby categories; # reorder var_groups (if any); """"""\; Draws brackets that represent groups of genes on the give axis.; For best results, this axis is located on top of an image whose; x axis contains gene names. The gene_groups_ax should share the x axis with the main ax. Eg: gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax). Parameters; ----------; gene_groups_ax; In this axis the gene marks are drawn; group_positions; Each item in the list, should contain the start and end position that the; bracket should cover.; Eg. [(0, 4), (5, 8)] means that there are two brackets, one for the var_names (eg genes); in positions 0-4 and other for positions 5-8; group_labels; List of group labels; left_adjustment; adjustment to plot the bracket start slightly before or after the first gene position.; If the value is negative the start is moved before.; right_adjustment; adjustment to plot the bracket end slightly before or after the last gene p",MatchSource.CODE_COMMENT,src/scanpy/plotting/_baseplot_class.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_baseplot_class.py
Modifiability,variab,variable,"""""""BasePlot for dotplot, matrixplot and stacked_violin""""""; """"""\; title; Title for the figure; colorbar_title; Title for the color bar. New line character (\\n) can be used.; cmap; String denoting matplotlib color map.; standard_scale; Whether or not to standardize the given dimension between 0 and 1, meaning for; each variable or group, subtract the minimum and divide each by its maximum.; swap_axes; By default, the x axis contains `var_names` (e.g. genes) and the y axis; the `groupby` categories. By setting `swap_axes` then x are the; `groupby` categories and y the `var_names`.; return_fig; Returns :class:`DotPlot` object. Useful for fine-tuning; the plot. Takes precedence over `show=False`.; """"""; """"""\; Generic class for the visualization of AnnData categories and; selected `var` (features or genes). Takes care of the visual location of a main plot, additional plots; in the margins (e.g. dendrogram, margin totals) and legends. Also; understand how to adapt the visual parameter if the plot is rotated. Classed based on BasePlot implement their own _mainplot() method. The BasePlot works by method chaining. For example:; BasePlot(adata, ...).legend(title='legend').style(cmap='binary').show(); """"""; # gridspec parameter. Sets the space between mainplot, dendrogram and legend; # maximum number of categories allowed to be plotted; # set default values for legend; # set style defaults; # style default parameters; # minimum height required for legends to plot properly; # after .render() is called the fig value is assigned and ax_dict; # contains a dictionary of the axes used in the plot; """"""; Plots a transposed image. By default, the x axis contains `var_names` (e.g. genes) and the y; axis the `groupby` categories. By setting `swap_axes` then x are; the `groupby` categories and y the `var_names`. Parameters; ----------; swap_axes; Boolean to turn on (True) or off (False) 'swap_axes'. Default True. Returns; -------; Returns `self` for method chaining. """"""; # dendrogram can onl",MatchSource.CODE_COMMENT,src/scanpy/plotting/_baseplot_class.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_baseplot_class.py
Safety,avoid,avoid,"plot the mainplot; # code from pandas.plot in add_totals adds; # minor ticks that need to be removed; """"""; Show the figure. Parameters; ----------; return_axes; If true return a dictionary with the figure axes. When return_axes is true; then :func:`matplotlib.pyplot.show` is not called. Returns; -------; If `return_axes=True`: Dict of :class:`matplotlib.axes.Axes`. The dict key; indicates the type of ax (eg. `mainplot_ax`). See also; --------; `render()`: Renders the plot but does not call :func:`matplotlib.pyplot.show`; `savefig()`: Saves the plot. Examples; -------; >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> markers = [""C1QA"", ""PSAP"", ""CD79A"", ""CD79B"", ""CST3"", ""LYZ""]; >>> sc.pl._baseplot_class.BasePlot(adata, markers, groupby=""bulk_labels"").show(); """"""; """"""; Save the current figure. Parameters; ----------; filename; Figure filename. Figure *format* is taken from the file ending unless; the parameter `format` is given.; bbox_inches; By default is set to 'tight' to avoid cropping of the legends.; kwargs; Passed to :func:`matplotlib.pyplot.savefig`. See also; --------; `render()`: Renders the plot but does not call :func:`matplotlib.pyplot.show`; `show()`: Renders and shows the plot. Examples; -------; >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> markers = [""C1QA"", ""PSAP"", ""CD79A"", ""CD79B"", ""CST3"", ""LYZ""]; >>> sc.pl._baseplot_class.BasePlot(; ... adata, markers, groupby=""bulk_labels""; ... ).savefig(""plot.pdf""); """"""; """"""\; Function used by plotting functions that need to reorder the the groupby; observations based on the dendrogram results. The function checks if a dendrogram has already been precomputed.; If not, `sc.tl.dendrogram` is run with default parameters. The results found in `.uns[dendrogram_key]` are used to reorder; `var_group_labels` and `var_group_positions`. Returns; -------; `None`, internally updates; 'categories_idx_ordered', 'var_group_names_idx_ordered',; 'var_group_labels' and 'var_group_posit",MatchSource.CODE_COMMENT,src/scanpy/plotting/_baseplot_class.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_baseplot_class.py
Availability,down,down,"s will draw a 'bracket' or a color block between the given start and end; positions. If the parameter `var_group_labels` is set, the corresponding; labels are added on top/left. E.g. `var_group_positions=[(4,10)]`; will add a bracket between the fourth `var_name` and the tenth `var_name`.; By giving more positions, more brackets/color blocks are drawn.; var_group_labels; Labels for each of the `var_group_positions` that want to be highlighted.; var_group_rotation; Label rotation degrees.; By default, labels larger than 4 characters are rotated 90 degrees.; layer; Name of the AnnData object layer that wants to be plotted. By default adata.raw.X is plotted.; If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name,; then the layer is plotted. `layer` takes precedence over `use_raw`.\; """"""; """"""\; adata; Annotated data matrix.; groups; The groups for which to show the gene ranking.; n_genes; Number of genes to show. This can be a negative number to show for; example the down regulated genes. eg: num_genes=-10. Is ignored if; `gene_names` is passed.; gene_symbols; Column name in `.var` DataFrame that stores gene symbols. By default `var_names`; refer to the index column of the `.var` DataFrame. Setting this option allows; alternative names to be used.; groupby; The key of the observation grouping to consider. By default,; the groupby is chosen from the rank genes groups parameter but; other groupby options can be used. It is expected that; groupby is a categorical. If groupby is not a categorical observation,; it would be subdivided into `num_categories` (see :func:`~scanpy.pl.dotplot`).; min_logfoldchange; Value to filter genes in groups if their logfoldchange is less than the; min_logfoldchange; key; Key used to store the ranking results in `adata.uns`.\; """"""; """"""\; values_to_plot; Instead of the mean gene value, plot the values computed by `sc.rank_genes_groups`.; The options are: ['scores', 'logfoldchanges', 'pvals', 'pvals_adj',; ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_docs.py
Integrability,depend,depending," first value is the width; of the border color as a fraction of the scatter dot size (default: 0.3). The second value is; width of the gap color (default: 0.05).\; """"""; """"""\; ncols; Number of panels per row.; wspace; Adjust the width of the space between multiple panels.; hspace; Adjust the height of the space between multiple panels.; return_fig; Return the matplotlib figure.\; """"""; # Docs for pl.pca, pl.tsne, … (everything in _tools.scatterplots); """"""\; show; Show the plot, do not return axis.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.\; """"""; """"""\; adata; Annotated data matrix.; var_names; `var_names` should be a valid subset of `adata.var_names`.; If `var_names` is a mapping, then the key is used as label; to group the values (see `var_group_labels`). The mapping values; should be sequences of valid `adata.var_names`. In this; case either coloring or 'brackets' are used for the grouping; of var names depending on the plot. When `var_names` is a mapping,; then the `var_group_labels` and `var_group_positions` are set.; groupby; The key of the observation grouping to consider.; use_raw; Use `raw` attribute of `adata` if present.; log; Plot on logarithmic axis.; num_categories; Only used if groupby observation is not categorical. This value; determines the number of groups into which the groupby observation; should be subdivided.; categories_order; Order in which to show the categories. Note: add_dendrogram or add_totals; can change the categories order.; figsize; Figure size when `multi_panel=True`.; Otherwise the `rcParam['figure.figsize]` value is used.; Format is (width, height); dendrogram; If True or a valid dendrogram key, a dendrogram based on the hierarchical; clustering between the `groupby` categories is added.; The dendrogram information is computed using :func:`scanpy.tl.dendrogram`.; If `tl.dendrogram` has not been called previously the func",MatchSource.CODE_COMMENT,src/scanpy/plotting/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_docs.py
Modifiability,variab,variables,"""""""\; Shared docstrings for plotting function parameters.; """"""; """"""\; adata; Annotated data matrix.; color; Keys for annotations of observations/cells or variables/genes, e.g.,; `'ann1'` or `['ann1', 'ann2']`.; gene_symbols; Column name in `.var` DataFrame that stores gene symbols. By default `var_names`; refer to the index column of the `.var` DataFrame. Setting this option allows; alternative names to be used.; use_raw; Use `.raw` attribute of `adata` for coloring with gene expression. If `None`,; defaults to `True` if `layer` isn't provided and `adata.raw` is present.; layer; Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.\; """"""; """"""\; edges; Show edges.; edges_width; Width of edges.; edges_color; Color of edges. See :func:`~networkx.drawing.nx_pylab.draw_networkx_edges`.; neighbors_key; Where to look for neighbors connectivities.; If not specified, this looks .obsp['connectivities'] for connectivities; (default storage place for pp.neighbors).; If specified, this looks; .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.; arrows; Show arrows (deprecated in favour of `scvelo.pl.velocity_embedding`).; arrows_kwds; Passed to :meth:`~matplotlib.axes.Axes.quiver`\; """"""; """"""\; color_map; Color map to use for continous variables. Can be a name or a; :class:`~matplotlib.colors.Colormap` instance (e.g. `""magma`"", `""viridis""`; or `mpl.cm.cividis`), see :func:`~matplotlib.pyplot.get_cmap`.; If `None`, the value of `mpl.rcParams[""image.cmap""]` is used.; The default `color_map` can be set using :func:`~scanpy.set_figure_params`.; palette; Colors to use for plotting categorical annotation groups.; The palette can be a valid :class:`~matplotlib.colors.ListedColormap` name; (`'Set2'`, `'tab20'`, …), a :class:`~cycler.Cycler` object, a dict mapping; categ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_docs.py
Testability,log,log,"ce; Adjust the height of the space between multiple panels.; return_fig; Return the matplotlib figure.\; """"""; # Docs for pl.pca, pl.tsne, … (everything in _tools.scatterplots); """"""\; show; Show the plot, do not return axis.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.\; """"""; """"""\; adata; Annotated data matrix.; var_names; `var_names` should be a valid subset of `adata.var_names`.; If `var_names` is a mapping, then the key is used as label; to group the values (see `var_group_labels`). The mapping values; should be sequences of valid `adata.var_names`. In this; case either coloring or 'brackets' are used for the grouping; of var names depending on the plot. When `var_names` is a mapping,; then the `var_group_labels` and `var_group_positions` are set.; groupby; The key of the observation grouping to consider.; use_raw; Use `raw` attribute of `adata` if present.; log; Plot on logarithmic axis.; num_categories; Only used if groupby observation is not categorical. This value; determines the number of groups into which the groupby observation; should be subdivided.; categories_order; Order in which to show the categories. Note: add_dendrogram or add_totals; can change the categories order.; figsize; Figure size when `multi_panel=True`.; Otherwise the `rcParam['figure.figsize]` value is used.; Format is (width, height); dendrogram; If True or a valid dendrogram key, a dendrogram based on the hierarchical; clustering between the `groupby` categories is added.; The dendrogram information is computed using :func:`scanpy.tl.dendrogram`.; If `tl.dendrogram` has not been called previously the function is called; with default parameters.; gene_symbols; Column name in `.var` DataFrame that stores gene symbols.; By default `var_names` refer to the index column of the `.var` DataFrame.; Setting this option allows alternative names to be used.; var_group_positions; Use thi",MatchSource.CODE_COMMENT,src/scanpy/plotting/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_docs.py
Availability,avail,available,"; expressed only if the expression value is greater than this threshold.; mean_only_expressed; If True, gene expression is averaged only over the cells; expressing the given genes.; dot_max; If none, the maximum dot size is set to the maximum fraction value found; (e.g. 0.6). If given, the value should be a number between 0 and 1.; All fractions larger than dot_max are clipped to this value.; dot_min; If none, the minimum dot size is set to 0. If given,; the value should be a number between 0 and 1.; All fractions smaller than dot_min are clipped to this value.; smallest_dot; If none, the smallest dot has size 0.; All expression levels with `dot_min` are plotted with this size.; {show_save_ax}; {vminmax}; kwds; Are passed to :func:`matplotlib.pyplot.scatter`. Returns; -------; If `return_fig` is `True`, returns a :class:`~scanpy.pl.DotPlot` object,; else if `show` is false, return axes dict. See also; --------; :class:`~scanpy.pl.DotPlot`: The DotPlot class can be used to to control; several visual parameters not available in this function.; :func:`~scanpy.pl.rank_genes_groups_dotplot`: to plot marker genes; identified using the :func:`~scanpy.tl.rank_genes_groups` function. Examples; --------. Create a dot plot using the given markers and the PBMC example dataset grouped by; the category 'bulk_labels'. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True). Using var_names as dict:. .. plot::; :context: close-figs. markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True). Get DotPlot object for fine tuning. .. plot::; :context: close-figs. dp = sc.pl.dotplot(adata, markers, 'bulk_labels', return_fig=True); dp.add_totals().style(dot_edge_color='black', dot_edge_lw=0.5).show(). The axes used can be obtained using the ge",MatchSource.CODE_COMMENT,src/scanpy/plotting/_dotplot.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_dotplot.py
Integrability,depend,depending,"idth. When `color_on='dot'` the default is no edge. When; `color_on='square'`, line width = 1.5; grid; Adds a grid to the plot; x_paddding; Space between the plot left/right borders and the dots center. A unit; is the distance between the x ticks. Only applied when color_on = dot; y_paddding; Space between the plot top/bottom borders and the dots center. A unit is; the distance between the y ticks. Only applied when color_on = dot; kwds; Are passed to :func:`matplotlib.pyplot.scatter`. Returns; -------; matplotlib.colors.Normalize, dot_min, dot_max. """"""; # make scatter plot in which; # x = var_names; # y = groupby category; # size = fraction; # color = mean expression; # +0.5 in y and x to set the dot center at 0.5 multiples; # this facilitates dendrogram and totals alignment for; # matrixplot, dotplot and stackec_violin using the same coordinates.; # clip frac between dot_min and dot_max; # re-scale frac between 0 and 1; # rescale size to match smallest_dot and largest_dot; # use either black or white for the edge color; # depending on the luminance of the background; # square color; # first make a heatmap similar to `sc.pl.matrixplot`; # (squares with the asigned colormap). Circles will be plotted; # on top; # to be consistent with the heatmap plot, is better to; # invert the order of the y-axis, such that the first group is on; # top; # add padding to the x and y lims when the color is not in the square; # default y range goes from 0.5 to num cols + 0.5; # and default x range goes from 0.5 to num rows + 0.5, thus; # the padding needs to be corrected.; # No need to have backwards compat for > 16 positional parameters; """"""\; Makes a *dot plot* of the expression values of `var_names`. For each var_name and each `groupby` category a dot is plotted.; Each dot represents two values: mean expression within each category; (visualized by color) and fraction of cells expressing the `var_name` in the; category (visualized by the size of the dot). If `groupby` is not given,;",MatchSource.CODE_COMMENT,src/scanpy/plotting/_dotplot.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_dotplot.py
Modifiability,variab,variable,"on of cells expressing the `var_name` in the; category (visualized by the size of the dot). If `groupby` is not given,; the dotplot assumes that all data belongs to a single category. .. note::; A gene is considered expressed if the expression value in the `adata` (or; `adata.raw`) is above the specified threshold which is zero by default. An example of dotplot usage is to visualize, for multiple marker genes,; the mean value and the percentage of cells expressing the gene; across multiple clusters. Parameters; ----------; {common_plot_args}; title; Title for the figure; expression_cutoff; Expression cutoff that is used for binarizing the gene expression and; determining the fraction of cells expressing given genes. A gene is; expressed only if the expression value is greater than this threshold.; mean_only_expressed; If True, gene expression is averaged only over the cells; expressing the given genes.; standard_scale; Whether or not to standardize that dimension between 0 and 1,; meaning for each variable or group,; subtract the minimum and divide each by its maximum.; kwds; Are passed to :func:`matplotlib.pyplot.scatter`. See also; --------; :func:`~scanpy.pl.dotplot`: Simpler way to call DotPlot but with less options.; :func:`~scanpy.pl.rank_genes_groups_dotplot`: to plot marker; genes identified using the :func:`~scanpy.tl.rank_genes_groups` function. Examples; --------. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels').show(). Using var_names as dict:. >>> markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels').show(). """"""; # default style parameters; # default legend parameters; # inches; # a unit is the distance between two x-axis ticks; # a unit is the distance between two y-axis ticks; # for if category defined by groupby (if any) compute for each va",MatchSource.CODE_COMMENT,src/scanpy/plotting/_dotplot.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_dotplot.py
Safety,avoid,avoid,"of the legends area. The unit is the same as in matplotlib (inches). Returns; -------; :class:`~scanpy.pl.DotPlot`. Examples; --------. Set color bar title:. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}; >>> dp = sc.pl.DotPlot(adata, markers, groupby='bulk_labels'); >>> dp.legend(colorbar_title='log(UMI counts + 1)').show(); """"""; # turn of legends by setting width to 0; # for the dot size legend, use step between dot_max and dot_min; # based on how different they are.; # a descending range that is afterwards inverted is used; # to guarantee that dot_max is in the legend.; # plot size bar; # remove y ticks and labels; # remove surrounding lines; # to maintain the fixed height size of the legends, a; # spacer of variable height is added at the bottom.; # The structure for the legends is:; # first row: variable space to keep the other rows of; # the same size (avoid stretching); # second row: legend for dot size; # third row: spacer to avoid color and size legend titles to overlap; # fourth row: colorbar; # work on a copy of the dataframes. This is to avoid changes; # on the original data frames after repetitive calls to the; # DotPlot object, for example once with swap_axes and other without; """"""\; Makes a *dot plot* given two data frames, one containing; the doc size and other containing the dot color. The indices and; columns of the data frame are used to label the output image. The dots are plotted using :func:`matplotlib.pyplot.scatter`. Thus, additional; arguments can be passed. Parameters; ----------; dot_size: Data frame containing the dot_size.; dot_color: Data frame containing the dot_color, should have the same,; shape, columns and indices as dot_size.; dot_ax: matplotlib axis; cmap; String denoting matplotlib color map.; color_on; Options are 'dot' or 'square'. Be default the colomap is applied to; the color of the dot. Optionally, the colormap can be applied to an",MatchSource.CODE_COMMENT,src/scanpy/plotting/_dotplot.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_dotplot.py
Testability,log,log,"efaults; # change only the values that had changed; """"""\; Configures dot size and the colorbar legends. Parameters; ----------; show; Set to `False` to hide the default plot of the legends. This sets the; legend width to zero, which will result in a wider main plot.; show_size_legend; Set to `False` to hide the dot size legend; show_colorbar; Set to `False` to hide the colorbar legend; size_title; Title for the dot size legend. Use '\\n' to add line breaks. Appears on top; of dot sizes; colorbar_title; Title for the color bar. Use '\\n' to add line breaks. Appears on top of the; color bar; width; Width of the legends area. The unit is the same as in matplotlib (inches). Returns; -------; :class:`~scanpy.pl.DotPlot`. Examples; --------. Set color bar title:. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}; >>> dp = sc.pl.DotPlot(adata, markers, groupby='bulk_labels'); >>> dp.legend(colorbar_title='log(UMI counts + 1)').show(); """"""; # turn of legends by setting width to 0; # for the dot size legend, use step between dot_max and dot_min; # based on how different they are.; # a descending range that is afterwards inverted is used; # to guarantee that dot_max is in the legend.; # plot size bar; # remove y ticks and labels; # remove surrounding lines; # to maintain the fixed height size of the legends, a; # spacer of variable height is added at the bottom.; # The structure for the legends is:; # first row: variable space to keep the other rows of; # the same size (avoid stretching); # second row: legend for dot size; # third row: spacer to avoid color and size legend titles to overlap; # fourth row: colorbar; # work on a copy of the dataframes. This is to avoid changes; # on the original data frames after repetitive calls to the; # DotPlot object, for example once with swap_axes and other without; """"""\; Makes a *dot plot* given two data frames, one containing; the doc size and other ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_dotplot.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_dotplot.py
Availability,avail,available," changed; # work on a copy of the dataframes. This is to avoid changes; # on the original data frames after repetitive calls to the; # MatrixPlot object, for example once with swap_axes and other without; # to be consistent with the heatmap plot, is better to; # invert the order of the y-axis, such that the first group is on; # top; # 17 positionals are enough for backwards compatibility; """"""\; Creates a heatmap of the mean expression values per group of each var_names. This function provides a convenient interface to the :class:`~scanpy.pl.MatrixPlot`; class. If you need more flexibility, you should use :class:`~scanpy.pl.MatrixPlot`; directly. Parameters; ----------; {common_plot_args}; {groupby_plots_args}; {show_save_ax}; {vminmax}; kwds; Are passed to :func:`matplotlib.pyplot.pcolor`. Returns; -------; If `return_fig` is `True`, returns a :class:`~scanpy.pl.MatrixPlot` object,; else if `show` is false, return axes dict. See also; --------; :class:`~scanpy.pl.MatrixPlot`: The MatrixPlot class can be used to to control; several visual parameters not available in this function.; :func:`~scanpy.pl.rank_genes_groups_matrixplot`: to plot marker genes; identified using the :func:`~scanpy.tl.rank_genes_groups` function. Examples; --------. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.matrixplot(adata, markers, groupby='bulk_labels', dendrogram=True). Using var_names as dict:. .. plot::; :context: close-figs. markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; sc.pl.matrixplot(adata, markers, groupby='bulk_labels', dendrogram=True). Get Matrix object for fine tuning:. .. plot::; :context: close-figs. mp = sc.pl.matrixplot(adata, markers, 'bulk_labels', return_fig=True); mp.add_totals().style(edge_color='black').show(). The axes used can be obtained using the get_axes() method. .. plot::; :context: close-figs. axes_dict = mp.get_axes(); """"""",MatchSource.CODE_COMMENT,src/scanpy/plotting/_matrixplot.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_matrixplot.py
Integrability,interface,interface,"t. Default is gray; edge_lw; Edge line width. Returns; -------; :class:`~scanpy.pl.MatrixPlot`. Examples; -------. .. plot::; :context: close-figs. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. Change color map and turn off edges:. .. plot::; :context: close-figs. (; sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels'); .style(cmap='Blues', edge_color='none'); .show(); ). """"""; # change only the values that had changed; # work on a copy of the dataframes. This is to avoid changes; # on the original data frames after repetitive calls to the; # MatrixPlot object, for example once with swap_axes and other without; # to be consistent with the heatmap plot, is better to; # invert the order of the y-axis, such that the first group is on; # top; # 17 positionals are enough for backwards compatibility; """"""\; Creates a heatmap of the mean expression values per group of each var_names. This function provides a convenient interface to the :class:`~scanpy.pl.MatrixPlot`; class. If you need more flexibility, you should use :class:`~scanpy.pl.MatrixPlot`; directly. Parameters; ----------; {common_plot_args}; {groupby_plots_args}; {show_save_ax}; {vminmax}; kwds; Are passed to :func:`matplotlib.pyplot.pcolor`. Returns; -------; If `return_fig` is `True`, returns a :class:`~scanpy.pl.MatrixPlot` object,; else if `show` is false, return axes dict. See also; --------; :class:`~scanpy.pl.MatrixPlot`: The MatrixPlot class can be used to to control; several visual parameters not available in this function.; :func:`~scanpy.pl.rank_genes_groups_matrixplot`: to plot marker genes; identified using the :func:`~scanpy.tl.rank_genes_groups` function. Examples; --------. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.matrixplot(adata, markers, groupby='bulk_labels', dendrogram=True). Using var_names as dict:",MatchSource.CODE_COMMENT,src/scanpy/plotting/_matrixplot.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_matrixplot.py
Modifiability,variab,variable,"""""""\; Allows the visualization of values using a color map. Parameters; ----------; {common_plot_args}; title; Title for the figure.; expression_cutoff; Expression cutoff that is used for binarizing the gene expression and; determining the fraction of cells expressing given genes. A gene is; expressed only if the expression value is greater than this threshold.; mean_only_expressed; If True, gene expression is averaged only over the cells; expressing the given genes.; standard_scale; Whether or not to standardize that dimension between 0 and 1,; meaning for each variable or group,; subtract the minimum and divide each by its maximum.; values_df; Optionally, a dataframe with the values to plot can be given. The; index should be the grouby categories and the columns the genes names. kwds; Are passed to :func:`matplotlib.pyplot.scatter`. See also; --------; :func:`~scanpy.pl.matrixplot`: Simpler way to call MatrixPlot but with less options.; :func:`~scanpy.pl.rank_genes_groups_matrixplot`: to plot marker genes identified; using the :func:`~scanpy.tl.rank_genes_groups` function. Examples; --------. Simple visualization of the average expression of a few genes grouped by; the category 'bulk_labels'. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels').show(). Same visualization but passing var_names as dict, which adds a grouping of; the genes on top of the image:. .. plot::; :context: close-figs. markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels').show(); """"""; # default style parameters; # compute mean value; """"""\; Modifies plot visual parameters. Parameters; ----------; cmap; String denoting matplotlib color map.; edge_color; Edge color between the squares of matrix plot. Default is gray; edge_lw; Edge line width. Returns; -------; :class:`~sc",MatchSource.CODE_COMMENT,src/scanpy/plotting/_matrixplot.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_matrixplot.py
Safety,avoid,avoid,"groupby='bulk_labels').show(); """"""; # default style parameters; # compute mean value; """"""\; Modifies plot visual parameters. Parameters; ----------; cmap; String denoting matplotlib color map.; edge_color; Edge color between the squares of matrix plot. Default is gray; edge_lw; Edge line width. Returns; -------; :class:`~scanpy.pl.MatrixPlot`. Examples; -------. .. plot::; :context: close-figs. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. Change color map and turn off edges:. .. plot::; :context: close-figs. (; sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels'); .style(cmap='Blues', edge_color='none'); .show(); ). """"""; # change only the values that had changed; # work on a copy of the dataframes. This is to avoid changes; # on the original data frames after repetitive calls to the; # MatrixPlot object, for example once with swap_axes and other without; # to be consistent with the heatmap plot, is better to; # invert the order of the y-axis, such that the first group is on; # top; # 17 positionals are enough for backwards compatibility; """"""\; Creates a heatmap of the mean expression values per group of each var_names. This function provides a convenient interface to the :class:`~scanpy.pl.MatrixPlot`; class. If you need more flexibility, you should use :class:`~scanpy.pl.MatrixPlot`; directly. Parameters; ----------; {common_plot_args}; {groupby_plots_args}; {show_save_ax}; {vminmax}; kwds; Are passed to :func:`matplotlib.pyplot.pcolor`. Returns; -------; If `return_fig` is `True`, returns a :class:`~scanpy.pl.MatrixPlot` object,; else if `show` is false, return axes dict. See also; --------; :class:`~scanpy.pl.MatrixPlot`: The MatrixPlot class can be used to to control; several visual parameters not available in this function.; :func:`~scanpy.pl.rank_genes_groups_matrixplot`: to plot marker genes; identified using the :func:`~scanpy.tl.rank_genes_groups` function. Examples; --------. .. ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_matrixplot.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_matrixplot.py
Testability,log,log,"# --------------------------------------------------------------------------------; # Plot result of preprocessing functions; # --------------------------------------------------------------------------------; """"""Plot dispersions or normalized variance versus means for genes. Produces Supp. Fig. 5c of Zheng et al. (2017) and MeanVarPlot() and; VariableFeaturePlot() of Seurat. Parameters; ----------; adata; Result of :func:`~scanpy.pp.highly_variable_genes`.; log; Plot on logarithmic axes.; show; Show the plot, do not return axis.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.; """"""; # there's a bug in autoscale; # backwards compat; """"""\; Plot dispersions versus means for genes. Produces Supp. Fig. 5c of Zheng et al. (2017) and MeanVarPlot() of Seurat. Parameters; ----------; result; Result of :func:`~scanpy.pp.filter_genes_dispersion`.; log; Plot on logarithmic axes.; show; Show the plot, do not return axis.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.; """"""",MatchSource.CODE_COMMENT,src/scanpy/plotting/_preprocessing.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_preprocessing.py
Safety,predict,predicted,"""""""\; Fraction of counts assigned to each gene over all cells. Computes, for each gene, the fraction of counts assigned to that gene within; a cell. The `n_top` genes with the highest mean fraction over all cells are; plotted as boxplots. This plot is similar to the `scater` package function `plotHighestExprs(type; = ""highest-expression"")`, see `here; <https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/vignette-qc.html>`__. Quoting; from there:. *We expect to see the “usual suspects”, i.e., mitochondrial genes, actin,; ribosomal protein, MALAT1. A few spike-in transcripts may also be; present here, though if all of the spike-ins are in the top 50, it; suggests that too much spike-in RNA was added. A large number of; pseudo-genes or predicted genes may indicate problems with alignment.*; -- Davis McCarthy and Aaron Lun. Parameters; ----------; adata; Annotated data matrix.; n_top; Number of top; {show_save_ax}; gene_symbols; Key for field in .var that stores gene symbols if you do not want to use .var_names.; log; Plot x-axis in log scale; **kwds; Are passed to :func:`~seaborn.boxplot`. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes`.; """"""; # Slow import, only import if called; # compute the percentage of each gene per cell; # identify the genes with the highest mean; # figsize is hardcoded to produce a tall image. To change the fig size,; # a matplotlib.axes.Axes object needs to be passed.",MatchSource.CODE_COMMENT,src/scanpy/plotting/_qc.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_qc.py
Testability,log,log,"""""""\; Fraction of counts assigned to each gene over all cells. Computes, for each gene, the fraction of counts assigned to that gene within; a cell. The `n_top` genes with the highest mean fraction over all cells are; plotted as boxplots. This plot is similar to the `scater` package function `plotHighestExprs(type; = ""highest-expression"")`, see `here; <https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/vignette-qc.html>`__. Quoting; from there:. *We expect to see the “usual suspects”, i.e., mitochondrial genes, actin,; ribosomal protein, MALAT1. A few spike-in transcripts may also be; present here, though if all of the spike-ins are in the top 50, it; suggests that too much spike-in RNA was added. A large number of; pseudo-genes or predicted genes may indicate problems with alignment.*; -- Davis McCarthy and Aaron Lun. Parameters; ----------; adata; Annotated data matrix.; n_top; Number of top; {show_save_ax}; gene_symbols; Key for field in .var that stores gene symbols if you do not want to use .var_names.; log; Plot x-axis in log scale; **kwds; Are passed to :func:`~seaborn.boxplot`. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes`.; """"""; # Slow import, only import if called; # compute the percentage of each gene per cell; # identify the genes with the highest mean; # figsize is hardcoded to produce a tall image. To change the fig size,; # a matplotlib.axes.Axes object needs to be passed.",MatchSource.CODE_COMMENT,src/scanpy/plotting/_qc.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_qc.py
Availability,avail,available,"ale the width of each violin.; If 'width' (the default), each violin will have the same width.; If 'area', each violin will have the same area.; If 'count', a violin’s width corresponds to the number of observations.; yticklabels; Set to true to view the y tick labels.; row_palette; Be default, median values are mapped to the violin color using a; color map (see `cmap` argument). Alternatively, a 'row_palette` can; be given to color each violin plot row using a different colors.; The value should be a valid seaborn or matplotlib palette name; (see :func:`~seaborn.color_palette`).; Alternatively, a single color name or hex value can be passed,; e.g. `'red'` or `'#cc33ff'`.; {show_save_ax}; {vminmax}; kwds; Are passed to :func:`~seaborn.violinplot`. Returns; -------; If `return_fig` is `True`, returns a :class:`~scanpy.pl.StackedViolin` object,; else if `show` is false, return axes dict. See also; --------; :class:`~scanpy.pl.StackedViolin`: The StackedViolin class can be used to to control; several visual parameters not available in this function.; :func:`~scanpy.pl.rank_genes_groups_stacked_violin` to plot marker genes identified; using the :func:`~scanpy.tl.rank_genes_groups` function. Examples; -------. Visualization of violin plots of a few genes grouped by the category `bulk_labels`:. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.stacked_violin(adata, markers, groupby='bulk_labels', dendrogram=True). Same visualization but passing var_names as dict, which adds a grouping of; the genes on top of the image:. .. plot::; :context: close-figs. markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; sc.pl.stacked_violin(adata, markers, groupby='bulk_labels', dendrogram=True). Get StackedViolin object for fine tuning. .. plot::; :context: close-figs. vp = sc.pl.stacked_violin(adata, markers, 'bulk_labels', return_fig=True); vp.add_totals().sty",MatchSource.CODE_COMMENT,src/scanpy/plotting/_stacked_violin.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_stacked_violin.py
Integrability,interface,interface,"color_df the values are not renamed as those; # values will be used to label the ticks); # use a single `color`` if row_colors[idx] is defined; # else use the palette; """"""; Configures each of the violin plot axes ticks like remove or add labels etc. """"""; # remove the default seaborn grids because in such a compact; # plot are unnecessary; # make line a bit ticker to see the extend of the yaxis in the; # final plot; # use only the smallest and the largest y ticks; # and align the firts label on top of the tick and; # the second below the tick. This avoid overlapping; # of nearby ticks; # use MaxNLocator to set 2 ticks; # remove labels; # 17 positionals are enough for backwards compatibility; """"""\; Stacked violin plots. Makes a compact image composed of individual violin plots; (from :func:`~seaborn.violinplot`) stacked on top of each other.; Useful to visualize gene expression per cluster. Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`. This function provides a convenient interface to the; :class:`~scanpy.pl.StackedViolin` class. If you need more flexibility,; you should use :class:`~scanpy.pl.StackedViolin` directly. Parameters; ----------; {common_plot_args}; {groupby_plots_args}; stripplot; Add a stripplot on top of the violin plot.; See :func:`~seaborn.stripplot`.; jitter; Add jitter to the stripplot (only when stripplot is True); See :func:`~seaborn.stripplot`.; size; Size of the jitter points.; order; Order in which to show the categories. Note: if `dendrogram=True`; the categories order will be given by the dendrogram and `order`; will be ignored.; scale; The method used to scale the width of each violin.; If 'width' (the default), each violin will have the same width.; If 'area', each violin will have the same area.; If 'count', a violin’s width corresponds to the number of observations.; yticklabels; Set to true to view the y tick labels.; row_palette; Be default, median values are mapped to the violin color using a; color map (see `cmap` argu",MatchSource.CODE_COMMENT,src/scanpy/plotting/_stacked_violin.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_stacked_violin.py
Modifiability,variab,variable,"; stripplot; Add a stripplot on top of the violin plot.; See :func:`~seaborn.stripplot`.; jitter; Add jitter to the stripplot (only when stripplot is True); See :func:`~seaborn.stripplot`.; size; Size of the jitter points.; order; Order in which to show the categories. Note: if `dendrogram=True`; the categories order will be given by the dendrogram and `order`; will be ignored.; density_norm; The method used to scale the width of each violin.; If 'width' (the default), each violin will have the same width.; If 'area', each violin will have the same area.; If 'count', a violin’s width corresponds to the number of observations.; row_palette; The row palette determines the colors to use for the stacked violins.; The value should be a valid seaborn or matplotlib palette name; (see :func:`~seaborn.color_palette`).; Alternatively, a single color name or hex value can be passed,; e.g. `'red'` or `'#cc33ff'`.; standard_scale; Whether or not to standardize a dimension between 0 and 1,; meaning for each variable or observation,; subtract the minimum and divide each by its maximum.; swap_axes; By default, the x axis contains `var_names` (e.g. genes) and the y axis; the `groupby` categories. By setting `swap_axes` then x are the `groupby`; categories and y the `var_names`. When swapping; axes var_group_positions are no longer used; kwds; Are passed to :func:`~seaborn.violinplot`. See also; --------; :func:`~scanpy.pl.stacked_violin`: simpler way to call StackedViolin but with less; options.; :func:`~scanpy.pl.violin` and :func:`~scanpy.pl.rank_genes_groups_stacked_violin`:; to plot marker genes identified using :func:`~scanpy.tl.rank_genes_groups`. Examples; -------. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; >>> sc.pl.StackedViolin(adata, markers, groupby='bulk_labels', dendrogram=True) # doctest: +ELLIPSIS; <scanpy.plotting._stacked_violin.StackedViolin object at 0x...>. Using var_names a",MatchSource.CODE_COMMENT,src/scanpy/plotting/_stacked_violin.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_stacked_violin.py
Safety,avoid,avoid,"lues for example. From seaborn.violin documentation:; #; # cut: Distance, in units of bandwidth size, to extend the density past; # the extreme datapoints. Set to 0 to limit the violin range within; # the range of the observed data (i.e., to have the same effect as; # trim=True in ggplot.; # inner{“box”, “quartile”, “point”, “stick”, None} (Default seaborn: box); # Representation of the datapoints in the violin interior. If box, draw a; # miniature boxplot. If quartiles, draw the quartiles of the distribution.; # If point or stick, show each underlying datapoint. Using; # None will draw unadorned violins.; """"""Called unconditionally when accessing an instance attribute""""""; # If the user has set the deprecated version on the class,; # and our code accesses the new version from the instance,; # return the user-specified version instead and warn.; # This is done because class properties are hard to do.; # Set default style parameters; # deprecated; # modify only values that had changed; # space needs to be added to avoid overlapping; # of labels and legend or dendrogram/totals.; # to make the stacked violin plots, the; # `ax` is subdivided horizontally and in each horizontal sub ax; # a seaborn violin plot is added.; # work on a copy of the dataframes. This is to avoid changes; # on the original data frames after repetitive calls to the; # StackedViolin object, for example once with swap_axes and other without; # get mean values for color and transform to color values; # using colormap; # turn on axis for `ax` as this is turned off; # by make_grid_spec when the axis is subdivided earlier.; # add tick labels; # 0.5 to position the ticks on the center of the violins; # 0.5 to position the ticks on the center of the violins; # rotate x tick labels if they are longer than 2 characters; # Slow import, only import if called; # when row_palette is used, there is no need for a legend; # All columns should have a unique name, yet, frequently; # gene names are repeated in self.var",MatchSource.CODE_COMMENT,src/scanpy/plotting/_stacked_violin.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_stacked_violin.py
Security,access,accessing,"ect at 0x...>; """"""; # a unit is the distance between two x-axis ticks; # a unit is the distance between two y-axis ticks; # set by default the violin plot cut=0 to limit the extend; # of the violin plot as this produces better plots that wont extend; # to negative values for example. From seaborn.violin documentation:; #; # cut: Distance, in units of bandwidth size, to extend the density past; # the extreme datapoints. Set to 0 to limit the violin range within; # the range of the observed data (i.e., to have the same effect as; # trim=True in ggplot.; # inner{“box”, “quartile”, “point”, “stick”, None} (Default seaborn: box); # Representation of the datapoints in the violin interior. If box, draw a; # miniature boxplot. If quartiles, draw the quartiles of the distribution.; # If point or stick, show each underlying datapoint. Using; # None will draw unadorned violins.; """"""Called unconditionally when accessing an instance attribute""""""; # If the user has set the deprecated version on the class,; # and our code accesses the new version from the instance,; # return the user-specified version instead and warn.; # This is done because class properties are hard to do.; # Set default style parameters; # deprecated; # modify only values that had changed; # space needs to be added to avoid overlapping; # of labels and legend or dendrogram/totals.; # to make the stacked violin plots, the; # `ax` is subdivided horizontally and in each horizontal sub ax; # a seaborn violin plot is added.; # work on a copy of the dataframes. This is to avoid changes; # on the original data frames after repetitive calls to the; # StackedViolin object, for example once with swap_axes and other without; # get mean values for color and transform to color values; # using colormap; # turn on axis for `ax` as this is turned off; # by make_grid_spec when the axis is subdivided earlier.; # add tick labels; # 0.5 to position the ticks on the center of the violins; # 0.5 to position the ticks on the center o",MatchSource.CODE_COMMENT,src/scanpy/plotting/_stacked_violin.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_stacked_violin.py
Usability,simpl,simpler,"fault), each violin will have the same width.; If 'area', each violin will have the same area.; If 'count', a violin’s width corresponds to the number of observations.; row_palette; The row palette determines the colors to use for the stacked violins.; The value should be a valid seaborn or matplotlib palette name; (see :func:`~seaborn.color_palette`).; Alternatively, a single color name or hex value can be passed,; e.g. `'red'` or `'#cc33ff'`.; standard_scale; Whether or not to standardize a dimension between 0 and 1,; meaning for each variable or observation,; subtract the minimum and divide each by its maximum.; swap_axes; By default, the x axis contains `var_names` (e.g. genes) and the y axis; the `groupby` categories. By setting `swap_axes` then x are the `groupby`; categories and y the `var_names`. When swapping; axes var_group_positions are no longer used; kwds; Are passed to :func:`~seaborn.violinplot`. See also; --------; :func:`~scanpy.pl.stacked_violin`: simpler way to call StackedViolin but with less; options.; :func:`~scanpy.pl.violin` and :func:`~scanpy.pl.rank_genes_groups_stacked_violin`:; to plot marker genes identified using :func:`~scanpy.tl.rank_genes_groups`. Examples; -------. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; >>> sc.pl.StackedViolin(adata, markers, groupby='bulk_labels', dendrogram=True) # doctest: +ELLIPSIS; <scanpy.plotting._stacked_violin.StackedViolin object at 0x...>. Using var_names as dict:. >>> markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; >>> sc.pl.StackedViolin(adata, markers, groupby='bulk_labels', dendrogram=True) # doctest: +ELLIPSIS; <scanpy.plotting._stacked_violin.StackedViolin object at 0x...>; """"""; # a unit is the distance between two x-axis ticks; # a unit is the distance between two y-axis ticks; # set by default the violin plot cut=0 to limit the extend; # of the violin plot as this produces better plo",MatchSource.CODE_COMMENT,src/scanpy/plotting/_stacked_violin.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_stacked_violin.py
Availability,error,errors,"Parameters; ----------; x, y : scalar or array_like, shape (n, ); Input data; s : scalar or array_like, shape (n, ); Radius of circles.; c : color or sequence of color, optional, default : 'b'; `c` can be a single color format string, or a sequence of color; specifications of length `N`, or a sequence of `N` numbers to be; mapped to colors using the `cmap` and `norm` specified via kwargs.; Note that `c` should not be a single numeric RGB or RGBA sequence; because that is indistinguishable from an array of values; to be colormapped. (If you insist, use `color` instead.); `c` can be a 2-D array in which the rows are RGB or RGBA, however.; vmin, vmax : scalar, optional, default: None; `vmin` and `vmax` are used in conjunction with `norm` to normalize; luminance data. If either are `None`, the min and max of the; color array is used.; kwargs : `~matplotlib.collections.Collection` properties; Eg. alpha, edgecolor(ec), facecolor(fc), linewidth(lw), linestyle(ls),; norm, cmap, transform, etc.; Returns; -------; paths : `~matplotlib.collections.PathCollection`; Examples; --------; a = np.arange(11); circles(a, a, s=a*0.2, c=a, alpha=0.5, ec='none'); pl.colorbar(); License; --------; This code is under [The BSD 3-Clause License]; (https://opensource.org/license/bsd-3-clause/); """"""; # You can set `facecolor` with an array for each patch,; # while you can only set `facecolors` with a value for all.; """"""; Given a dictionary of plot parameters (kwds_dict) and a dict of kwds,; merge the parameters into a single consolidated dictionary to avoid; argument duplication errors. If kwds_dict an kwargs have the same key, only the value in kwds_dict is kept. Parameters; ----------; kwds_dict kwds_dictionary; kwargs. Returns; -------; kwds_dict merged with kwargs. Examples; --------. >>> def _example(**kwds):; ... return fix_kwds(kwds, key1=""value1"", key2=""value2""); >>> _example(key1=""value10"", key3=""value3""); {'key1': 'value10', 'key2': 'value2', 'key3': 'value3'}; """"""; # matplotlib<3.2",MatchSource.CODE_COMMENT,src/scanpy/plotting/_utils.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_utils.py
Deployability,continuous,continuous,"# TODO: more; # These are needed by _wraps_plot_scatter; """"""Intersection between Axes and SubplotBase: Has methods of both""""""; # -------------------------------------------------------------------------------; # Simple plotting functions; # -------------------------------------------------------------------------------; """"""Plot a matrix.""""""; # need a figure instance for colorbar; """"""Plot X. See timeseries_subplot.""""""; """"""\; Plot X. Parameters; ----------; X; Call this with:; X with one column, color categorical.; X with one column, color continuous.; X with n columns, color is of length n.; """"""; """"""\; Plot timeseries as heatmap. Parameters; ----------; X; Data array.; var_names; Array of strings naming variables stored in columns of X.; """"""; # transpose X; # insert space into X; # generate new array with highlights_x; # integer; # update variables; # -------------------------------------------------------------------------------; # Colors in addition to matplotlib's colors; # -------------------------------------------------------------------------------; # -------------------------------------------------------------------------------; # Helper functions; # -------------------------------------------------------------------------------; """"""Save current figure to file. The `filename` is generated as follows:. filename = settings.figdir / (writekey + settings.plot_suffix + '.' + settings.file_format_figs); """"""; # we need this as in notebooks, the internal figures are also influenced by 'savefig.dpi' this...; # output the following msg at warning level; it's really important for the user; # check whether `save` contains a figure extension; # append it; # clear figure; """"""; checks if the list of colors in adata.uns[f'{key}_colors'] is valid; and updates the color list in adata.uns[f'{key}_colors'] if needed. Not only valid matplotlib colors are checked but also if the color name; is a valid R color name, in which case it will be translated to a valid name; """"""; # check ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_utils.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_utils.py
Energy Efficiency,allocate,allocated,"bel closer to the axis; # scale limits to match data; """"""Plot scatter plot of data. Parameters; ----------; ax; Axis to plot on.; Y; Data array, data to be plotted needs to be in the first two columns.; """"""; """"""; Plot arrows of transitions in data matrix. Parameters; ----------; ax; Axis object from matplotlib.; X; Data array, any representation wished (X, psi, phi, etc).; indices; Indices storing the transitions.; """"""; # don't plot arrow of length 0; # pretty scientific notation; """"""Remove trailing zeros.""""""; """"""Take some 1d data and scale it so that min matches 0 and max 1.""""""; """"""Tree layout for networkx graph. See https://stackoverflow.com/questions/29586520/can-one-get-hierarchical-graphs-from-networkx-with-python-3; answer by burubum. If there is a cycle that is reachable from root, then this will see; infinite recursion. Parameters; ----------; G: the graph; root: the root node; levels: a dictionary; key: level number (starting from 0); value: number of nodes in this level; width: horizontal space allocated for drawing; height: vertical space allocated for drawing; """"""; """"""Compute the number of nodes for each level""""""; """"""Zoom into axis. Parameters; ----------; """"""; """"""Get axis size. Parameters; ----------; ax; Axis object from matplotlib.; fig; Figure.; """"""; """"""For a width in axis coordinates, return the corresponding in data; coordinates. Parameters; ----------; ax; Axis object from matplotlib.; width; Width in xaxis coordinates.; """"""; """"""Map points in axis coordinates to data coordinates. Uses matplotlib.transform. Parameters; ----------; ax; Axis object from matplotlib.; points_axis; Points in axis coordinates.; """"""; """"""Map points in data coordinates to axis coordinates. Uses matplotlib.transform. Parameters; ----------; ax; Axis object from matplotlib.; points_data; Points in data coordinates.; """"""; """"""Validation for projection argument.""""""; """"""; Taken from here: https://gist.github.com/syrte/592a062c562cd2a98a83; Make a scatter plot of circles.; Similar ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_utils.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_utils.py
Modifiability,variab,variables,"# TODO: more; # These are needed by _wraps_plot_scatter; """"""Intersection between Axes and SubplotBase: Has methods of both""""""; # -------------------------------------------------------------------------------; # Simple plotting functions; # -------------------------------------------------------------------------------; """"""Plot a matrix.""""""; # need a figure instance for colorbar; """"""Plot X. See timeseries_subplot.""""""; """"""\; Plot X. Parameters; ----------; X; Call this with:; X with one column, color categorical.; X with one column, color continuous.; X with n columns, color is of length n.; """"""; """"""\; Plot timeseries as heatmap. Parameters; ----------; X; Data array.; var_names; Array of strings naming variables stored in columns of X.; """"""; # transpose X; # insert space into X; # generate new array with highlights_x; # integer; # update variables; # -------------------------------------------------------------------------------; # Colors in addition to matplotlib's colors; # -------------------------------------------------------------------------------; # -------------------------------------------------------------------------------; # Helper functions; # -------------------------------------------------------------------------------; """"""Save current figure to file. The `filename` is generated as follows:. filename = settings.figdir / (writekey + settings.plot_suffix + '.' + settings.file_format_figs); """"""; # we need this as in notebooks, the internal figures are also influenced by 'savefig.dpi' this...; # output the following msg at warning level; it's really important for the user; # check whether `save` contains a figure extension; # append it; # clear figure; """"""; checks if the list of colors in adata.uns[f'{key}_colors'] is valid; and updates the color list in adata.uns[f'{key}_colors'] if needed. Not only valid matplotlib colors are checked but also if the color name; is a valid R color name, in which case it will be translated to a valid name; """"""; # check ",MatchSource.CODE_COMMENT,src/scanpy/plotting/_utils.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_utils.py
Safety,avoid,avoid,"Parameters; ----------; x, y : scalar or array_like, shape (n, ); Input data; s : scalar or array_like, shape (n, ); Radius of circles.; c : color or sequence of color, optional, default : 'b'; `c` can be a single color format string, or a sequence of color; specifications of length `N`, or a sequence of `N` numbers to be; mapped to colors using the `cmap` and `norm` specified via kwargs.; Note that `c` should not be a single numeric RGB or RGBA sequence; because that is indistinguishable from an array of values; to be colormapped. (If you insist, use `color` instead.); `c` can be a 2-D array in which the rows are RGB or RGBA, however.; vmin, vmax : scalar, optional, default: None; `vmin` and `vmax` are used in conjunction with `norm` to normalize; luminance data. If either are `None`, the min and max of the; color array is used.; kwargs : `~matplotlib.collections.Collection` properties; Eg. alpha, edgecolor(ec), facecolor(fc), linewidth(lw), linestyle(ls),; norm, cmap, transform, etc.; Returns; -------; paths : `~matplotlib.collections.PathCollection`; Examples; --------; a = np.arange(11); circles(a, a, s=a*0.2, c=a, alpha=0.5, ec='none'); pl.colorbar(); License; --------; This code is under [The BSD 3-Clause License]; (https://opensource.org/license/bsd-3-clause/); """"""; # You can set `facecolor` with an array for each patch,; # while you can only set `facecolors` with a value for all.; """"""; Given a dictionary of plot parameters (kwds_dict) and a dict of kwds,; merge the parameters into a single consolidated dictionary to avoid; argument duplication errors. If kwds_dict an kwargs have the same key, only the value in kwds_dict is kept. Parameters; ----------; kwds_dict kwds_dictionary; kwargs. Returns; -------; kwds_dict merged with kwargs. Examples; --------. >>> def _example(**kwds):; ... return fix_kwds(kwds, key1=""value1"", key2=""value2""); >>> _example(key1=""value10"", key3=""value3""); {'key1': 'value10', 'key2': 'value2', 'key3': 'value3'}; """"""; # matplotlib<3.2",MatchSource.CODE_COMMENT,src/scanpy/plotting/_utils.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_utils.py
Usability,clear,clear,"p. Parameters; ----------; X; Data array.; var_names; Array of strings naming variables stored in columns of X.; """"""; # transpose X; # insert space into X; # generate new array with highlights_x; # integer; # update variables; # -------------------------------------------------------------------------------; # Colors in addition to matplotlib's colors; # -------------------------------------------------------------------------------; # -------------------------------------------------------------------------------; # Helper functions; # -------------------------------------------------------------------------------; """"""Save current figure to file. The `filename` is generated as follows:. filename = settings.figdir / (writekey + settings.plot_suffix + '.' + settings.file_format_figs); """"""; # we need this as in notebooks, the internal figures are also influenced by 'savefig.dpi' this...; # output the following msg at warning level; it's really important for the user; # check whether `save` contains a figure extension; # append it; # clear figure; """"""; checks if the list of colors in adata.uns[f'{key}_colors'] is valid; and updates the color list in adata.uns[f'{key}_colors'] if needed. Not only valid matplotlib colors are checked but also if the color name; is a valid R color name, in which case it will be translated to a valid name; """"""; # check if the color is a valid R color and translate it; # to a valid hex color value; # Don’t modify if nothing changed; """"""; Sets the adata.uns[value_to_plot + '_colors'] according to the given palette. Parameters; ----------; adata; annData object; value_to_plot; name of a valid categorical observation; palette; Palette should be either a valid :func:`~matplotlib.pyplot.colormaps` string,; a sequence of colors (in a format that can be understood by matplotlib,; eg. RGB, RGBS, hex, or a cycler object with key='color'. Returns; -------; None; """"""; # check is palette is a valid matplotlib colormap; # this creates a palette from a col",MatchSource.CODE_COMMENT,src/scanpy/plotting/_utils.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_utils.py
Deployability,patch,patches,"the matrix that stores the; arrows, for instance `'transitions_confidence'`.; solid_edges; Key for `.uns['paga']` that specifies the matrix that stores the edges; to be drawn solid black.; dashed_edges; Key for `.uns['paga']` that specifies the matrix that stores the edges; to be drawn dashed grey. If `None`, no dashed edges are drawn.; single_component; Restrict to largest connected component.; fontsize; Font size for node labels.; fontoutline; Width of the white outline around fonts.; text_kwds; Keywords for :meth:`~matplotlib.axes.Axes.text`.; node_size_scale; Increase or decrease the size of the nodes.; node_size_power; The power with which groups sizes influence the radius of the nodes.; edge_width_scale; Edge with scale in units of `rcParams['lines.linewidth']`.; min_edge_width; Min width of solid edges.; max_edge_width; Max width of solid and dashed edges.; arrowsize; For directed graphs, choose the size of the arrow head head's length and; width. See :py:class: `matplotlib.patches.FancyArrowPatch` for attribute; `mutation_scale` for more info.; export_to_gexf; Export to gexf format to be read by graph visualization programs such as; Gephi.; normalize_to_color; Whether to normalize categorical plots to `color` or the underlying; grouping.; cmap; The color map.; cax; A matplotlib axes object for a potential colorbar.; cb_kwds; Keyword arguments for :class:`~matplotlib.colorbar.Colorbar`,; for instance, `ticks`.; add_pos; Add the positions to `adata.uns['paga']`.; title; Provide a title.; frameon; Draw a frame around the PAGA graph.; plot; If `False`, do not create the figure, simply compute the layout.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on \\{`'.pdf'`, `'.png'`, `'.svg'`\\}.; ax; A matplotlib axes object. Returns; -------; If `show==False`, one or more :class:`~matplotlib.axes.Axes` objects.; Adds `'pos'` to `adata.uns['paga']` if `add_pos` is `True`. Examples; --------. ..",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/paga.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/paga.py
Energy Efficiency,power,power,"node or a list; of root node indices. If this is a non-empty vector then the supplied; node IDs are used as the roots of the trees (or a single tree if the; graph is connected). If this is `None` or an empty list, the root; vertices are automatically calculated based on topological sorting.; transitions; Key for `.uns['paga']` that specifies the matrix that stores the; arrows, for instance `'transitions_confidence'`.; solid_edges; Key for `.uns['paga']` that specifies the matrix that stores the edges; to be drawn solid black.; dashed_edges; Key for `.uns['paga']` that specifies the matrix that stores the edges; to be drawn dashed grey. If `None`, no dashed edges are drawn.; single_component; Restrict to largest connected component.; fontsize; Font size for node labels.; fontoutline; Width of the white outline around fonts.; text_kwds; Keywords for :meth:`~matplotlib.axes.Axes.text`.; node_size_scale; Increase or decrease the size of the nodes.; node_size_power; The power with which groups sizes influence the radius of the nodes.; edge_width_scale; Edge with scale in units of `rcParams['lines.linewidth']`.; min_edge_width; Min width of solid edges.; max_edge_width; Max width of solid and dashed edges.; arrowsize; For directed graphs, choose the size of the arrow head head's length and; width. See :py:class: `matplotlib.patches.FancyArrowPatch` for attribute; `mutation_scale` for more info.; export_to_gexf; Export to gexf format to be read by graph visualization programs such as; Gephi.; normalize_to_color; Whether to normalize categorical plots to `color` or the underlying; grouping.; cmap; The color map.; cax; A matplotlib axes object for a potential colorbar.; cb_kwds; Keyword arguments for :class:`~matplotlib.colorbar.Colorbar`,; for instance, `ticks`.; add_pos; Add the positions to `adata.uns['paga']`.; title; Provide a title.; frameon; Draw a frame around the PAGA graph.; plot; If `False`, do not create the figure, simply compute the layout.; save; If `True` or a",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/paga.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/paga.py
Modifiability,variab,variables,"aga; pl.paga_compare; pl.paga_path; """"""; # backwards compat; # handle paga pie, remap string keys to integers; # labels is a list that contains no lists; # define the adjacency matrices; # default threshold; # compute positions; # rename for clarity; # convert pos to array and dict; # read the node definition from the file; # convert to dictionary; # uniform color; # color degree of the graph; # see also tools.paga.paga_degrees; # plot gene expression; # plot continuous annotation; # plot categorical annotation; # count number of connected components; # edge widths; # draw dashed edges; # draw solid edges; # draw directed edges; # groups sizes; # usual scatter plot; # else pie chart plot; """"""\; Gene expression and annotation changes along paths in the abstracted graph. Parameters; ----------; adata; An annotated data matrix.; nodes; A path through nodes of the abstracted graph, that is, names or indices; (within `.categories`) of groups that have been used to run PAGA.; keys; Either variables in `adata.var_names` or annotations in; `adata.obs`. They are plotted using `color_map`.; use_raw; Use `adata.raw` for retrieving gene expressions if it has been set.; annotations; Plot these keys with `color_maps_annotations`. Need to be keys for; `adata.obs`.; color_map; Matplotlib colormap.; color_maps_annotations; Color maps for plotting the annotations. Keys of the dictionary must; appear in `annotations`.; palette_groups; Ususally, use the same `sc.pl.palettes...` as used for coloring the; abstracted graph.; n_avg; Number of data points to include in computation of running average.; groups_key; Key of the grouping used to run PAGA. If `None`, defaults to; `adata.uns['paga']['groups']`.; as_heatmap; Plot the timeseries as heatmap. If not plotting as heatmap,; `annotations` have no effect.; show_node_names; Plot the node names on the nodes bar.; show_colorbar; Show the colorbar.; show_yticks; Show the y ticks.; normalize_to_zero_one; Shift and scale the running average to [0",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/paga.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/paga.py
Performance,optimiz,optimization,"`. If the fractions; do not sum to 1, a new category called `'rest'` colored grey will be created.; labels; The node labels. If `None`, this defaults to the group labels stored in; the categorical for which :func:`~scanpy.tl.paga` has been computed.; pos; Two-column array-like storing the x and y coordinates for drawing.; Otherwise, path to a `.gdf` file that has been exported from Gephi or; a similar graph visualization software.; layout; Plotting layout that computes positions.; `'fa'` stands for “ForceAtlas2”,; `'fr'` stands for “Fruchterman-Reingold”,; `'rt'` stands for “Reingold-Tilford”,; `'eq_tree'` stands for “eqally spaced tree”.; All but `'fa'` and `'eq_tree'` are igraph layouts.; All other igraph layouts are also permitted.; See also parameter `pos` and :func:`~scanpy.tl.draw_graph`.; layout_kwds; Keywords for the layout.; init_pos; Two-column array storing the x and y coordinates for initializing the; layout.; random_state; For layouts with random initialization like `'fr'`, change this to use; different intial states for the optimization. If `None`, the initial; state is not reproducible.; root; If choosing a tree layout, this is the index of the root node or a list; of root node indices. If this is a non-empty vector then the supplied; node IDs are used as the roots of the trees (or a single tree if the; graph is connected). If this is `None` or an empty list, the root; vertices are automatically calculated based on topological sorting.; transitions; Key for `.uns['paga']` that specifies the matrix that stores the; arrows, for instance `'transitions_confidence'`.; solid_edges; Key for `.uns['paga']` that specifies the matrix that stores the edges; to be drawn solid black.; dashed_edges; Key for `.uns['paga']` that specifies the matrix that stores the edges; to be drawn dashed grey. If `None`, no dashed edges are drawn.; single_component; Restrict to largest connected component.; fontsize; Font size for node labels.; fontoutline; Width of the white outli",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/paga.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/paga.py
Usability,clear,clearer," Performance; # Tolerance; # NOT IMPLEMENTED; # Tuning; # Log; # igraph layouts; # I don't know why this is necessary; # np.random.seed(random_state); # this is a super-weird hack that is necessary as igraph’s; # layout function seems to do some strange stuff here; # hack for empty graphs...; # 17 positionals are enough for backwards compat; # TODO: this seems to be unused; # backwards compat; # backwards compat; """"""\; Plot the PAGA graph through thresholding low-connectivity edges. Compute a coarse-grained layout of the data. Reuse this by passing; `init_pos='paga'` to :func:`~scanpy.tl.umap` or; :func:`~scanpy.tl.draw_graph` and obtain embeddings with more meaningful; global topology :cite:p:`Wolf2019`. This uses ForceAtlas2 or igraph's layout algorithms for most layouts :cite:p:`Csardi2006`. Parameters; ----------; adata; Annotated data matrix.; threshold; Do not draw edges for weights below this threshold. Set to 0 if you want; all edges. Discarding low-connectivity edges helps in getting a much; clearer picture of the graph.; color; Gene name or `obs` annotation defining the node colors.; Also plots the degree of the abstracted graph when; passing {`'degree_dashed'`, `'degree_solid'`}. Can be also used to visualize pie chart at each node in the following form:; `{<group name or index>: {<color>: <fraction>, ...}, ...}`. If the fractions; do not sum to 1, a new category called `'rest'` colored grey will be created.; labels; The node labels. If `None`, this defaults to the group labels stored in; the categorical for which :func:`~scanpy.tl.paga` has been computed.; pos; Two-column array-like storing the x and y coordinates for drawing.; Otherwise, path to a `.gdf` file that has been exported from Gephi or; a similar graph visualization software.; layout; Plotting layout that computes positions.; `'fa'` stands for “ForceAtlas2”,; `'fr'` stands for “Fruchterman-Reingold”,; `'rt'` stands for “Reingold-Tilford”,; `'eq_tree'` stands for “eqally spaced tree”.; All but `",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/paga.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/paga.py
Availability,mask,mask,"# noqa: TCH003; # noqa: TCH003; # noqa: TCH003; # noqa: TCH002; # noqa: TCH002; # noqa: TCH002; # noqa: TCH002; # noqa: TCH002; # noqa: TCH002; # noqa: TCH001; # noqa: TCH001; # noqa: TCH001; # noqa: TCH001; # noqa: TCH001; # noqa: TCH001; """"""\; Scatter plot for user specified embedding basis (e.g. umap, pca, etc). Parameters; ----------; basis; Name of the `obsm` basis to use.; {adata_color_etc}; {edges_arrows}; {scatter_bulk}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; """"""; #####################; # Argument handling #; #####################; # Checking the mask format and if used together with groups; # Figure out if we're using raw; # check if adata.raw is set; # Color map; # Prevents warnings during legend creation; # by default turn off edge color. Otherwise, for; # very small sizes the edge will not reduce its size; # (https://github.com/scverse/scanpy/issues/293); # Vectorized arguments; # turn color into a python list; # turn marker into a python list; # turn title into a python list if not None; # turn vmax and vmin into a sequence; # Size; # check if size is any type of sequence, and if so; # set as ndarray; ##########; # Layout #; ##########; # Most of the code is for the case when multiple plots are required; # try to set a wspace that is not too large or too small given the; # current figure size; # 'color' is a list of names that want to be plotted.; # Eg. ['Gene1', 'louvain', 'Gene2'].; # component_list is a list of components [[0,1], [1,2]]; # each plot needs to be its own panel; ############; # Plotting #; ############; # use itertools.product to make a plot for each color and for each component; # For example if color=[gene1, gene2] and components=['1,2, '2,3'].; # The plots are: [; # color=gene1, components=[1,2], color=gene1, components=[2,3],; # color=gene2, components = [1, 2], color=gene2, components=[2,3],; # ]; # is potentially mutated for each plot; # Order points; # Higher values pl",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/scatterplots.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/scatterplots.py
Deployability,continuous,continuous,"dd a legend to the passed Axes.""""""; # Shrink current axis by 10% to fit legend and match; # size of plots that are not categorical; # identify centroids to put labels; # Have to sort_index since if observed=True and categorical is unordered; # the order of values in .index is undefined. Related issue:; # https://github.com/pandas-dev/pandas/issues/25167; """"""Get array for basis from anndata. Just tries to add 'X_'.""""""; """"""; Get array from adata that colors will be based on.; """"""; # Points will be plotted with `na_color`. Ideally this would work; # with the ""bad color"" in a color map but that throws a warning. Instead; # _color_vector handles this.; # https://github.com/matplotlib/matplotlib/issues/18294; # We should probably just make an index for this, and share it over runs; # TODO: Throw helpful error if this doesn't work; # set a default palette in case that no colors or few colors are found; """"""; Map array of values to array of hex (plus alpha) codes. For categorical data, the return value is list of colors taken; from the category palette or from the given `palette` value. For continuous values, the input array is returned (may change in future).; """"""; ###; # when plotting, the color of the dots is determined for each plot; # the data is either categorical or continuous and the data could be in; # 'obs' or in 'var'; # If color_map does not have unique values, this can be slow as the; # result is not categorical; # Set color to 'missing color' for all missing values; """"""; converts the 'basis' into the proper name.; """"""; """"""; Resolve spot_size value. This is a required argument for spatial plots.; """"""; """"""Resolve scale_factor, defaults to 1.""""""; """"""; Given a mapping, try and extract a library id/ mapping with spatial data. Assumes this is `.uns` from how we parse visium data.; """"""; """"""; Resolve image for spatial plots.; """"""; # Throws StopIteration Error if keys not present; """"""Handle cropping with image or basis.""""""; """"""Broadcasts arguments to a common length.""""""",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/scatterplots.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/scatterplots.py
Energy Efficiency,reduce,reduce,"# noqa: TCH003; # noqa: TCH003; # noqa: TCH003; # noqa: TCH002; # noqa: TCH002; # noqa: TCH002; # noqa: TCH002; # noqa: TCH002; # noqa: TCH002; # noqa: TCH001; # noqa: TCH001; # noqa: TCH001; # noqa: TCH001; # noqa: TCH001; # noqa: TCH001; """"""\; Scatter plot for user specified embedding basis (e.g. umap, pca, etc). Parameters; ----------; basis; Name of the `obsm` basis to use.; {adata_color_etc}; {edges_arrows}; {scatter_bulk}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; """"""; #####################; # Argument handling #; #####################; # Checking the mask format and if used together with groups; # Figure out if we're using raw; # check if adata.raw is set; # Color map; # Prevents warnings during legend creation; # by default turn off edge color. Otherwise, for; # very small sizes the edge will not reduce its size; # (https://github.com/scverse/scanpy/issues/293); # Vectorized arguments; # turn color into a python list; # turn marker into a python list; # turn title into a python list if not None; # turn vmax and vmin into a sequence; # Size; # check if size is any type of sequence, and if so; # set as ndarray; ##########; # Layout #; ##########; # Most of the code is for the case when multiple plots are required; # try to set a wspace that is not too large or too small given the; # current figure size; # 'color' is a list of names that want to be plotted.; # Eg. ['Gene1', 'louvain', 'Gene2'].; # component_list is a list of components [[0,1], [1,2]]; # each plot needs to be its own panel; ############; # Plotting #; ############; # use itertools.product to make a plot for each color and for each component; # For example if color=[gene1, gene2] and components=['1,2, '2,3'].; # The plots are: [; # color=gene1, components=[1,2], color=gene1, components=[2,3],; # color=gene2, components = [1, 2], color=gene2, components=[2,3],; # ]; # is potentially mutated for each plot; # Order points; # Higher values pl",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/scatterplots.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/scatterplots.py
Integrability,wrap,wrapper,"e of rcParams['figure.figsize']; """"""; Evaluates the value of vmin, vmax and vcenter, which could be a; str in which case is interpreted as a percentile and should; be specified in the form 'pN' where N is the percentile.; Eg. for a percentile of 85 the format would be 'p85'.; Floats are accepted as p99.9. Alternatively, vmin/vmax could be a function that is applied to; the list of color values (`colors`). E.g. def my_vmax(colors): np.percentile(colors, p=80). Parameters; ----------; index; This index of the plot; colors; Values for the plot. Returns; -------. (vmin, vmax, vcenter, norm) containing None or float values for; vmin, vmax, vcenter and matplotlib.colors.Normalize or None for norm. """"""; # this case usually happens when the user sets eg vmax=0.9, which; # is internally converted into list of len=1, but is expected that this; # value applies to all plots.; # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'; # interpret vmin/vmax as function; """"""Update the wrapper function to use the correct signature.""""""; # Python 3.9 does not support `eval_str`, so we only support this in 3.10+; # API; """"""\; Scatter plot in UMAP basis. Parameters; ----------; {adata_color_etc}; {edges_arrows}; {scatter_bulk}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. Examples; --------. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.pl.umap(adata). Colour points by discrete variable (Louvain clusters). .. plot::; :context: close-figs. sc.pl.umap(adata, color=""louvain""). Colour points by gene expression. .. plot::; :context: close-figs. sc.pl.umap(adata, color=""HES4""). Plot muliple umaps for different gene expressions. .. plot::; :context: close-figs. sc.pl.umap(adata, color=[""HES4"", ""TNFRSF4""]). .. currentmodule:: scanpy. See also; --------; tl.umap; """"""; """"""\; Scatter plot in tSNE basis. Parameters; ----------; {adata_color_etc}; {edges_arrows}; {scatter_bulk",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/scatterplots.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/scatterplots.py
Modifiability,variab,variable,"of the plot; colors; Values for the plot. Returns; -------. (vmin, vmax, vcenter, norm) containing None or float values for; vmin, vmax, vcenter and matplotlib.colors.Normalize or None for norm. """"""; # this case usually happens when the user sets eg vmax=0.9, which; # is internally converted into list of len=1, but is expected that this; # value applies to all plots.; # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'; # interpret vmin/vmax as function; """"""Update the wrapper function to use the correct signature.""""""; # Python 3.9 does not support `eval_str`, so we only support this in 3.10+; # API; """"""\; Scatter plot in UMAP basis. Parameters; ----------; {adata_color_etc}; {edges_arrows}; {scatter_bulk}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. Examples; --------. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.pl.umap(adata). Colour points by discrete variable (Louvain clusters). .. plot::; :context: close-figs. sc.pl.umap(adata, color=""louvain""). Colour points by gene expression. .. plot::; :context: close-figs. sc.pl.umap(adata, color=""HES4""). Plot muliple umaps for different gene expressions. .. plot::; :context: close-figs. sc.pl.umap(adata, color=[""HES4"", ""TNFRSF4""]). .. currentmodule:: scanpy. See also; --------; tl.umap; """"""; """"""\; Scatter plot in tSNE basis. Parameters; ----------; {adata_color_etc}; {edges_arrows}; {scatter_bulk}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. Examples; --------; .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.tl.tsne(adata); sc.pl.tsne(adata, color='bulk_labels'). .. currentmodule:: scanpy. See also; --------; tl.tsne; """"""; """"""\; Scatter plot in Diffusion Map basis. Parameters; ----------; {adata_color_etc}; {scatter_bulk}; {show_save_ax}. Returns; -------; If `show==False` a :class:`~matpl",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/scatterplots.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/scatterplots.py
Modifiability,variab,variables,"ot. Call pca_ranking separately; if you want to change the default settings. Parameters; ----------; adata; Annotated data matrix.; color; Keys for observation/cell annotation either as list `[""ann1"", ""ann2""]` or; string `""ann1,ann2,...""`.; use_raw; Use `raw` attribute of `adata` if present.; {scatter_bulk}; show; Show the plot, do not return axis.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.; Examples; --------; .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc3k_processed(); sc.pl.pca_overview(adata, color=""louvain""). .. currentmodule:: scanpy. See also; --------; pp.pca; """"""; # backwards compat; """"""\; Rank genes according to contributions to PCs. Parameters; ----------; adata; Annotated data matrix.; components; For example, ``'1,2,3'`` means ``[1, 2, 3]``, first, second, third; principal component.; include_lowest; Whether to show the variables with both highest and lowest loadings.; show; Show the plot, do not return axis.; n_points; Number of variables to plot for each component.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}. Examples; --------; .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc3k_processed(). Show first 3 components loadings. .. plot::; :context: close-figs. sc.pl.pca_loadings(adata, components = '1,2,3'). """"""; """"""\; Plot the variance ratio. Parameters; ----------; n_pcs; Number of PCs to show.; log; Plot on logarithmic scale..; show; Show the plot, do not return axis.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.; """"""; # ------------------------------------------------------------------------------; # Subgroup identification and ordering – clustering, ps",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/__init__.py
Performance,load,loadings,"ot. Call pca_ranking separately; if you want to change the default settings. Parameters; ----------; adata; Annotated data matrix.; color; Keys for observation/cell annotation either as list `[""ann1"", ""ann2""]` or; string `""ann1,ann2,...""`.; use_raw; Use `raw` attribute of `adata` if present.; {scatter_bulk}; show; Show the plot, do not return axis.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.; Examples; --------; .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc3k_processed(); sc.pl.pca_overview(adata, color=""louvain""). .. currentmodule:: scanpy. See also; --------; pp.pca; """"""; # backwards compat; """"""\; Rank genes according to contributions to PCs. Parameters; ----------; adata; Annotated data matrix.; components; For example, ``'1,2,3'`` means ``[1, 2, 3]``, first, second, third; principal component.; include_lowest; Whether to show the variables with both highest and lowest loadings.; show; Show the plot, do not return axis.; n_points; Number of variables to plot for each component.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}. Examples; --------; .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc3k_processed(). Show first 3 components loadings. .. plot::; :context: close-figs. sc.pl.pca_loadings(adata, components = '1,2,3'). """"""; """"""\; Plot the variance ratio. Parameters; ----------; n_pcs; Number of PCs to show.; log; Plot on logarithmic scale..; show; Show the plot, do not return axis.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.; """"""; # ------------------------------------------------------------------------------; # Subgroup identification and ordering – clustering, ps",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/__init__.py
Safety,avoid,avoid,"for background data points not in the `group`.; fg_dotsize; Dot size for foreground data points in the `group`.; {vminmax}; {panels}; {show_save_ax}. Examples; --------. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.tl.umap(adata); sc.tl.embedding_density(adata, basis='umap', groupby='phase'). Plot all categories be default. .. plot::; :context: close-figs. sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase'). Plot selected categories. .. plot::; :context: close-figs. sc.pl.embedding_density(; adata,; basis='umap',; key='umap_density_phase',; group=['G1', 'S'],; ). .. currentmodule:: scanpy. See also; --------; tl.embedding_density; """"""; # Test user inputs; # turn group into a list if needed; # try to set a wspace that is not too large or too small given the; # current figure size; # Make the color map; # a name to store the density values is needed. To avoid; # overwriting a user name a new random name is created; # if group is set, then plot it using multiple panels; # (even if only one group is set); # Define plotting data; # Saved with 1 based indexing; # Ensure title is blank as default; # Plot the graph; # Saved with 1 based indexing; # remove temporary column name; """"""; If rank_genes_groups has been called, this function; prepares a dataframe containing scores, pvalues, logfoldchange etc to be plotted; as dotplot or matrixplot. The dataframe index are the given groups and the columns are the gene_names. used by rank_genes_groups_dotplot. Parameters; ----------; adata; values_to_plot; name of the value to plot; gene_names; gene names; groups; groupby categories; key; adata.uns key where the rank_genes_groups is stored.; By default 'rank_genes_groups'; gene_symbols; Key for field in .var that stores gene symbols.; Returns; -------; pandas DataFrame index=groups, columns=gene_names. """"""; # check that all genes are present in the df as sc.tl.rank_genes_groups; # can be called with only top genes",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/__init__.py
Testability,log,log,"erview(adata, color=""louvain""). .. currentmodule:: scanpy. See also; --------; pp.pca; """"""; # backwards compat; """"""\; Rank genes according to contributions to PCs. Parameters; ----------; adata; Annotated data matrix.; components; For example, ``'1,2,3'`` means ``[1, 2, 3]``, first, second, third; principal component.; include_lowest; Whether to show the variables with both highest and lowest loadings.; show; Show the plot, do not return axis.; n_points; Number of variables to plot for each component.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}. Examples; --------; .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc3k_processed(). Show first 3 components loadings. .. plot::; :context: close-figs. sc.pl.pca_loadings(adata, components = '1,2,3'). """"""; """"""\; Plot the variance ratio. Parameters; ----------; n_pcs; Number of PCs to show.; log; Plot on logarithmic scale..; show; Show the plot, do not return axis.; save; If `True` or a `str`, save the figure.; A string is appended to the default filename.; Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.; """"""; # ------------------------------------------------------------------------------; # Subgroup identification and ordering – clustering, pseudotime, branching; # and tree inference tools; # ------------------------------------------------------------------------------; """"""\; Heatmap of pseudotime series. Parameters; ----------; as_heatmap; Plot the timeseries as heatmap.; """"""; # only if number of genes is not too high; # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; # plot time series as gene expression vs time; """"""\; Plot groups and pseudotime. Parameters; ----------; adata; Annotated data matrix.; {cm_palette}; {show_save}; marker; Marker style. See :mod:`~matplotlib.markers` for details.; """"""; """"""\; Plot ranking of genes. Parameters; --------",MatchSource.CODE_COMMENT,src/scanpy/plotting/_tools/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/plotting/_tools/__init__.py
Deployability,update,updated," the batch annotation; # standardize across genes using a pooled variance estimator; # fitting the parameters on the standardized data; # first estimate of the additive batch effect; # first estimate for the multiplicative batch effect; # empirically fix the prior hyperparameters; # a_prior and b_prior are the priors on lambda and theta from Johnson and Li (2006); # gamma star and delta star will be our empirical bayes (EB) estimators; # for the additive and multiplicative batch effect per batch and cell; # temp stores our estimates for the batch effect parameters.; # temp[0] is the additive batch effect; # temp[1] is the multiplicative batch effect; # we now apply the parametric adjustment to the standardized data from above; # loop over all batches in the data; # we basically substract the additive batch effect, rescale by the ratio; # of multiplicative batch effect to pooled variance and add the overall gene; # wise mean; # put back into the adata object or return; """"""\; Iteratively compute the conditional posterior means for gamma and delta. gamma is an estimator for the additive batch effect, deltat is an estimator; for the multiplicative batch effect. We use an EB framework to estimate these; two. Analytical expressions exist for both parameters, which however depend on each other.; We therefore iteratively evalutate these two expressions until convergence is reached. Parameters; --------; s_data; Contains the standardized Data; g_hat; Initial guess for gamma; d_hat; Initial guess for delta; g_bar, t2, a, b; Hyperparameters; conv: float, optional (default: `0.0001`); convergence criterium. Returns:; --------; gamma; estimated value for gamma; delta; estimated value for delta; """"""; # They need to be initialized for numba to properly infer types; # we place a normally distributed prior on gamma and and inverse gamma prior on delta; # in the loop, gamma and delta are updated together. they depend on each other. we iterate until convergence.; # .copy(); # .copy()",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_combat.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_combat.py
Energy Efficiency,power,power,"ers; --------; model; Contains the batch annotation; batch_key; Name of the batch column; batch_levels; Levels of the batch annotation. Returns; --------; The design matrix for the regression problem; """"""; """"""\; Standardizes the data per gene. The aim here is to make mean and variance be comparable across batches. Parameters; --------; model; Contains the batch annotation; data; Contains the Data; batch_key; Name of the batch column in the model matrix. Returns; --------; s_data; Standardized Data; design; Batch assignment as one-hot encodings; var_pooled; Pooled variance per gene; stand_mean; Gene-wise mean; """"""; # compute the design matrix; # compute pooled variance estimator; # Compute the means; # need to be a bit careful with the zero variance genes; # just set the zero variance genes to zero in the standardized data; """"""\; ComBat function for batch effect correction :cite:p:`Johnson2006,Leek2012,Pedersen2012`. Corrects for batch effects by fitting linear models, gains statistical power; via an EB framework where information is borrowed across genes.; This uses the implementation `combat.py`_ :cite:p:`Pedersen2012`. .. _combat.py: https://github.com/brentp/combat.py. Parameters; ----------; adata; Annotated data matrix; key; Key to a categorical annotation from :attr:`~anndata.AnnData.obs`; that will be used for batch effect removal.; covariates; Additional covariates besides the batch variable such as adjustment; variables or biological condition. This parameter refers to the design; matrix `X` in Equation 2.1 in :cite:t:`Johnson2006` and to the `mod` argument in; the original combat function in the sva R package.; Note that not including covariates may introduce bias or lead to the; removal of biological signal in unbalanced designs.; inplace; Whether to replace adata.X or to return the corrected data. Returns; -------; Returns :class:`numpy.ndarray` if `inplace=True`, else returns `None` and sets the following field in the `adata` object:. `adata.X` : :class",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_combat.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_combat.py
Integrability,depend,depend," the batch annotation; # standardize across genes using a pooled variance estimator; # fitting the parameters on the standardized data; # first estimate of the additive batch effect; # first estimate for the multiplicative batch effect; # empirically fix the prior hyperparameters; # a_prior and b_prior are the priors on lambda and theta from Johnson and Li (2006); # gamma star and delta star will be our empirical bayes (EB) estimators; # for the additive and multiplicative batch effect per batch and cell; # temp stores our estimates for the batch effect parameters.; # temp[0] is the additive batch effect; # temp[1] is the multiplicative batch effect; # we now apply the parametric adjustment to the standardized data from above; # loop over all batches in the data; # we basically substract the additive batch effect, rescale by the ratio; # of multiplicative batch effect to pooled variance and add the overall gene; # wise mean; # put back into the adata object or return; """"""\; Iteratively compute the conditional posterior means for gamma and delta. gamma is an estimator for the additive batch effect, deltat is an estimator; for the multiplicative batch effect. We use an EB framework to estimate these; two. Analytical expressions exist for both parameters, which however depend on each other.; We therefore iteratively evalutate these two expressions until convergence is reached. Parameters; --------; s_data; Contains the standardized Data; g_hat; Initial guess for gamma; d_hat; Initial guess for delta; g_bar, t2, a, b; Hyperparameters; conv: float, optional (default: `0.0001`); convergence criterium. Returns:; --------; gamma; estimated value for gamma; delta; estimated value for delta; """"""; # They need to be initialized for numba to properly infer types; # we place a normally distributed prior on gamma and and inverse gamma prior on delta; # in the loop, gamma and delta are updated together. they depend on each other. we iterate until convergence.; # .copy(); # .copy()",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_combat.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_combat.py
Modifiability,variab,variable,"f the batch column in the model matrix. Returns; --------; s_data; Standardized Data; design; Batch assignment as one-hot encodings; var_pooled; Pooled variance per gene; stand_mean; Gene-wise mean; """"""; # compute the design matrix; # compute pooled variance estimator; # Compute the means; # need to be a bit careful with the zero variance genes; # just set the zero variance genes to zero in the standardized data; """"""\; ComBat function for batch effect correction :cite:p:`Johnson2006,Leek2012,Pedersen2012`. Corrects for batch effects by fitting linear models, gains statistical power; via an EB framework where information is borrowed across genes.; This uses the implementation `combat.py`_ :cite:p:`Pedersen2012`. .. _combat.py: https://github.com/brentp/combat.py. Parameters; ----------; adata; Annotated data matrix; key; Key to a categorical annotation from :attr:`~anndata.AnnData.obs`; that will be used for batch effect removal.; covariates; Additional covariates besides the batch variable such as adjustment; variables or biological condition. This parameter refers to the design; matrix `X` in Equation 2.1 in :cite:t:`Johnson2006` and to the `mod` argument in; the original combat function in the sva R package.; Note that not including covariates may introduce bias or lead to the; removal of biological signal in unbalanced designs.; inplace; Whether to replace adata.X or to return the corrected data. Returns; -------; Returns :class:`numpy.ndarray` if `inplace=True`, else returns `None` and sets the following field in the `adata` object:. `adata.X` : :class:`numpy.ndarray` (dtype `float`); Corrected data matrix.; """"""; # check the input; # only works on dense matrices so far; # construct a pandas series of the batch annotation; # standardize across genes using a pooled variance estimator; # fitting the parameters on the standardized data; # first estimate of the additive batch effect; # first estimate for the multiplicative batch effect; # empirically fix the prior hyp",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_combat.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_combat.py
Usability,simpl,simple,"""""""\; Computes a simple design matrix. Parameters; --------; model; Contains the batch annotation; batch_key; Name of the batch column; batch_levels; Levels of the batch annotation. Returns; --------; The design matrix for the regression problem; """"""; """"""\; Standardizes the data per gene. The aim here is to make mean and variance be comparable across batches. Parameters; --------; model; Contains the batch annotation; data; Contains the Data; batch_key; Name of the batch column in the model matrix. Returns; --------; s_data; Standardized Data; design; Batch assignment as one-hot encodings; var_pooled; Pooled variance per gene; stand_mean; Gene-wise mean; """"""; # compute the design matrix; # compute pooled variance estimator; # Compute the means; # need to be a bit careful with the zero variance genes; # just set the zero variance genes to zero in the standardized data; """"""\; ComBat function for batch effect correction :cite:p:`Johnson2006,Leek2012,Pedersen2012`. Corrects for batch effects by fitting linear models, gains statistical power; via an EB framework where information is borrowed across genes.; This uses the implementation `combat.py`_ :cite:p:`Pedersen2012`. .. _combat.py: https://github.com/brentp/combat.py. Parameters; ----------; adata; Annotated data matrix; key; Key to a categorical annotation from :attr:`~anndata.AnnData.obs`; that will be used for batch effect removal.; covariates; Additional covariates besides the batch variable such as adjustment; variables or biological condition. This parameter refers to the design; matrix `X` in Equation 2.1 in :cite:t:`Johnson2006` and to the `mod` argument in; the original combat function in the sva R package.; Note that not including covariates may introduce bias or lead to the; removal of biological signal in unbalanced designs.; inplace; Whether to replace adata.X or to return the corrected data. Returns; -------; Returns :class:`numpy.ndarray` if `inplace=True`, else returns `None` and sets the following fie",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_combat.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_combat.py
Availability,avail,available,"""""""Shared docstrings for preprocessing function parameters.""""""; """"""\; adata; Annotated data matrix.\; """"""; """"""\; layer; If provided, use `adata.layers[layer]` for expression values instead; of `adata.X`.; use_raw; If True, use `adata.raw.X` for expression values instead of `adata.X`.\; """"""; """"""\; mask_var; To run only on a certain set of genes given by a boolean array; or a string referring to an array in :attr:`~anndata.AnnData.var`.; By default, uses `.var['highly_variable']` if available, else everything.; use_highly_variable; Whether to use highly variable genes only, stored in; `.var['highly_variable']`.; By default uses them if they have been determined beforehand. .. deprecated:: 1.10.0; Use `mask_var` instead; """"""; """"""\; qc_vars; Keys for boolean columns of `.var` which identify variables you could; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top; List of ranks (where genes are ranked by expression) at which the cumulative; proportion of expression will be reported as a percentage. This can be used to; assess library complexity. Ranks are considered 1-indexed, and if empty or None; don't calculate. E.g. `percent_top=[50]` finds cumulative proportion to the 50th most expressed gene.; """"""; """"""\; expr_type; Name of kind of values in X.; var_type; The kind of thing the variables are.\; """"""; """"""\; Observation level metrics include:. `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts in a cell.; `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; `pct_{expr_type}_in_top_{n}_{var_type}` – for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts; for 50 most expressed genes in a cell.; `total_{expr_type}_{qc_var}` – for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number of counts for variables in; `qc_vars`.; `pct_{expr_type}_{qc_var}` – for `qc_var` in `qc_vars`; E.g. ""pct_counts_mito"". Proportion of total counts for a cell which; are",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_docs.py
Modifiability,layers,layers,"""""""Shared docstrings for preprocessing function parameters.""""""; """"""\; adata; Annotated data matrix.\; """"""; """"""\; layer; If provided, use `adata.layers[layer]` for expression values instead; of `adata.X`.; use_raw; If True, use `adata.raw.X` for expression values instead of `adata.X`.\; """"""; """"""\; mask_var; To run only on a certain set of genes given by a boolean array; or a string referring to an array in :attr:`~anndata.AnnData.var`.; By default, uses `.var['highly_variable']` if available, else everything.; use_highly_variable; Whether to use highly variable genes only, stored in; `.var['highly_variable']`.; By default uses them if they have been determined beforehand. .. deprecated:: 1.10.0; Use `mask_var` instead; """"""; """"""\; qc_vars; Keys for boolean columns of `.var` which identify variables you could; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top; List of ranks (where genes are ranked by expression) at which the cumulative; proportion of expression will be reported as a percentage. This can be used to; assess library complexity. Ranks are considered 1-indexed, and if empty or None; don't calculate. E.g. `percent_top=[50]` finds cumulative proportion to the 50th most expressed gene.; """"""; """"""\; expr_type; Name of kind of values in X.; var_type; The kind of thing the variables are.\; """"""; """"""\; Observation level metrics include:. `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts in a cell.; `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; `pct_{expr_type}_in_top_{n}_{var_type}` – for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts; for 50 most expressed genes in a cell.; `total_{expr_type}_{qc_var}` – for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number of counts for variables in; `qc_vars`.; `pct_{expr_type}_{qc_var}` – for `qc_var` in `qc_vars`; E.g. ""pct_counts_mito"". Proportion of total counts for a cell which; are",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_docs.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_docs.py
Availability,mask,mask,"variable_rank : :class:`float`; Rank of the gene according to normalized variance, median rank in the case of multiple batches.; highly_variable_nbatches : :class:`int`; If batch_key is given, this denotes in how many batches genes are detected as HVG.; """"""; # clip large values as in Seurat; # argsort twice gives ranks, small rank means most variable; # this is done in SelectIntegrationFeatures() in Seurat v3; """"""\; See `highly_variable_genes`. Returns; -------; A DataFrame that contains the columns; `highly_variable`, `means`, `dispersions`, and `dispersions_norm`.; """"""; # AnnData array view; # For compatibility with anndata<0.9; # Doesn't actually copy memory, just removes View class wrapper; # use out if possible. only possible since we copy the data matrix; # now actually compute the dispersion; # set entries equal to zero to small value; # logarithmized mean as in Seurat; # all of the following quantities are ""per-gene"" here; # actually do the normalization; # retrieve those genes that have nan std, these are the ones where; # only a single gene fell in the bin and implicitly set them to have; # a normalized disperion of 1; # MAD calculation raises the warning: ""Mean of empty slice""; """"""Get boolean mask of genes with normalized dispersion in bounds.""""""; # similar to Seurat; # interestingly, np.argpartition is slightly slower; # Filter to genes that are in the dataset; # TODO use groupby or so instead of materialize_as_ndarray; # Add 0 values for genes that were filtered out; # sort genes by how often they selected as hvg within each batch and; # break ties with normalized dispersion across batches; # similar to Seurat; """"""\; Annotate highly variable genes :cite:p:`Satija2015,Zheng2017,Stuart2019`. Expects logarithmized data, except when `flavor='seurat_v3'`/`'seurat_v3_paper'`, in which count; data is expected. Depending on `flavor`, this reproduces the R-implementations of Seurat; :cite:p:`Satija2015`, Cell Ranger :cite:p:`Zheng2017`, and Seurat v3 :cite:p:`St",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_highly_variable_genes.py
Deployability,update,updates,"""""""\; See `highly_variable_genes`. For further implementation details see https://www.overleaf.com/read/ckptrbgzzzpg. Returns; -------; Depending on `inplace` returns calculated metrics (:class:`~pd.DataFrame`) or; updates `.var` with the following fields:. highly_variable : :class:`bool`; boolean indicator of highly-variable genes.; **means**; means per gene.; **variances**; variance per gene.; **variances_norm**; normalized variance per gene, averaged in the case of multiple batches.; highly_variable_rank : :class:`float`; Rank of the gene according to normalized variance, median rank in the case of multiple batches.; highly_variable_nbatches : :class:`int`; If batch_key is given, this denotes in how many batches genes are detected as HVG.; """"""; # clip large values as in Seurat; # argsort twice gives ranks, small rank means most variable; # this is done in SelectIntegrationFeatures() in Seurat v3; """"""\; See `highly_variable_genes`. Returns; -------; A DataFrame that contains the columns; `highly_variable`, `means`, `dispersions`, and `dispersions_norm`.; """"""; # AnnData array view; # For compatibility with anndata<0.9; # Doesn't actually copy memory, just removes View class wrapper; # use out if possible. only possible since we copy the data matrix; # now actually compute the dispersion; # set entries equal to zero to small value; # logarithmized mean as in Seurat; # all of the following quantities are ""per-gene"" here; # actually do the normalization; # retrieve those genes that have nan std, these are the ones where; # only a single gene fell in the bin and implicitly set them to have; # a normalized disperion of 1; # MAD calculation raises the warning: ""Mean of empty slice""; """"""Get boolean mask of genes with normalized dispersion in bounds.""""""; # similar to Seurat; # interestingly, np.argpartition is slightly slower; # Filter to genes that are in the dataset; # TODO use groupby or so instead of materialize_as_ndarray; # Add 0 values for genes that were filtered ou",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_highly_variable_genes.py
Integrability,wrap,wrapper,"trics (:class:`~pd.DataFrame`) or; updates `.var` with the following fields:. highly_variable : :class:`bool`; boolean indicator of highly-variable genes.; **means**; means per gene.; **variances**; variance per gene.; **variances_norm**; normalized variance per gene, averaged in the case of multiple batches.; highly_variable_rank : :class:`float`; Rank of the gene according to normalized variance, median rank in the case of multiple batches.; highly_variable_nbatches : :class:`int`; If batch_key is given, this denotes in how many batches genes are detected as HVG.; """"""; # clip large values as in Seurat; # argsort twice gives ranks, small rank means most variable; # this is done in SelectIntegrationFeatures() in Seurat v3; """"""\; See `highly_variable_genes`. Returns; -------; A DataFrame that contains the columns; `highly_variable`, `means`, `dispersions`, and `dispersions_norm`.; """"""; # AnnData array view; # For compatibility with anndata<0.9; # Doesn't actually copy memory, just removes View class wrapper; # use out if possible. only possible since we copy the data matrix; # now actually compute the dispersion; # set entries equal to zero to small value; # logarithmized mean as in Seurat; # all of the following quantities are ""per-gene"" here; # actually do the normalization; # retrieve those genes that have nan std, these are the ones where; # only a single gene fell in the bin and implicitly set them to have; # a normalized disperion of 1; # MAD calculation raises the warning: ""Mean of empty slice""; """"""Get boolean mask of genes with normalized dispersion in bounds.""""""; # similar to Seurat; # interestingly, np.argpartition is slightly slower; # Filter to genes that are in the dataset; # TODO use groupby or so instead of materialize_as_ndarray; # Add 0 values for genes that were filtered out; # sort genes by how often they selected as hvg within each batch and; # break ties with normalized dispersion across batches; # similar to Seurat; """"""\; Annotate highly variabl",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_highly_variable_genes.py
Modifiability,variab,variable,"""""""\; See `highly_variable_genes`. For further implementation details see https://www.overleaf.com/read/ckptrbgzzzpg. Returns; -------; Depending on `inplace` returns calculated metrics (:class:`~pd.DataFrame`) or; updates `.var` with the following fields:. highly_variable : :class:`bool`; boolean indicator of highly-variable genes.; **means**; means per gene.; **variances**; variance per gene.; **variances_norm**; normalized variance per gene, averaged in the case of multiple batches.; highly_variable_rank : :class:`float`; Rank of the gene according to normalized variance, median rank in the case of multiple batches.; highly_variable_nbatches : :class:`int`; If batch_key is given, this denotes in how many batches genes are detected as HVG.; """"""; # clip large values as in Seurat; # argsort twice gives ranks, small rank means most variable; # this is done in SelectIntegrationFeatures() in Seurat v3; """"""\; See `highly_variable_genes`. Returns; -------; A DataFrame that contains the columns; `highly_variable`, `means`, `dispersions`, and `dispersions_norm`.; """"""; # AnnData array view; # For compatibility with anndata<0.9; # Doesn't actually copy memory, just removes View class wrapper; # use out if possible. only possible since we copy the data matrix; # now actually compute the dispersion; # set entries equal to zero to small value; # logarithmized mean as in Seurat; # all of the following quantities are ""per-gene"" here; # actually do the normalization; # retrieve those genes that have nan std, these are the ones where; # only a single gene fell in the bin and implicitly set them to have; # a normalized disperion of 1; # MAD calculation raises the warning: ""Mean of empty slice""; """"""Get boolean mask of genes with normalized dispersion in bounds.""""""; # similar to Seurat; # interestingly, np.argpartition is slightly slower; # Filter to genes that are in the dataset; # TODO use groupby or so instead of materialize_as_ndarray; # Add 0 values for genes that were filtered ou",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_highly_variable_genes.py
Safety,detect,detected,"""""""\; See `highly_variable_genes`. For further implementation details see https://www.overleaf.com/read/ckptrbgzzzpg. Returns; -------; Depending on `inplace` returns calculated metrics (:class:`~pd.DataFrame`) or; updates `.var` with the following fields:. highly_variable : :class:`bool`; boolean indicator of highly-variable genes.; **means**; means per gene.; **variances**; variance per gene.; **variances_norm**; normalized variance per gene, averaged in the case of multiple batches.; highly_variable_rank : :class:`float`; Rank of the gene according to normalized variance, median rank in the case of multiple batches.; highly_variable_nbatches : :class:`int`; If batch_key is given, this denotes in how many batches genes are detected as HVG.; """"""; # clip large values as in Seurat; # argsort twice gives ranks, small rank means most variable; # this is done in SelectIntegrationFeatures() in Seurat v3; """"""\; See `highly_variable_genes`. Returns; -------; A DataFrame that contains the columns; `highly_variable`, `means`, `dispersions`, and `dispersions_norm`.; """"""; # AnnData array view; # For compatibility with anndata<0.9; # Doesn't actually copy memory, just removes View class wrapper; # use out if possible. only possible since we copy the data matrix; # now actually compute the dispersion; # set entries equal to zero to small value; # logarithmized mean as in Seurat; # all of the following quantities are ""per-gene"" here; # actually do the normalization; # retrieve those genes that have nan std, these are the ones where; # only a single gene fell in the bin and implicitly set them to have; # a normalized disperion of 1; # MAD calculation raises the warning: ""Mean of empty slice""; """"""Get boolean mask of genes with normalized dispersion in bounds.""""""; # similar to Seurat; # interestingly, np.argpartition is slightly slower; # Filter to genes that are in the dataset; # TODO use groupby or so instead of materialize_as_ndarray; # Add 0 values for genes that were filtered ou",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_highly_variable_genes.py
Testability,log,logarithmized,"variable_rank : :class:`float`; Rank of the gene according to normalized variance, median rank in the case of multiple batches.; highly_variable_nbatches : :class:`int`; If batch_key is given, this denotes in how many batches genes are detected as HVG.; """"""; # clip large values as in Seurat; # argsort twice gives ranks, small rank means most variable; # this is done in SelectIntegrationFeatures() in Seurat v3; """"""\; See `highly_variable_genes`. Returns; -------; A DataFrame that contains the columns; `highly_variable`, `means`, `dispersions`, and `dispersions_norm`.; """"""; # AnnData array view; # For compatibility with anndata<0.9; # Doesn't actually copy memory, just removes View class wrapper; # use out if possible. only possible since we copy the data matrix; # now actually compute the dispersion; # set entries equal to zero to small value; # logarithmized mean as in Seurat; # all of the following quantities are ""per-gene"" here; # actually do the normalization; # retrieve those genes that have nan std, these are the ones where; # only a single gene fell in the bin and implicitly set them to have; # a normalized disperion of 1; # MAD calculation raises the warning: ""Mean of empty slice""; """"""Get boolean mask of genes with normalized dispersion in bounds.""""""; # similar to Seurat; # interestingly, np.argpartition is slightly slower; # Filter to genes that are in the dataset; # TODO use groupby or so instead of materialize_as_ndarray; # Add 0 values for genes that were filtered out; # sort genes by how often they selected as hvg within each batch and; # break ties with normalized dispersion across batches; # similar to Seurat; """"""\; Annotate highly variable genes :cite:p:`Satija2015,Zheng2017,Stuart2019`. Expects logarithmized data, except when `flavor='seurat_v3'`/`'seurat_v3_paper'`, in which count; data is expected. Depending on `flavor`, this reproduces the R-implementations of Seurat; :cite:p:`Satija2015`, Cell Ranger :cite:p:`Zheng2017`, and Seurat v3 :cite:p:`St",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_highly_variable_genes.py
Usability,simpl,simple,"rmalized dispersions are ignored. Ignored if `flavor='seurat_v3'`.; span; The fraction of the data (cells) used when estimating the variance in the loess; model fit if `flavor='seurat_v3'`.; n_bins; Number of bins for binning the mean gene expression. Normalization is; done with respect to each bin. If just a single gene falls into a bin,; the normalized dispersion is artificially set to 1. You'll be informed; about this if you set `settings.verbosity = 4`.; flavor; Choose the flavor for identifying highly variable genes. For the dispersion; based methods in their default workflows, Seurat passes the cutoffs whereas; Cell Ranger passes `n_top_genes`.; subset; Inplace subset to highly-variable genes if `True` otherwise merely indicate; highly variable genes.; inplace; Whether to place calculated metrics in `.var` or return them.; batch_key; If specified, highly-variable genes are selected within each batch separately and merged.; This simple process avoids the selection of batch-specific genes and acts as a; lightweight batch correction method. For all flavors, except `seurat_v3`, genes are first sorted; by how many batches they are a HVG. For dispersion-based flavors ties are broken; by normalized dispersion. For `flavor = 'seurat_v3_paper'`, ties are broken by the median; (across batches) rank based on within-batch normalized variance.; check_values; Check if counts in selected layer are integers. A Warning is returned if set to True.; Only used if `flavor='seurat_v3'`/`'seurat_v3_paper'`. Returns; -------; Returns a :class:`pandas.DataFrame` with calculated metrics if `inplace=True`, else returns an `AnnData` object where it sets the following field:. `adata.var['highly_variable']` : :class:`pandas.Series` (dtype `bool`); boolean indicator of highly-variable genes; `adata.var['means']` : :class:`pandas.Series` (dtype `float`); means per gene; `adata.var['dispersions']` : :class:`pandas.Series` (dtype `float`); For dispersion-based flavors, dispersions per gene; `a",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_highly_variable_genes.py
Deployability,update,update,".; target_sum; If `None`, after normalization, each observation (cell) has a total; count equal to the median of total counts for observations (cells); before normalization.; exclude_highly_expressed; Exclude (very) highly expressed genes for the computation of the; normalization factor (size factor) for each cell. A gene is considered; highly expressed, if it has more than `max_fraction` of the total counts; in at least one cell. The not-excluded genes will sum up to; `target_sum`. Providing this argument when `adata.X` is a :class:`~dask.array.Array`; will incur blocking `.compute()` calls on the array.; max_fraction; If `exclude_highly_expressed=True`, consider cells as highly expressed; that have more counts than `max_fraction` of the original total counts; in at least one cell.; key_added; Name of the field in `adata.obs` where the normalization factor is; stored.; layer; Layer to normalize instead of `X`. If `None`, `X` is normalized.; inplace; Whether to update `adata` or return dictionary with normalized copies of; `adata.X` and `adata.layers`.; copy; Whether to modify copied input object. Not compatible with inplace=False. Returns; -------; Returns dictionary with normalized copies of `adata.X` and `adata.layers`; or updates `adata` with normalized version of the original; `adata.X` and `adata.layers`, depending on `inplace`. Example; --------; >>> import sys; >>> from anndata import AnnData; >>> import scanpy as sc; >>> sc.settings.verbosity = 'info'; >>> sc.settings.logfile = sys.stdout # for doctests; >>> np.set_printoptions(precision=2); >>> adata = AnnData(np.array([; ... [3, 3, 3, 6, 6],; ... [1, 1, 1, 2, 2],; ... [1, 22, 1, 2, 2],; ... ], dtype='float32')); >>> adata.X; array([[ 3., 3., 3., 6., 6.],; [ 1., 1., 1., 2., 2.],; [ 1., 22., 1., 2., 2.]], dtype=float32); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; normalizing counts per cell; finished (0:00:00); >>> X_norm; array([[0.14, 0.14, 0.14, 0.29, 0.29],; [0.14, 0.14",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_normalization.py
Integrability,depend,depending,"ighly expressed, if it has more than `max_fraction` of the total counts; in at least one cell. The not-excluded genes will sum up to; `target_sum`. Providing this argument when `adata.X` is a :class:`~dask.array.Array`; will incur blocking `.compute()` calls on the array.; max_fraction; If `exclude_highly_expressed=True`, consider cells as highly expressed; that have more counts than `max_fraction` of the original total counts; in at least one cell.; key_added; Name of the field in `adata.obs` where the normalization factor is; stored.; layer; Layer to normalize instead of `X`. If `None`, `X` is normalized.; inplace; Whether to update `adata` or return dictionary with normalized copies of; `adata.X` and `adata.layers`.; copy; Whether to modify copied input object. Not compatible with inplace=False. Returns; -------; Returns dictionary with normalized copies of `adata.X` and `adata.layers`; or updates `adata` with normalized version of the original; `adata.X` and `adata.layers`, depending on `inplace`. Example; --------; >>> import sys; >>> from anndata import AnnData; >>> import scanpy as sc; >>> sc.settings.verbosity = 'info'; >>> sc.settings.logfile = sys.stdout # for doctests; >>> np.set_printoptions(precision=2); >>> adata = AnnData(np.array([; ... [3, 3, 3, 6, 6],; ... [1, 1, 1, 2, 2],; ... [1, 22, 1, 2, 2],; ... ], dtype='float32')); >>> adata.X; array([[ 3., 3., 3., 6., 6.],; [ 1., 1., 1., 2., 2.],; [ 1., 22., 1., 2., 2.]], dtype=float32); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; normalizing counts per cell; finished (0:00:00); >>> X_norm; array([[0.14, 0.14, 0.14, 0.29, 0.29],; [0.14, 0.14, 0.14, 0.29, 0.29],; [0.04, 0.79, 0.04, 0.07, 0.07]], dtype=float32); >>> X_norm = sc.pp.normalize_total(; ... adata, target_sum=1, exclude_highly_expressed=True,; ... max_fraction=0.2, inplace=False; ... )['X']; normalizing counts per cell. The following highly-expressed genes are not considered during normalization factor computation:;",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_normalization.py
Modifiability,layers,layers,"(cell) has a total; count equal to the median of total counts for observations (cells); before normalization.; exclude_highly_expressed; Exclude (very) highly expressed genes for the computation of the; normalization factor (size factor) for each cell. A gene is considered; highly expressed, if it has more than `max_fraction` of the total counts; in at least one cell. The not-excluded genes will sum up to; `target_sum`. Providing this argument when `adata.X` is a :class:`~dask.array.Array`; will incur blocking `.compute()` calls on the array.; max_fraction; If `exclude_highly_expressed=True`, consider cells as highly expressed; that have more counts than `max_fraction` of the original total counts; in at least one cell.; key_added; Name of the field in `adata.obs` where the normalization factor is; stored.; layer; Layer to normalize instead of `X`. If `None`, `X` is normalized.; inplace; Whether to update `adata` or return dictionary with normalized copies of; `adata.X` and `adata.layers`.; copy; Whether to modify copied input object. Not compatible with inplace=False. Returns; -------; Returns dictionary with normalized copies of `adata.X` and `adata.layers`; or updates `adata` with normalized version of the original; `adata.X` and `adata.layers`, depending on `inplace`. Example; --------; >>> import sys; >>> from anndata import AnnData; >>> import scanpy as sc; >>> sc.settings.verbosity = 'info'; >>> sc.settings.logfile = sys.stdout # for doctests; >>> np.set_printoptions(precision=2); >>> adata = AnnData(np.array([; ... [3, 3, 3, 6, 6],; ... [1, 1, 1, 2, 2],; ... [1, 22, 1, 2, 2],; ... ], dtype='float32')); >>> adata.X; array([[ 3., 3., 3., 6., 6.],; [ 1., 1., 1., 2., 2.],; [ 1., 22., 1., 2., 2.]], dtype=float32); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; normalizing counts per cell; finished (0:00:00); >>> X_norm; array([[0.14, 0.14, 0.14, 0.29, 0.29],; [0.14, 0.14, 0.14, 0.29, 0.29],; [0.04, 0.79, 0.04, 0.07, 0.07]], dtype=flo",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_normalization.py
Testability,log,logfile,"nt when `adata.X` is a :class:`~dask.array.Array`; will incur blocking `.compute()` calls on the array.; max_fraction; If `exclude_highly_expressed=True`, consider cells as highly expressed; that have more counts than `max_fraction` of the original total counts; in at least one cell.; key_added; Name of the field in `adata.obs` where the normalization factor is; stored.; layer; Layer to normalize instead of `X`. If `None`, `X` is normalized.; inplace; Whether to update `adata` or return dictionary with normalized copies of; `adata.X` and `adata.layers`.; copy; Whether to modify copied input object. Not compatible with inplace=False. Returns; -------; Returns dictionary with normalized copies of `adata.X` and `adata.layers`; or updates `adata` with normalized version of the original; `adata.X` and `adata.layers`, depending on `inplace`. Example; --------; >>> import sys; >>> from anndata import AnnData; >>> import scanpy as sc; >>> sc.settings.verbosity = 'info'; >>> sc.settings.logfile = sys.stdout # for doctests; >>> np.set_printoptions(precision=2); >>> adata = AnnData(np.array([; ... [3, 3, 3, 6, 6],; ... [1, 1, 1, 2, 2],; ... [1, 22, 1, 2, 2],; ... ], dtype='float32')); >>> adata.X; array([[ 3., 3., 3., 6., 6.],; [ 1., 1., 1., 2., 2.],; [ 1., 22., 1., 2., 2.]], dtype=float32); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; normalizing counts per cell; finished (0:00:00); >>> X_norm; array([[0.14, 0.14, 0.14, 0.29, 0.29],; [0.14, 0.14, 0.14, 0.29, 0.29],; [0.04, 0.79, 0.04, 0.07, 0.07]], dtype=float32); >>> X_norm = sc.pp.normalize_total(; ... adata, target_sum=1, exclude_highly_expressed=True,; ... max_fraction=0.2, inplace=False; ... )['X']; normalizing counts per cell. The following highly-expressed genes are not considered during normalization factor computation:; ['1', '3', '4']; finished (0:00:00); >>> X_norm; array([[ 0.5, 0.5, 0.5, 1. , 1. ],; [ 0.5, 0.5, 0.5, 1. , 1. ],; [ 0.5, 11. , 0.5, 1. , 1. ]], dtype=float32); """"""; # D",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_normalization.py
Availability,avail,available,"ix.; If `False`, omit zero-centering variables; (uses *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` or; *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD`),; which allows to handle sparse input efficiently.; Passing `None` decides automatically based on sparseness of the data.; svd_solver; SVD solver to use:. `None`; See `chunked` and `zero_center` descriptions to determine which class will be used.; Depending on the class and the type of X different values for default will be set.; If *scikit-learn* :class:`~sklearn.decomposition.PCA` is used, will give `'arpack'`,; if *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` is used, will give `'randomized'`,; if *dask-ml* :class:`~dask_ml.decomposition.PCA` or :class:`~dask_ml.decomposition.IncrementalPCA` is used, will give `'auto'`,; if *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD` is used, will give `'tsqr'`; `'arpack'`; for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`); Not available with *dask* arrays.; `'randomized'`; for the randomized algorithm due to Halko (2009). For *dask* arrays,; this will use :func:`~dask.array.linalg.svd_compressed`.; `'auto'`; chooses automatically depending on the size of the problem.; `'lobpcg'`; An alternative SciPy solver. Not available with dask arrays.; `'tsqr'`; Only available with *dask* arrays. ""tsqr""; algorithm from Benson et. al. (2013). .. versionchanged:: 1.9.3; Default value changed from `'arpack'` to None.; .. versionchanged:: 1.4.5; Default value changed from `'auto'` to `'arpack'`. Efficient computation of the principal components of a sparse matrix; currently only works with the `'arpack`' or `'lobpcg'` solvers. If X is a *dask* array, *dask-ml* classes :class:`~dask_ml.decomposition.PCA`,; :class:`~dask_ml.decomposition.IncrementalPCA`, or; :class:`~dask_ml.decomposition.TruncatedSVD` will be used.; Otherwise their *scikit-learn* counterparts :class:`~sklearn.decomposition.PCA`,; :class:`~sklearn.decomposition.Increm",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_pca.py
Deployability,update,updated,"ar_hvg}; layer; Layer of `adata` to use as expression values.; dtype; Numpy data type string to which to convert the result.; copy; If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned. Is ignored otherwise.; chunked; If `True`, perform an incremental PCA on segments of `chunk_size`.; The incremental PCA automatically zero centers and ignores settings of; `random_seed` and `svd_solver`. Uses sklearn :class:`~sklearn.decomposition.IncrementalPCA` or; *dask-ml* :class:`~dask_ml.decomposition.IncrementalPCA`. If `False`, perform a full PCA and; use sklearn :class:`~sklearn.decomposition.PCA` or; *dask-ml* :class:`~dask_ml.decomposition.PCA`; chunk_size; Number of observations to include in each chunk.; Required if `chunked=True` was passed. Returns; -------; If `data` is array-like and `return_info=False` was passed,; this function returns the PCA representation of `data` as an; array of the same type as the input array. Otherwise, it returns `None` if `copy=False`, else an updated `AnnData` object.; Sets the following fields:. `.obsm['X_pca']` : :class:`~scipy.sparse.spmatrix` | :class:`~numpy.ndarray` (shape `(adata.n_obs, n_comps)`); PCA representation of data.; `.varm['PCs']` : :class:`~numpy.ndarray` (shape `(adata.n_vars, n_comps)`); The principal components containing the loadings.; `.uns['pca']['variance_ratio']` : :class:`~numpy.ndarray` (shape `(n_comps,)`); Ratio of explained variance.; `.uns['pca']['variance']` : :class:`~numpy.ndarray` (shape `(n_comps,)`); Explained variance, equivalent to the eigenvalues of the; covariance matrix.; """"""; # Current chunking implementation relies on pca being called on X; # chunked calculation is not randomized, anyways; # Unify new mask argument and deprecated use_highly_varible argument; # See: https://github.com/scverse/scanpy/pull/2816#issuecomment-1932650529; # check_random_state returns a numpy RandomState when passed an int but; # dask needs an int for random state; # This is for backward",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_pca.py
Energy Efficiency,efficient,efficiently,"riance decomposition.; Uses the implementation of *scikit-learn* :cite:p:`Pedregosa2011`. .. versionchanged:: 1.5.0. In previous versions, computing a PCA on a sparse matrix would make; a dense copy of the array for mean centering.; As of scanpy 1.5.0, mean centering is implicit.; While results are extremely similar, they are not exactly the same.; If you would like to reproduce the old results, pass a dense array. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; n_comps; Number of principal components to compute. Defaults to 50, or 1 - minimum; dimension size of selected representation.; layer; If provided, which element of layers to use for PCA.; zero_center; If `True`, compute standard PCA from covariance matrix.; If `False`, omit zero-centering variables; (uses *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` or; *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD`),; which allows to handle sparse input efficiently.; Passing `None` decides automatically based on sparseness of the data.; svd_solver; SVD solver to use:. `None`; See `chunked` and `zero_center` descriptions to determine which class will be used.; Depending on the class and the type of X different values for default will be set.; If *scikit-learn* :class:`~sklearn.decomposition.PCA` is used, will give `'arpack'`,; if *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` is used, will give `'randomized'`,; if *dask-ml* :class:`~dask_ml.decomposition.PCA` or :class:`~dask_ml.decomposition.IncrementalPCA` is used, will give `'auto'`,; if *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD` is used, will give `'tsqr'`; `'arpack'`; for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`); Not available with *dask* arrays.; `'randomized'`; for the randomized algorithm due to Halko (2009). For *dask* arrays,; this will use :func:`~dask.array.linalg.svd_compressed`.; `'auto'`; chooses automat",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_pca.py
Integrability,wrap,wrapper,"rs to use for PCA.; zero_center; If `True`, compute standard PCA from covariance matrix.; If `False`, omit zero-centering variables; (uses *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` or; *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD`),; which allows to handle sparse input efficiently.; Passing `None` decides automatically based on sparseness of the data.; svd_solver; SVD solver to use:. `None`; See `chunked` and `zero_center` descriptions to determine which class will be used.; Depending on the class and the type of X different values for default will be set.; If *scikit-learn* :class:`~sklearn.decomposition.PCA` is used, will give `'arpack'`,; if *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` is used, will give `'randomized'`,; if *dask-ml* :class:`~dask_ml.decomposition.PCA` or :class:`~dask_ml.decomposition.IncrementalPCA` is used, will give `'auto'`,; if *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD` is used, will give `'tsqr'`; `'arpack'`; for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`); Not available with *dask* arrays.; `'randomized'`; for the randomized algorithm due to Halko (2009). For *dask* arrays,; this will use :func:`~dask.array.linalg.svd_compressed`.; `'auto'`; chooses automatically depending on the size of the problem.; `'lobpcg'`; An alternative SciPy solver. Not available with dask arrays.; `'tsqr'`; Only available with *dask* arrays. ""tsqr""; algorithm from Benson et. al. (2013). .. versionchanged:: 1.9.3; Default value changed from `'arpack'` to None.; .. versionchanged:: 1.4.5; Default value changed from `'auto'` to `'arpack'`. Efficient computation of the principal components of a sparse matrix; currently only works with the `'arpack`' or `'lobpcg'` solvers. If X is a *dask* array, *dask-ml* classes :class:`~dask_ml.decomposition.PCA`,; :class:`~dask_ml.decomposition.IncrementalPCA`, or; :class:`~dask_ml.decomposition.TruncatedSVD` will be used.; Otherwise their *scikit-learn* co",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_pca.py
Modifiability,layers,layers,"""""""\; Principal component analysis :cite:p:`Pedregosa2011`. Computes PCA coordinates, loadings and variance decomposition.; Uses the implementation of *scikit-learn* :cite:p:`Pedregosa2011`. .. versionchanged:: 1.5.0. In previous versions, computing a PCA on a sparse matrix would make; a dense copy of the array for mean centering.; As of scanpy 1.5.0, mean centering is implicit.; While results are extremely similar, they are not exactly the same.; If you would like to reproduce the old results, pass a dense array. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; n_comps; Number of principal components to compute. Defaults to 50, or 1 - minimum; dimension size of selected representation.; layer; If provided, which element of layers to use for PCA.; zero_center; If `True`, compute standard PCA from covariance matrix.; If `False`, omit zero-centering variables; (uses *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` or; *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD`),; which allows to handle sparse input efficiently.; Passing `None` decides automatically based on sparseness of the data.; svd_solver; SVD solver to use:. `None`; See `chunked` and `zero_center` descriptions to determine which class will be used.; Depending on the class and the type of X different values for default will be set.; If *scikit-learn* :class:`~sklearn.decomposition.PCA` is used, will give `'arpack'`,; if *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` is used, will give `'randomized'`,; if *dask-ml* :class:`~dask_ml.decomposition.PCA` or :class:`~dask_ml.decomposition.IncrementalPCA` is used, will give `'auto'`,; if *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD` is used, will give `'tsqr'`; `'arpack'`; for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`); Not available with *dask* arrays.; `'randomized'`; for the randomized algorithm due to Halko (2009). Fo",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_pca.py
Performance,load,loadings,"""""""\; Principal component analysis :cite:p:`Pedregosa2011`. Computes PCA coordinates, loadings and variance decomposition.; Uses the implementation of *scikit-learn* :cite:p:`Pedregosa2011`. .. versionchanged:: 1.5.0. In previous versions, computing a PCA on a sparse matrix would make; a dense copy of the array for mean centering.; As of scanpy 1.5.0, mean centering is implicit.; While results are extremely similar, they are not exactly the same.; If you would like to reproduce the old results, pass a dense array. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; n_comps; Number of principal components to compute. Defaults to 50, or 1 - minimum; dimension size of selected representation.; layer; If provided, which element of layers to use for PCA.; zero_center; If `True`, compute standard PCA from covariance matrix.; If `False`, omit zero-centering variables; (uses *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` or; *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD`),; which allows to handle sparse input efficiently.; Passing `None` decides automatically based on sparseness of the data.; svd_solver; SVD solver to use:. `None`; See `chunked` and `zero_center` descriptions to determine which class will be used.; Depending on the class and the type of X different values for default will be set.; If *scikit-learn* :class:`~sklearn.decomposition.PCA` is used, will give `'arpack'`,; if *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` is used, will give `'randomized'`,; if *dask-ml* :class:`~dask_ml.decomposition.PCA` or :class:`~dask_ml.decomposition.IncrementalPCA` is used, will give `'auto'`,; if *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD` is used, will give `'tsqr'`; `'arpack'`; for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`); Not available with *dask* arrays.; `'randomized'`; for the randomized algorithm due to Halko (2009). Fo",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_pca.py
Security,validat,validated,"mposition.IncrementalPCA`. If `False`, perform a full PCA and; use sklearn :class:`~sklearn.decomposition.PCA` or; *dask-ml* :class:`~dask_ml.decomposition.PCA`; chunk_size; Number of observations to include in each chunk.; Required if `chunked=True` was passed. Returns; -------; If `data` is array-like and `return_info=False` was passed,; this function returns the PCA representation of `data` as an; array of the same type as the input array. Otherwise, it returns `None` if `copy=False`, else an updated `AnnData` object.; Sets the following fields:. `.obsm['X_pca']` : :class:`~scipy.sparse.spmatrix` | :class:`~numpy.ndarray` (shape `(adata.n_obs, n_comps)`); PCA representation of data.; `.varm['PCs']` : :class:`~numpy.ndarray` (shape `(adata.n_vars, n_comps)`); The principal components containing the loadings.; `.uns['pca']['variance_ratio']` : :class:`~numpy.ndarray` (shape `(n_comps,)`); Ratio of explained variance.; `.uns['pca']['variance']` : :class:`~numpy.ndarray` (shape `(n_comps,)`); Explained variance, equivalent to the eigenvalues of the; covariance matrix.; """"""; # Current chunking implementation relies on pca being called on X; # chunked calculation is not randomized, anyways; # Unify new mask argument and deprecated use_highly_varible argument; # See: https://github.com/scverse/scanpy/pull/2816#issuecomment-1932650529; # check_random_state returns a numpy RandomState when passed an int but; # dask needs an int for random state; # This is for backwards compat. Better behaviour would be to either error or use arpack.; # this is just a wrapper for the results; """"""\; Unify new mask argument and deprecated use_highly_varible argument. Returns both the normalized mask parameter and the validated mask array.; """"""; # First, verify and possibly warn; # Handle default case and explicit use_highly_variable=True; # Without highly variable genes, we don’t use a mask by default; # u_based_decision was changed in https://github.com/scikit-learn/scikit-learn/pull/27491",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_pca.py
Usability,learn,learn,"""""""\; Principal component analysis :cite:p:`Pedregosa2011`. Computes PCA coordinates, loadings and variance decomposition.; Uses the implementation of *scikit-learn* :cite:p:`Pedregosa2011`. .. versionchanged:: 1.5.0. In previous versions, computing a PCA on a sparse matrix would make; a dense copy of the array for mean centering.; As of scanpy 1.5.0, mean centering is implicit.; While results are extremely similar, they are not exactly the same.; If you would like to reproduce the old results, pass a dense array. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; n_comps; Number of principal components to compute. Defaults to 50, or 1 - minimum; dimension size of selected representation.; layer; If provided, which element of layers to use for PCA.; zero_center; If `True`, compute standard PCA from covariance matrix.; If `False`, omit zero-centering variables; (uses *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` or; *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD`),; which allows to handle sparse input efficiently.; Passing `None` decides automatically based on sparseness of the data.; svd_solver; SVD solver to use:. `None`; See `chunked` and `zero_center` descriptions to determine which class will be used.; Depending on the class and the type of X different values for default will be set.; If *scikit-learn* :class:`~sklearn.decomposition.PCA` is used, will give `'arpack'`,; if *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` is used, will give `'randomized'`,; if *dask-ml* :class:`~dask_ml.decomposition.PCA` or :class:`~dask_ml.decomposition.IncrementalPCA` is used, will give `'auto'`,; if *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD` is used, will give `'tsqr'`; `'arpack'`; for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`); Not available with *dask* arrays.; `'randomized'`; for the randomized algorithm due to Halko (2009). Fo",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_pca.py
Deployability,update,updates,"trics for variables in adata. If inplace, values are placed into the; AnnData's `.var` dataframe. {doc_var_qc_returns}; """"""; # Handle whether X is passed; # COO not subscriptable; # Current memory bottleneck for csr matrices:; # Relabel; """"""\; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section; `Returns` for specifics. Largely based on `calculateQCMetrics` from scater; :cite:p:`McCarthy2017`. Currently is most efficient on a sparse CSR or dense matrix. Note that this method can take a while to compile on the first call. That; result is then cached to disk to be used later. Parameters; ----------; {doc_adata_basic}; {doc_qc_metric_naming}; {doc_obs_qc_args}; {doc_expr_reps}; inplace; Whether to place calculated metrics in `adata`'s `.obs` and `.var`.; log1p; Set to `False` to skip computing `log1p` transformed annotations. Returns; -------; Depending on `inplace` returns calculated metrics; (as :class:`~pandas.DataFrame`) or updates `adata`'s `obs` and `var`. {doc_obs_qc_returns}. {doc_var_qc_returns}. Example; -------; Calculate qc metrics for visualization. .. plot::; :context: close-figs. import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc3k(); pbmc.var[""mito""] = pbmc.var_names.str.startswith(""MT-""); sc.pp.calculate_qc_metrics(pbmc, qc_vars=[""mito""], inplace=True); sns.jointplot(; data=pbmc.obs,; x=""log1p_total_counts"",; y=""log1p_n_genes_by_counts"",; kind=""hex"",; ). .. plot::; :context: close-figs. sns.histplot(pbmc.obs[""pct_counts_mito""]); """"""; # Pass X so I only have to do it once; # COO not subscriptable; # Convert qc_vars to list if str; """"""\; Calculates cumulative proportions of top expressed genes. Parameters; ----------; mtx; Matrix, where each row is a sample, each column a feature.; n; Rank to calculate proportions up to. Value is treated as 1-indexed,; `n=50` will calculate cumulative proportions up to the 50th most; expressed gene.; """"""; # Allowing numba to do more; # Not a view; # Sort",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_qc.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_qc.py
Energy Efficiency,efficient,efficient,"values are placed into; the AnnData's `.obs` dataframe. {doc_obs_qc_returns}; """"""; # Handle whether X is passed; # COO not subscriptable; """"""\; Describe variables of anndata. Calculates a number of qc metrics for variables in AnnData object. See; section `Returns` for a description of those metrics. Params; ------; {doc_adata_basic}; {doc_qc_metric_naming}; {doc_expr_reps}; inplace; Whether to place calculated metrics in `adata.var`.; X; Matrix to calculate values on. Meant for internal usage. Returns; -------; QC metrics for variables in adata. If inplace, values are placed into the; AnnData's `.var` dataframe. {doc_var_qc_returns}; """"""; # Handle whether X is passed; # COO not subscriptable; # Current memory bottleneck for csr matrices:; # Relabel; """"""\; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section; `Returns` for specifics. Largely based on `calculateQCMetrics` from scater; :cite:p:`McCarthy2017`. Currently is most efficient on a sparse CSR or dense matrix. Note that this method can take a while to compile on the first call. That; result is then cached to disk to be used later. Parameters; ----------; {doc_adata_basic}; {doc_qc_metric_naming}; {doc_obs_qc_args}; {doc_expr_reps}; inplace; Whether to place calculated metrics in `adata`'s `.obs` and `.var`.; log1p; Set to `False` to skip computing `log1p` transformed annotations. Returns; -------; Depending on `inplace` returns calculated metrics; (as :class:`~pandas.DataFrame`) or updates `adata`'s `obs` and `var`. {doc_obs_qc_returns}. {doc_var_qc_returns}. Example; -------; Calculate qc metrics for visualization. .. plot::; :context: close-figs. import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc3k(); pbmc.var[""mito""] = pbmc.var_names.str.startswith(""MT-""); sc.pp.calculate_qc_metrics(pbmc, qc_vars=[""mito""], inplace=True); sns.jointplot(; data=pbmc.obs,; x=""log1p_total_counts"",; y=""log1p_n_genes_by_counts"",; kind=""hex"",; ). .. plot::; :context",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_qc.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_qc.py
Modifiability,variab,variables,"""""""\; Describe observations of anndata. Calculates a number of qc metrics for observations in AnnData object. See; section `Returns` for a description of those metrics. Note that this method can take a while to compile on the first call. That; result is then cached to disk to be used later. Params; ------; {doc_adata_basic}; {doc_qc_metric_naming}; {doc_obs_qc_args}; {doc_expr_reps}; log1p; Add `log1p` transformed metrics.; inplace; Whether to place calculated metrics in `adata.obs`.; X; Matrix to calculate values on. Meant for internal usage. Returns; -------; QC metrics for observations in adata. If inplace, values are placed into; the AnnData's `.obs` dataframe. {doc_obs_qc_returns}; """"""; # Handle whether X is passed; # COO not subscriptable; """"""\; Describe variables of anndata. Calculates a number of qc metrics for variables in AnnData object. See; section `Returns` for a description of those metrics. Params; ------; {doc_adata_basic}; {doc_qc_metric_naming}; {doc_expr_reps}; inplace; Whether to place calculated metrics in `adata.var`.; X; Matrix to calculate values on. Meant for internal usage. Returns; -------; QC metrics for variables in adata. If inplace, values are placed into the; AnnData's `.var` dataframe. {doc_var_qc_returns}; """"""; # Handle whether X is passed; # COO not subscriptable; # Current memory bottleneck for csr matrices:; # Relabel; """"""\; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section; `Returns` for specifics. Largely based on `calculateQCMetrics` from scater; :cite:p:`McCarthy2017`. Currently is most efficient on a sparse CSR or dense matrix. Note that this method can take a while to compile on the first call. That; result is then cached to disk to be used later. Parameters; ----------; {doc_adata_basic}; {doc_qc_metric_naming}; {doc_obs_qc_args}; {doc_expr_reps}; inplace; Whether to place calculated metrics in `adata`'s `.obs` and `.var`.; log1p; Set to `False` to skip computing `log1p`",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_qc.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_qc.py
Performance,cache,cached,"""""""\; Describe observations of anndata. Calculates a number of qc metrics for observations in AnnData object. See; section `Returns` for a description of those metrics. Note that this method can take a while to compile on the first call. That; result is then cached to disk to be used later. Params; ------; {doc_adata_basic}; {doc_qc_metric_naming}; {doc_obs_qc_args}; {doc_expr_reps}; log1p; Add `log1p` transformed metrics.; inplace; Whether to place calculated metrics in `adata.obs`.; X; Matrix to calculate values on. Meant for internal usage. Returns; -------; QC metrics for observations in adata. If inplace, values are placed into; the AnnData's `.obs` dataframe. {doc_obs_qc_returns}; """"""; # Handle whether X is passed; # COO not subscriptable; """"""\; Describe variables of anndata. Calculates a number of qc metrics for variables in AnnData object. See; section `Returns` for a description of those metrics. Params; ------; {doc_adata_basic}; {doc_qc_metric_naming}; {doc_expr_reps}; inplace; Whether to place calculated metrics in `adata.var`.; X; Matrix to calculate values on. Meant for internal usage. Returns; -------; QC metrics for variables in adata. If inplace, values are placed into the; AnnData's `.var` dataframe. {doc_var_qc_returns}; """"""; # Handle whether X is passed; # COO not subscriptable; # Current memory bottleneck for csr matrices:; # Relabel; """"""\; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section; `Returns` for specifics. Largely based on `calculateQCMetrics` from scater; :cite:p:`McCarthy2017`. Currently is most efficient on a sparse CSR or dense matrix. Note that this method can take a while to compile on the first call. That; result is then cached to disk to be used later. Parameters; ----------; {doc_adata_basic}; {doc_qc_metric_naming}; {doc_obs_qc_args}; {doc_expr_reps}; inplace; Whether to place calculated metrics in `adata`'s `.obs` and `.var`.; log1p; Set to `False` to skip computing `log1p`",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_qc.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_qc.py
Usability,simpl,simple,"_basic}; {doc_qc_metric_naming}; {doc_obs_qc_args}; {doc_expr_reps}; inplace; Whether to place calculated metrics in `adata`'s `.obs` and `.var`.; log1p; Set to `False` to skip computing `log1p` transformed annotations. Returns; -------; Depending on `inplace` returns calculated metrics; (as :class:`~pandas.DataFrame`) or updates `adata`'s `obs` and `var`. {doc_obs_qc_returns}. {doc_var_qc_returns}. Example; -------; Calculate qc metrics for visualization. .. plot::; :context: close-figs. import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc3k(); pbmc.var[""mito""] = pbmc.var_names.str.startswith(""MT-""); sc.pp.calculate_qc_metrics(pbmc, qc_vars=[""mito""], inplace=True); sns.jointplot(; data=pbmc.obs,; x=""log1p_total_counts"",; y=""log1p_n_genes_by_counts"",; kind=""hex"",; ). .. plot::; :context: close-figs. sns.histplot(pbmc.obs[""pct_counts_mito""]); """"""; # Pass X so I only have to do it once; # COO not subscriptable; # Convert qc_vars to list if str; """"""\; Calculates cumulative proportions of top expressed genes. Parameters; ----------; mtx; Matrix, where each row is a sample, each column a feature.; n; Rank to calculate proportions up to. Value is treated as 1-indexed,; `n=50` will calculate cumulative proportions up to the 50th most; expressed gene.; """"""; # Allowing numba to do more; # Not a view; # Sorting on a reversed view (e.g. a descending sort); # Is this not just vec.sum()?; """"""; Calculates total percentage of counts in top ns genes. Parameters; ----------; mtx; Matrix, where each row is a sample, each column a feature.; ns; Positions to calculate cumulative proportion at. Values are considered; 1-indexed, e.g. `ns=[50]` will calculate cumulative proportion up to; the 50th most expressed gene.; """"""; # Pretty much just does dispatch; # Currently ns is considered to be 1 indexed; # work around https://github.com/numba/numba/issues/5056; # Just to keep it simple, as a dense matrix; # can’t use enumerate due to https://github.com/numba/numba/issues/2625",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_qc.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_qc.py
Deployability,update,update,"""""""Preprocessing recipes from the literature""""""; """"""\; Normalization and filtering as of :cite:p:`Weinreb2017`. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. Parameters; ----------; adata; Annotated data matrix.; log; Logarithmize data?; copy; Return a copy if true.; """"""; # this modifies the object itself; # update adata; """"""\; Normalization and filtering as of Seurat :cite:p:`Satija2015`. This uses a particular preprocessing. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. Parameters; ----------; adata; Annotated data matrix.; log; Logarithmize data?; plot; Show a plot of the gene dispersion vs. mean relation.; copy; Return a copy if true.; """"""; # filter genes; """"""\; Normalization and filtering as of :cite:t:`Zheng2017`. Reproduces the preprocessing of :cite:t:`Zheng2017` – the Cell Ranger R Kit of 10x; Genomics. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. The recipe runs the following steps. .. code:: python. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'; ); filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes; adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False; ); adata = adata[:, filter_result.gene_subset] # subset the genes; sc.pp.normalize_per_cell(adata) # renormalize after filtering; if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1); sc.pp.scale(adata) # scale to unit variance and shift to zero mean. Parameters; ----------; adata; Annotated data matrix.; n_top_genes; Number of genes to keep.; log; Take logarithm.; plot; Show a plot of the gene dispersion vs. mean relation.; copy; Return a copy of `adata` instead of updating it. Returns; -------; Returns or updates `adata` depending on `copy`.; """"""; # only consider genes with more than 1 count; # normalize wi",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_recipes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_recipes.py
Integrability,depend,depending,"; copy; Return a copy if true.; """"""; # this modifies the object itself; # update adata; """"""\; Normalization and filtering as of Seurat :cite:p:`Satija2015`. This uses a particular preprocessing. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. Parameters; ----------; adata; Annotated data matrix.; log; Logarithmize data?; plot; Show a plot of the gene dispersion vs. mean relation.; copy; Return a copy if true.; """"""; # filter genes; """"""\; Normalization and filtering as of :cite:t:`Zheng2017`. Reproduces the preprocessing of :cite:t:`Zheng2017` – the Cell Ranger R Kit of 10x; Genomics. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. The recipe runs the following steps. .. code:: python. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'; ); filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes; adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False; ); adata = adata[:, filter_result.gene_subset] # subset the genes; sc.pp.normalize_per_cell(adata) # renormalize after filtering; if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1); sc.pp.scale(adata) # scale to unit variance and shift to zero mean. Parameters; ----------; adata; Annotated data matrix.; n_top_genes; Number of genes to keep.; log; Take logarithm.; plot; Show a plot of the gene dispersion vs. mean relation.; copy; Return a copy of `adata` instead of updating it. Returns; -------; Returns or updates `adata` depending on `copy`.; """"""; # only consider genes with more than 1 count; # normalize with total UMI count per cell; # should not import at the top of the file; # actually filter the genes, the following is the inplace version of; # adata = adata[:, filter_result.gene_subset]; # filter genes; # renormalize after filtering; # log transform: X = log(X + 1)",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_recipes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_recipes.py
Modifiability,variab,variable,"; copy; Return a copy if true.; """"""; # this modifies the object itself; # update adata; """"""\; Normalization and filtering as of Seurat :cite:p:`Satija2015`. This uses a particular preprocessing. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. Parameters; ----------; adata; Annotated data matrix.; log; Logarithmize data?; plot; Show a plot of the gene dispersion vs. mean relation.; copy; Return a copy if true.; """"""; # filter genes; """"""\; Normalization and filtering as of :cite:t:`Zheng2017`. Reproduces the preprocessing of :cite:t:`Zheng2017` – the Cell Ranger R Kit of 10x; Genomics. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. The recipe runs the following steps. .. code:: python. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'; ); filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes; adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False; ); adata = adata[:, filter_result.gene_subset] # subset the genes; sc.pp.normalize_per_cell(adata) # renormalize after filtering; if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1); sc.pp.scale(adata) # scale to unit variance and shift to zero mean. Parameters; ----------; adata; Annotated data matrix.; n_top_genes; Number of genes to keep.; log; Take logarithm.; plot; Show a plot of the gene dispersion vs. mean relation.; copy; Return a copy of `adata` instead of updating it. Returns; -------; Returns or updates `adata` depending on `copy`.; """"""; # only consider genes with more than 1 count; # normalize with total UMI count per cell; # should not import at the top of the file; # actually filter the genes, the following is the inplace version of; # adata = adata[:, filter_result.gene_subset]; # filter genes; # renormalize after filtering; # log transform: X = log(X + 1)",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_recipes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_recipes.py
Testability,log,logarithmized,"""""""Preprocessing recipes from the literature""""""; """"""\; Normalization and filtering as of :cite:p:`Weinreb2017`. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. Parameters; ----------; adata; Annotated data matrix.; log; Logarithmize data?; copy; Return a copy if true.; """"""; # this modifies the object itself; # update adata; """"""\; Normalization and filtering as of Seurat :cite:p:`Satija2015`. This uses a particular preprocessing. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. Parameters; ----------; adata; Annotated data matrix.; log; Logarithmize data?; plot; Show a plot of the gene dispersion vs. mean relation.; copy; Return a copy if true.; """"""; # filter genes; """"""\; Normalization and filtering as of :cite:t:`Zheng2017`. Reproduces the preprocessing of :cite:t:`Zheng2017` – the Cell Ranger R Kit of 10x; Genomics. Expects non-logarithmized data.; If using logarithmized data, pass `log=False`. The recipe runs the following steps. .. code:: python. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'; ); filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes; adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False; ); adata = adata[:, filter_result.gene_subset] # subset the genes; sc.pp.normalize_per_cell(adata) # renormalize after filtering; if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1); sc.pp.scale(adata) # scale to unit variance and shift to zero mean. Parameters; ----------; adata; Annotated data matrix.; n_top_genes; Number of genes to keep.; log; Take logarithm.; plot; Show a plot of the gene dispersion vs. mean relation.; copy; Return a copy of `adata` instead of updating it. Returns; -------; Returns or updates `adata` depending on `copy`.; """"""; # only consider genes with more than 1 count; # normalize wi",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_recipes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_recipes.py
Availability,avail,available,"# install dask if available; """"""\; Scale data to unit variance and zero mean. .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and (for zero_center==True) set to 0; during this operation. In the future, they might be set to NaNs. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; Whether this function should be performed inplace. If an AnnData object; is passed, this also determines if a copy is returned.; layer; If provided, which element of layers to scale.; obsm; If provided, which element of obsm to scale.; mask_obs; Restrict both the derivation of scaling parameters and the scaling itself; to a certain set of observations. The mask is specified as a boolean array; or a string referring to an array in :attr:`~anndata.AnnData.obs`.; This will transform data from csc to csr format if `issparse(data)`. Returns; -------; Returns `None` if `copy=False`, else returns an updated `AnnData` object. Sets the following fields:. `adata.X` | `adata.layers[layer]` : :class:`numpy.ndarray` | :class:`scipy.sparse._csr.csr_matrix` (dtype `float`); Scaled count data matrix.; `adata.var['mean']` : :class:`pandas.Series` (dtype `float`); Means per gene before scaling.; `adata.var['std']` : :class:`pandas.Series` (dtype `float`); Standard deviations per gene before scaling.; `adata.var['var']` : :class:`pandas.Series` (dtype `float`); Variances per gene before scaling.; """"""; # Be careful of what? This should be more specific; # do the clipping; # need to add the following here to make inplace logic work; # Since the data has been copied; # because a copy has already been made, if it were to be made",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scale.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scale.py
Deployability,install,install,"# install dask if available; """"""\; Scale data to unit variance and zero mean. .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and (for zero_center==True) set to 0; during this operation. In the future, they might be set to NaNs. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; Whether this function should be performed inplace. If an AnnData object; is passed, this also determines if a copy is returned.; layer; If provided, which element of layers to scale.; obsm; If provided, which element of obsm to scale.; mask_obs; Restrict both the derivation of scaling parameters and the scaling itself; to a certain set of observations. The mask is specified as a boolean array; or a string referring to an array in :attr:`~anndata.AnnData.obs`.; This will transform data from csc to csr format if `issparse(data)`. Returns; -------; Returns `None` if `copy=False`, else returns an updated `AnnData` object. Sets the following fields:. `adata.X` | `adata.layers[layer]` : :class:`numpy.ndarray` | :class:`scipy.sparse._csr.csr_matrix` (dtype `float`); Scaled count data matrix.; `adata.var['mean']` : :class:`pandas.Series` (dtype `float`); Means per gene before scaling.; `adata.var['std']` : :class:`pandas.Series` (dtype `float`); Standard deviations per gene before scaling.; `adata.var['var']` : :class:`pandas.Series` (dtype `float`); Variances per gene before scaling.; """"""; # Be careful of what? This should be more specific; # do the clipping; # need to add the following here to make inplace logic work; # Since the data has been copied; # because a copy has already been made, if it were to be made",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scale.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scale.py
Energy Efficiency,efficient,efficiently,"# install dask if available; """"""\; Scale data to unit variance and zero mean. .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and (for zero_center==True) set to 0; during this operation. In the future, they might be set to NaNs. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; Whether this function should be performed inplace. If an AnnData object; is passed, this also determines if a copy is returned.; layer; If provided, which element of layers to scale.; obsm; If provided, which element of obsm to scale.; mask_obs; Restrict both the derivation of scaling parameters and the scaling itself; to a certain set of observations. The mask is specified as a boolean array; or a string referring to an array in :attr:`~anndata.AnnData.obs`.; This will transform data from csc to csr format if `issparse(data)`. Returns; -------; Returns `None` if `copy=False`, else returns an updated `AnnData` object. Sets the following fields:. `adata.X` | `adata.layers[layer]` : :class:`numpy.ndarray` | :class:`scipy.sparse._csr.csr_matrix` (dtype `float`); Scaled count data matrix.; `adata.var['mean']` : :class:`pandas.Series` (dtype `float`); Means per gene before scaling.; `adata.var['std']` : :class:`pandas.Series` (dtype `float`); Standard deviations per gene before scaling.; `adata.var['var']` : :class:`pandas.Series` (dtype `float`); Variances per gene before scaling.; """"""; # Be careful of what? This should be more specific; # do the clipping; # need to add the following here to make inplace logic work; # Since the data has been copied; # because a copy has already been made, if it were to be made",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scale.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scale.py
Modifiability,variab,variables,"# install dask if available; """"""\; Scale data to unit variance and zero mean. .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and (for zero_center==True) set to 0; during this operation. In the future, they might be set to NaNs. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; Whether this function should be performed inplace. If an AnnData object; is passed, this also determines if a copy is returned.; layer; If provided, which element of layers to scale.; obsm; If provided, which element of obsm to scale.; mask_obs; Restrict both the derivation of scaling parameters and the scaling itself; to a certain set of observations. The mask is specified as a boolean array; or a string referring to an array in :attr:`~anndata.AnnData.obs`.; This will transform data from csc to csr format if `issparse(data)`. Returns; -------; Returns `None` if `copy=False`, else returns an updated `AnnData` object. Sets the following fields:. `adata.X` | `adata.layers[layer]` : :class:`numpy.ndarray` | :class:`scipy.sparse._csr.csr_matrix` (dtype `float`); Scaled count data matrix.; `adata.var['mean']` : :class:`pandas.Series` (dtype `float`); Means per gene before scaling.; `adata.var['std']` : :class:`pandas.Series` (dtype `float`); Standard deviations per gene before scaling.; `adata.var['var']` : :class:`pandas.Series` (dtype `float`); Variances per gene before scaling.; """"""; # Be careful of what? This should be more specific; # do the clipping; # need to add the following here to make inplace logic work; # Since the data has been copied; # because a copy has already been made, if it were to be made",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scale.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scale.py
Performance,perform,performed,"# install dask if available; """"""\; Scale data to unit variance and zero mean. .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and (for zero_center==True) set to 0; during this operation. In the future, they might be set to NaNs. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; Whether this function should be performed inplace. If an AnnData object; is passed, this also determines if a copy is returned.; layer; If provided, which element of layers to scale.; obsm; If provided, which element of obsm to scale.; mask_obs; Restrict both the derivation of scaling parameters and the scaling itself; to a certain set of observations. The mask is specified as a boolean array; or a string referring to an array in :attr:`~anndata.AnnData.obs`.; This will transform data from csc to csr format if `issparse(data)`. Returns; -------; Returns `None` if `copy=False`, else returns an updated `AnnData` object. Sets the following fields:. `adata.X` | `adata.layers[layer]` : :class:`numpy.ndarray` | :class:`scipy.sparse._csr.csr_matrix` (dtype `float`); Scaled count data matrix.; `adata.var['mean']` : :class:`pandas.Series` (dtype `float`); Means per gene before scaling.; `adata.var['std']` : :class:`pandas.Series` (dtype `float`); Standard deviations per gene before scaling.; `adata.var['var']` : :class:`pandas.Series` (dtype `float`); Variances per gene before scaling.; """"""; # Be careful of what? This should be more specific; # do the clipping; # need to add the following here to make inplace logic work; # Since the data has been copied; # because a copy has already been made, if it were to be made",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scale.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scale.py
Testability,log,logic,"# install dask if available; """"""\; Scale data to unit variance and zero mean. .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and (for zero_center==True) set to 0; during this operation. In the future, they might be set to NaNs. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; Whether this function should be performed inplace. If an AnnData object; is passed, this also determines if a copy is returned.; layer; If provided, which element of layers to scale.; obsm; If provided, which element of obsm to scale.; mask_obs; Restrict both the derivation of scaling parameters and the scaling itself; to a certain set of observations. The mask is specified as a boolean array; or a string referring to an array in :attr:`~anndata.AnnData.obs`.; This will transform data from csc to csr format if `issparse(data)`. Returns; -------; Returns `None` if `copy=False`, else returns an updated `AnnData` object. Sets the following fields:. `adata.X` | `adata.layers[layer]` : :class:`numpy.ndarray` | :class:`scipy.sparse._csr.csr_matrix` (dtype `float`); Scaled count data matrix.; `adata.var['mean']` : :class:`pandas.Series` (dtype `float`); Means per gene before scaling.; `adata.var['std']` : :class:`pandas.Series` (dtype `float`); Standard deviations per gene before scaling.; `adata.var['var']` : :class:`pandas.Series` (dtype `float`); Variances per gene before scaling.; """"""; # Be careful of what? This should be more specific; # do the clipping; # need to add the following here to make inplace logic work; # Since the data has been copied; # because a copy has already been made, if it were to be made",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scale.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scale.py
Availability,avail,available,"""""""Simple Preprocessing Functions. Compositions of these functions are found in sc.preprocess.recipes.; """"""; # install dask if available; # backwards compat; # noqa: F401; """"""\; Filter cell outliers based on counts and numbers of genes expressed. For instance, only keep cells with at least `min_counts` counts or; `min_genes` genes expressed. This is to filter measurement outliers,; i.e. “unreliable” observations. Only provide one of the optional parameters `min_counts`, `min_genes`,; `max_counts`, `max_genes` per call. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; min_counts; Minimum number of counts required for a cell to pass filtering.; min_genes; Minimum number of genes expressed required for a cell to pass filtering.; max_counts; Maximum number of counts required for a cell to pass filtering.; max_genes; Maximum number of genes expressed required for a cell to pass filtering.; inplace; Perform computation inplace or return result. Returns; -------; Depending on `inplace`, returns the following arrays or directly subsets; and annotates the data matrix:. cells_subset; Boolean index mask that does filtering. `True` means that the; cell is kept. `False` means the cell is removed.; number_per_cell; Depending on what was thresholded (`counts` or `genes`),; the array stores `n_counts` or `n_cells` per gene. Examples; --------; >>> import scanpy as sc; >>> adata = sc.datasets.krumsiek11(); UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; utils.warn_names_duplicates(""obs""); >>> adata.obs_names_make_unique(); >>> adata.n_obs; 640; >>> adata.var_names.tolist() # doctest: +NORMALIZE_WHITESPACE; ['Gata2', 'Gata1', 'Fog1', 'EKLF', 'Fli1', 'SCL',; 'Cebpa', 'Pu.1', 'cJun', 'EgrNab', 'Gfi1']; >>> # add some true zeros; >>> adata.X[adata.X < 0.3] = 0; >>> # simply compute the number of genes per cell; >>> sc.pp.filter_cells(adata, min_genes=",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_simple.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_simple.py
Deployability,install,install,"""""""Simple Preprocessing Functions. Compositions of these functions are found in sc.preprocess.recipes.; """"""; # install dask if available; # backwards compat; # noqa: F401; """"""\; Filter cell outliers based on counts and numbers of genes expressed. For instance, only keep cells with at least `min_counts` counts or; `min_genes` genes expressed. This is to filter measurement outliers,; i.e. “unreliable” observations. Only provide one of the optional parameters `min_counts`, `min_genes`,; `max_counts`, `max_genes` per call. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; min_counts; Minimum number of counts required for a cell to pass filtering.; min_genes; Minimum number of genes expressed required for a cell to pass filtering.; max_counts; Maximum number of counts required for a cell to pass filtering.; max_genes; Maximum number of genes expressed required for a cell to pass filtering.; inplace; Perform computation inplace or return result. Returns; -------; Depending on `inplace`, returns the following arrays or directly subsets; and annotates the data matrix:. cells_subset; Boolean index mask that does filtering. `True` means that the; cell is kept. `False` means the cell is removed.; number_per_cell; Depending on what was thresholded (`counts` or `genes`),; the array stores `n_counts` or `n_cells` per gene. Examples; --------; >>> import scanpy as sc; >>> adata = sc.datasets.krumsiek11(); UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; utils.warn_names_duplicates(""obs""); >>> adata.obs_names_make_unique(); >>> adata.n_obs; 640; >>> adata.var_names.tolist() # doctest: +NORMALIZE_WHITESPACE; ['Gata2', 'Gata1', 'Fog1', 'EKLF', 'Fli1', 'SCL',; 'Cebpa', 'Pu.1', 'cJun', 'EgrNab', 'Gfi1']; >>> # add some true zeros; >>> adata.X[adata.X < 0.3] = 0; >>> # simply compute the number of genes per cell; >>> sc.pp.filter_cells(adata, min_genes=",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_simple.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_simple.py
Energy Efficiency,reduce,reduce,"it (`copy == True`).; """"""; """"""\; Downsample counts from count matrix. If `counts_per_cell` is specified, each cell will downsampled.; If `total_counts` is specified, expression matrix will be downsampled to; contain at most `total_counts`. Parameters; ----------; adata; Annotated data matrix.; counts_per_cell; Target total counts per cell. If a cell has more than 'counts_per_cell',; it will be downsampled to this number. Resulting counts can be specified; on a per cell basis by passing an array.Should be an integer or integer; ndarray with same length as number of obs.; total_counts; Target total counts. If the count matrix has more than `total_counts`; it will be downsampled to have this number.; random_state; Random seed for subsampling.; replace; Whether to sample the counts with replacement.; copy; Determines whether a copy of `adata` is returned. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:. `adata.X` : :class:`numpy.ndarray` | :class:`scipy.sparse.spmatrix` (dtype `float`); Downsampled counts matrix.; """"""; # This logic is all dispatch; # np.random.choice needs int arguments in numba code:; # Faster for csr matrix; # Put it back; """"""\; Evenly reduce counts in cell to target amount. This is an internal function and has some restrictions:. * total counts in cell must be less than target; """"""; # --------------------------------------------------------------------------------; # Helper Functions; # --------------------------------------------------------------------------------; # mean center the data; # calculate the covariance matrix; # calculate eigenvectors & eigenvalues of the covariance matrix; # use 'eigh' rather than 'eig' since C is symmetric,; # the performance gain is substantial; # evals, evecs = np.linalg.eigh(C); # sort eigenvalues in decreasing order; # select the first n eigenvectors (n is desired dimension; # of rescaled data array, or n_comps); # project data points on eigenvectors",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_simple.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_simple.py
Integrability,depend,depending,"x mask that does filtering. `True` means that the; gene is kept. `False` means the gene is removed.; number_per_gene; Depending on what was thresholded (`counts` or `cells`), the array stores; `n_counts` or `n_cells` per gene.; """"""; # proceed with processing the data matrix; """"""\; Logarithmize the data matrix. Computes :math:`X = \\log(X + 1)`,; where :math:`log` denotes the natural logarithm unless a different base is given. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; base; Base of the logarithm. Natural logarithm is used by default.; copy; If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned.; chunked; Process the data matrix in chunks, which will save memory.; Applies only to :class:`~anndata.AnnData`.; chunk_size; `n_obs` of the chunks to process the data in.; layer; Entry of layers to transform.; obsm; Entry of obsm to transform. Returns; -------; Returns or updates `data`, depending on `copy`.; """"""; # Can force arrays to be np.ndarrays, but would be useful to not; # X = check_array(X, dtype=(np.float64, np.float32), ensure_2d=False, copy=copy); """"""\; Square root the data matrix. Computes :math:`X = \\sqrt(X)`. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; copy; If an :class:`~anndata.AnnData` object is passed,; determines whether a copy is returned.; chunked; Process the data matrix in chunks, which will save memory.; Applies only to :class:`~anndata.AnnData`.; chunk_size; `n_obs` of the chunks to process the data in. Returns; -------; Returns or updates `data`, depending on `copy`.; """"""; # proceed with data matrix; # noqa: PLR0917; """"""\; Normalize total counts per cell. .. warning::; .. deprecated:: 1.3.7; Use :func:`~scanpy.pp.normalize_total` instead.; The new function is equivalent to the present; function, except that. * the new function doesn't fi",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_simple.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_simple.py
Modifiability,layers,layers,"ng arrays or directly subsets; and annotates the data matrix. gene_subset; Boolean index mask that does filtering. `True` means that the; gene is kept. `False` means the gene is removed.; number_per_gene; Depending on what was thresholded (`counts` or `cells`), the array stores; `n_counts` or `n_cells` per gene.; """"""; # proceed with processing the data matrix; """"""\; Logarithmize the data matrix. Computes :math:`X = \\log(X + 1)`,; where :math:`log` denotes the natural logarithm unless a different base is given. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; base; Base of the logarithm. Natural logarithm is used by default.; copy; If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned.; chunked; Process the data matrix in chunks, which will save memory.; Applies only to :class:`~anndata.AnnData`.; chunk_size; `n_obs` of the chunks to process the data in.; layer; Entry of layers to transform.; obsm; Entry of obsm to transform. Returns; -------; Returns or updates `data`, depending on `copy`.; """"""; # Can force arrays to be np.ndarrays, but would be useful to not; # X = check_array(X, dtype=(np.float64, np.float32), ensure_2d=False, copy=copy); """"""\; Square root the data matrix. Computes :math:`X = \\sqrt(X)`. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; copy; If an :class:`~anndata.AnnData` object is passed,; determines whether a copy is returned.; chunked; Process the data matrix in chunks, which will save memory.; Applies only to :class:`~anndata.AnnData`.; chunk_size; `n_obs` of the chunks to process the data in. Returns; -------; Returns or updates `data`, depending on `copy`.; """"""; # proceed with data matrix; # noqa: PLR0917; """"""\; Normalize total counts per cell. .. warning::; .. deprecated:: 1.3.7; Use :func:`~scanpy.pp.normalize_total` instead.; The new func",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_simple.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_simple.py
Performance,perform,performance,"it (`copy == True`).; """"""; """"""\; Downsample counts from count matrix. If `counts_per_cell` is specified, each cell will downsampled.; If `total_counts` is specified, expression matrix will be downsampled to; contain at most `total_counts`. Parameters; ----------; adata; Annotated data matrix.; counts_per_cell; Target total counts per cell. If a cell has more than 'counts_per_cell',; it will be downsampled to this number. Resulting counts can be specified; on a per cell basis by passing an array.Should be an integer or integer; ndarray with same length as number of obs.; total_counts; Target total counts. If the count matrix has more than `total_counts`; it will be downsampled to have this number.; random_state; Random seed for subsampling.; replace; Whether to sample the counts with replacement.; copy; Determines whether a copy of `adata` is returned. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:. `adata.X` : :class:`numpy.ndarray` | :class:`scipy.sparse.spmatrix` (dtype `float`); Downsampled counts matrix.; """"""; # This logic is all dispatch; # np.random.choice needs int arguments in numba code:; # Faster for csr matrix; # Put it back; """"""\; Evenly reduce counts in cell to target amount. This is an internal function and has some restrictions:. * total counts in cell must be less than target; """"""; # --------------------------------------------------------------------------------; # Helper Functions; # --------------------------------------------------------------------------------; # mean center the data; # calculate the covariance matrix; # calculate eigenvectors & eigenvalues of the covariance matrix; # use 'eigh' rather than 'eig' since C is symmetric,; # the performance gain is substantial; # evals, evecs = np.linalg.eigh(C); # sort eigenvalues in decreasing order; # select the first n eigenvectors (n is desired dimension; # of rescaled data array, or n_comps); # project data points on eigenvectors",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_simple.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_simple.py
Testability,log,log,"An annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; min_counts; Minimum number of counts required for a gene to pass filtering.; min_cells; Minimum number of cells expressed required for a gene to pass filtering.; max_counts; Maximum number of counts required for a gene to pass filtering.; max_cells; Maximum number of cells expressed required for a gene to pass filtering.; inplace; Perform computation inplace or return result. Returns; -------; Depending on `inplace`, returns the following arrays or directly subsets; and annotates the data matrix. gene_subset; Boolean index mask that does filtering. `True` means that the; gene is kept. `False` means the gene is removed.; number_per_gene; Depending on what was thresholded (`counts` or `cells`), the array stores; `n_counts` or `n_cells` per gene.; """"""; # proceed with processing the data matrix; """"""\; Logarithmize the data matrix. Computes :math:`X = \\log(X + 1)`,; where :math:`log` denotes the natural logarithm unless a different base is given. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; base; Base of the logarithm. Natural logarithm is used by default.; copy; If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned.; chunked; Process the data matrix in chunks, which will save memory.; Applies only to :class:`~anndata.AnnData`.; chunk_size; `n_obs` of the chunks to process the data in.; layer; Entry of layers to transform.; obsm; Entry of obsm to transform. Returns; -------; Returns or updates `data`, depending on `copy`.; """"""; # Can force arrays to be np.ndarrays, but would be useful to not; # X = check_array(X, dtype=(np.float64, np.float32), ensure_2d=False, copy=copy); """"""\; Square root the data matrix. Computes :math:`X = \\sqrt(X)`. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells an",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_simple.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_simple.py
Usability,simpl,simply,"expressed required for a cell to pass filtering.; inplace; Perform computation inplace or return result. Returns; -------; Depending on `inplace`, returns the following arrays or directly subsets; and annotates the data matrix:. cells_subset; Boolean index mask that does filtering. `True` means that the; cell is kept. `False` means the cell is removed.; number_per_cell; Depending on what was thresholded (`counts` or `genes`),; the array stores `n_counts` or `n_cells` per gene. Examples; --------; >>> import scanpy as sc; >>> adata = sc.datasets.krumsiek11(); UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; utils.warn_names_duplicates(""obs""); >>> adata.obs_names_make_unique(); >>> adata.n_obs; 640; >>> adata.var_names.tolist() # doctest: +NORMALIZE_WHITESPACE; ['Gata2', 'Gata1', 'Fog1', 'EKLF', 'Fli1', 'SCL',; 'Cebpa', 'Pu.1', 'cJun', 'EgrNab', 'Gfi1']; >>> # add some true zeros; >>> adata.X[adata.X < 0.3] = 0; >>> # simply compute the number of genes per cell; >>> sc.pp.filter_cells(adata, min_genes=0); >>> adata.n_obs; 640; >>> adata.obs['n_genes'].min(); 1; >>> # filter manually; >>> adata_copy = adata[adata.obs['n_genes'] >= 3]; >>> adata_copy.n_obs; 554; >>> adata_copy.obs['n_genes'].min(); 3; >>> # actually do some filtering; >>> sc.pp.filter_cells(adata, min_genes=3); >>> adata.n_obs; 554; >>> adata.obs['n_genes'].min(); 3; """"""; # proceed with processing the data matrix; """"""\; Filter genes based on number of cells or counts. Keep genes that have at least `min_counts` counts or are expressed in at; least `min_cells` cells or have at most `max_counts` counts or are expressed; in at most `max_cells` cells. Only provide one of the optional parameters `min_counts`, `min_cells`,; `max_counts`, `max_cells` per call. Parameters; ----------; data; An annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; min_counts; Minimum number of counts required for a gene to pass filter",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_simple.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_simple.py
Availability,error,error," to each bin. If just a single gene falls into a bin,; the normalized dispersion is artificially set to 1. You'll be informed; about this if you set `settings.verbosity = 4`.; n_top_genes; Number of highly-variable genes to keep.; log; Use the logarithm of the mean to variance ratio.; subset; Keep highly-variable genes only (if True) else write a bool array for h; ighly-variable genes while keeping all genes; copy; If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned. Returns; -------; If an AnnData `adata` is passed, returns or updates `adata` depending on; `copy`. It filters the `adata` and adds the annotations. **means** : adata.var; Means per gene. Logarithmized when `log` is `True`.; **dispersions** : adata.var; Dispersions per gene. Logarithmized when `log` is `True`.; **dispersions_norm** : adata.var; Normalized dispersions per gene. Logarithmized when `log` is `True`. If a data matrix `X` is passed, the annotation is returned as `np.recarray`; with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`.; """"""; # no copy necessary, X remains unchanged in the following; # now actually compute the dispersion; # set entries equal to zero to small value; # logarithmized mean as in Seurat; # all of the following quantities are ""per-gene"" here; # retrieve those genes that have nan std, these are the ones where; # only a single gene fell in the bin and implicitly set them to have; # a normalized disperion of 1; # Circumvent pandas 0.23 bug. Both sides of the assignment have dtype==float32,; # but there’s still a dtype error without “.value”.; # actually do the normalization; # use values here as index differs; # the next line raises the warning: ""Mean of empty slice""; # interestingly, np.argpartition is slightly slower; # similar to Seurat; """"""Filter genes by coefficient of variance and mean.""""""; """"""Filter genes by fano factor and mean.""""""; """"""See `filter_genes_dispersion` :cite:p:`Weinreb2017`.""""""",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_deprecated/highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_deprecated/highly_variable_genes.py
Deployability,update,updates,"l_ranger', this is usually called for logarithmized data; – in this case you should set `log` to `False`. In their default; workflows, Seurat passes the cutoffs whereas Cell Ranger passes; `n_top_genes`.; min_mean; max_mean; min_disp; max_disp; If `n_top_genes` unequals `None`, these cutoffs for the means and the; normalized dispersions are ignored.; n_bins; Number of bins for binning the mean gene expression. Normalization is; done with respect to each bin. If just a single gene falls into a bin,; the normalized dispersion is artificially set to 1. You'll be informed; about this if you set `settings.verbosity = 4`.; n_top_genes; Number of highly-variable genes to keep.; log; Use the logarithm of the mean to variance ratio.; subset; Keep highly-variable genes only (if True) else write a bool array for h; ighly-variable genes while keeping all genes; copy; If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned. Returns; -------; If an AnnData `adata` is passed, returns or updates `adata` depending on; `copy`. It filters the `adata` and adds the annotations. **means** : adata.var; Means per gene. Logarithmized when `log` is `True`.; **dispersions** : adata.var; Dispersions per gene. Logarithmized when `log` is `True`.; **dispersions_norm** : adata.var; Normalized dispersions per gene. Logarithmized when `log` is `True`. If a data matrix `X` is passed, the annotation is returned as `np.recarray`; with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`.; """"""; # no copy necessary, X remains unchanged in the following; # now actually compute the dispersion; # set entries equal to zero to small value; # logarithmized mean as in Seurat; # all of the following quantities are ""per-gene"" here; # retrieve those genes that have nan std, these are the ones where; # only a single gene fell in the bin and implicitly set them to have; # a normalized disperion of 1; # Circumvent pandas 0.23 bug. Both sides of the",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_deprecated/highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_deprecated/highly_variable_genes.py
Integrability,depend,depending,"l_ranger', this is usually called for logarithmized data; – in this case you should set `log` to `False`. In their default; workflows, Seurat passes the cutoffs whereas Cell Ranger passes; `n_top_genes`.; min_mean; max_mean; min_disp; max_disp; If `n_top_genes` unequals `None`, these cutoffs for the means and the; normalized dispersions are ignored.; n_bins; Number of bins for binning the mean gene expression. Normalization is; done with respect to each bin. If just a single gene falls into a bin,; the normalized dispersion is artificially set to 1. You'll be informed; about this if you set `settings.verbosity = 4`.; n_top_genes; Number of highly-variable genes to keep.; log; Use the logarithm of the mean to variance ratio.; subset; Keep highly-variable genes only (if True) else write a bool array for h; ighly-variable genes while keeping all genes; copy; If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned. Returns; -------; If an AnnData `adata` is passed, returns or updates `adata` depending on; `copy`. It filters the `adata` and adds the annotations. **means** : adata.var; Means per gene. Logarithmized when `log` is `True`.; **dispersions** : adata.var; Dispersions per gene. Logarithmized when `log` is `True`.; **dispersions_norm** : adata.var; Normalized dispersions per gene. Logarithmized when `log` is `True`. If a data matrix `X` is passed, the annotation is returned as `np.recarray`; with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`.; """"""; # no copy necessary, X remains unchanged in the following; # now actually compute the dispersion; # set entries equal to zero to small value; # logarithmized mean as in Seurat; # all of the following quantities are ""per-gene"" here; # retrieve those genes that have nan std, these are the ones where; # only a single gene fell in the bin and implicitly set them to have; # a normalized disperion of 1; # Circumvent pandas 0.23 bug. Both sides of the",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_deprecated/highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_deprecated/highly_variable_genes.py
Modifiability,variab,variable,"# noqa: PLR0917; """"""\; Extract highly variable genes :cite:p:`Satija2015,Zheng2017`. .. warning::; .. deprecated:: 1.3.6; Use :func:`~scanpy.pp.highly_variable_genes`; instead. The new function is equivalent to the present; function, except that. * the new function always expects logarithmized data; * `subset=False` in the new function, it suffices to; merely annotate the genes, tools like `pp.pca` will; detect the annotation; * you can now call: `sc.pl.highly_variable_genes(adata)`; * `copy` is replaced by `inplace`. If trying out parameters, pass the data matrix instead of AnnData. Depending on `flavor`, this reproduces the R-implementations of Seurat; :cite:p:`Satija2015` and Cell Ranger :cite:p:`Zheng2017`. The normalized dispersion is obtained by scaling with the mean and standard; deviation of the dispersions for genes falling into a given bin for mean; expression of genes. This means that for each bin of mean expression, highly; variable genes are selected. Use `flavor='cell_ranger'` with care and in the same way as in; :func:`~scanpy.pp.recipe_zheng17`. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; flavor; Choose the flavor for computing normalized dispersion. If choosing; 'seurat', this expects non-logarithmized data – the logarithm of mean; and dispersion is taken internally when `log` is at its default value; `True`. For 'cell_ranger', this is usually called for logarithmized data; – in this case you should set `log` to `False`. In their default; workflows, Seurat passes the cutoffs whereas Cell Ranger passes; `n_top_genes`.; min_mean; max_mean; min_disp; max_disp; If `n_top_genes` unequals `None`, these cutoffs for the means and the; normalized dispersions are ignored.; n_bins; Number of bins for binning the mean gene expression. Normalization is; done with respect to each bin. If just a single gene falls into a bin,; the normalized dispersion is artificially set to ",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_deprecated/highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_deprecated/highly_variable_genes.py
Safety,detect,detect,"# noqa: PLR0917; """"""\; Extract highly variable genes :cite:p:`Satija2015,Zheng2017`. .. warning::; .. deprecated:: 1.3.6; Use :func:`~scanpy.pp.highly_variable_genes`; instead. The new function is equivalent to the present; function, except that. * the new function always expects logarithmized data; * `subset=False` in the new function, it suffices to; merely annotate the genes, tools like `pp.pca` will; detect the annotation; * you can now call: `sc.pl.highly_variable_genes(adata)`; * `copy` is replaced by `inplace`. If trying out parameters, pass the data matrix instead of AnnData. Depending on `flavor`, this reproduces the R-implementations of Seurat; :cite:p:`Satija2015` and Cell Ranger :cite:p:`Zheng2017`. The normalized dispersion is obtained by scaling with the mean and standard; deviation of the dispersions for genes falling into a given bin for mean; expression of genes. This means that for each bin of mean expression, highly; variable genes are selected. Use `flavor='cell_ranger'` with care and in the same way as in; :func:`~scanpy.pp.recipe_zheng17`. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; flavor; Choose the flavor for computing normalized dispersion. If choosing; 'seurat', this expects non-logarithmized data – the logarithm of mean; and dispersion is taken internally when `log` is at its default value; `True`. For 'cell_ranger', this is usually called for logarithmized data; – in this case you should set `log` to `False`. In their default; workflows, Seurat passes the cutoffs whereas Cell Ranger passes; `n_top_genes`.; min_mean; max_mean; min_disp; max_disp; If `n_top_genes` unequals `None`, these cutoffs for the means and the; normalized dispersions are ignored.; n_bins; Number of bins for binning the mean gene expression. Normalization is; done with respect to each bin. If just a single gene falls into a bin,; the normalized dispersion is artificially set to ",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_deprecated/highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_deprecated/highly_variable_genes.py
Testability,log,logarithmized,"# noqa: PLR0917; """"""\; Extract highly variable genes :cite:p:`Satija2015,Zheng2017`. .. warning::; .. deprecated:: 1.3.6; Use :func:`~scanpy.pp.highly_variable_genes`; instead. The new function is equivalent to the present; function, except that. * the new function always expects logarithmized data; * `subset=False` in the new function, it suffices to; merely annotate the genes, tools like `pp.pca` will; detect the annotation; * you can now call: `sc.pl.highly_variable_genes(adata)`; * `copy` is replaced by `inplace`. If trying out parameters, pass the data matrix instead of AnnData. Depending on `flavor`, this reproduces the R-implementations of Seurat; :cite:p:`Satija2015` and Cell Ranger :cite:p:`Zheng2017`. The normalized dispersion is obtained by scaling with the mean and standard; deviation of the dispersions for genes falling into a given bin for mean; expression of genes. This means that for each bin of mean expression, highly; variable genes are selected. Use `flavor='cell_ranger'` with care and in the same way as in; :func:`~scanpy.pp.recipe_zheng17`. Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; flavor; Choose the flavor for computing normalized dispersion. If choosing; 'seurat', this expects non-logarithmized data – the logarithm of mean; and dispersion is taken internally when `log` is at its default value; `True`. For 'cell_ranger', this is usually called for logarithmized data; – in this case you should set `log` to `False`. In their default; workflows, Seurat passes the cutoffs whereas Cell Ranger passes; `n_top_genes`.; min_mean; max_mean; min_disp; max_disp; If `n_top_genes` unequals `None`, these cutoffs for the means and the; normalized dispersions are ignored.; n_bins; Number of bins for binning the mean gene expression. Normalization is; done with respect to each bin. If just a single gene falls into a bin,; the normalized dispersion is artificially set to ",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_deprecated/highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_deprecated/highly_variable_genes.py
Modifiability,variab,variable,"""""""\; Normalize each cell :cite:p:`Weinreb2017`. This is a deprecated version. See `normalize_per_cell` instead. Normalize each cell by UMI count, so that every cell has the same total; count. Parameters; ----------; X; Expression matrix. Rows correspond to cells and columns to genes.; max_fraction; Only use genes that make up more than max_fraction of the total; reads in every cell.; mult_with_mean; Multiply the result with the mean of total counts. Returns; -------; Normalized version of the original expression matrix.; """"""; """"""\; Z-score standardize each variable/gene in X :cite:p:`Weinreb2017`. Use `scale` instead. Parameters; ----------; X; Data matrix. Rows correspond to cells and columns to genes. Returns; -------; Z-score standardized version of the data matrix.; """"""",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_deprecated/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_deprecated/__init__.py
Availability,mask,mask,"; Initialize Scrublet object with counts matrix and doublet prediction parameters. Parameters; ----------; counts_obs; Matrix with shape (n_cells, n_genes) containing raw (unnormalized); UMI-based transcript counts.; Converted into a :class:`scipy.sparse.csc_matrix`. total_counts_obs; Array with shape (n_cells,) of total UMI counts per cell.; If `None`, this is calculated as the row sums of `counts_obs`. sim_doublet_ratio; Number of doublets to simulate relative to the number of observed; transcriptomes. n_neighbors; Number of neighbors used to construct the KNN graph of observed; transcriptomes and simulated doublets.; If `None`, this is set to round(0.5 * sqrt(n_cells)). expected_doublet_rate; The estimated doublet rate for the experiment. stdev_doublet_rate; Uncertainty in the expected doublet rate. random_state; Random state for doublet simulation, approximate; nearest neighbor search, and PCA/TruncatedSVD.; """"""; # init fields; # private fields; # Fields set by methods; """"""(shape: n_cells); Boolean mask of predicted doublets in the observed transcriptomes.; """"""; """"""(shape: n_cells); Doublet scores for observed transcriptomes.; """"""; """"""(shape: n_doublets); Doublet scores for simulated doublets.; """"""; """"""(shape: n_cells); Standard error in the doublet scores for observed transcriptomes.; """"""; """"""(shape: n_doublets); Standard error in the doublet scores for simulated doublets.; """"""; """"""Doublet score threshold for calling a transcriptome a doublet.""""""; """"""(shape: n_cells); Z-score conveying confidence in doublet calls.; Z = `(doublet_score_obs_ - threhsold_) / doublet_errors_obs_`; """"""; """"""Fraction of observed transcriptomes that have been called doublets.""""""; """"""Estimated fraction of doublets that are detectable, i.e.,; fraction of simulated doublets with doublet scores above `threshold_`; """"""; """"""Estimated overall doublet rate,; `detected_doublet_rate_ / detectable_doublet_fraction_`.; Should agree (roughly) with `expected_doublet_rate`.; """"""; """"""(shape: n_cells ×",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/core.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/core.py
Safety,predict,prediction,"# noqa: E731; # noqa: E731; """"""\; Initialize Scrublet object with counts matrix and doublet prediction parameters. Parameters; ----------; counts_obs; Matrix with shape (n_cells, n_genes) containing raw (unnormalized); UMI-based transcript counts.; Converted into a :class:`scipy.sparse.csc_matrix`. total_counts_obs; Array with shape (n_cells,) of total UMI counts per cell.; If `None`, this is calculated as the row sums of `counts_obs`. sim_doublet_ratio; Number of doublets to simulate relative to the number of observed; transcriptomes. n_neighbors; Number of neighbors used to construct the KNN graph of observed; transcriptomes and simulated doublets.; If `None`, this is set to round(0.5 * sqrt(n_cells)). expected_doublet_rate; The estimated doublet rate for the experiment. stdev_doublet_rate; Uncertainty in the expected doublet rate. random_state; Random state for doublet simulation, approximate; nearest neighbor search, and PCA/TruncatedSVD.; """"""; # init fields; # private fields; # Fields set by methods; """"""(shape: n_cells); Boolean mask of predicted doublets in the observed transcriptomes.; """"""; """"""(shape: n_cells); Doublet scores for observed transcriptomes.; """"""; """"""(shape: n_doublets); Doublet scores for simulated doublets.; """"""; """"""(shape: n_cells); Standard error in the doublet scores for observed transcriptomes.; """"""; """"""(shape: n_doublets); Standard error in the doublet scores for simulated doublets.; """"""; """"""Doublet score threshold for calling a transcriptome a doublet.""""""; """"""(shape: n_cells); Z-score conveying confidence in doublet calls.; Z = `(doublet_score_obs_ - threhsold_) / doublet_errors_obs_`; """"""; """"""Fraction of observed transcriptomes that have been called doublets.""""""; """"""Estimated fraction of doublets that are detectable, i.e.,; fraction of simulated doublets with doublet scores above `threshold_`; """"""; """"""Estimated overall doublet rate,; `detected_doublet_rate_ / detectable_doublet_fraction_`.; Should agree (roughly) with `expected_doublet_ra",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/core.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/core.py
Testability,log,log,"und using; the union of `manifold_obs` (see above) and `manifold_sim`. Sets; ----; manifold_obs_, manifold_sim_,; """"""; """"""\; Calculate doublet scores for observed transcriptomes and simulated doublets. Requires that manifold_obs_ and manifold_sim_ have already been set. Arguments; ---------; use_approx_neighbors; Use approximate nearest neighbor method (annoy) for the KNN; classifier. distance_metric; Distance metric used when finding nearest neighbors. For list of; valid values, see the documentation for annoy (if `use_approx_neighbors`; is True) or sklearn.neighbors.NearestNeighbors (if `use_approx_neighbors`; is False). get_doublet_neighbor_parents; If True, return the parent transcriptomes that generated the; doublet neighbors of each observed transcriptome. This information can; be used to infer the cell states that generated a given; doublet state. Sets; ----; doublet_scores_obs_, doublet_scores_sim_,; doublet_errors_obs_, doublet_errors_sim_,; doublet_neighbor_parents_; """"""; # Adjust k (number of nearest neighbors) based on the ratio of simulated to observed cells; # Find k_adj nearest neighbors; # Calculate doublet score based on ratio of simulated cell neighbors vs. observed cell neighbors; # Bayesian; # get parents of doublet neighbors, if requested; """"""\; Call trancriptomes as doublets or singlets. Arguments; ---------; threshold; Doublet score threshold for calling a transcriptome; a doublet. If `None`, this is set automatically by looking; for the minimum between the two modes of the `doublet_scores_sim_`; histogram. It is best practice to check the threshold visually; using the `doublet_scores_sim_` histogram and/or based on; co-localization of predicted doublets in a 2-D embedding. verbose; If True, log summary statistics. Sets; ----; predicted_doublets_, z_scores_, threshold_,; detected_doublet_rate_, detectable_doublet_fraction,; overall_doublet_rate_; """"""; # automatic threshold detection; # http://scikit-image.org/docs/dev/api/skimage.filters.html",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/core.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/core.py
Usability,simpl,simply,"the union of `manifold_obs_` and `manifold_sim_` (see below).; """"""; """"""shape (n_doublets × n_features); The single-cell ""manifold"" coordinates (e.g., PCA coordinates); for simulated doublets. Nearest neighbors are found using; the union of `manifold_obs_` (see above) and `manifold_sim_`.; """"""; """"""(shape: n_doublets × 2); Indices of the observed transcriptomes used to generate the; simulated doublets.; """"""; """"""(length: n_cells); A list of arrays of the indices of the doublet neighbors of; each observed transcriptome (the ith entry is an array of; the doublet neighbors of transcriptome i).; """"""; """"""Simulate doublets by adding the counts of random observed transcriptome pairs. Arguments; ---------; sim_doublet_ratio; Number of doublets to simulate relative to the number of observed; transcriptomes. If `None`, self.sim_doublet_ratio is used. synthetic_doublet_umi_subsampling; Rate for sampling UMIs when creating synthetic doublets.; If 1.0, each doublet is created by simply adding the UMIs from two randomly; sampled observed transcriptomes.; For values less than 1, the UMI counts are added and then randomly sampled; at the specified rate. Sets; ----; doublet_parents_; """"""; """"""\; Set the manifold coordinates used in k-nearest-neighbor graph construction. Arguments; ---------; manifold_obs; (shape: n_cells × n_features); The single-cell ""manifold"" coordinates (e.g., PCA coordinates); for observed transcriptomes. Nearest neighbors are found using; the union of `manifold_obs` and `manifold_sim` (see below). manifold_sim; (shape: n_doublets × n_features); The single-cell ""manifold"" coordinates (e.g., PCA coordinates); for simulated doublets. Nearest neighbors are found using; the union of `manifold_obs` (see above) and `manifold_sim`. Sets; ----; manifold_obs_, manifold_sim_,; """"""; """"""\; Calculate doublet scores for observed transcriptomes and simulated doublets. Requires that manifold_obs_ and manifold_sim_ have already been set. Arguments; ---------; use_approx_neighbors; ",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/core.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/core.py
Availability,down,downstream,"eters']``; Dictionary of Scrublet parameters; """"""; # Estimate n_neighbors if not provided, and create scrublet object.; # Note: Scrublet() will sparse adata_obs.X if it's not already, but this; # matrix won't get used if we pre-set the normalised slots.; # Ensure normalised matrix sparseness as Scrublet does; # https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100; # Call scrublet-specific preprocessing where specified; # Do PCA. Scrublet fits to the observed matrix and decomposes both observed; # and simulated based on that fit, so we'll just let it do its thing rather; # than trying to use Scanpy's PCA wrapper of the same functions.; # Score the doublets; # Actually call doublets; # Store results in AnnData for return; # Store doublet Scrublet metadata; # If threshold hasn't been located successfully then we couldn't make any; # predictions. The user will get a warning from Scrublet, but we need to; # set the boolean so that any downstream filtering on; # predicted_doublet=False doesn't incorrectly filter cells. The user can; # still use this object to generate the plot and derive a threshold; # manually.; """"""\; Simulate doublets by adding the counts of random observed transcriptome pairs. Parameters; ----------; adata; The annotated data matrix of shape ``n_obs`` × ``n_vars``. Rows; correspond to cells and columns to genes. Genes should have been; filtered for expression and variability, and the object should contain; raw expression of the same dimensions.; layer; Layer of adata where raw values are stored, or 'X' if values are in .X.; sim_doublet_ratio; Number of doublets to simulate relative to the number of observed; transcriptomes. If `None`, self.sim_doublet_ratio is used.; synthetic_doublet_umi_subsampling; Rate for sampling UMIs when creating synthetic doublets. If 1.0,; each doublet is created by simply adding the UMIs from two randomly; sampled observed transcriptomes. For values less than 1, the;",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/__init__.py
Deployability,update,updates,"transcriptomes prior; to k-nearest-neighbor graph construction.; use_approx_neighbors; Use approximate nearest neighbor method (annoy) for the KNN; classifier.; get_doublet_neighbor_parents; If True, return (in .uns) the parent transcriptomes that generated the; doublet neighbors of each observed transcriptome. This information can; be used to infer the cell states that generated a given doublet state.; n_neighbors; Number of neighbors used to construct the KNN graph of observed; transcriptomes and simulated doublets. If ``None``, this is; automatically set to ``np.round(0.5 * np.sqrt(n_obs))``.; threshold; Doublet score threshold for calling a transcriptome a doublet. If; `None`, this is set automatically by looking for the minimum between; the two modes of the `doublet_scores_sim_` histogram. It is best; practice to check the threshold visually using the; `doublet_scores_sim_` histogram and/or based on co-localization of; predicted doublets in a 2-D embedding.; verbose; If :data:`True`, log progress updates.; copy; If :data:`True`, return a copy of the input ``adata`` with Scrublet results; added. Otherwise, Scrublet results are added in place.; random_state; Initial state for doublet simulation and nearest neighbors. Returns; -------; if ``copy=True`` it returns or else adds fields to ``adata``. Those fields:. ``.obs['doublet_score']``; Doublet scores for each observed transcriptome. ``.obs['predicted_doublet']``; Boolean indicating predicted doublet status. ``.uns['scrublet']['doublet_scores_sim']``; Doublet scores for each simulated doublet transcriptome. ``.uns['scrublet']['doublet_parents']``; Pairs of ``.obs_names`` used to generate each simulated doublet; transcriptome. ``.uns['scrublet']['parameters']``; Dictionary of Scrublet parameters. See also; --------; :func:`~scanpy.pp.scrublet_simulate_doublets`: Run Scrublet's doublet; simulation separately for advanced usage.; :func:`~scanpy.pl.scrublet_score_distribution`: Plot histogram of doublet; scores for ob",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/__init__.py
Integrability,wrap,wrapper,"""""""\; Predict doublets using Scrublet :cite:p:`Wolock2019`. Predict cell doublets using a nearest-neighbor classifier of observed; transcriptomes and simulated doublets. Works best if the input is a raw; (unnormalized) counts matrix from a single sample or a collection of; similar samples from the same experiment.; This function is a wrapper around functions that pre-process using Scanpy; and directly call functions of Scrublet(). You may also undertake your own; preprocessing, simulate doublets with; :func:`~scanpy.pp.scrublet_simulate_doublets`, and run the core scrublet; function :func:`~scanpy.pp.scrublet` with ``adata_sim`` set. Parameters; ----------; adata; The annotated data matrix of shape ``n_obs`` × ``n_vars``. Rows; correspond to cells and columns to genes. Expected to be un-normalised; where adata_sim is not supplied, in which case doublets will be; simulated and pre-processing applied to both objects. If adata_sim is; supplied, this should be the observed transcriptomes processed; consistently (filtering, transform, normalisaton, hvg) with adata_sim.; adata_sim; (Advanced use case) Optional annData object generated by; :func:`~scanpy.pp.scrublet_simulate_doublets`, with same number of vars; as adata. This should have been built from adata_obs after; filtering genes and cells and selcting highly-variable genes.; batch_key; Optional :attr:`~anndata.AnnData.obs` column name discriminating between batches.; sim_doublet_ratio; Number of doublets to simulate relative to the number of observed; transcriptomes.; expected_doublet_rate; Where adata_sim not suplied, the estimated doublet rate for the; experiment.; stdev_doublet_rate; Where adata_sim not suplied, uncertainty in the expected doublet rate.; synthetic_doublet_umi_subsampling; Where adata_sim not suplied, rate for sampling UMIs when creating; synthetic doublets. If 1.0, each doublet is created by simply adding; the UMI counts from two randomly sampled observed transcriptomes. For; values less than 1, t",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/__init__.py
Modifiability,variab,variable," from the same experiment.; This function is a wrapper around functions that pre-process using Scanpy; and directly call functions of Scrublet(). You may also undertake your own; preprocessing, simulate doublets with; :func:`~scanpy.pp.scrublet_simulate_doublets`, and run the core scrublet; function :func:`~scanpy.pp.scrublet` with ``adata_sim`` set. Parameters; ----------; adata; The annotated data matrix of shape ``n_obs`` × ``n_vars``. Rows; correspond to cells and columns to genes. Expected to be un-normalised; where adata_sim is not supplied, in which case doublets will be; simulated and pre-processing applied to both objects. If adata_sim is; supplied, this should be the observed transcriptomes processed; consistently (filtering, transform, normalisaton, hvg) with adata_sim.; adata_sim; (Advanced use case) Optional annData object generated by; :func:`~scanpy.pp.scrublet_simulate_doublets`, with same number of vars; as adata. This should have been built from adata_obs after; filtering genes and cells and selcting highly-variable genes.; batch_key; Optional :attr:`~anndata.AnnData.obs` column name discriminating between batches.; sim_doublet_ratio; Number of doublets to simulate relative to the number of observed; transcriptomes.; expected_doublet_rate; Where adata_sim not suplied, the estimated doublet rate for the; experiment.; stdev_doublet_rate; Where adata_sim not suplied, uncertainty in the expected doublet rate.; synthetic_doublet_umi_subsampling; Where adata_sim not suplied, rate for sampling UMIs when creating; synthetic doublets. If 1.0, each doublet is created by simply adding; the UMI counts from two randomly sampled observed transcriptomes. For; values less than 1, the UMI counts are added and then randomly sampled; at the specified rate.; knn_dist_metric; Distance metric used when finding nearest neighbors. For list of; valid values, see the documentation for annoy (if `use_approx_neighbors`; is True) or sklearn.neighbors.NearestNeighbors (if `use_",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/__init__.py
Safety,predict,predicted,"` will be used for dimensionality; reduction.; n_prin_comps; Number of principal components used to embed the transcriptomes prior; to k-nearest-neighbor graph construction.; use_approx_neighbors; Use approximate nearest neighbor method (annoy) for the KNN; classifier.; get_doublet_neighbor_parents; If True, return (in .uns) the parent transcriptomes that generated the; doublet neighbors of each observed transcriptome. This information can; be used to infer the cell states that generated a given doublet state.; n_neighbors; Number of neighbors used to construct the KNN graph of observed; transcriptomes and simulated doublets. If ``None``, this is; automatically set to ``np.round(0.5 * np.sqrt(n_obs))``.; threshold; Doublet score threshold for calling a transcriptome a doublet. If; `None`, this is set automatically by looking for the minimum between; the two modes of the `doublet_scores_sim_` histogram. It is best; practice to check the threshold visually using the; `doublet_scores_sim_` histogram and/or based on co-localization of; predicted doublets in a 2-D embedding.; verbose; If :data:`True`, log progress updates.; copy; If :data:`True`, return a copy of the input ``adata`` with Scrublet results; added. Otherwise, Scrublet results are added in place.; random_state; Initial state for doublet simulation and nearest neighbors. Returns; -------; if ``copy=True`` it returns or else adds fields to ``adata``. Those fields:. ``.obs['doublet_score']``; Doublet scores for each observed transcriptome. ``.obs['predicted_doublet']``; Boolean indicating predicted doublet status. ``.uns['scrublet']['doublet_scores_sim']``; Doublet scores for each simulated doublet transcriptome. ``.uns['scrublet']['doublet_parents']``; Pairs of ``.obs_names`` used to generate each simulated doublet; transcriptome. ``.uns['scrublet']['parameters']``; Dictionary of Scrublet parameters. See also; --------; :func:`~scanpy.pp.scrublet_simulate_doublets`: Run Scrublet's doublet; simulation separately",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/__init__.py
Testability,log,log-transform,"estimated doublet rate for the; experiment.; stdev_doublet_rate; Where adata_sim not suplied, uncertainty in the expected doublet rate.; synthetic_doublet_umi_subsampling; Where adata_sim not suplied, rate for sampling UMIs when creating; synthetic doublets. If 1.0, each doublet is created by simply adding; the UMI counts from two randomly sampled observed transcriptomes. For; values less than 1, the UMI counts are added and then randomly sampled; at the specified rate.; knn_dist_metric; Distance metric used when finding nearest neighbors. For list of; valid values, see the documentation for annoy (if `use_approx_neighbors`; is True) or sklearn.neighbors.NearestNeighbors (if `use_approx_neighbors`; is False).; normalize_variance; If True, normalize the data such that each gene has a variance of 1.; :class:`sklearn.decomposition.TruncatedSVD` will be used for dimensionality; reduction, unless `mean_center` is True.; log_transform; Whether to use :func:`~scanpy.pp.log1p` to log-transform the data; prior to PCA.; mean_center; If True, center the data such that each gene has a mean of 0.; :class:`sklearn.decomposition.PCA` will be used for dimensionality; reduction.; n_prin_comps; Number of principal components used to embed the transcriptomes prior; to k-nearest-neighbor graph construction.; use_approx_neighbors; Use approximate nearest neighbor method (annoy) for the KNN; classifier.; get_doublet_neighbor_parents; If True, return (in .uns) the parent transcriptomes that generated the; doublet neighbors of each observed transcriptome. This information can; be used to infer the cell states that generated a given doublet state.; n_neighbors; Number of neighbors used to construct the KNN graph of observed; transcriptomes and simulated doublets. If ``None``, this is; automatically set to ``np.round(0.5 * np.sqrt(n_obs))``.; threshold; Doublet score threshold for calling a transcriptome a doublet. If; `None`, this is set automatically by looking for the minimum between; the ",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/__init__.py
Usability,simpl,simply," objects. If adata_sim is; supplied, this should be the observed transcriptomes processed; consistently (filtering, transform, normalisaton, hvg) with adata_sim.; adata_sim; (Advanced use case) Optional annData object generated by; :func:`~scanpy.pp.scrublet_simulate_doublets`, with same number of vars; as adata. This should have been built from adata_obs after; filtering genes and cells and selcting highly-variable genes.; batch_key; Optional :attr:`~anndata.AnnData.obs` column name discriminating between batches.; sim_doublet_ratio; Number of doublets to simulate relative to the number of observed; transcriptomes.; expected_doublet_rate; Where adata_sim not suplied, the estimated doublet rate for the; experiment.; stdev_doublet_rate; Where adata_sim not suplied, uncertainty in the expected doublet rate.; synthetic_doublet_umi_subsampling; Where adata_sim not suplied, rate for sampling UMIs when creating; synthetic doublets. If 1.0, each doublet is created by simply adding; the UMI counts from two randomly sampled observed transcriptomes. For; values less than 1, the UMI counts are added and then randomly sampled; at the specified rate.; knn_dist_metric; Distance metric used when finding nearest neighbors. For list of; valid values, see the documentation for annoy (if `use_approx_neighbors`; is True) or sklearn.neighbors.NearestNeighbors (if `use_approx_neighbors`; is False).; normalize_variance; If True, normalize the data such that each gene has a variance of 1.; :class:`sklearn.decomposition.TruncatedSVD` will be used for dimensionality; reduction, unless `mean_center` is True.; log_transform; Whether to use :func:`~scanpy.pp.log1p` to log-transform the data; prior to PCA.; mean_center; If True, center the data such that each gene has a mean of 0.; :class:`sklearn.decomposition.PCA` will be used for dimensionality; reduction.; n_prin_comps; Number of principal components used to embed the transcriptomes prior; to k-nearest-neighbor graph construction.; use_appro",MatchSource.CODE_COMMENT,src/scanpy/preprocessing/_scrublet/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/preprocessing/_scrublet/__init__.py
Integrability,interface,interface,"""""""\; org; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.\; """"""; """"""\; host; A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org"").\; """"""; """"""\; use_cache; Whether pybiomart should use a cache for requests. Will create a; `.pybiomart.sqlite` file in current directory if used.\; """"""; """"""\; A simple interface to biomart. Params; ------; {doc_org}; attrs; What you want returned.; filters; What you want to pick out.; {doc_host}; {doc_use_cache}; """"""; """"""\; Retrieve gene annotations from ensembl biomart. Parameters; ----------; {doc_org}; attrs; Attributes to query biomart for.; {doc_host}; {doc_use_cache}. Returns; -------; Dataframe containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> import scanpy as sc; >>> annot = sc.queries.biomart_annotations(; ... ""hsapiens"",; ... [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ... ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; """"""; """"""\; Retrieve gene coordinates for specific organism through BioMart. Parameters; ----------; {doc_org}; gene_name; The gene symbol (e.g. ""hgnc_symbol"" for human) for which to retrieve; coordinates.; gene_attr; The biomart attribute the gene symbol should show up for.; chr_exclude; A list of chromosomes to exclude from query.; {doc_host}; {doc_use_cache}. Returns; -------; Dataframe containing gene coordinates for the specified gene symbol. Examples; --------; >>> import scanpy as sc; >>> sc.queries.gene_coordinates(""hsapiens"", ""MT-TF""); """"""; """"""\; Mitochondrial gene symbols for specific organism through BioMart. Parameters; ----------; {doc_org}; attrname; Biomart attribute field to return. Possible values include; ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",; and ""zfin_id_symbol"".; {doc_host}; {doc_use_cache}; chromosome; Mitochrondrial chromosome ",MatchSource.CODE_COMMENT,src/scanpy/queries/_queries.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/queries/_queries.py
Performance,cache,cache,"""""""\; org; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.\; """"""; """"""\; host; A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org"").\; """"""; """"""\; use_cache; Whether pybiomart should use a cache for requests. Will create a; `.pybiomart.sqlite` file in current directory if used.\; """"""; """"""\; A simple interface to biomart. Params; ------; {doc_org}; attrs; What you want returned.; filters; What you want to pick out.; {doc_host}; {doc_use_cache}; """"""; """"""\; Retrieve gene annotations from ensembl biomart. Parameters; ----------; {doc_org}; attrs; Attributes to query biomart for.; {doc_host}; {doc_use_cache}. Returns; -------; Dataframe containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> import scanpy as sc; >>> annot = sc.queries.biomart_annotations(; ... ""hsapiens"",; ... [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ... ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; """"""; """"""\; Retrieve gene coordinates for specific organism through BioMart. Parameters; ----------; {doc_org}; gene_name; The gene symbol (e.g. ""hgnc_symbol"" for human) for which to retrieve; coordinates.; gene_attr; The biomart attribute the gene symbol should show up for.; chr_exclude; A list of chromosomes to exclude from query.; {doc_host}; {doc_use_cache}. Returns; -------; Dataframe containing gene coordinates for the specified gene symbol. Examples; --------; >>> import scanpy as sc; >>> sc.queries.gene_coordinates(""hsapiens"", ""MT-TF""); """"""; """"""\; Mitochondrial gene symbols for specific organism through BioMart. Parameters; ----------; {doc_org}; attrname; Biomart attribute field to return. Possible values include; ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",; and ""zfin_id_symbol"".; {doc_host}; {doc_use_cache}; chromosome; Mitochrondrial chromosome ",MatchSource.CODE_COMMENT,src/scanpy/queries/_queries.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/queries/_queries.py
Usability,simpl,simple,"""""""\; org; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.\; """"""; """"""\; host; A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org"").\; """"""; """"""\; use_cache; Whether pybiomart should use a cache for requests. Will create a; `.pybiomart.sqlite` file in current directory if used.\; """"""; """"""\; A simple interface to biomart. Params; ------; {doc_org}; attrs; What you want returned.; filters; What you want to pick out.; {doc_host}; {doc_use_cache}; """"""; """"""\; Retrieve gene annotations from ensembl biomart. Parameters; ----------; {doc_org}; attrs; Attributes to query biomart for.; {doc_host}; {doc_use_cache}. Returns; -------; Dataframe containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> import scanpy as sc; >>> annot = sc.queries.biomart_annotations(; ... ""hsapiens"",; ... [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ... ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; """"""; """"""\; Retrieve gene coordinates for specific organism through BioMart. Parameters; ----------; {doc_org}; gene_name; The gene symbol (e.g. ""hgnc_symbol"" for human) for which to retrieve; coordinates.; gene_attr; The biomart attribute the gene symbol should show up for.; chr_exclude; A list of chromosomes to exclude from query.; {doc_host}; {doc_use_cache}. Returns; -------; Dataframe containing gene coordinates for the specified gene symbol. Examples; --------; >>> import scanpy as sc; >>> sc.queries.gene_coordinates(""hsapiens"", ""MT-TF""); """"""; """"""\; Mitochondrial gene symbols for specific organism through BioMart. Parameters; ----------; {doc_org}; attrname; Biomart attribute field to return. Possible values include; ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",; and ""zfin_id_symbol"".; {doc_host}; {doc_use_cache}; chromosome; Mitochrondrial chromosome ",MatchSource.CODE_COMMENT,src/scanpy/queries/_queries.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/queries/_queries.py
Availability,avail,available,"""""""; Computes a dendrogram based on a given categorical observation.; """"""; """"""\; Computes a hierarchical clustering for the given `groupby` categories. By default, the PCA representation is used unless `.X`; has less than 50 variables. Alternatively, a list of `var_names` (e.g. genes) can be given. Average values of either `var_names` or components are used; to compute a correlation matrix. The hierarchical clustering can be visualized using; :func:`scanpy.pl.dendrogram` or multiple other visualizations that can; include a dendrogram: :func:`~scanpy.pl.matrixplot`,; :func:`~scanpy.pl.heatmap`, :func:`~scanpy.pl.dotplot`,; and :func:`~scanpy.pl.stacked_violin`. .. note::; The computation of the hierarchical clustering is based on predefined; groups and not per cell. The correlation matrix is computed using by; default pearson but other methods are available. Parameters; ----------; adata; Annotated data matrix; {n_pcs}; {use_rep}; var_names; List of var_names to use for computing the hierarchical clustering.; If `var_names` is given, then `use_rep` and `n_pcs` is ignored.; use_raw; Only when `var_names` is not None.; Use `raw` attribute of `adata` if present.; cor_method; correlation method to use.; Options are 'pearson', 'kendall', and 'spearman'; linkage_method; linkage method to use. See :func:`scipy.cluster.hierarchy.linkage`; for more information.; optimal_ordering; Same as the optimal_ordering argument of :func:`scipy.cluster.hierarchy.linkage`; which reorders the linkage matrix so that the distance between successive; leaves is minimal.; key_added; By default, the dendrogram information is added to; `.uns[f'dendrogram_{{groupby}}']`.; Notice that the `groupby` information is added to the dendrogram.; inplace; If `True`, adds dendrogram information to `adata.uns[key_added]`,; else this function returns the information. Returns; -------; Returns `None` if `inplace=True`, else returns a `dict` with dendrogram information. Sets the following field if `inplace=True`",MatchSource.CODE_COMMENT,src/scanpy/tools/_dendrogram.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_dendrogram.py
Modifiability,variab,variables,"""""""; Computes a dendrogram based on a given categorical observation.; """"""; """"""\; Computes a hierarchical clustering for the given `groupby` categories. By default, the PCA representation is used unless `.X`; has less than 50 variables. Alternatively, a list of `var_names` (e.g. genes) can be given. Average values of either `var_names` or components are used; to compute a correlation matrix. The hierarchical clustering can be visualized using; :func:`scanpy.pl.dendrogram` or multiple other visualizations that can; include a dendrogram: :func:`~scanpy.pl.matrixplot`,; :func:`~scanpy.pl.heatmap`, :func:`~scanpy.pl.dotplot`,; and :func:`~scanpy.pl.stacked_violin`. .. note::; The computation of the hierarchical clustering is based on predefined; groups and not per cell. The correlation matrix is computed using by; default pearson but other methods are available. Parameters; ----------; adata; Annotated data matrix; {n_pcs}; {use_rep}; var_names; List of var_names to use for computing the hierarchical clustering.; If `var_names` is given, then `use_rep` and `n_pcs` is ignored.; use_raw; Only when `var_names` is not None.; Use `raw` attribute of `adata` if present.; cor_method; correlation method to use.; Options are 'pearson', 'kendall', and 'spearman'; linkage_method; linkage method to use. See :func:`scipy.cluster.hierarchy.linkage`; for more information.; optimal_ordering; Same as the optimal_ordering argument of :func:`scipy.cluster.hierarchy.linkage`; which reorders the linkage matrix so that the distance between successive; leaves is minimal.; key_added; By default, the dendrogram information is added to; `.uns[f'dendrogram_{{groupby}}']`.; Notice that the `groupby` information is added to the dendrogram.; inplace; If `True`, adds dendrogram information to `adata.uns[key_added]`,; else this function returns the information. Returns; -------; Returns `None` if `inplace=True`, else returns a `dict` with dendrogram information. Sets the following field if `inplace=True`",MatchSource.CODE_COMMENT,src/scanpy/tools/_dendrogram.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_dendrogram.py
Energy Efficiency,adapt,adapted,"""""""\; Diffusion Maps :cite:p:`Coifman2005,Haghverdi2015,Wolf2018`. Diffusion maps :cite:p:`Coifman2005` has been proposed for visualizing single-cell; data by :cite:t:`Haghverdi2015`. The tool uses the adapted Gaussian kernel suggested; by :cite:t:`Haghverdi2016` in the implementation of :cite:t:`Wolf2018`. The width (""sigma"") of the connectivity kernel is implicitly determined by; the number of neighbors used to compute the single-cell graph in; :func:`~scanpy.pp.neighbors`. To reproduce the original implementation; using a Gaussian kernel, use `method=='gauss'` in; :func:`~scanpy.pp.neighbors`. To use an exponential kernel, use the default; `method=='umap'`. Differences between these options shouldn't usually be; dramatic. Parameters; ----------; adata; Annotated data matrix.; n_comps; The number of dimensions of the representation.; neighbors_key; If not specified, diffmap looks .uns['neighbors'] for neighbors settings; and .obsp['connectivities'], .obsp['distances'] for connectivities and; distances respectively (default storage places for pp.neighbors).; If specified, diffmap looks .uns[neighbors_key] for neighbors settings and; .obsp[.uns[neighbors_key]['connectivities_key']],; .obsp[.uns[neighbors_key]['distances_key']] for connectivities and distances; respectively.; random_state; A numpy random seed; copy; Return a copy instead of writing to adata. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:. `adata.obsm['X_diffmap']` : :class:`numpy.ndarray` (dtype `float`); Diffusion map representation of data, which is the right eigen basis of; the transition matrix with eigenvectors as columns. `adata.uns['diffmap_evals']` : :class:`numpy.ndarray` (dtype `float`); Array of size (number of eigen vectors).; Eigenvalues of transition matrix. Notes; -----; The 0-th column in `adata.obsm[""X_diffmap""]` is the steady-state solution,; which is non-informative in diffusion maps.; Therefore, the first diffusion com",MatchSource.CODE_COMMENT,src/scanpy/tools/_diffmap.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_diffmap.py
Modifiability,adapt,adapted,"""""""\; Diffusion Maps :cite:p:`Coifman2005,Haghverdi2015,Wolf2018`. Diffusion maps :cite:p:`Coifman2005` has been proposed for visualizing single-cell; data by :cite:t:`Haghverdi2015`. The tool uses the adapted Gaussian kernel suggested; by :cite:t:`Haghverdi2016` in the implementation of :cite:t:`Wolf2018`. The width (""sigma"") of the connectivity kernel is implicitly determined by; the number of neighbors used to compute the single-cell graph in; :func:`~scanpy.pp.neighbors`. To reproduce the original implementation; using a Gaussian kernel, use `method=='gauss'` in; :func:`~scanpy.pp.neighbors`. To use an exponential kernel, use the default; `method=='umap'`. Differences between these options shouldn't usually be; dramatic. Parameters; ----------; adata; Annotated data matrix.; n_comps; The number of dimensions of the representation.; neighbors_key; If not specified, diffmap looks .uns['neighbors'] for neighbors settings; and .obsp['connectivities'], .obsp['distances'] for connectivities and; distances respectively (default storage places for pp.neighbors).; If specified, diffmap looks .uns[neighbors_key] for neighbors settings and; .obsp[.uns[neighbors_key]['connectivities_key']],; .obsp[.uns[neighbors_key]['distances_key']] for connectivities and distances; respectively.; random_state; A numpy random seed; copy; Return a copy instead of writing to adata. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:. `adata.obsm['X_diffmap']` : :class:`numpy.ndarray` (dtype `float`); Diffusion map representation of data, which is the right eigen basis of; the transition matrix with eigenvectors as columns. `adata.uns['diffmap_evals']` : :class:`numpy.ndarray` (dtype `float`); Array of size (number of eigen vectors).; Eigenvalues of transition matrix. Notes; -----; The 0-th column in `adata.obsm[""X_diffmap""]` is the steady-state solution,; which is non-informative in diffusion maps.; Therefore, the first diffusion com",MatchSource.CODE_COMMENT,src/scanpy/tools/_diffmap.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_diffmap.py
Availability,error,errors,"]['connectivities_key']],; .obsp[.uns[neighbors_key]['distances_key']] for connectivities and distances; respectively.; copy; Copy instance before computation and return a copy.; Otherwise, perform computation inplace and return `None`. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields (If `n_branchings==0`, no field `adata.obs['dpt_groups']` will be written):. `adata.obs['dpt_pseudotime']` : :class:`pandas.Series` (dtype `float`); Array of dim (number of samples) that stores the pseudotime of each; cell, that is, the DPT distance with respect to the root cell.; `adata.obs['dpt_groups']` : :class:`pandas.Series` (dtype `category`); Array of dim (number of samples) that stores the subgroup id ('0',; '1', ...) for each cell. The groups typically correspond to; 'progenitor cells', 'undecided cells' or 'branches' of a process. Notes; -----; The tool is similar to the R package `destiny` of :cite:t:`Angerer2015`.; """"""; # standard errors, warnings etc.; # start with the actual computation; # pseudotimes are distances from root point; # update iroot, might have changed when subsampling, for example; # detect branchings and partition the data into segments; # the ""change points"" separate segments in the ordering above; # the tip points of segments; # the ordering according to segments and pseudotime; """"""\; Hierarchical Diffusion Pseudotime.; """"""; # just for debugging purposes; """"""\; Detect branchings and partition the data into corresponding segments. Detect all branchings up to `n_branchings`. Writes; ------; segs : :class:`~numpy.ndarray`; Array of dimension (number of segments) × (number of data; points). Each row stores a mask array that defines a segment.; segs_tips : :class:`~numpy.ndarray`; Array of dimension (number of segments) × 2. Each row stores the; indices of the two tip points of each segment.; segs_names : :class:`~numpy.ndarray`; Array of dimension (number of data points). Stores an integer label; f",MatchSource.CODE_COMMENT,src/scanpy/tools/_dpt.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_dpt.py
Deployability,update,update," and return `None`. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields (If `n_branchings==0`, no field `adata.obs['dpt_groups']` will be written):. `adata.obs['dpt_pseudotime']` : :class:`pandas.Series` (dtype `float`); Array of dim (number of samples) that stores the pseudotime of each; cell, that is, the DPT distance with respect to the root cell.; `adata.obs['dpt_groups']` : :class:`pandas.Series` (dtype `category`); Array of dim (number of samples) that stores the subgroup id ('0',; '1', ...) for each cell. The groups typically correspond to; 'progenitor cells', 'undecided cells' or 'branches' of a process. Notes; -----; The tool is similar to the R package `destiny` of :cite:t:`Angerer2015`.; """"""; # standard errors, warnings etc.; # start with the actual computation; # pseudotimes are distances from root point; # update iroot, might have changed when subsampling, for example; # detect branchings and partition the data into segments; # the ""change points"" separate segments in the ordering above; # the tip points of segments; # the ordering according to segments and pseudotime; """"""\; Hierarchical Diffusion Pseudotime.; """"""; # just for debugging purposes; """"""\; Detect branchings and partition the data into corresponding segments. Detect all branchings up to `n_branchings`. Writes; ------; segs : :class:`~numpy.ndarray`; Array of dimension (number of segments) × (number of data; points). Each row stores a mask array that defines a segment.; segs_tips : :class:`~numpy.ndarray`; Array of dimension (number of segments) × 2. Each row stores the; indices of the two tip points of each segment.; segs_names : :class:`~numpy.ndarray`; Array of dimension (number of data points). Stores an integer label; for each segment.; """"""; """"""\; Detect all branchings up to `n_branchings`. Writes Attributes; -----------------; segs : :class:`~numpy.ndarray`; List of integer index arrays.; segs_tips : :class:`~numpy.ndarray`; List ",MatchSource.CODE_COMMENT,src/scanpy/tools/_dpt.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_dpt.py
Modifiability,variab,variable,"ints that have maximal; # distance in the segment, the ""tips"" of the segment; #; # the rest of the points in the segment is then defined by demanding; # them to ""be close to the line segment that connects the tips"", that; # is, for such a point, the normalized added distance to both tips is; # smaller than one:; # (D[tips[0],i] + D[tips[1],i])/D[tips[0],tips[1] < 1; # of course, this condition is fulfilled by the full cylindrical; # subspace surrounding that line segment, where the radius of the; # cylinder can be infinite; #; # if D denotes a euclidian distance matrix, a line segment is a linear; # object, and the name ""line"" is justified. if we take the; # diffusion-based distance matrix Dchosen, which approximates geodesic; # distance, with ""line"", we mean the shortest path between two points,; # which can be highly non-linear in the original space; #; # let us define the tips of the whole data set; # this is safe, but not compatible with on-the-fly computation; # we keep a list of the tips of each segment; # [third start end]; # detect branching and update segs and segs_tips; # store as class members; # the following is a bit too much, but this allows easy storage; # noqa: F841 TODO Evaluate whether to assign the variable or not; # exclude the first point, the segment itself; # update adjacency matrix within the loop!; # self.segs_adjacency[iseg, neighbor_segs > 0] = 0; # self.segs_adjacency[iseg, closest_segs] = np.array(distance_segs)[closest_segs]; # self.segs_adjacency[neighbor_segs > 0, iseg] = 0; # self.segs_adjacency[closest_segs, iseg] = np.array(distance_segs)[closest_segs].reshape(len(closest_segs), 1); # n_edges_per_seg = np.sum(self.segs_adjacency > 0, axis=1).A1; # print(self.segs_adjacency); # self.segs_adjacency.eliminate_zeros(); """"""\; Out of a list of line segments, choose segment that has the most; distant second data point. Assume the distance matrix Ddiff is sorted according to seg_idcs.; Compute all the distances. Returns; -------; iseg; Ind",MatchSource.CODE_COMMENT,src/scanpy/tools/_dpt.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_dpt.py
Performance,perform,perform,"chings to detect.; min_group_size; During recursive splitting of branches ('dpt groups') for `n_branchings`; > 1, do not consider groups that contain less than `min_group_size` data; points. If a float, `min_group_size` refers to a fraction of the total; number of data points.; allow_kendall_tau_shift; If a very small branch is detected upon splitting, shift away from; maximum correlation in Kendall tau criterion of :cite:t:`Haghverdi2016` to; stabilize the splitting.; neighbors_key; If not specified, dpt looks .uns['neighbors'] for neighbors settings; and .obsp['connectivities'], .obsp['distances'] for connectivities and; distances respectively (default storage places for pp.neighbors).; If specified, dpt looks .uns[neighbors_key] for neighbors settings and; .obsp[.uns[neighbors_key]['connectivities_key']],; .obsp[.uns[neighbors_key]['distances_key']] for connectivities and distances; respectively.; copy; Copy instance before computation and return a copy.; Otherwise, perform computation inplace and return `None`. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields (If `n_branchings==0`, no field `adata.obs['dpt_groups']` will be written):. `adata.obs['dpt_pseudotime']` : :class:`pandas.Series` (dtype `float`); Array of dim (number of samples) that stores the pseudotime of each; cell, that is, the DPT distance with respect to the root cell.; `adata.obs['dpt_groups']` : :class:`pandas.Series` (dtype `category`); Array of dim (number of samples) that stores the subgroup id ('0',; '1', ...) for each cell. The groups typically correspond to; 'progenitor cells', 'undecided cells' or 'branches' of a process. Notes; -----; The tool is similar to the R package `destiny` of :cite:t:`Angerer2015`.; """"""; # standard errors, warnings etc.; # start with the actual computation; # pseudotimes are distances from root point; # update iroot, might have changed when subsampling, for example; # detect branchings and partition the",MatchSource.CODE_COMMENT,src/scanpy/tools/_dpt.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_dpt.py
Safety,detect,detect,"""""""\; Infer progression of cells through geodesic distance along the graph; :cite:p:`Haghverdi2016,Wolf2019`. Reconstruct the progression of a biological process from snapshot; data. `Diffusion Pseudotime` has been introduced by :cite:t:`Haghverdi2016` and; implemented within Scanpy :cite:p:`Wolf2018`. Here, we use a further developed; version, which is able to deal with disconnected graphs :cite:p:`Wolf2019` and can; be run in a `hierarchical` mode by setting the parameter; `n_branchings>1`. We recommend, however, to only use; :func:`~scanpy.tl.dpt` for computing pseudotime (`n_branchings=0`) and; to detect branchings via :func:`~scanpy.tl.paga`. For pseudotime, you need; to annotate your data with a root cell. For instance::. adata.uns['iroot'] = np.flatnonzero(adata.obs['cell_types'] == 'Stem')[0]. This requires to run :func:`~scanpy.pp.neighbors`, first. In order to; reproduce the original implementation of DPT, use `method=='gauss'` in; this. Using the default `method=='umap'` only leads to minor quantitative; differences, though. .. versionadded:: 1.1. :func:`~scanpy.tl.dpt` also requires to run; :func:`~scanpy.tl.diffmap` first. As previously,; :func:`~scanpy.tl.dpt` came with a default parameter of ``n_dcs=10`` but; :func:`~scanpy.tl.diffmap` has a default parameter of ``n_comps=15``,; you need to pass ``n_comps=10`` in :func:`~scanpy.tl.diffmap` in order; to exactly reproduce previous :func:`~scanpy.tl.dpt` results. Parameters; ----------; adata; Annotated data matrix.; n_dcs; The number of diffusion components to use.; n_branchings; Number of branchings to detect.; min_group_size; During recursive splitting of branches ('dpt groups') for `n_branchings`; > 1, do not consider groups that contain less than `min_group_size` data; points. If a float, `min_group_size` refers to a fraction of the total; number of data points.; allow_kendall_tau_shift; If a very small branch is detected upon splitting, shift away from; maximum correlation in Kendall tau criterion o",MatchSource.CODE_COMMENT,src/scanpy/tools/_dpt.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_dpt.py
Testability,log,logg,"ssign the variable or not; # exclude the first point, the segment itself; # update adjacency matrix within the loop!; # self.segs_adjacency[iseg, neighbor_segs > 0] = 0; # self.segs_adjacency[iseg, closest_segs] = np.array(distance_segs)[closest_segs]; # self.segs_adjacency[neighbor_segs > 0, iseg] = 0; # self.segs_adjacency[closest_segs, iseg] = np.array(distance_segs)[closest_segs].reshape(len(closest_segs), 1); # n_edges_per_seg = np.sum(self.segs_adjacency > 0, axis=1).A1; # print(self.segs_adjacency); # self.segs_adjacency.eliminate_zeros(); """"""\; Out of a list of line segments, choose segment that has the most; distant second data point. Assume the distance matrix Ddiff is sorted according to seg_idcs.; Compute all the distances. Returns; -------; iseg; Index identifying the position within the list of line segments.; tips3; Positions of tips within chosen segment.; """"""; # do not consider too small segments; # restrict distance matrix to points in segment; # check that none of our tips ""connects"" with a tip of the; # other segments; # take the inner tip, the ""second tip"" of the segment; # logg.debug(; # ' group', iseg, 'with tip', segs_tips[iseg][itip],; # 'connects with', jseg, 'with tip', segs_tips[jseg][1],; # ); # logg.debug(' do not use the tip for ""triangulation""'); # map the global position to the position within the segment; # find the third point on the segment that has maximal; # added distance from the two tip points; # add this point to tips, it's a third tip, we store it at the first; # position in an array called tips3; # find a fourth point that has maximal distance to all three; # compute the score as ratio of the added distance to the third tip,; # to what it would be if it were on the straight line between the; # two first tips, given by Dseg[tips[:2]]; # if we did not normalize, there would be a danger of simply; # assigning the highest score to the longest segment; # simply the number of points; # write result; """"""Convert the format of the ",MatchSource.CODE_COMMENT,src/scanpy/tools/_dpt.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_dpt.py
Usability,simpl,simply,"cond data point. Assume the distance matrix Ddiff is sorted according to seg_idcs.; Compute all the distances. Returns; -------; iseg; Index identifying the position within the list of line segments.; tips3; Positions of tips within chosen segment.; """"""; # do not consider too small segments; # restrict distance matrix to points in segment; # check that none of our tips ""connects"" with a tip of the; # other segments; # take the inner tip, the ""second tip"" of the segment; # logg.debug(; # ' group', iseg, 'with tip', segs_tips[iseg][itip],; # 'connects with', jseg, 'with tip', segs_tips[jseg][1],; # ); # logg.debug(' do not use the tip for ""triangulation""'); # map the global position to the position within the segment; # find the third point on the segment that has maximal; # added distance from the two tip points; # add this point to tips, it's a third tip, we store it at the first; # position in an array called tips3; # find a fourth point that has maximal distance to all three; # compute the score as ratio of the added distance to the third tip,; # to what it would be if it were on the straight line between the; # two first tips, given by Dseg[tips[:2]]; # if we did not normalize, there would be a danger of simply; # assigning the highest score to the longest segment; # simply the number of points; # write result; """"""Convert the format of the segment class members.""""""; # make segs a list of mask arrays, it's easier to store; # as there is a hdf5 equivalent; # convert to arrays; """"""Return a single array that stores integer segment labels.""""""; """"""\; Define indices that reflect segment and pseudotime order. Writes; ------; indices : :class:`~numpy.ndarray`; Index array of shape n, which stores an ordering of the data points; with respect to increasing segment index and increasing pseudotime.; changepoints : :class:`~numpy.ndarray`; Index array of shape len(ssegs)-1, which stores the indices of; points where the segment index changes, with respect to the ordering; of ind",MatchSource.CODE_COMMENT,src/scanpy/tools/_dpt.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_dpt.py
Availability,avail,available,"""""""\; Force-directed graph drawing :cite:p:`Islam2011,Jacomy2014,Chippada2018`. An alternative to tSNE that often preserves the topology of the data; better. This requires to run :func:`~scanpy.pp.neighbors`, first. The default layout ('fa', `ForceAtlas2`, :cite:t:`Jacomy2014`) uses the package |fa2|_; :cite:p:`Chippada2018`, which can be installed via `pip install fa2`. `Force-directed graph drawing`_ describes a class of long-established; algorithms for visualizing graphs.; It has been suggested for visualizing single-cell data by :cite:t:`Islam2011`.; Many other layouts as implemented in igraph :cite:p:`Csardi2006` are available.; Similar approaches have been used by :cite:t:`Zunder2015` or :cite:t:`Weinreb2017`. .. |fa2| replace:: `fa2`; .. _fa2: https://github.com/bhargavchippada/forceatlas2; .. _Force-directed graph drawing: https://en.wikipedia.org/wiki/Force-directed_graph_drawing. Parameters; ----------; adata; Annotated data matrix.; layout; 'fa' (`ForceAtlas2`) or any valid `igraph layout; <https://igraph.org/c/doc/igraph-Layout.html>`__. Of particular interest; are 'fr' (Fruchterman Reingold), 'grid_fr' (Grid Fruchterman Reingold,; faster than 'fr'), 'kk' (Kamadi Kawai', slower than 'fr'), 'lgl' (Large; Graph, very fast), 'drl' (Distributed Recursive Layout, pretty fast) and; 'rt' (Reingold Tilford tree layout).; root; Root for tree layouts.; random_state; For layouts with random initialization like 'fr', change this to use; different intial states for the optimization. If `None`, no seed is set.; adjacency; Sparse adjacency matrix of the graph, defaults to neighbors connectivities.; key_added_ext; By default, append `layout`.; proceed; Continue computation, starting off with 'X_draw_graph_`layout`'.; init_pos; `'paga'`/`True`, `None`/`False`, or any valid 2d-`.obsm` key.; Use precomputed coordinates for initialization.; If `False`/`None` (the default), initialize randomly.; neighbors_key; If not specified, draw_graph looks .obsp['connectivities'] for con",MatchSource.CODE_COMMENT,src/scanpy/tools/_draw_graph.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_draw_graph.py
Deployability,install,installed,"""""""\; Force-directed graph drawing :cite:p:`Islam2011,Jacomy2014,Chippada2018`. An alternative to tSNE that often preserves the topology of the data; better. This requires to run :func:`~scanpy.pp.neighbors`, first. The default layout ('fa', `ForceAtlas2`, :cite:t:`Jacomy2014`) uses the package |fa2|_; :cite:p:`Chippada2018`, which can be installed via `pip install fa2`. `Force-directed graph drawing`_ describes a class of long-established; algorithms for visualizing graphs.; It has been suggested for visualizing single-cell data by :cite:t:`Islam2011`.; Many other layouts as implemented in igraph :cite:p:`Csardi2006` are available.; Similar approaches have been used by :cite:t:`Zunder2015` or :cite:t:`Weinreb2017`. .. |fa2| replace:: `fa2`; .. _fa2: https://github.com/bhargavchippada/forceatlas2; .. _Force-directed graph drawing: https://en.wikipedia.org/wiki/Force-directed_graph_drawing. Parameters; ----------; adata; Annotated data matrix.; layout; 'fa' (`ForceAtlas2`) or any valid `igraph layout; <https://igraph.org/c/doc/igraph-Layout.html>`__. Of particular interest; are 'fr' (Fruchterman Reingold), 'grid_fr' (Grid Fruchterman Reingold,; faster than 'fr'), 'kk' (Kamadi Kawai', slower than 'fr'), 'lgl' (Large; Graph, very fast), 'drl' (Distributed Recursive Layout, pretty fast) and; 'rt' (Reingold Tilford tree layout).; root; Root for tree layouts.; random_state; For layouts with random initialization like 'fr', change this to use; different intial states for the optimization. If `None`, no seed is set.; adjacency; Sparse adjacency matrix of the graph, defaults to neighbors connectivities.; key_added_ext; By default, append `layout`.; proceed; Continue computation, starting off with 'X_draw_graph_`layout`'.; init_pos; `'paga'`/`True`, `None`/`False`, or any valid 2d-`.obsm` key.; Use precomputed coordinates for initialization.; If `False`/`None` (the default), initialize randomly.; neighbors_key; If not specified, draw_graph looks .obsp['connectivities'] for con",MatchSource.CODE_COMMENT,src/scanpy/tools/_draw_graph.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_draw_graph.py
Performance,optimiz,optimization,"hed; algorithms for visualizing graphs.; It has been suggested for visualizing single-cell data by :cite:t:`Islam2011`.; Many other layouts as implemented in igraph :cite:p:`Csardi2006` are available.; Similar approaches have been used by :cite:t:`Zunder2015` or :cite:t:`Weinreb2017`. .. |fa2| replace:: `fa2`; .. _fa2: https://github.com/bhargavchippada/forceatlas2; .. _Force-directed graph drawing: https://en.wikipedia.org/wiki/Force-directed_graph_drawing. Parameters; ----------; adata; Annotated data matrix.; layout; 'fa' (`ForceAtlas2`) or any valid `igraph layout; <https://igraph.org/c/doc/igraph-Layout.html>`__. Of particular interest; are 'fr' (Fruchterman Reingold), 'grid_fr' (Grid Fruchterman Reingold,; faster than 'fr'), 'kk' (Kamadi Kawai', slower than 'fr'), 'lgl' (Large; Graph, very fast), 'drl' (Distributed Recursive Layout, pretty fast) and; 'rt' (Reingold Tilford tree layout).; root; Root for tree layouts.; random_state; For layouts with random initialization like 'fr', change this to use; different intial states for the optimization. If `None`, no seed is set.; adjacency; Sparse adjacency matrix of the graph, defaults to neighbors connectivities.; key_added_ext; By default, append `layout`.; proceed; Continue computation, starting off with 'X_draw_graph_`layout`'.; init_pos; `'paga'`/`True`, `None`/`False`, or any valid 2d-`.obsm` key.; Use precomputed coordinates for initialization.; If `False`/`None` (the default), initialize randomly.; neighbors_key; If not specified, draw_graph looks .obsp['connectivities'] for connectivities; (default storage place for pp.neighbors).; If specified, draw_graph looks; .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.; obsp; Use .obsp[obsp] as adjacency. You can't specify both; `obsp` and `neighbors_key` at the same time.; copy; Return a copy instead of writing to adata.; **kwds; Parameters of chosen igraph layout. See e.g.; :meth:`~igraph.GraphBase.layout_fruchterman_reingold` :cite:p:`Fruchterm",MatchSource.CODE_COMMENT,src/scanpy/tools/_draw_graph.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_draw_graph.py
Performance,perform,performed,"""""""\; Calculate density of cells in embeddings; """"""; """"""\; Calculates the density of points in 2 dimensions.; """"""; # Calculate the point density; # Scale between 0 and 1; """"""\; Calculate the density of cells in an embedding (per condition). Gaussian kernel density estimation is used to calculate the density of; cells in an embedded space. This can be performed per category over a; categorical cell annotation. The cell density can be plotted using the; `pl.embedding_density` function. Note that density values are scaled to be between 0 and 1. Thus, the; density value at each cell is only comparable to densities in; the same category. Beware that the KDE estimate used (`scipy.stats.gaussian_kde`) becomes; unreliable if you don't have enough cells in a category. This function was written by Sophie Tritschler and implemented into; Scanpy by Malte Luecken. Parameters; ----------; adata; The annotated data matrix.; basis; The embedding over which the density will be calculated. This embedded; representation should be found in `adata.obsm['X_[basis]']``.; groupby; Key for categorical observation/cell annotation for which densities; are calculated per category.; key_added; Name of the `.obs` covariate that will be added with the density; estimates.; components; The embedding dimensions over which the density should be calculated.; This is limited to two components. Returns; -------; Sets the following fields (`key_added` defaults to `[basis]_density_[groupby]`, where `[basis]` is one of `umap`, `diffmap`, `pca`, `tsne`, or `draw_graph_fa` and `[groupby]` denotes the parameter input):. `adata.obs[key_added]` : :class:`numpy.ndarray` (dtype `float`); Embedding density values for each cell.; `adata.uns['[key_added]_params']` : :class:`dict`; A dict with the values for the parameters `covariate` (for the `groupby` parameter) and `components`. Examples; --------. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.tl.umap(adata); sc.tl.",MatchSource.CODE_COMMENT,src/scanpy/tools/_embedding_density.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_embedding_density.py
Testability,test,test,"unction was written by Sophie Tritschler and implemented into; Scanpy by Malte Luecken. Parameters; ----------; adata; The annotated data matrix.; basis; The embedding over which the density will be calculated. This embedded; representation should be found in `adata.obsm['X_[basis]']``.; groupby; Key for categorical observation/cell annotation for which densities; are calculated per category.; key_added; Name of the `.obs` covariate that will be added with the density; estimates.; components; The embedding dimensions over which the density should be calculated.; This is limited to two components. Returns; -------; Sets the following fields (`key_added` defaults to `[basis]_density_[groupby]`, where `[basis]` is one of `umap`, `diffmap`, `pca`, `tsne`, or `draw_graph_fa` and `[groupby]` denotes the parameter input):. `adata.obs[key_added]` : :class:`numpy.ndarray` (dtype `float`); Embedding density values for each cell.; `adata.uns['[key_added]_params']` : :class:`dict`; A dict with the values for the parameters `covariate` (for the `groupby` parameter) and `components`. Examples; --------. .. plot::; :context: close-figs. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.tl.umap(adata); sc.tl.embedding_density(adata, basis='umap', groupby='phase'); sc.pl.embedding_density(; adata, basis='umap', key='umap_density_phase', group='G1'; ). .. plot::; :context: close-figs. sc.pl.embedding_density(; adata, basis='umap', key='umap_density_phase', group='S'; ). .. currentmodule:: scanpy. See also; --------; pl.embedding_density; """"""; # to ensure that newly created covariates are categorical; # to test for category numbers; # Test user inputs; # Define new covariate name; # Calculate the densities over each category in the groupby column; # if groupby is None; # Calculate the density over the whole embedding without subsetting; # Reduce diffmap components for labeling; # Note: plot_scatter takes care of correcting diffmap components; # for plotting automatically",MatchSource.CODE_COMMENT,src/scanpy/tools/_embedding_density.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_embedding_density.py
Deployability,integrat,integrating-data-using-ingest,"""""""\; Map labels and embeddings from reference data to new data. :doc:`/tutorials/basics/integrating-data-using-ingest`. Integrates embeddings and annotations of an `adata` with a reference dataset; `adata_ref` through projecting on a PCA (or alternate; model) that has been fitted on the reference data. The function uses a knn; classifier for mapping labels and the UMAP package :cite:p:`McInnes2018` for mapping; the embeddings. .. note::. We refer to this *asymmetric* dataset integration as *ingesting*; annotations from reference data to new data. This is different from; learning a joint representation that integrates both datasets in an; unbiased way, as CCA (e.g. in Seurat) or a conditional VAE (e.g. in; scVI) would do. You need to run :func:`~scanpy.pp.neighbors` on `adata_ref` before; passing it. Parameters; ----------; adata; The annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes. This is the dataset without labels and; embeddings.; adata_ref; The annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; Variables (`n_vars` and `var_names`) of `adata_ref` should be the same; as in `adata`.; This is the dataset with labels and embeddings; which need to be mapped to `adata`.; obs; Labels' keys in `adata_ref.obs` which need to be mapped to `adata.obs`; (inferred for observation of `adata`).; embedding_method; Embeddings in `adata_ref` which need to be mapped to `adata`.; The only supported values are 'umap' and 'pca'.; labeling_method; The method to map labels in `adata_ref.obs` to `adata.obs`.; The only supported value is 'knn'.; neighbors_key; If not specified, ingest looks adata_ref.uns['neighbors']; for neighbors settings and adata_ref.obsp['distances'] for; distances (default storage places for pp.neighbors).; If specified, ingest looks adata_ref.uns[neighbors_key] for; neighbors settings and; adata_ref.obsp[adata_ref.uns[neighbors_key]['distances_key']] for distances.; in",MatchSource.CODE_COMMENT,src/scanpy/tools/_ingest.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_ingest.py
Integrability,integrat,integrating-data-using-ingest,"""""""\; Map labels and embeddings from reference data to new data. :doc:`/tutorials/basics/integrating-data-using-ingest`. Integrates embeddings and annotations of an `adata` with a reference dataset; `adata_ref` through projecting on a PCA (or alternate; model) that has been fitted on the reference data. The function uses a knn; classifier for mapping labels and the UMAP package :cite:p:`McInnes2018` for mapping; the embeddings. .. note::. We refer to this *asymmetric* dataset integration as *ingesting*; annotations from reference data to new data. This is different from; learning a joint representation that integrates both datasets in an; unbiased way, as CCA (e.g. in Seurat) or a conditional VAE (e.g. in; scVI) would do. You need to run :func:`~scanpy.pp.neighbors` on `adata_ref` before; passing it. Parameters; ----------; adata; The annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes. This is the dataset without labels and; embeddings.; adata_ref; The annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; Variables (`n_vars` and `var_names`) of `adata_ref` should be the same; as in `adata`.; This is the dataset with labels and embeddings; which need to be mapped to `adata`.; obs; Labels' keys in `adata_ref.obs` which need to be mapped to `adata.obs`; (inferred for observation of `adata`).; embedding_method; Embeddings in `adata_ref` which need to be mapped to `adata`.; The only supported values are 'umap' and 'pca'.; labeling_method; The method to map labels in `adata_ref.obs` to `adata.obs`.; The only supported value is 'knn'.; neighbors_key; If not specified, ingest looks adata_ref.uns['neighbors']; for neighbors settings and adata_ref.obsp['distances'] for; distances (default storage places for pp.neighbors).; If specified, ingest looks adata_ref.uns[neighbors_key] for; neighbors settings and; adata_ref.obsp[adata_ref.uns[neighbors_key]['distances_key']] for distances.; in",MatchSource.CODE_COMMENT,src/scanpy/tools/_ingest.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_ingest.py
Usability,learn,learning,"""""""\; Map labels and embeddings from reference data to new data. :doc:`/tutorials/basics/integrating-data-using-ingest`. Integrates embeddings and annotations of an `adata` with a reference dataset; `adata_ref` through projecting on a PCA (or alternate; model) that has been fitted on the reference data. The function uses a knn; classifier for mapping labels and the UMAP package :cite:p:`McInnes2018` for mapping; the embeddings. .. note::. We refer to this *asymmetric* dataset integration as *ingesting*; annotations from reference data to new data. This is different from; learning a joint representation that integrates both datasets in an; unbiased way, as CCA (e.g. in Seurat) or a conditional VAE (e.g. in; scVI) would do. You need to run :func:`~scanpy.pp.neighbors` on `adata_ref` before; passing it. Parameters; ----------; adata; The annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes. This is the dataset without labels and; embeddings.; adata_ref; The annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; Variables (`n_vars` and `var_names`) of `adata_ref` should be the same; as in `adata`.; This is the dataset with labels and embeddings; which need to be mapped to `adata`.; obs; Labels' keys in `adata_ref.obs` which need to be mapped to `adata.obs`; (inferred for observation of `adata`).; embedding_method; Embeddings in `adata_ref` which need to be mapped to `adata`.; The only supported values are 'umap' and 'pca'.; labeling_method; The method to map labels in `adata_ref.obs` to `adata.obs`.; The only supported value is 'knn'.; neighbors_key; If not specified, ingest looks adata_ref.uns['neighbors']; for neighbors settings and adata_ref.obsp['distances'] for; distances (default storage places for pp.neighbors).; If specified, ingest looks adata_ref.uns[neighbors_key] for; neighbors settings and; adata_ref.obsp[adata_ref.uns[neighbors_key]['distances_key']] for distances.; in",MatchSource.CODE_COMMENT,src/scanpy/tools/_ingest.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_ingest.py
Availability,avail,available,"ution_parameter`.; random_state; Change the initialization of the optimization.; restrict_to; Restrict the clustering to the categories within the key for sample; annotation, tuple needs to contain `(obs_key, list_of_categories)`.; key_added; `adata.obs` key under which to add the cluster labels.; adjacency; Sparse adjacency matrix of the graph, defaults to neighbors connectivities.; directed; Whether to treat the graph as directed or undirected.; use_weights; If `True`, edge weights from the graph are used in the computation; (placing more emphasis on stronger edges).; n_iterations; How many iterations of the Leiden clustering algorithm to perform.; Positive values above 2 define the total number of iterations to perform,; -1 has the algorithm run until it reaches its optimal clustering.; 2 is faster and the default for underlying packages.; partition_type; Type of partition to use.; Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`.; For the available options, consult the documentation for; :func:`~leidenalg.find_partition`.; neighbors_key; Use neighbors connectivities as adjacency.; If not specified, leiden looks .obsp['connectivities'] for connectivities; (default storage place for pp.neighbors).; If specified, leiden looks; .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.; obsp; Use .obsp[obsp] as adjacency. You can't specify both; `obsp` and `neighbors_key` at the same time.; copy; Whether to copy `adata` or modify it inplace.; flavor; Which package's implementation to use.; **clustering_args; Any further arguments to pass to :func:`~leidenalg.find_partition` (which in turn passes arguments to the `partition_type`); or :meth:`igraph.Graph.community_leiden` from `igraph`. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:. `adata.obs['leiden' | key_added]` : :class:`pandas.Series` (dtype ``category``); Array of dim (number of samples) that stores the subgroup id; (``'0'",MatchSource.CODE_COMMENT,src/scanpy/tools/_leiden.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_leiden.py
Performance,optimiz,optimization,"""""""\; Cluster cells into subgroups :cite:p:`Traag2019`. Cluster cells using the Leiden algorithm :cite:p:`Traag2019`,; an improved version of the Louvain algorithm :cite:p:`Blondel2008`.; It has been proposed for single-cell analysis by :cite:t:`Levine2015`. This requires having ran :func:`~scanpy.pp.neighbors` or; :func:`~scanpy.external.pp.bbknn` first. Parameters; ----------; adata; The annotated data matrix.; resolution; A parameter value controlling the coarseness of the clustering.; Higher values lead to more clusters.; Set to `None` if overriding `partition_type`; to one that doesn’t accept a `resolution_parameter`.; random_state; Change the initialization of the optimization.; restrict_to; Restrict the clustering to the categories within the key for sample; annotation, tuple needs to contain `(obs_key, list_of_categories)`.; key_added; `adata.obs` key under which to add the cluster labels.; adjacency; Sparse adjacency matrix of the graph, defaults to neighbors connectivities.; directed; Whether to treat the graph as directed or undirected.; use_weights; If `True`, edge weights from the graph are used in the computation; (placing more emphasis on stronger edges).; n_iterations; How many iterations of the Leiden clustering algorithm to perform.; Positive values above 2 define the total number of iterations to perform,; -1 has the algorithm run until it reaches its optimal clustering.; 2 is faster and the default for underlying packages.; partition_type; Type of partition to use.; Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`.; For the available options, consult the documentation for; :func:`~leidenalg.find_partition`.; neighbors_key; Use neighbors connectivities as adjacency.; If not specified, leiden looks .obsp['connectivities'] for connectivities; (default storage place for pp.neighbors).; If specified, leiden looks; .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.; obsp; Use .obsp[obsp] as adjacency. You can't specify bo",MatchSource.CODE_COMMENT,src/scanpy/tools/_leiden.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_leiden.py
Energy Efficiency,power,powerful," proposed for single-cell; analysis by :cite:t:`Levine2015`. This requires having ran :func:`~scanpy.pp.neighbors` or; :func:`~scanpy.external.pp.bbknn` first,; or explicitly passing a ``adjacency`` matrix. Parameters; ----------; adata; The annotated data matrix.; resolution; For the default flavor (``'vtraag'``) or for ```RAPIDS```, you can provide a; resolution (higher resolution means finding more and smaller clusters),; which defaults to 1.0.; See “Time as a resolution parameter” in :cite:t:`Lambiotte2014`.; random_state; Change the initialization of the optimization.; restrict_to; Restrict the clustering to the categories within the key for sample; annotation, tuple needs to contain ``(obs_key, list_of_categories)``.; key_added; Key under which to add the cluster labels. (default: ``'louvain'``); adjacency; Sparse adjacency matrix of the graph, defaults to neighbors connectivities.; flavor; Choose between to packages for computing the clustering. ``'vtraag'``; Much more powerful than ``'igraph'``, and the default.; ``'igraph'``; Built in ``igraph`` method.; ``'rapids'``; GPU accelerated implementation. .. deprecated:: 1.10.0; Use :func:`rapids_singlecell.tl.louvain` instead.; directed; Interpret the ``adjacency`` matrix as directed graph?; use_weights; Use weights from knn graph.; partition_type; Type of partition to use.; Only a valid argument if ``flavor`` is ``'vtraag'``.; partition_kwargs; Key word arguments to pass to partitioning,; if ``vtraag`` method is being used.; neighbors_key; Use neighbors connectivities as adjacency.; If not specified, louvain looks .obsp['connectivities'] for connectivities; (default storage place for pp.neighbors).; If specified, louvain looks; .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.; obsp; Use .obsp[obsp] as adjacency. You can't specify both; `obsp` and `neighbors_key` at the same time.; copy; Copy adata or modify it inplace. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData",MatchSource.CODE_COMMENT,src/scanpy/tools/_louvain.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_louvain.py
Performance,optimiz,optimization,"""""""\; Cluster cells into subgroups :cite:p:`Blondel2008,Levine2015,Traag2017`. Cluster cells using the Louvain algorithm :cite:p:`Blondel2008` in the implementation; of :cite:t:`Traag2017`. The Louvain algorithm has been proposed for single-cell; analysis by :cite:t:`Levine2015`. This requires having ran :func:`~scanpy.pp.neighbors` or; :func:`~scanpy.external.pp.bbknn` first,; or explicitly passing a ``adjacency`` matrix. Parameters; ----------; adata; The annotated data matrix.; resolution; For the default flavor (``'vtraag'``) or for ```RAPIDS```, you can provide a; resolution (higher resolution means finding more and smaller clusters),; which defaults to 1.0.; See “Time as a resolution parameter” in :cite:t:`Lambiotte2014`.; random_state; Change the initialization of the optimization.; restrict_to; Restrict the clustering to the categories within the key for sample; annotation, tuple needs to contain ``(obs_key, list_of_categories)``.; key_added; Key under which to add the cluster labels. (default: ``'louvain'``); adjacency; Sparse adjacency matrix of the graph, defaults to neighbors connectivities.; flavor; Choose between to packages for computing the clustering. ``'vtraag'``; Much more powerful than ``'igraph'``, and the default.; ``'igraph'``; Built in ``igraph`` method.; ``'rapids'``; GPU accelerated implementation. .. deprecated:: 1.10.0; Use :func:`rapids_singlecell.tl.louvain` instead.; directed; Interpret the ``adjacency`` matrix as directed graph?; use_weights; Use weights from knn graph.; partition_type; Type of partition to use.; Only a valid argument if ``flavor`` is ``'vtraag'``.; partition_kwargs; Key word arguments to pass to partitioning,; if ``vtraag`` method is being used.; neighbors_key; Use neighbors connectivities as adjacency.; If not specified, louvain looks .obsp['connectivities'] for connectivities; (default storage place for pp.neighbors).; If specified, louvain looks; .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.;",MatchSource.CODE_COMMENT,src/scanpy/tools/_louvain.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_louvain.py
Testability,test,test,"rker genes are used. If `adj_pval_threshold` is set along with; `top_n_markers`, then `adj_pval_threshold` is ignored.; adj_pval_threshold; A significance threshold on the adjusted p-values to select marker; genes. This can only be used when adjusted p-values are calculated by; `sc.tl.rank_genes_groups()`. If `adj_pval_threshold` is set along with; `top_n_markers`, then `adj_pval_threshold` is ignored.; key_added; Name of the `.uns` field that will contain the marker overlap scores.; inplace; Return a marker gene dataframe or store it inplace in `adata.uns`. Returns; -------; Returns :class:`pandas.DataFrame` if `inplace=True`, else returns an `AnnData` object where it sets the following field:. `adata.uns[key_added]` : :class:`pandas.DataFrame` (dtype `float`); Marker gene overlap scores. Default for `key_added` is `'marker_gene_overlap'`. Examples; --------; >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> sc.pp.pca(adata, svd_solver='arpack'); >>> sc.pp.neighbors(adata); >>> sc.tl.leiden(adata); >>> sc.tl.rank_genes_groups(adata, groupby='leiden'); >>> marker_genes = {; ... 'CD4 T cells': {'IL7R'},; ... 'CD14+ Monocytes': {'CD14', 'LYZ'},; ... 'B cells': {'MS4A1'},; ... 'CD8 T cells': {'CD8A'},; ... 'NK cells': {'GNLY', 'NKG7'},; ... 'FCGR3A+ Monocytes': {'FCGR3A', 'MS4A7'},; ... 'Dendritic Cells': {'FCER1A', 'CST3'},; ... 'Megakaryocytes': {'PPBP'}; ... }; >>> marker_matches = sc.tl.marker_gene_overlap(adata, marker_genes); """"""; # Test user inputs; # Get data-derived marker genes in a dictionary of sets; # Use top 100 markers as default if top_n_markers = None; # Find overlaps; # Ensure rows sum to 1; # Ensure columns sum to 1; # Note:; # Could add an 'enrich' option here; # (fisher's exact test or hypergeometric test),; # but that would require knowledge of the size of the space from which; # the reference marker gene set was taken.; # This is at best approximately known.; # Create a pandas dataframe with the results; # Store the results",MatchSource.CODE_COMMENT,src/scanpy/tools/_marker_gene_overlap.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_marker_gene_overlap.py
Performance,perform,perform,"data.obs`. You can pass your predefined groups; by choosing any categorical annotation of observations. Default:; The first present key of `'leiden'` or `'louvain'`.; use_rna_velocity; Use RNA velocity to orient edges in the abstracted graph and estimate; transitions. Requires that `adata.uns` contains a directed single-cell; graph with key `['velocity_graph']`. This feature might be subject; to change in the future.; model; The PAGA connectivity model.; neighbors_key; If not specified, paga looks `.uns['neighbors']` for neighbors settings; and `.obsp['connectivities']`, `.obsp['distances']` for connectivities and; distances respectively (default storage places for `pp.neighbors`).; If specified, paga looks `.uns[neighbors_key]` for neighbors settings and; `.obsp[.uns[neighbors_key]['connectivities_key']]`,; `.obsp[.uns[neighbors_key]['distances_key']]` for connectivities and distances; respectively.; copy; Copy `adata` before computation and return a copy. Otherwise, perform; computation inplace and return `None`. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:. `adata.uns['connectivities']` : :class:`numpy.ndarray` (dtype `float`); The full adjacency matrix of the abstracted graph, weights correspond to; confidence in the connectivities of partitions.; `adata.uns['connectivities_tree']` : :class:`scipy.sparse.csr_matrix` (dtype `float`); The adjacency matrix of the tree-like subgraph that best explains; the topology. Notes; -----; Together with a random walk-based distance measure; (e.g. :func:`scanpy.tl.dpt`) this generates a partial coordinatization of; data useful for exploring and explaining its variation. .. currentmodule:: scanpy. See Also; --------; pl.paga; pl.paga_path; pl.paga_compare; """"""; # only add if not present; # adata.uns['paga']['expected_n_edges_random'] = paga.expected_n_edges_random; # adata.uns['paga']['transitions_ttest'] = paga.transitions_ttest; # "" 'paga/transitions_ttest', t-t",MatchSource.CODE_COMMENT,src/scanpy/tools/_paga.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_paga.py
Testability,test,test,"rm; computation inplace and return `None`. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:. `adata.uns['connectivities']` : :class:`numpy.ndarray` (dtype `float`); The full adjacency matrix of the abstracted graph, weights correspond to; confidence in the connectivities of partitions.; `adata.uns['connectivities_tree']` : :class:`scipy.sparse.csr_matrix` (dtype `float`); The adjacency matrix of the tree-like subgraph that best explains; the topology. Notes; -----; Together with a random walk-based distance measure; (e.g. :func:`scanpy.tl.dpt`) this generates a partial coordinatization of; data useful for exploring and explaining its variation. .. currentmodule:: scanpy. See Also; --------; pl.paga; pl.paga_path; pl.paga_compare; """"""; # only add if not present; # adata.uns['paga']['expected_n_edges_random'] = paga.expected_n_edges_random; # adata.uns['paga']['transitions_ttest'] = paga.transitions_ttest; # "" 'paga/transitions_ttest', t-test on transitions (adata.uns)""; # should be directed if we deal with distances; # \epsilon_i + \epsilon_j; # set attributes; # have n_neighbors**2 inside sqrt for backwards compat; # set attributes; # restore this at some point; # if 'expected_n_edges_random' not in self._adata.uns['paga']:; # raise ValueError(; # 'Before running PAGA with `use_rna_velocity=True`, run it with `False`.'); # set combine_edges to False if you want self loops; # total_n_sum = sum(total_n); # expected_n_edges_random = self._adata.uns['paga']['expected_n_edges_random']; # if expected_n_edges_random[i, j] != 0:; # # factor 0.5 because of asymmetry; # reference = 0.5 * expected_n_edges_random[i, j]; # else:; # # approximate; # reference = self._neighbors.n_neighbors * total_n[i] * total_n[j] / total_n_sum; # transpose in order to match convention of stochastic matrices; # entry ij means transition from j to i; # this stores all single-cell edges in the cluster graph; # this is the boolean version",MatchSource.CODE_COMMENT,src/scanpy/tools/_paga.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_paga.py
Usability,simpl,simpler,"""""""\; Mapping out the coarse-grained connectivity structures of complex manifolds :cite:p:`Wolf2019`. By quantifying the connectivity of partitions (groups, clusters) of the; single-cell graph, partition-based graph abstraction (PAGA) generates a much; simpler abstracted graph (*PAGA graph*) of partitions, in which edge weights; represent confidence in the presence of connections. By thresholding this; confidence in :func:`~scanpy.pl.paga`, a much simpler representation of the; manifold data is obtained, which is nonetheless faithful to the topology of; the manifold. The confidence should be interpreted as the ratio of the actual versus the; expected value of connections under the null model of randomly connecting; partitions. We do not provide a p-value as this null model does not; precisely capture what one would consider ""connected"" in real data, hence it; strongly overestimates the expected value. See an extensive discussion of; this in :cite:t:`Wolf2019`. .. note::; Note that you can use the result of :func:`~scanpy.pl.paga` in; :func:`~scanpy.tl.umap` and :func:`~scanpy.tl.draw_graph` via; `init_pos='paga'` to get single-cell embeddings that are typically more; faithful to the global topology. Parameters; ----------; adata; An annotated data matrix.; groups; Key for categorical in `adata.obs`. You can pass your predefined groups; by choosing any categorical annotation of observations. Default:; The first present key of `'leiden'` or `'louvain'`.; use_rna_velocity; Use RNA velocity to orient edges in the abstracted graph and estimate; transitions. Requires that `adata.uns` contains a directed single-cell; graph with key `['velocity_graph']`. This feature might be subject; to change in the future.; model; The PAGA connectivity model.; neighbors_key; If not specified, paga looks `.uns['neighbors']` for neighbors settings; and `.obsp['connectivities']`, `.obsp['distances']` for connectivities and; distances respectively (default storage places for `pp.neighbors`).;",MatchSource.CODE_COMMENT,src/scanpy/tools/_paga.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_paga.py
Availability,error,errors,"""""""Rank genes according to differential expression.""""""; # Used with get_args; # Calculate chunk frames; # Singlet groups cause division by zero errors; # for correct getnnz calculation; # for logreg only; """"""Set self.{means,vars,pts}{,_rest} depending on X.""""""; # deleting the next line causes a memory leak for some reason; # this can be costly for sparse data; # deleting the next line causes a memory leak for some reason; # hack for overestimating the variance for small groups; # TODO: Come up with better solution. Mask unexpressed genes?; # See https://github.com/scipy/scipy/issues/10269; # Welch's; # I think it's only nan when means are the same and vars are 0; # This also has to happen for Benjamini Hochberg; # First loop: Loop over all genes; # initialize space for z-scores; # initialize space for tie correction coefficients; # Calculate rank sums for each chunk for the current mask; # If no reference group exists,; # ranking needs only to be done once (full mask); # sum up adjusted_ranks to calculate W_m,n; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; # Indexing with a series causes issues, possibly segfault; # not all codes necessarily appear in data; # binary logistic regression; # cat code is index of cat value in .categories; # index of scores row is index of cat code in array of existing codes; # add small value to remove 0's; """"""\; Rank genes for characterizing groups. Expects logarithmized data. Parameters; ----------; adata; Annotated data matrix.; groupby; The key of the observations grouping to consider.; mask_var; Select subset of genes to use in statistical tests.; use_raw; Use `raw` attribute of `adata` if present.; layer; Key from `adata.layers` whose value will be used to perform tests on.; groups; Subset of groups, e.g. [`'g1'`, `'g2'`, `'g3'`], to which comparison; shall be restricted, or `'all'` (default), for all g",MatchSource.CODE_COMMENT,src/scanpy/tools/_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_rank_genes_groups.py
Integrability,depend,depending,"""""""Rank genes according to differential expression.""""""; # Used with get_args; # Calculate chunk frames; # Singlet groups cause division by zero errors; # for correct getnnz calculation; # for logreg only; """"""Set self.{means,vars,pts}{,_rest} depending on X.""""""; # deleting the next line causes a memory leak for some reason; # this can be costly for sparse data; # deleting the next line causes a memory leak for some reason; # hack for overestimating the variance for small groups; # TODO: Come up with better solution. Mask unexpressed genes?; # See https://github.com/scipy/scipy/issues/10269; # Welch's; # I think it's only nan when means are the same and vars are 0; # This also has to happen for Benjamini Hochberg; # First loop: Loop over all genes; # initialize space for z-scores; # initialize space for tie correction coefficients; # Calculate rank sums for each chunk for the current mask; # If no reference group exists,; # ranking needs only to be done once (full mask); # sum up adjusted_ranks to calculate W_m,n; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; # Indexing with a series causes issues, possibly segfault; # not all codes necessarily appear in data; # binary logistic regression; # cat code is index of cat value in .categories; # index of scores row is index of cat code in array of existing codes; # add small value to remove 0's; """"""\; Rank genes for characterizing groups. Expects logarithmized data. Parameters; ----------; adata; Annotated data matrix.; groupby; The key of the observations grouping to consider.; mask_var; Select subset of genes to use in statistical tests.; use_raw; Use `raw` attribute of `adata` if present.; layer; Key from `adata.layers` whose value will be used to perform tests on.; groups; Subset of groups, e.g. [`'g1'`, `'g2'`, `'g3'`], to which comparison; shall be restricted, or `'all'` (default), for all g",MatchSource.CODE_COMMENT,src/scanpy/tools/_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_rank_genes_groups.py
Modifiability,layers,layers,"# Calculate rank sums for each chunk for the current mask; # If no reference group exists,; # ranking needs only to be done once (full mask); # sum up adjusted_ranks to calculate W_m,n; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; # Indexing with a series causes issues, possibly segfault; # not all codes necessarily appear in data; # binary logistic regression; # cat code is index of cat value in .categories; # index of scores row is index of cat code in array of existing codes; # add small value to remove 0's; """"""\; Rank genes for characterizing groups. Expects logarithmized data. Parameters; ----------; adata; Annotated data matrix.; groupby; The key of the observations grouping to consider.; mask_var; Select subset of genes to use in statistical tests.; use_raw; Use `raw` attribute of `adata` if present.; layer; Key from `adata.layers` whose value will be used to perform tests on.; groups; Subset of groups, e.g. [`'g1'`, `'g2'`, `'g3'`], to which comparison; shall be restricted, or `'all'` (default), for all groups. Note that if; `reference='rest'` all groups will still be used as the reference, not; just those specified in `groups`.; reference; If `'rest'`, compare each group to the union of the rest of the group.; If a group identifier, compare with respect to this group.; n_genes; The number of genes that appear in the returned tables.; Defaults to all genes.; method; The default method is `'t-test'`,; `'t-test_overestim_var'` overestimates variance of each group,; `'wilcoxon'` uses Wilcoxon rank-sum,; `'logreg'` uses logistic regression. See :cite:t:`Ntranos2019`,; `here <https://github.com/scverse/scanpy/issues/95>`__ and `here; <https://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__,; for why this is meaningful.; corr_method; p-value correction method.; Used only for `'t-test'`, `'t-test_overestim_var'`, and `'wilco",MatchSource.CODE_COMMENT,src/scanpy/tools/_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_rank_genes_groups.py
Performance,perform,perform,"# Calculate rank sums for each chunk for the current mask; # If no reference group exists,; # ranking needs only to be done once (full mask); # sum up adjusted_ranks to calculate W_m,n; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; # Indexing with a series causes issues, possibly segfault; # not all codes necessarily appear in data; # binary logistic regression; # cat code is index of cat value in .categories; # index of scores row is index of cat code in array of existing codes; # add small value to remove 0's; """"""\; Rank genes for characterizing groups. Expects logarithmized data. Parameters; ----------; adata; Annotated data matrix.; groupby; The key of the observations grouping to consider.; mask_var; Select subset of genes to use in statistical tests.; use_raw; Use `raw` attribute of `adata` if present.; layer; Key from `adata.layers` whose value will be used to perform tests on.; groups; Subset of groups, e.g. [`'g1'`, `'g2'`, `'g3'`], to which comparison; shall be restricted, or `'all'` (default), for all groups. Note that if; `reference='rest'` all groups will still be used as the reference, not; just those specified in `groups`.; reference; If `'rest'`, compare each group to the union of the rest of the group.; If a group identifier, compare with respect to this group.; n_genes; The number of genes that appear in the returned tables.; Defaults to all genes.; method; The default method is `'t-test'`,; `'t-test_overestim_var'` overestimates variance of each group,; `'wilcoxon'` uses Wilcoxon rank-sum,; `'logreg'` uses logistic regression. See :cite:t:`Ntranos2019`,; `here <https://github.com/scverse/scanpy/issues/95>`__ and `here; <https://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__,; for why this is meaningful.; corr_method; p-value correction method.; Used only for `'t-test'`, `'t-test_overestim_var'`, and `'wilco",MatchSource.CODE_COMMENT,src/scanpy/tools/_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_rank_genes_groups.py
Safety,predict,predictors,"`'wilcoxon'` uses Wilcoxon rank-sum,; `'logreg'` uses logistic regression. See :cite:t:`Ntranos2019`,; `here <https://github.com/scverse/scanpy/issues/95>`__ and `here; <https://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__,; for why this is meaningful.; corr_method; p-value correction method.; Used only for `'t-test'`, `'t-test_overestim_var'`, and `'wilcoxon'`.; tie_correct; Use tie correction for `'wilcoxon'` scores.; Used only for `'wilcoxon'`.; rankby_abs; Rank genes by the absolute value of the score, not by the; score. The returned scores are never the absolute values.; pts; Compute the fraction of cells expressing the genes.; key_added; The key in `adata.uns` information is saved to.; copy; Whether to copy `adata` or modify it inplace.; kwds; Are passed to test methods. Currently this affects only parameters that; are passed to :class:`sklearn.linear_model.LogisticRegression`.; For instance, you can pass `penalty='l1'` to try to come up with a; minimal set of genes that are good predictors (sparse solution meaning; few non-zero fitted coefficients). Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:. `adata.uns['rank_genes_groups' | key_added]['names']` : structured :class:`numpy.ndarray` (dtype `object`); Structured array to be indexed by group id storing the gene; names. Ordered according to scores.; `adata.uns['rank_genes_groups' | key_added]['scores']` : structured :class:`numpy.ndarray` (dtype `object`); Structured array to be indexed by group id storing the z-score; underlying the computation of a p-value for each gene for each; group. Ordered according to scores.; `adata.uns['rank_genes_groups' | key_added]['logfoldchanges']` : structured :class:`numpy.ndarray` (dtype `object`); Structured array to be indexed by group id storing the log2; fold change for each gene for each group. Ordered according to; scores. Only provided if method is 't-test' like.; Note: this is an approxima",MatchSource.CODE_COMMENT,src/scanpy/tools/_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_rank_genes_groups.py
Testability,log,logreg,"""""""Rank genes according to differential expression.""""""; # Used with get_args; # Calculate chunk frames; # Singlet groups cause division by zero errors; # for correct getnnz calculation; # for logreg only; """"""Set self.{means,vars,pts}{,_rest} depending on X.""""""; # deleting the next line causes a memory leak for some reason; # this can be costly for sparse data; # deleting the next line causes a memory leak for some reason; # hack for overestimating the variance for small groups; # TODO: Come up with better solution. Mask unexpressed genes?; # See https://github.com/scipy/scipy/issues/10269; # Welch's; # I think it's only nan when means are the same and vars are 0; # This also has to happen for Benjamini Hochberg; # First loop: Loop over all genes; # initialize space for z-scores; # initialize space for tie correction coefficients; # Calculate rank sums for each chunk for the current mask; # If no reference group exists,; # ranking needs only to be done once (full mask); # sum up adjusted_ranks to calculate W_m,n; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; # Indexing with a series causes issues, possibly segfault; # not all codes necessarily appear in data; # binary logistic regression; # cat code is index of cat value in .categories; # index of scores row is index of cat code in array of existing codes; # add small value to remove 0's; """"""\; Rank genes for characterizing groups. Expects logarithmized data. Parameters; ----------; adata; Annotated data matrix.; groupby; The key of the observations grouping to consider.; mask_var; Select subset of genes to use in statistical tests.; use_raw; Use `raw` attribute of `adata` if present.; layer; Key from `adata.layers` whose value will be used to perform tests on.; groups; Subset of groups, e.g. [`'g1'`, `'g2'`, `'g3'`], to which comparison; shall be restricted, or `'all'` (default), for all g",MatchSource.CODE_COMMENT,src/scanpy/tools/_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_rank_genes_groups.py
Availability,avail,availnodes," hill function.""""""; """"""Inhibiting hill function. Is equivalent to 1-hill_a(self,x,power,threshold).; """"""; """"""Normalized activating hill function.""""""; """"""Normalized inhibiting hill function. Is equivalent to 1-nhill_a(self,x,power,threshold).; """"""; """"""Read the model and the couplings from the model file.""""""; # read model; # read couplings via names; # open(self.model.replace('/model','/couplList')):; # adjancecy matrices; # build bool coefficients (necessary for odefy type; # version of the discrete model); """"""Construct the coupling matrix (and adjacancy matrix) from predefined models; or via sampling.; """"""; # vector auto regressive process; # sinknodes = np.random.choice(np.arange(0,self.dim),; # size=n_sinknodes,replace=False); # assume sinknodes have feeback; # # allow negative feedback; # if self.model == 10:; # plus_minus = (np.random.randint(0,2,n_sinknodes) - 0.5)*2; # self.Adj_signed[sinknodes,sinknodes] = plus_minus; # settings.m(0,leafnodes,availnodes); # parent; # children; # update leafnodes; # update availnodes; # settings.m(0,availnodes); # settings.m(0,leafnodes); # settings.m(0,self.Adj); # settings.m(0,'-'); # this number includes parents (other variables); # and the variable itself, therefore its; # self.maxnpar+2 in the following line; #; # settings.m(0,self.Adj); """"""Using the adjacency matrix, sample a coupling matrix.""""""; # we already built the coupling matrix in set_coupl20(); # if there is a 1 in Adj, specify co and antiregulation; # and strength of regulation; # set a lower bound for the coupling parameters; # they ought not to be smaller than 0.1; # and not be larger than 0.4; # set sign for coupling; # enforce certain requirements on models; # output; """"""In model 1, we want enforce the following signs; on the couplings. Model 2 has the same couplings; but arbitrary signs.; """"""; """"""Toggle switch.""""""; """"""Variant of toggle switch.""""""; """"""Variant of toggle switch.""""""; # reduce the value of the coupling of the repressing genes; # otherwise complet",MatchSource.CODE_COMMENT,src/scanpy/tools/_sim.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_sim.py
Deployability,update,update,"""; # init variables; # step size for saving the figure; # how many files?; # simple vector auto regressive process or; # hill kinetics process simulation; # create instance, set seed; # random topology / for a given edge density; # only consider off-diagonal edges; # check that the coupling matrix does not have eigenvalues; # greater than 1, which would lead to an exploding var process; # init type; # slightly break symmetry in initial conditions; # check branching; # append some zeros; # more complex models; # choose initial conditions such that branchings result; # vary initial conditions around mean; # generate random initial conditions within [0.3,0.7]; # check branching; # load the last simulation file; """"""Write simulated data. Accounts for saving at the same time an ID; and a model file.; """"""; # update file with sample ids; # dimension; # write files with adjacancy and coupling matrices; # due to 'update formulation' of model, there; # is always a diagonal dependence; # write model file; # write coupling via names; # write simulated data; # the binary mode option in the following line is a fix for python 3; # variable names; """"""; Simlulation of stochastic dynamic systems. Main application: simulation of gene expression dynamics. Also standard models are implemented.; """"""; """"""; Params; ------; model; either string for predefined model,; or directory with a model file and a couple matrix files; """"""; # number of nodes / dimension of system; # maximal number of parents; # fraction of independent genes; # string characterizing a specific initial; # checks; # noqa: F841 # TODO FIX; # read from file; # set the coupling matrix, and with that the adjacency matrix; # seed; # header; # params; """"""Simulate the model.""""""; #; # run simulation; # add dynamic noise; """"""Build Xdiff from coefficients of boolean network,; that is, using self.boolCoeff. The employed functions; are Hill type activation and deactivation functions. See Wittmann et al., BMC Syst. Biol. 3, 98 (2009),;",MatchSource.CODE_COMMENT,src/scanpy/tools/_sim.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_sim.py
Energy Efficiency,power,power,"/ dimension of system; # maximal number of parents; # fraction of independent genes; # string characterizing a specific initial; # checks; # noqa: F841 # TODO FIX; # read from file; # set the coupling matrix, and with that the adjacency matrix; # seed; # header; # params; """"""Simulate the model.""""""; #; # run simulation; # add dynamic noise; """"""Build Xdiff from coefficients of boolean network,; that is, using self.boolCoeff. The employed functions; are Hill type activation and deactivation functions. See Wittmann et al., BMC Syst. Biol. 3, 98 (2009),; doi:10.1186/1752-0509-3-98 for more details.; """"""; # check whether list of parents is non-empty,; # otherwise continue; # synthesize term; # loop over all tuples for which the boolean update; # rule returns true, these are stored in self.boolCoeff; # multiply with degradation term; """"""""""""; # subtract the current state; # add the information from the past; """"""Activating hill function.""""""; """"""Inhibiting hill function. Is equivalent to 1-hill_a(self,x,power,threshold).; """"""; """"""Normalized activating hill function.""""""; """"""Normalized inhibiting hill function. Is equivalent to 1-nhill_a(self,x,power,threshold).; """"""; """"""Read the model and the couplings from the model file.""""""; # read model; # read couplings via names; # open(self.model.replace('/model','/couplList')):; # adjancecy matrices; # build bool coefficients (necessary for odefy type; # version of the discrete model); """"""Construct the coupling matrix (and adjacancy matrix) from predefined models; or via sampling.; """"""; # vector auto regressive process; # sinknodes = np.random.choice(np.arange(0,self.dim),; # size=n_sinknodes,replace=False); # assume sinknodes have feeback; # # allow negative feedback; # if self.model == 10:; # plus_minus = (np.random.randint(0,2,n_sinknodes) - 0.5)*2; # self.Adj_signed[sinknodes,sinknodes] = plus_minus; # settings.m(0,leafnodes,availnodes); # parent; # children; # update leafnodes; # update availnodes; # settings.m(0,availnodes); # sett",MatchSource.CODE_COMMENT,src/scanpy/tools/_sim.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_sim.py
Integrability,depend,dependence,"""; # init variables; # step size for saving the figure; # how many files?; # simple vector auto regressive process or; # hill kinetics process simulation; # create instance, set seed; # random topology / for a given edge density; # only consider off-diagonal edges; # check that the coupling matrix does not have eigenvalues; # greater than 1, which would lead to an exploding var process; # init type; # slightly break symmetry in initial conditions; # check branching; # append some zeros; # more complex models; # choose initial conditions such that branchings result; # vary initial conditions around mean; # generate random initial conditions within [0.3,0.7]; # check branching; # load the last simulation file; """"""Write simulated data. Accounts for saving at the same time an ID; and a model file.; """"""; # update file with sample ids; # dimension; # write files with adjacancy and coupling matrices; # due to 'update formulation' of model, there; # is always a diagonal dependence; # write model file; # write coupling via names; # write simulated data; # the binary mode option in the following line is a fix for python 3; # variable names; """"""; Simlulation of stochastic dynamic systems. Main application: simulation of gene expression dynamics. Also standard models are implemented.; """"""; """"""; Params; ------; model; either string for predefined model,; or directory with a model file and a couple matrix files; """"""; # number of nodes / dimension of system; # maximal number of parents; # fraction of independent genes; # string characterizing a specific initial; # checks; # noqa: F841 # TODO FIX; # read from file; # set the coupling matrix, and with that the adjacency matrix; # seed; # header; # params; """"""Simulate the model.""""""; #; # run simulation; # add dynamic noise; """"""Build Xdiff from coefficients of boolean network,; that is, using self.boolCoeff. The employed functions; are Hill type activation and deactivation functions. See Wittmann et al., BMC Syst. Biol. 3, 98 (2009),;",MatchSource.CODE_COMMENT,src/scanpy/tools/_sim.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_sim.py
Modifiability,variab,variables," per realization of time series.; branching; Only write realizations that contain new branches.; nrRealizations; Number of realizations.; noiseObs; Observatory/Measurement noise.; noiseDyn; Dynamic noise.; step; Interval for saving state of system.; seed; Seed for generation of random numbers.; writedir; Path to directory for writing output files. Returns; -------; Annotated data matrix. Examples; --------; See this `use case <https://github.com/scverse/scanpy_usage/tree/master/170430_krumsiek11>`__; """"""; """"""; Update parser with tool specific arguments. This overwrites was is done in utils.uns_args.; """"""; # dictionary for adding arguments; """"""; Helper function.; """"""; # init variables; # step size for saving the figure; # how many files?; # simple vector auto regressive process or; # hill kinetics process simulation; # create instance, set seed; # random topology / for a given edge density; # only consider off-diagonal edges; # check that the coupling matrix does not have eigenvalues; # greater than 1, which would lead to an exploding var process; # init type; # slightly break symmetry in initial conditions; # check branching; # append some zeros; # more complex models; # choose initial conditions such that branchings result; # vary initial conditions around mean; # generate random initial conditions within [0.3,0.7]; # check branching; # load the last simulation file; """"""Write simulated data. Accounts for saving at the same time an ID; and a model file.; """"""; # update file with sample ids; # dimension; # write files with adjacancy and coupling matrices; # due to 'update formulation' of model, there; # is always a diagonal dependence; # write model file; # write coupling via names; # write simulated data; # the binary mode option in the following line is a fix for python 3; # variable names; """"""; Simlulation of stochastic dynamic systems. Main application: simulation of gene expression dynamics. Also standard models are implemented.; """"""; """"""; Params; ------; model; ",MatchSource.CODE_COMMENT,src/scanpy/tools/_sim.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_sim.py
Performance,load,load,"ed data matrix. Examples; --------; See this `use case <https://github.com/scverse/scanpy_usage/tree/master/170430_krumsiek11>`__; """"""; """"""; Update parser with tool specific arguments. This overwrites was is done in utils.uns_args.; """"""; # dictionary for adding arguments; """"""; Helper function.; """"""; # init variables; # step size for saving the figure; # how many files?; # simple vector auto regressive process or; # hill kinetics process simulation; # create instance, set seed; # random topology / for a given edge density; # only consider off-diagonal edges; # check that the coupling matrix does not have eigenvalues; # greater than 1, which would lead to an exploding var process; # init type; # slightly break symmetry in initial conditions; # check branching; # append some zeros; # more complex models; # choose initial conditions such that branchings result; # vary initial conditions around mean; # generate random initial conditions within [0.3,0.7]; # check branching; # load the last simulation file; """"""Write simulated data. Accounts for saving at the same time an ID; and a model file.; """"""; # update file with sample ids; # dimension; # write files with adjacancy and coupling matrices; # due to 'update formulation' of model, there; # is always a diagonal dependence; # write model file; # write coupling via names; # write simulated data; # the binary mode option in the following line is a fix for python 3; # variable names; """"""; Simlulation of stochastic dynamic systems. Main application: simulation of gene expression dynamics. Also standard models are implemented.; """"""; """"""; Params; ------; model; either string for predefined model,; or directory with a model file and a couple matrix files; """"""; # number of nodes / dimension of system; # maximal number of parents; # fraction of independent genes; # string characterizing a specific initial; # checks; # noqa: F841 # TODO FIX; # read from file; # set the coupling matrix, and with that the adjacency matrix; # seed; # he",MatchSource.CODE_COMMENT,src/scanpy/tools/_sim.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_sim.py
Usability,simpl,simple," per realization of time series.; branching; Only write realizations that contain new branches.; nrRealizations; Number of realizations.; noiseObs; Observatory/Measurement noise.; noiseDyn; Dynamic noise.; step; Interval for saving state of system.; seed; Seed for generation of random numbers.; writedir; Path to directory for writing output files. Returns; -------; Annotated data matrix. Examples; --------; See this `use case <https://github.com/scverse/scanpy_usage/tree/master/170430_krumsiek11>`__; """"""; """"""; Update parser with tool specific arguments. This overwrites was is done in utils.uns_args.; """"""; # dictionary for adding arguments; """"""; Helper function.; """"""; # init variables; # step size for saving the figure; # how many files?; # simple vector auto regressive process or; # hill kinetics process simulation; # create instance, set seed; # random topology / for a given edge density; # only consider off-diagonal edges; # check that the coupling matrix does not have eigenvalues; # greater than 1, which would lead to an exploding var process; # init type; # slightly break symmetry in initial conditions; # check branching; # append some zeros; # more complex models; # choose initial conditions such that branchings result; # vary initial conditions around mean; # generate random initial conditions within [0.3,0.7]; # check branching; # load the last simulation file; """"""Write simulated data. Accounts for saving at the same time an ID; and a model file.; """"""; # update file with sample ids; # dimension; # write files with adjacancy and coupling matrices; # due to 'update formulation' of model, there; # is always a diagonal dependence; # write model file; # write coupling via names; # write simulated data; # the binary mode option in the following line is a fix for python 3; # variable names; """"""; Simlulation of stochastic dynamic systems. Main application: simulation of gene expression dynamics. Also standard models are implemented.; """"""; """"""; Params; ------; model; ",MatchSource.CODE_COMMENT,src/scanpy/tools/_sim.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_sim.py
Availability,mask,mask,"ls for advanced gene ranking and exploration of genes; """"""; """"""\; Calculate correlation matrix. Calculate a correlation matrix for genes strored in sample annotation; using :func:`~scanpy.tl.rank_genes_groups`. Parameters; ----------; adata; Annotated data matrix.; name_list; Takes a list of genes for which to calculate the correlation matrix; groupby; If no name list is passed, genes are selected from the; results of rank_gene_groups. Then this is the key of the sample grouping to consider.; Note that in this case also a group index has to be specified.; group; Group index for which the correlation matrix for top_ranked genes should be calculated.; Currently only int is supported, will change very soon; n_genes; For how many genes to calculate correlation matrix? If specified, cuts the name list; (in whatever order it is passed).; data; At the moment, this is only relevant for the case that name_list is drawn from rank_gene_groups results.; If specified, collects mask for the called group and then takes only those cells specified.; If 'Complete', calculate correlation using full data; If 'Group', calculate correlation within the selected group.; If 'Rest', calculate corrlation for everything except the group; method; Which kind of correlation coefficient to use. pearson; standard correlation coefficient; kendall; Kendall Tau correlation coefficient; spearman; Spearman rank correlation; annotation_key; Allows to define the name of the anndata entry where results are stored.; """"""; # TODO: At the moment, only works for int identifiers; # If no genes are passed, selects ranked genes from sample annotation.; # At the moment, only calculate one table (Think about what comes next); # If special method (later) , truncate; # This line just makes group_mask access easier. Nothing else but 'all' will stand here.; # get group_mask; # Distinguish between sparse and non-sparse data; """"""\; Calculate correlation matrix. Calculate a correlation matrix for genes strored in sample ann",MatchSource.CODE_COMMENT,src/scanpy/tools/_top_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_top_genes.py
Energy Efficiency,adapt,adapted,"rix for genes strored in sample annotation. Parameters; ----------; adata; Annotated data matrix.; groupby; The key of the sample grouping to consider.; group; Group name or index for which the correlation matrix for top ranked; genes should be calculated.; If no parameter is passed, ROC/AUC is calculated for all groups; n_genes; For how many genes to calculate ROC and AUC. If no parameter is passed,; calculation is done for all stored top ranked genes.; """"""; # TODO: Loop over all groups instead of just taking one.; # Assume group takes an int value for one group for the moment.; # TODO: For the moment, see that everything works for comparison against the rest. Resolve issues later.; # Use usual convention, better for looping later.; # TODO: Allow for sample weighting requires better mask access... later; # We store calculated data in dict, access it via dict to dict. Check if this is the best way.; # Simple method that can be called by rank_gene_group. It uses masks that have been passed to the function and; # calculates how much has to be subsampled in order to reach a certain precision with a certain probability; # Then it subsamples for mask, mask rest; # Since convergence speed varies, we take the slower one, i.e. the variance. This might have future speed-up; # potential; # TODO: DO precision calculation for mean variance shared; # TODO: Subsample; # This tool has the purpose to take a set of genes (possibly already pre-selected) and analyze AUC.; # Those and only those are eliminated who are dominated completely; # TODO: Potentially (But not till tomorrow), this can be adapted to only consider the AUC in the given; # TODO: optimization frame; # This tool serves to; # It is not thought to be addressed directly but rather using rank_genes_group or ROC analysis or comparable; # TODO: Pass back a truncated adata object with only those genes that fullfill thresholding criterias; # This function should be accessible by both rank_genes_groups and ROC_curve analysis",MatchSource.CODE_COMMENT,src/scanpy/tools/_top_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_top_genes.py
Modifiability,adapt,adapted,"rix for genes strored in sample annotation. Parameters; ----------; adata; Annotated data matrix.; groupby; The key of the sample grouping to consider.; group; Group name or index for which the correlation matrix for top ranked; genes should be calculated.; If no parameter is passed, ROC/AUC is calculated for all groups; n_genes; For how many genes to calculate ROC and AUC. If no parameter is passed,; calculation is done for all stored top ranked genes.; """"""; # TODO: Loop over all groups instead of just taking one.; # Assume group takes an int value for one group for the moment.; # TODO: For the moment, see that everything works for comparison against the rest. Resolve issues later.; # Use usual convention, better for looping later.; # TODO: Allow for sample weighting requires better mask access... later; # We store calculated data in dict, access it via dict to dict. Check if this is the best way.; # Simple method that can be called by rank_gene_group. It uses masks that have been passed to the function and; # calculates how much has to be subsampled in order to reach a certain precision with a certain probability; # Then it subsamples for mask, mask rest; # Since convergence speed varies, we take the slower one, i.e. the variance. This might have future speed-up; # potential; # TODO: DO precision calculation for mean variance shared; # TODO: Subsample; # This tool has the purpose to take a set of genes (possibly already pre-selected) and analyze AUC.; # Those and only those are eliminated who are dominated completely; # TODO: Potentially (But not till tomorrow), this can be adapted to only consider the AUC in the given; # TODO: optimization frame; # This tool serves to; # It is not thought to be addressed directly but rather using rank_genes_group or ROC analysis or comparable; # TODO: Pass back a truncated adata object with only those genes that fullfill thresholding criterias; # This function should be accessible by both rank_genes_groups and ROC_curve analysis",MatchSource.CODE_COMMENT,src/scanpy/tools/_top_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_top_genes.py
Performance,optimiz,optimization,"rix for genes strored in sample annotation. Parameters; ----------; adata; Annotated data matrix.; groupby; The key of the sample grouping to consider.; group; Group name or index for which the correlation matrix for top ranked; genes should be calculated.; If no parameter is passed, ROC/AUC is calculated for all groups; n_genes; For how many genes to calculate ROC and AUC. If no parameter is passed,; calculation is done for all stored top ranked genes.; """"""; # TODO: Loop over all groups instead of just taking one.; # Assume group takes an int value for one group for the moment.; # TODO: For the moment, see that everything works for comparison against the rest. Resolve issues later.; # Use usual convention, better for looping later.; # TODO: Allow for sample weighting requires better mask access... later; # We store calculated data in dict, access it via dict to dict. Check if this is the best way.; # Simple method that can be called by rank_gene_group. It uses masks that have been passed to the function and; # calculates how much has to be subsampled in order to reach a certain precision with a certain probability; # Then it subsamples for mask, mask rest; # Since convergence speed varies, we take the slower one, i.e. the variance. This might have future speed-up; # potential; # TODO: DO precision calculation for mean variance shared; # TODO: Subsample; # This tool has the purpose to take a set of genes (possibly already pre-selected) and analyze AUC.; # Those and only those are eliminated who are dominated completely; # TODO: Potentially (But not till tomorrow), this can be adapted to only consider the AUC in the given; # TODO: optimization frame; # This tool serves to; # It is not thought to be addressed directly but rather using rank_genes_group or ROC analysis or comparable; # TODO: Pass back a truncated adata object with only those genes that fullfill thresholding criterias; # This function should be accessible by both rank_genes_groups and ROC_curve analysis",MatchSource.CODE_COMMENT,src/scanpy/tools/_top_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_top_genes.py
Security,access,access,"n; n_genes; For how many genes to calculate correlation matrix? If specified, cuts the name list; (in whatever order it is passed).; data; At the moment, this is only relevant for the case that name_list is drawn from rank_gene_groups results.; If specified, collects mask for the called group and then takes only those cells specified.; If 'Complete', calculate correlation using full data; If 'Group', calculate correlation within the selected group.; If 'Rest', calculate corrlation for everything except the group; method; Which kind of correlation coefficient to use. pearson; standard correlation coefficient; kendall; Kendall Tau correlation coefficient; spearman; Spearman rank correlation; annotation_key; Allows to define the name of the anndata entry where results are stored.; """"""; # TODO: At the moment, only works for int identifiers; # If no genes are passed, selects ranked genes from sample annotation.; # At the moment, only calculate one table (Think about what comes next); # If special method (later) , truncate; # This line just makes group_mask access easier. Nothing else but 'all' will stand here.; # get group_mask; # Distinguish between sparse and non-sparse data; """"""\; Calculate correlation matrix. Calculate a correlation matrix for genes strored in sample annotation. Parameters; ----------; adata; Annotated data matrix.; groupby; The key of the sample grouping to consider.; group; Group name or index for which the correlation matrix for top ranked; genes should be calculated.; If no parameter is passed, ROC/AUC is calculated for all groups; n_genes; For how many genes to calculate ROC and AUC. If no parameter is passed,; calculation is done for all stored top ranked genes.; """"""; # TODO: Loop over all groups instead of just taking one.; # Assume group takes an int value for one group for the moment.; # TODO: For the moment, see that everything works for comparison against the rest. Resolve issues later.; # Use usual convention, better for looping later.; # ",MatchSource.CODE_COMMENT,src/scanpy/tools/_top_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_top_genes.py
Deployability,install,install,"""""""\; t-SNE :cite:p:`vanDerMaaten2008,Amir2013,Pedregosa2011`. t-distributed stochastic neighborhood embedding (tSNE, :cite:t:`vanDerMaaten2008`) has been; proposed for visualizating single-cell data by :cite:t:`Amir2013`. Here, by default,; we use the implementation of *scikit-learn* :cite:p:`Pedregosa2011`. You can achieve; a huge speedup and better convergence if you install Multicore-tSNE_; by :cite:t:`Ulyanov2016`, which will be automatically detected by Scanpy. .. _multicore-tsne: https://github.com/DmitryUlyanov/Multicore-TSNE. Parameters; ----------; adata; Annotated data matrix.; {doc_n_pcs}; {use_rep}; perplexity; The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter.; metric; Distance metric calculate neighbors on.; early_exaggeration; Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high.; learning_rate; Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes.; random_state; Change this to use different intial states for the optimization.; If `None`, the initial state is not reproducible.; n_jobs; Number of jobs for parallel computation.; `None` means u",MatchSource.CODE_COMMENT,src/scanpy/tools/_tsne.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_tsne.py
Performance,optimiz,optimization,"tter convergence if you install Multicore-tSNE_; by :cite:t:`Ulyanov2016`, which will be automatically detected by Scanpy. .. _multicore-tsne: https://github.com/DmitryUlyanov/Multicore-TSNE. Parameters; ----------; adata; Annotated data matrix.; {doc_n_pcs}; {use_rep}; perplexity; The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter.; metric; Distance metric calculate neighbors on.; early_exaggeration; Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high.; learning_rate; Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes.; random_state; Change this to use different intial states for the optimization.; If `None`, the initial state is not reproducible.; n_jobs; Number of jobs for parallel computation.; `None` means using :attr:`scanpy._settings.ScanpyConfig.n_jobs`.; copy; Return a copy instead of writing to `adata`. Returns; -------; Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:. `adata.obsm['X_tsne']` : :class:`numpy.ndarray` (dtype `float`); tSNE coordinates of data.; `adata.uns['tsne']` : :class:`dict`; tSNE ",MatchSource.CODE_COMMENT,src/scanpy/tools/_tsne.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_tsne.py
Safety,detect,detected,"""""""\; t-SNE :cite:p:`vanDerMaaten2008,Amir2013,Pedregosa2011`. t-distributed stochastic neighborhood embedding (tSNE, :cite:t:`vanDerMaaten2008`) has been; proposed for visualizating single-cell data by :cite:t:`Amir2013`. Here, by default,; we use the implementation of *scikit-learn* :cite:p:`Pedregosa2011`. You can achieve; a huge speedup and better convergence if you install Multicore-tSNE_; by :cite:t:`Ulyanov2016`, which will be automatically detected by Scanpy. .. _multicore-tsne: https://github.com/DmitryUlyanov/Multicore-TSNE. Parameters; ----------; adata; Annotated data matrix.; {doc_n_pcs}; {use_rep}; perplexity; The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter.; metric; Distance metric calculate neighbors on.; early_exaggeration; Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high.; learning_rate; Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes.; random_state; Change this to use different intial states for the optimization.; If `None`, the initial state is not reproducible.; n_jobs; Number of jobs for parallel computation.; `None` means u",MatchSource.CODE_COMMENT,src/scanpy/tools/_tsne.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_tsne.py
Usability,learn,learn,"""""""\; t-SNE :cite:p:`vanDerMaaten2008,Amir2013,Pedregosa2011`. t-distributed stochastic neighborhood embedding (tSNE, :cite:t:`vanDerMaaten2008`) has been; proposed for visualizating single-cell data by :cite:t:`Amir2013`. Here, by default,; we use the implementation of *scikit-learn* :cite:p:`Pedregosa2011`. You can achieve; a huge speedup and better convergence if you install Multicore-tSNE_; by :cite:t:`Ulyanov2016`, which will be automatically detected by Scanpy. .. _multicore-tsne: https://github.com/DmitryUlyanov/Multicore-TSNE. Parameters; ----------; adata; Annotated data matrix.; {doc_n_pcs}; {use_rep}; perplexity; The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter.; metric; Distance metric calculate neighbors on.; early_exaggeration; Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high.; learning_rate; Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes.; random_state; Change this to use different intial states for the optimization.; If `None`, the initial state is not reproducible.; n_jobs; Number of jobs for parallel computation.; `None` means u",MatchSource.CODE_COMMENT,src/scanpy/tools/_tsne.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_tsne.py
Performance,optimiz,optimizes,"""""""\; Embed the neighborhood graph using UMAP :cite:p:`McInnes2018`. UMAP (Uniform Manifold Approximation and Projection) is a manifold learning; technique suitable for visualizing high-dimensional data. Besides tending to; be faster than tSNE, it optimizes the embedding such that it best reflects; the topology of the data, which we represent throughout Scanpy using a; neighborhood graph. tSNE, by contrast, optimizes the distribution of; nearest-neighbor distances in the embedding such that these best match the; distribution of distances in the high-dimensional space.; We use the implementation of umap-learn_ :cite:p:`McInnes2018`.; For a few comparisons of UMAP with tSNE, see :cite:t:`Becht2018`. .. _umap-learn: https://github.com/lmcinnes/umap. Parameters; ----------; adata; Annotated data matrix.; min_dist; The effective minimum distance between embedded points. Smaller values; will result in a more clustered/clumped embedding where nearby points on; the manifold are drawn closer together, while larger values will result; on a more even dispersal of points. The value should be set relative to; the ``spread`` value, which determines the scale at which embedded; points will be spread out. The default of in the `umap-learn` package is; 0.1.; spread; The effective scale of embedded points. In combination with `min_dist`; this determines how clustered/clumped the embedded points are.; n_components; The number of dimensions of the embedding.; maxiter; The number of iterations (epochs) of the optimization. Called `n_epochs`; in the original UMAP.; alpha; The initial learning rate for the embedding optimization.; gamma; Weighting applied to negative samples in low dimensional embedding; optimization. Values higher than one will result in greater weight; being given to negative samples.; negative_sample_rate; The number of negative edge/1-simplex samples to use per positive; edge/1-simplex sample in optimizing the low dimensional embedding.; init_pos; How to initialize the",MatchSource.CODE_COMMENT,src/scanpy/tools/_umap.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_umap.py
Usability,learn,learning,"""""""\; Embed the neighborhood graph using UMAP :cite:p:`McInnes2018`. UMAP (Uniform Manifold Approximation and Projection) is a manifold learning; technique suitable for visualizing high-dimensional data. Besides tending to; be faster than tSNE, it optimizes the embedding such that it best reflects; the topology of the data, which we represent throughout Scanpy using a; neighborhood graph. tSNE, by contrast, optimizes the distribution of; nearest-neighbor distances in the embedding such that these best match the; distribution of distances in the high-dimensional space.; We use the implementation of umap-learn_ :cite:p:`McInnes2018`.; For a few comparisons of UMAP with tSNE, see :cite:t:`Becht2018`. .. _umap-learn: https://github.com/lmcinnes/umap. Parameters; ----------; adata; Annotated data matrix.; min_dist; The effective minimum distance between embedded points. Smaller values; will result in a more clustered/clumped embedding where nearby points on; the manifold are drawn closer together, while larger values will result; on a more even dispersal of points. The value should be set relative to; the ``spread`` value, which determines the scale at which embedded; points will be spread out. The default of in the `umap-learn` package is; 0.1.; spread; The effective scale of embedded points. In combination with `min_dist`; this determines how clustered/clumped the embedded points are.; n_components; The number of dimensions of the embedding.; maxiter; The number of iterations (epochs) of the optimization. Called `n_epochs`; in the original UMAP.; alpha; The initial learning rate for the embedding optimization.; gamma; Weighting applied to negative samples in low dimensional embedding; optimization. Values higher than one will result in greater weight; being given to negative samples.; negative_sample_rate; The number of negative edge/1-simplex samples to use per positive; edge/1-simplex sample in optimizing the low dimensional embedding.; init_pos; How to initialize the",MatchSource.CODE_COMMENT,src/scanpy/tools/_umap.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/tools/_umap.py
Integrability,depend,dependency,"""""""Mark function with doctest dependency.""""""; """"""Mark function so doctest is skipped.""""""; """"""Mark function so doctest gets the internet mark.""""""",MatchSource.CODE_COMMENT,src/scanpy/_utils/_doctests.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_utils/_doctests.py
Availability,mask,masked,"n the future; """"""Transform string annotations to categoricals.""""""; """"""Moving average over one-dimensional array. Parameters; ----------; a; One-dimensional array.; n; Number of entries to average over. n=2 means averaging over the currrent; the previous entry. Returns; -------; An array view storing the moving average.; """"""; # --------------------------------------------------------------------------------; # Deal with tool parameters; # --------------------------------------------------------------------------------; """"""\; Update old_params with new_params. If check==False, this merely adds and overwrites the content of old_params. If check==True, this only allows updating of parameters that are already; present in old_params. Parameters; ----------; old_params; new_params; check. Returns; -------; updated_params; """"""; # allow for new_params to be None; # --------------------------------------------------------------------------------; # Others; # --------------------------------------------------------------------------------; # returns coo_matrix, so cast back to input type; # masked operations on sparse produce which numpy matrices gives the same API issues handled here; # returns a np.matrix normally, which is undesireable; """"""Checks values of X to ensure it is count data""""""; # Check no negatives; # Check all are integers; """"""Get subset of groups in adata.obs[key].""""""; # if the name is not found, fallback to index retrieval; # fallback to index retrieval; # noqa: PLR0917; """"""Get full tracebacks when warning is raised by setting. warnings.showwarning = warn_with_traceback. See also; --------; https://stackoverflow.com/questions/22373927/get-traceback-of-warnings; """"""; # noqa: F841 # TODO Does this need fixing?; # You'd think `'once'` works, but it doesn't at the repl and in notebooks; """"""\; Subsample a fraction of 1/subsample samples from the rows of X. Parameters; ----------; X; Data array.; subsample; 1/subsample is the fraction of data sampled, n = X.shape[0]/",MatchSource.CODE_COMMENT,src/scanpy/_utils/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_utils/__init__.py
Safety,predict,prediction," should start with ``\\`` in the first line for proper formatting.; """"""; """"""Checks for invalid arguments when an array is passed. Helper for functions that work on either AnnData objects or array-likes.; """"""; # TODO: Figure out a better solution for documenting dispatched functions; """"""; Normalize checking `use_raw`. My intentention here is to also provide a single place to throw a deprecation warning from in future.; """"""; # --------------------------------------------------------------------------------; # Graph stuff; # --------------------------------------------------------------------------------; """"""Get igraph graph from adjacency matrix.""""""; # this adds adjacency.shape[0] vertices; # --------------------------------------------------------------------------------; # Group stuff; # --------------------------------------------------------------------------------; """"""Compute overlaps between groups. See ``identify_groups`` for identifying the groups. Parameters; ----------; adata; prediction; Field name of adata.obs.; reference; Field name of adata.obs.; normalization; Whether to normalize with respect to the predicted groups or the; reference groups.; threshold; Do not consider associations whose overlap is below this fraction.; max_n_names; Control how many reference names you want to be associated with per; predicted name. Set to `None`, if you want all. Returns; -------; asso_names; List of associated reference names; (`max_n_names` for each predicted name).; asso_matrix; Matrix where rows correspond to the predicted labels and columns to the; reference labels, entries are proportional to degree of association.; """"""; # starting from numpy version 1.13, subtractions of boolean arrays are deprecated; # e.g. if the pred group is contained in mask_ref, mask_ref and; # mask_ref_or_pred are the same; # compute which fraction of the predicted group is contained in; # the ref group; # compute which fraction of the reference group is contained in; # the predicted gro",MatchSource.CODE_COMMENT,src/scanpy/_utils/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_utils/__init__.py
Security,access,access,"warnings.showwarning = warn_with_traceback. See also; --------; https://stackoverflow.com/questions/22373927/get-traceback-of-warnings; """"""; # noqa: F841 # TODO Does this need fixing?; # You'd think `'once'` works, but it doesn't at the repl and in notebooks; """"""\; Subsample a fraction of 1/subsample samples from the rows of X. Parameters; ----------; X; Data array.; subsample; 1/subsample is the fraction of data sampled, n = X.shape[0]/subsample.; seed; Seed for sampling. Returns; -------; Xsampled; Subsampled X.; rows; Indices of rows that are stored in Xsampled.; """"""; # this sequence is defined simply by skipping rows; # is faster than sampling; """"""Subsample n samples from rows of array. Parameters; ----------; X; Data array.; n; Sample size.; seed; Seed for sampling. Returns; -------; Xsampled; Subsampled X.; rows; Indices of rows that are stored in Xsampled.; """"""; """"""Check if file is present otherwise download.""""""; """"""Imports a module in a way that it’s only executed on member access""""""; # Make module with proper locking and get it inserted into sys.modules.; # --------------------------------------------------------------------------------; # Neighbors; # --------------------------------------------------------------------------------; """"""Convenience class for accessing neighbors graph representations. Allows to access neighbors distances, connectivities and settings; dictionary in a uniform manner. Parameters; ----------. adata; AnnData object.; key; This defines where to look for neighbors dictionary,; connectivities, distances. neigh = NeighborsView(adata, key); neigh['distances']; neigh['connectivities']; neigh['params']; 'connectivities' in neigh; 'params' in neigh. is the same as. adata.obsp[adata.uns[key]['distances_key']]; adata.obsp[adata.uns[key]['connectivities_key']]; adata.uns[key]['params']; adata.uns[key]['connectivities_key'] in adata.obsp; 'params' in adata.uns[key]; """"""; # fallback to uns; """"""Choose connectivities from neighbbors or another o",MatchSource.CODE_COMMENT,src/scanpy/_utils/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_utils/__init__.py
Usability,learn,learn,"""""""Utility functions and classes. This file largely consists of the old _utils.py file. Over time, these functions; should be moved of this file.; """"""; # noqa: F401; # e.g. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html; # maybe in the future random.Generator; """"""; Random number generator for ipgraph so global seed is not changed.; See :func:`igraph.set_random_number_generator` for the requirements.; """"""; # Python’s import mechanism seems to add this to `scanpy`’s attributes; """"""\; Docstrings should start with ``\\`` in the first line for proper formatting.; """"""; """"""Checks for invalid arguments when an array is passed. Helper for functions that work on either AnnData objects or array-likes.; """"""; # TODO: Figure out a better solution for documenting dispatched functions; """"""; Normalize checking `use_raw`. My intentention here is to also provide a single place to throw a deprecation warning from in future.; """"""; # --------------------------------------------------------------------------------; # Graph stuff; # --------------------------------------------------------------------------------; """"""Get igraph graph from adjacency matrix.""""""; # this adds adjacency.shape[0] vertices; # --------------------------------------------------------------------------------; # Group stuff; # --------------------------------------------------------------------------------; """"""Compute overlaps between groups. See ``identify_groups`` for identifying the groups. Parameters; ----------; adata; prediction; Field name of adata.obs.; reference; Field name of adata.obs.; normalization; Whether to normalize with respect to the predicted groups or the; reference groups.; threshold; Do not consider associations whose overlap is below this fraction.; max_n_names; Control how many reference names you want to be associated with per; predicted name. Set to `None`, if you want all. Returns; -------; asso_names; List of associated reference names; (`max_n_names` for e",MatchSource.CODE_COMMENT,src/scanpy/_utils/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_utils/__init__.py
Energy Efficiency,reduce,reduce,"""""""; Check whether values in array are constant. Params; ------; a; Array to check; axis; Axis to reduce over. Returns; -------; Boolean array, True values were constant. Example; -------. >>> a = np.array([[0, 1], [0, 0]]); >>> a; array([[0, 1],; [0, 0]]); >>> is_constant(a); False; >>> is_constant(a, axis=0); array([ True, False]); >>> is_constant(a, axis=1); array([False, True]); """"""; # Should eventually support nd, not now.; # TODO: use overlapping blocks and reduction instead of `drop_axis`",MatchSource.CODE_COMMENT,src/scanpy/_utils/compute/is_constant.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/scanpy/_utils/compute/is_constant.py
Availability,error,error,"""""""; This file contains helper functions for the scanpy test suite.; """"""; # TODO: Report more context on the fields being compared on error; # TODO: Allow specifying paths to ignore on comparison; ###########################; # Representation choice; ###########################; # These functions can be used to check that functions are correctly using arugments like `layers`, `obsm`, etc.; """"""Constructor for anndata that uses dtype of X for test compatibility with older versions of AnnData. Once the minimum version of AnnData is 0.9, this function can be replaced with the default constructor.; """"""; """"""Check that only the array meant to be modified is modified.""""""; # Modified fields; # Unmodified fields; """"""Checks that the results of a computation add values/ mutate the anndata object in a consistent way.""""""; # Gen data; # Apply function; # Reset X; """"""; Runs `function` on `adata` with provided arguments `kwargs` twice:; once with `check_values=True` and once with `check_values=False`.; Checks that the `expected_warning` is only raised whtn `check_values=True`.; """"""; # expecting 0 no-int warnings; # expecting 1 no-int warning; # Delayed imports for case where we aren't using dask",MatchSource.CODE_COMMENT,src/testing/scanpy/_helpers/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/testing/scanpy/_helpers/__init__.py
Modifiability,layers,layers,"""""""; This file contains helper functions for the scanpy test suite.; """"""; # TODO: Report more context on the fields being compared on error; # TODO: Allow specifying paths to ignore on comparison; ###########################; # Representation choice; ###########################; # These functions can be used to check that functions are correctly using arugments like `layers`, `obsm`, etc.; """"""Constructor for anndata that uses dtype of X for test compatibility with older versions of AnnData. Once the minimum version of AnnData is 0.9, this function can be replaced with the default constructor.; """"""; """"""Check that only the array meant to be modified is modified.""""""; # Modified fields; # Unmodified fields; """"""Checks that the results of a computation add values/ mutate the anndata object in a consistent way.""""""; # Gen data; # Apply function; # Reset X; """"""; Runs `function` on `adata` with provided arguments `kwargs` twice:; once with `check_values=True` and once with `check_values=False`.; Checks that the `expected_warning` is only raised whtn `check_values=True`.; """"""; # expecting 0 no-int warnings; # expecting 1 no-int warning; # Delayed imports for case where we aren't using dask",MatchSource.CODE_COMMENT,src/testing/scanpy/_helpers/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/testing/scanpy/_helpers/__init__.py
Testability,test,test,"""""""; This file contains helper functions for the scanpy test suite.; """"""; # TODO: Report more context on the fields being compared on error; # TODO: Allow specifying paths to ignore on comparison; ###########################; # Representation choice; ###########################; # These functions can be used to check that functions are correctly using arugments like `layers`, `obsm`, etc.; """"""Constructor for anndata that uses dtype of X for test compatibility with older versions of AnnData. Once the minimum version of AnnData is 0.9, this function can be replaced with the default constructor.; """"""; """"""Check that only the array meant to be modified is modified.""""""; # Modified fields; # Unmodified fields; """"""Checks that the results of a computation add values/ mutate the anndata object in a consistent way.""""""; # Gen data; # Apply function; # Reset X; """"""; Runs `function` on `adata` with provided arguments `kwargs` twice:; once with `check_values=True` and once with `check_values=False`.; Checks that the `expected_warning` is only raised whtn `check_values=True`.; """"""; # expecting 0 no-int warnings; # expecting 1 no-int warning; # Delayed imports for case where we aren't using dask",MatchSource.CODE_COMMENT,src/testing/scanpy/_helpers/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/testing/scanpy/_helpers/__init__.py
Testability,test,tests,"""""""Distribution name for matching modules""""""; """"""; Pytest skip marker evaluated at module import. This allows us to see the amount of skipped tests at the start of a test run.; :func:`pytest.importorskip` skips tests after they started running.; """"""; # _generate_next_value_ needs to come before members, also it’s finnicky:; # https://github.com/python/mypy/issues/7591#issuecomment-652800625; # external",MatchSource.CODE_COMMENT,src/testing/scanpy/_pytest/marks.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/testing/scanpy/_pytest/marks.py
Modifiability,flexible,flexible,"""""""Like fixtures, but more flexible""""""; # probably not necessary to also do csc",MatchSource.CODE_COMMENT,src/testing/scanpy/_pytest/params.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/testing/scanpy/_pytest/params.py
Modifiability,plugin,plugin,"""""""A private pytest plugin""""""; # noqa: F403; # Defining it here because it’s autouse.; """"""Switch to agg backend, reset settings, and close all figures at teardown.""""""; # make sure seaborn is imported and did its thing; # noqa: F401; """"""Limit number of threads used per worker when using pytest-xdist. Prevents oversubscription of the CPU when multiple tests with parallel code are; running at once.; """"""; # All tests marked with `pytest.mark.internet` get skipped unless; # `--run-internet` passed; # Dask AnnData tests require anndata > 0.10",MatchSource.CODE_COMMENT,src/testing/scanpy/_pytest/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/testing/scanpy/_pytest/__init__.py
Testability,test,tests,"""""""A private pytest plugin""""""; # noqa: F403; # Defining it here because it’s autouse.; """"""Switch to agg backend, reset settings, and close all figures at teardown.""""""; # make sure seaborn is imported and did its thing; # noqa: F401; """"""Limit number of threads used per worker when using pytest-xdist. Prevents oversubscription of the CPU when multiple tests with parallel code are; running at once.; """"""; # All tests marked with `pytest.mark.internet` get skipped unless; # `--run-internet` passed; # Dask AnnData tests require anndata > 0.10",MatchSource.CODE_COMMENT,src/testing/scanpy/_pytest/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/testing/scanpy/_pytest/__init__.py
Availability,error,errors,"""""""This file contains some common fixtures for use in tests. This is kept seperate from the helpers file because it relies on pytest.; """"""; # noqa: PLR0917; # make errors visible and the rest ignored",MatchSource.CODE_COMMENT,src/testing/scanpy/_pytest/fixtures/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/testing/scanpy/_pytest/fixtures/__init__.py
Testability,test,tests,"""""""This file contains some common fixtures for use in tests. This is kept seperate from the helpers file because it relies on pytest.; """"""; # noqa: PLR0917; # make errors visible and the rest ignored",MatchSource.CODE_COMMENT,src/testing/scanpy/_pytest/fixtures/__init__.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/src/testing/scanpy/_pytest/fixtures/__init__.py
Testability,test,tests,"# just import for the IMPORTED check; # noqa: F401; # So editors understand that we’re using those fixtures; # noqa: F403; # define this after importing scanpy but before running tests; """"""Remove handlers from all loggers on session teardown. Fixes <https://github.com/scverse/scanpy/issues/1736>.; See also <https://github.com/pytest-dev/pytest/issues/5502>.; """"""; # loggerDict can contain `logging.Placeholder`s; """"""Allow use of scanpy’s logger with caplog""""""; """"""\; Image files did not match.; RMS Value: {rms}; Expected: {expected}; Actual: {actual}; Difference: {diff}; Tolerance: {tol}; """"""",MatchSource.CODE_COMMENT,tests/conftest.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/conftest.py
Availability,error,error,"""""""Ensure the two implementations are the same for the same args.""""""; """"""Ensure that the old leidenalg defaults are close enough to the current default outputs.""""""; """"""Ensure that popping this as a `clustering_kwargs` and using it does not error out.""""""; # Get new clustering labels; # Only original cluster's cells assigned to new categories; # Original category's cells assigned only to new categories; # Run clustering with default key, then custom keys; # ensure that all clustering parameters are added to user provided keys and not overwritten",MatchSource.CODE_COMMENT,tests/test_clustering.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_clustering.py
Performance,load,load,"# this test trivially checks whether mean normalisation worked; # load in data; # construct a pandas series of the batch annotation; # standardize the data; # Test for fix to #1170; # Non-unique index; # this test checks wether combat can align data from several gaussians; # it checks this by computing the silhouette coefficient in a pca embedding; # load in data; # apply combat; # compute pca; # compute silhouette coefficient in pca",MatchSource.CODE_COMMENT,tests/test_combat.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_combat.py
Testability,test,test,"# this test trivially checks whether mean normalisation worked; # load in data; # construct a pandas series of the batch annotation; # standardize the data; # Test for fix to #1170; # Non-unique index; # this test checks wether combat can align data from several gaussians; # it checks this by computing the silhouette coefficient in a pca embedding; # load in data; # apply combat; # compute pca; # compute silhouette coefficient in pca",MatchSource.CODE_COMMENT,tests/test_combat.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_combat.py
Availability,down,downloading,"""""""; Tests to make sure the example datasets load.; """"""; # https://foss.heptapod.net/openpyxl/openpyxl/-/issues/2051; # The shape changes sometimes; """"""Tests that reading/ downloading works and is does not have global effects.""""""; """"""Test that changing the dataset dir doesn't break reading.""""""; """"""Test that image download works and is does not have global effects.""""""; # Test that downloading tissue image works; # Test that tissue image exists and is a valid image file; # Test that tissue image is a tif image file (using `file`); # make process output string; # These are tested via doctest; # These have parameters that affect shape and so on; # Additional marks for datasets besides “internet”",MatchSource.CODE_COMMENT,tests/test_datasets.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_datasets.py
Performance,load,load,"""""""; Tests to make sure the example datasets load.; """"""; # https://foss.heptapod.net/openpyxl/openpyxl/-/issues/2051; # The shape changes sometimes; """"""Tests that reading/ downloading works and is does not have global effects.""""""; """"""Test that changing the dataset dir doesn't break reading.""""""; """"""Test that image download works and is does not have global effects.""""""; # Test that downloading tissue image works; # Test that tissue image exists and is a valid image file; # Test that tissue image is a tif image file (using `file`); # make process output string; # These are tested via doctest; # These have parameters that affect shape and so on; # Additional marks for datasets besides “internet”",MatchSource.CODE_COMMENT,tests/test_datasets.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_datasets.py
Testability,test,tested,"""""""; Tests to make sure the example datasets load.; """"""; # https://foss.heptapod.net/openpyxl/openpyxl/-/issues/2051; # The shape changes sometimes; """"""Tests that reading/ downloading works and is does not have global effects.""""""; """"""Test that changing the dataset dir doesn't break reading.""""""; """"""Test that image download works and is does not have global effects.""""""; # Test that downloading tissue image works; # Test that tissue image exists and is a valid image file; # Test that tissue image is a tif image file (using `file`); # make process output string; # These are tested via doctest; # These have parameters that affect shape and so on; # Additional marks for datasets besides “internet”",MatchSource.CODE_COMMENT,tests/test_datasets.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_datasets.py
Availability,error,error,"# Test that density values are scaled; # Test that the highest value is in the middle for a grid layout; # Test that sc.pl.embedding_density() runs without error",MatchSource.CODE_COMMENT,tests/test_embedding_density.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_embedding_density.py
Deployability,continuous,continuous,"""""""A bit cute.""""""; # Just using to calculate the hex coords; # Adding some missing values; """"""Returns a Request object. Allows you to access names of parameterized tests from within a test.; """"""; # Passing through a dict so it's easier to use default values; # Passing through a dict so it's easier to use default values; # making a copy so colors aren't saved; # TODO: Deprecate components kwarg; # with pytest.warns(FutureWarning, match=r""components .* deprecated""):; # Spatial specific; # standard visium data; # default values; # Points default to transparent if an image is included; # visium coordinates but image empty; # general coordinates; # spatial data don't have imgs, so remove entry from uns; # Required argument for now; # category; # continuous; # external image; # Shape of the image for particular fixture, should not be hardcoded like this; """"""; Tests that manually passing values to sc.pl.spatial is similar to automatic extraction.; """"""; # Has no meaning when there is no image; # These arguments don't make sense for this check; """"""; Check that na_color defaults to transparent when an image is present, light gray when not.; """"""",MatchSource.CODE_COMMENT,tests/test_embedding_plots.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_embedding_plots.py
Modifiability,parameteriz,parameterized,"""""""A bit cute.""""""; # Just using to calculate the hex coords; # Adding some missing values; """"""Returns a Request object. Allows you to access names of parameterized tests from within a test.; """"""; # Passing through a dict so it's easier to use default values; # Passing through a dict so it's easier to use default values; # making a copy so colors aren't saved; # TODO: Deprecate components kwarg; # with pytest.warns(FutureWarning, match=r""components .* deprecated""):; # Spatial specific; # standard visium data; # default values; # Points default to transparent if an image is included; # visium coordinates but image empty; # general coordinates; # spatial data don't have imgs, so remove entry from uns; # Required argument for now; # category; # continuous; # external image; # Shape of the image for particular fixture, should not be hardcoded like this; """"""; Tests that manually passing values to sc.pl.spatial is similar to automatic extraction.; """"""; # Has no meaning when there is no image; # These arguments don't make sense for this check; """"""; Check that na_color defaults to transparent when an image is present, light gray when not.; """"""",MatchSource.CODE_COMMENT,tests/test_embedding_plots.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_embedding_plots.py
Security,access,access,"""""""A bit cute.""""""; # Just using to calculate the hex coords; # Adding some missing values; """"""Returns a Request object. Allows you to access names of parameterized tests from within a test.; """"""; # Passing through a dict so it's easier to use default values; # Passing through a dict so it's easier to use default values; # making a copy so colors aren't saved; # TODO: Deprecate components kwarg; # with pytest.warns(FutureWarning, match=r""components .* deprecated""):; # Spatial specific; # standard visium data; # default values; # Points default to transparent if an image is included; # visium coordinates but image empty; # general coordinates; # spatial data don't have imgs, so remove entry from uns; # Required argument for now; # category; # continuous; # external image; # Shape of the image for particular fixture, should not be hardcoded like this; """"""; Tests that manually passing values to sc.pl.spatial is similar to automatic extraction.; """"""; # Has no meaning when there is no image; # These arguments don't make sense for this check; """"""; Check that na_color defaults to transparent when an image is present, light gray when not.; """"""",MatchSource.CODE_COMMENT,tests/test_embedding_plots.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_embedding_plots.py
Testability,test,tests,"""""""A bit cute.""""""; # Just using to calculate the hex coords; # Adding some missing values; """"""Returns a Request object. Allows you to access names of parameterized tests from within a test.; """"""; # Passing through a dict so it's easier to use default values; # Passing through a dict so it's easier to use default values; # making a copy so colors aren't saved; # TODO: Deprecate components kwarg; # with pytest.warns(FutureWarning, match=r""components .* deprecated""):; # Spatial specific; # standard visium data; # default values; # Points default to transparent if an image is included; # visium coordinates but image empty; # general coordinates; # spatial data don't have imgs, so remove entry from uns; # Required argument for now; # category; # continuous; # external image; # Shape of the image for particular fixture, should not be hardcoded like this; """"""; Tests that manually passing values to sc.pl.spatial is similar to automatic extraction.; """"""; # Has no meaning when there is no image; # These arguments don't make sense for this check; """"""; Check that na_color defaults to transparent when an image is present, light gray when not.; """"""",MatchSource.CODE_COMMENT,tests/test_embedding_plots.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_embedding_plots.py
Testability,test,test,"# fix filter defaults; # test compare_abs",MatchSource.CODE_COMMENT,tests/test_filter_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_filter_rank_genes_groups.py
Availability,error,error,"# Override so warning gets caught; """"""; adata.X is np.ones((2, 2)); adata.layers['double'] is sparse np.ones((2,2)) * 2 to also test sparse matrices; """"""; ########################; # obs_df, var_df tests #; ########################; # make raw with different genes than adata; # test only obs; # test only var; # test handling of duplicated keys (in this case repeated gene names); # test non unique index; """"""; Gene symbols column allows repeats, but we can't unambiguously get data for these values.; """"""; """"""compares backed vs. memory""""""; # get location test h5ad file in datasets; # use non-sequential list of genes; # use non-sequential list of cell indices; """"""uses a larger dataset to test column order and content""""""; # test that columns content is correct for obs_df; # test that columns content is correct for var_df; # test only cells; # test only var columns; # test handling of duplicated keys (in this case repeated cell names); # https://github.com/scverse/scanpy/issues/1634; # Test for error where just passing obsm_keys, but not keys, would cause error.; ##################################; # Test errors for obs_df, var_df #; ##################################; # (?s) is inline re.DOTALL; # This one could be reverted, see:; # https://github.com/scverse/scanpy/pull/1583#issuecomment-770641710; # This should error; # This shouldn't error; ##############################; # rank_genes_groups_df tests #; ##############################; # get all groups",MatchSource.CODE_COMMENT,tests/test_get.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_get.py
Modifiability,layers,layers,"# Override so warning gets caught; """"""; adata.X is np.ones((2, 2)); adata.layers['double'] is sparse np.ones((2,2)) * 2 to also test sparse matrices; """"""; ########################; # obs_df, var_df tests #; ########################; # make raw with different genes than adata; # test only obs; # test only var; # test handling of duplicated keys (in this case repeated gene names); # test non unique index; """"""; Gene symbols column allows repeats, but we can't unambiguously get data for these values.; """"""; """"""compares backed vs. memory""""""; # get location test h5ad file in datasets; # use non-sequential list of genes; # use non-sequential list of cell indices; """"""uses a larger dataset to test column order and content""""""; # test that columns content is correct for obs_df; # test that columns content is correct for var_df; # test only cells; # test only var columns; # test handling of duplicated keys (in this case repeated cell names); # https://github.com/scverse/scanpy/issues/1634; # Test for error where just passing obsm_keys, but not keys, would cause error.; ##################################; # Test errors for obs_df, var_df #; ##################################; # (?s) is inline re.DOTALL; # This one could be reverted, see:; # https://github.com/scverse/scanpy/pull/1583#issuecomment-770641710; # This should error; # This shouldn't error; ##############################; # rank_genes_groups_df tests #; ##############################; # get all groups",MatchSource.CODE_COMMENT,tests/test_get.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_get.py
Testability,test,test,"# Override so warning gets caught; """"""; adata.X is np.ones((2, 2)); adata.layers['double'] is sparse np.ones((2,2)) * 2 to also test sparse matrices; """"""; ########################; # obs_df, var_df tests #; ########################; # make raw with different genes than adata; # test only obs; # test only var; # test handling of duplicated keys (in this case repeated gene names); # test non unique index; """"""; Gene symbols column allows repeats, but we can't unambiguously get data for these values.; """"""; """"""compares backed vs. memory""""""; # get location test h5ad file in datasets; # use non-sequential list of genes; # use non-sequential list of cell indices; """"""uses a larger dataset to test column order and content""""""; # test that columns content is correct for obs_df; # test that columns content is correct for var_df; # test only cells; # test only var columns; # test handling of duplicated keys (in this case repeated cell names); # https://github.com/scverse/scanpy/issues/1634; # Test for error where just passing obsm_keys, but not keys, would cause error.; ##################################; # Test errors for obs_df, var_df #; ##################################; # (?s) is inline re.DOTALL; # This one could be reverted, see:; # https://github.com/scverse/scanpy/pull/1583#issuecomment-770641710; # This should error; # This shouldn't error; ##############################; # rank_genes_groups_df tests #; ##############################; # get all groups",MatchSource.CODE_COMMENT,tests/test_get.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_get.py
Availability,error,error,"""""""Tests that, with `n_top_genes=None` the returned dataframe has the expected columns.""""""; # cell_ranger flavor can raise error if many 0 genes; # depending on check_values, warnings should be raised for non-integer data; # errors should be raised for invalid theta values; # cleanup var; # compute reference output; # lazyly sort by residual variance and take top N; # (results in sorted ""gene order"" in reference); # compute output to be tested; # compare inplace=True and inplace=False output; # check output is complete; # check consistency with normalization method; # sort values before comparing as reference is sorted as well for subset case; # check hvg flag; # check ranks; # more general checks on ranks, hvg flag and residual variance; # cleanup var; # compare inplace=True and inplace=False output; # check output is complete; # general checks on ranks, hvg flag and residual variance; # check intersection flag; # check ranks (with batch_key these are the median of within-batch ranks); # check nbatches; # check subsetting; # noqa: PLR0917; # (still) Not equal to tolerance rtol=2e-05, atol=2e-05; # np.testing.assert_allclose(4, 3.9999, rtol=2e-05, atol=2e-05); ### test without batch; # this doesnt do anything btw; ### test with batch; # introduce a dummy ""technical covariate""; this is used in Seurat's SelectIntegrationFeatures; # true_mean, true_var = _get_mean_var(pbmc.X); """"""Tests that, with `n_top_genes=n`; - `inplace` and `subset` interact correctly; - for both the `seurat` and `cell_ranger` flavors; - for dask arrays and non-dask arrays; """"""",MatchSource.CODE_COMMENT,tests/test_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_highly_variable_genes.py
Integrability,depend,depending,"""""""Tests that, with `n_top_genes=None` the returned dataframe has the expected columns.""""""; # cell_ranger flavor can raise error if many 0 genes; # depending on check_values, warnings should be raised for non-integer data; # errors should be raised for invalid theta values; # cleanup var; # compute reference output; # lazyly sort by residual variance and take top N; # (results in sorted ""gene order"" in reference); # compute output to be tested; # compare inplace=True and inplace=False output; # check output is complete; # check consistency with normalization method; # sort values before comparing as reference is sorted as well for subset case; # check hvg flag; # check ranks; # more general checks on ranks, hvg flag and residual variance; # cleanup var; # compare inplace=True and inplace=False output; # check output is complete; # general checks on ranks, hvg flag and residual variance; # check intersection flag; # check ranks (with batch_key these are the median of within-batch ranks); # check nbatches; # check subsetting; # noqa: PLR0917; # (still) Not equal to tolerance rtol=2e-05, atol=2e-05; # np.testing.assert_allclose(4, 3.9999, rtol=2e-05, atol=2e-05); ### test without batch; # this doesnt do anything btw; ### test with batch; # introduce a dummy ""technical covariate""; this is used in Seurat's SelectIntegrationFeatures; # true_mean, true_var = _get_mean_var(pbmc.X); """"""Tests that, with `n_top_genes=n`; - `inplace` and `subset` interact correctly; - for both the `seurat` and `cell_ranger` flavors; - for dask arrays and non-dask arrays; """"""",MatchSource.CODE_COMMENT,tests/test_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_highly_variable_genes.py
Testability,test,tested,"""""""Tests that, with `n_top_genes=None` the returned dataframe has the expected columns.""""""; # cell_ranger flavor can raise error if many 0 genes; # depending on check_values, warnings should be raised for non-integer data; # errors should be raised for invalid theta values; # cleanup var; # compute reference output; # lazyly sort by residual variance and take top N; # (results in sorted ""gene order"" in reference); # compute output to be tested; # compare inplace=True and inplace=False output; # check output is complete; # check consistency with normalization method; # sort values before comparing as reference is sorted as well for subset case; # check hvg flag; # check ranks; # more general checks on ranks, hvg flag and residual variance; # cleanup var; # compare inplace=True and inplace=False output; # check output is complete; # general checks on ranks, hvg flag and residual variance; # check intersection flag; # check ranks (with batch_key these are the median of within-batch ranks); # check nbatches; # check subsetting; # noqa: PLR0917; # (still) Not equal to tolerance rtol=2e-05, atol=2e-05; # np.testing.assert_allclose(4, 3.9999, rtol=2e-05, atol=2e-05); ### test without batch; # this doesnt do anything btw; ### test with batch; # introduce a dummy ""technical covariate""; this is used in Seurat's SelectIntegrationFeatures; # true_mean, true_var = _get_mean_var(pbmc.X); """"""Tests that, with `n_top_genes=n`; - `inplace` and `subset` interact correctly; - for both the `seurat` and `cell_ranger` flavors; - for dask arrays and non-dask arrays; """"""",MatchSource.CODE_COMMENT,tests/test_highly_variable_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_highly_variable_genes.py
Availability,error,error,"# setting a logfile removes all handlers; # setting a logfile removes all handlers; """"""; Tests that these functions print to stdout and don't error. Checks that https://github.com/scverse/scanpy/issues/1437 is fixed.; """"""",MatchSource.CODE_COMMENT,tests/test_logging.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_logging.py
Testability,log,logfile,"# setting a logfile removes all handlers; # setting a logfile removes all handlers; """"""; Tests that these functions print to stdout and don't error. Checks that https://github.com/scverse/scanpy/issues/1437 is fixed.; """"""",MatchSource.CODE_COMMENT,tests/test_logging.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_logging.py
Testability,test,test,"# test functions with neighbors_key and obsp; # no obsp in umap, paga",MatchSource.CODE_COMMENT,tests/test_neighbors_key_added.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_neighbors_key_added.py
Availability,mask,masked,"# TODO: Add support for sparse-in-dask; # Test that layer kwarg works; # depending on check_values, warnings should be raised for non-integer data; # toy data; # compute reference residuals; # Poisson case; # NB case; # compute output to test; # check for correct new `adata.uns` keys; # test against inplace; # default clipping: compare to sqrt(n) threshold; # no clipping: compare to raw residuals; # custom clipping: compare to custom threshold; # number of variables in output if inplace=False; # inplace=False; # inplace=True modifies the input adata object; # inplace adatas should always retains original shape; # check adata shape to see if all genes or only HVGs are in the returned adata; # check PC shapes to see whether or not HVGs were used for PCA; # check if there are columns of all-zeros in the PCs shapes; # to see whether or not HVGs were used for PCA; # either no all-zero-colums or all number corresponding to non-hvgs should exist; # compare PCA results beteen inplace / copied; ### inplace = False ###; # outputs the (potentially hvg-restricted) adata_pca object; # PCA on all genes; # check PCA fields; # check adata output shape (only HVGs in output); # check PC shape (non-hvgs are removed, so only `n_hvgs` genes); # check hvg df; ### inplace = True ###; # modifies the input adata object; # PCA on all genes; # check PCA fields and output shape; # check adata shape (no change to input); # check PC shape (non-hvgs are masked with 0s, so original number of genes); # number of all-zero-colums should be number of non-hvgs",MatchSource.CODE_COMMENT,tests/test_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_normalization.py
Integrability,depend,depending,"# TODO: Add support for sparse-in-dask; # Test that layer kwarg works; # depending on check_values, warnings should be raised for non-integer data; # toy data; # compute reference residuals; # Poisson case; # NB case; # compute output to test; # check for correct new `adata.uns` keys; # test against inplace; # default clipping: compare to sqrt(n) threshold; # no clipping: compare to raw residuals; # custom clipping: compare to custom threshold; # number of variables in output if inplace=False; # inplace=False; # inplace=True modifies the input adata object; # inplace adatas should always retains original shape; # check adata shape to see if all genes or only HVGs are in the returned adata; # check PC shapes to see whether or not HVGs were used for PCA; # check if there are columns of all-zeros in the PCs shapes; # to see whether or not HVGs were used for PCA; # either no all-zero-colums or all number corresponding to non-hvgs should exist; # compare PCA results beteen inplace / copied; ### inplace = False ###; # outputs the (potentially hvg-restricted) adata_pca object; # PCA on all genes; # check PCA fields; # check adata output shape (only HVGs in output); # check PC shape (non-hvgs are removed, so only `n_hvgs` genes); # check hvg df; ### inplace = True ###; # modifies the input adata object; # PCA on all genes; # check PCA fields and output shape; # check adata shape (no change to input); # check PC shape (non-hvgs are masked with 0s, so original number of genes); # number of all-zero-colums should be number of non-hvgs",MatchSource.CODE_COMMENT,tests/test_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_normalization.py
Modifiability,variab,variables,"# TODO: Add support for sparse-in-dask; # Test that layer kwarg works; # depending on check_values, warnings should be raised for non-integer data; # toy data; # compute reference residuals; # Poisson case; # NB case; # compute output to test; # check for correct new `adata.uns` keys; # test against inplace; # default clipping: compare to sqrt(n) threshold; # no clipping: compare to raw residuals; # custom clipping: compare to custom threshold; # number of variables in output if inplace=False; # inplace=False; # inplace=True modifies the input adata object; # inplace adatas should always retains original shape; # check adata shape to see if all genes or only HVGs are in the returned adata; # check PC shapes to see whether or not HVGs were used for PCA; # check if there are columns of all-zeros in the PCs shapes; # to see whether or not HVGs were used for PCA; # either no all-zero-colums or all number corresponding to non-hvgs should exist; # compare PCA results beteen inplace / copied; ### inplace = False ###; # outputs the (potentially hvg-restricted) adata_pca object; # PCA on all genes; # check PCA fields; # check adata output shape (only HVGs in output); # check PC shape (non-hvgs are removed, so only `n_hvgs` genes); # check hvg df; ### inplace = True ###; # modifies the input adata object; # PCA on all genes; # check PCA fields and output shape; # check adata shape (no change to input); # check PC shape (non-hvgs are masked with 0s, so original number of genes); # number of all-zero-colums should be number of non-hvgs",MatchSource.CODE_COMMENT,tests/test_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_normalization.py
Testability,test,test,"# TODO: Add support for sparse-in-dask; # Test that layer kwarg works; # depending on check_values, warnings should be raised for non-integer data; # toy data; # compute reference residuals; # Poisson case; # NB case; # compute output to test; # check for correct new `adata.uns` keys; # test against inplace; # default clipping: compare to sqrt(n) threshold; # no clipping: compare to raw residuals; # custom clipping: compare to custom threshold; # number of variables in output if inplace=False; # inplace=False; # inplace=True modifies the input adata object; # inplace adatas should always retains original shape; # check adata shape to see if all genes or only HVGs are in the returned adata; # check PC shapes to see whether or not HVGs were used for PCA; # check if there are columns of all-zeros in the PCs shapes; # to see whether or not HVGs were used for PCA; # either no all-zero-colums or all number corresponding to non-hvgs should exist; # compare PCA results beteen inplace / copied; ### inplace = False ###; # outputs the (potentially hvg-restricted) adata_pca object; # PCA on all genes; # check PCA fields; # check adata output shape (only HVGs in output); # check PC shape (non-hvgs are removed, so only `n_hvgs` genes); # check hvg df; ### inplace = True ###; # modifies the input adata object; # PCA on all genes; # check PCA fields and output shape; # check adata shape (no change to input); # check PC shape (non-hvgs are masked with 0s, so original number of genes); # number of all-zero-colums should be number of non-hvgs",MatchSource.CODE_COMMENT,tests/test_normalization.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_normalization.py
Availability,reliab,reliably,"# If one uses dask for PCA it will always require dask-ml; # TODO: are these right for sparse?; # explicit check for special case; # TODO: Fix this case, maybe by increasing test data size.; # https://github.com/scverse/scanpy/issues/2744; # This warning test is out of the fixture because it is a special case in the logic of the function; """"""; Tests that n_comps behaves correctly; See https://github.com/scverse/scanpy/issues/1051; """"""; """"""; Tests that implicitly centered pca on sparse arrays returns equivalent results to; explicit centering on dense arrays.; """"""; # Test that changing random seed changes result; # Does not show up reliably with 32 bit computation; """"""; See https://github.com/scverse/scanpy/issues/1590; But this is also a more general test; """"""; # Subsetting for speed of test; # Taking absolute value since sometimes dimensions are flipped; """"""; Tests that the n_pcs parameter also works for; representations not called ""X_pca""; """"""; # We use all ARRAY_TYPES here since this error should be raised before; # PCA can realize that it got a Dask array; """"""Check if use_highly_variable=True throws an error if the annotation is missing.""""""; """"""Check error for n_obs / mask length mismatch.""""""; """"""Test if pca result is equal when given mask as boolarray vs string""""""; # There are slight difference based on whether the matrix was column or row major; """"""; Test if pca result is equal without highly variable and with-but mask is None; and if pca takes highly variable as mask as default; """"""; """"""; Tests that layers works the same way as .X; """"""",MatchSource.CODE_COMMENT,tests/test_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_pca.py
Modifiability,variab,variable,"# If one uses dask for PCA it will always require dask-ml; # TODO: are these right for sparse?; # explicit check for special case; # TODO: Fix this case, maybe by increasing test data size.; # https://github.com/scverse/scanpy/issues/2744; # This warning test is out of the fixture because it is a special case in the logic of the function; """"""; Tests that n_comps behaves correctly; See https://github.com/scverse/scanpy/issues/1051; """"""; """"""; Tests that implicitly centered pca on sparse arrays returns equivalent results to; explicit centering on dense arrays.; """"""; # Test that changing random seed changes result; # Does not show up reliably with 32 bit computation; """"""; See https://github.com/scverse/scanpy/issues/1590; But this is also a more general test; """"""; # Subsetting for speed of test; # Taking absolute value since sometimes dimensions are flipped; """"""; Tests that the n_pcs parameter also works for; representations not called ""X_pca""; """"""; # We use all ARRAY_TYPES here since this error should be raised before; # PCA can realize that it got a Dask array; """"""Check if use_highly_variable=True throws an error if the annotation is missing.""""""; """"""Check error for n_obs / mask length mismatch.""""""; """"""Test if pca result is equal when given mask as boolarray vs string""""""; # There are slight difference based on whether the matrix was column or row major; """"""; Test if pca result is equal without highly variable and with-but mask is None; and if pca takes highly variable as mask as default; """"""; """"""; Tests that layers works the same way as .X; """"""",MatchSource.CODE_COMMENT,tests/test_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_pca.py
Testability,test,test,"# If one uses dask for PCA it will always require dask-ml; # TODO: are these right for sparse?; # explicit check for special case; # TODO: Fix this case, maybe by increasing test data size.; # https://github.com/scverse/scanpy/issues/2744; # This warning test is out of the fixture because it is a special case in the logic of the function; """"""; Tests that n_comps behaves correctly; See https://github.com/scverse/scanpy/issues/1051; """"""; """"""; Tests that implicitly centered pca on sparse arrays returns equivalent results to; explicit centering on dense arrays.; """"""; # Test that changing random seed changes result; # Does not show up reliably with 32 bit computation; """"""; See https://github.com/scverse/scanpy/issues/1590; But this is also a more general test; """"""; # Subsetting for speed of test; # Taking absolute value since sometimes dimensions are flipped; """"""; Tests that the n_pcs parameter also works for; representations not called ""X_pca""; """"""; # We use all ARRAY_TYPES here since this error should be raised before; # PCA can realize that it got a Dask array; """"""Check if use_highly_variable=True throws an error if the annotation is missing.""""""; """"""Check error for n_obs / mask length mismatch.""""""; """"""Test if pca result is equal when given mask as boolarray vs string""""""; # There are slight difference based on whether the matrix was column or row major; """"""; Test if pca result is equal without highly variable and with-but mask is None; and if pca takes highly variable as mask as default; """"""; """"""; Tests that layers works the same way as .X; """"""",MatchSource.CODE_COMMENT,tests/test_pca.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_pca.py
Availability,down,down,"column():; # set as numeric column the vales for the first gene on the matrix; # test var/obs standardization and layer; # test standard_scale_obs; # test var_names as dict; # call umap to trigger colors for the clusters; # test that plot elements are well aligned; # small; # test dotplot dot_min, dot_max, color_map, and var_groups; # test layer, var standardization, smallest_dot,; # color title, size_title return_fig and dot_edge; # only testing stacked_violin, matrixplot and dotplot; # test use of layer; # TODO: Generalize test to more plotting types; # https://github.com/scverse/scanpy/issues/1546; # add gene symbol; """"""Create two anndata objects which are equivalent except for var_names. Both have ensembl ids and hgnc symbols as columns in var. The first has ensembl; ids as var_names, the second has symbols.; """"""; # Cutting down on size for plotting, tracksplot and stacked_violin are slow; # Creating variations; # Computing DE; # TODO: add other rank_genes_groups plots here once they work; # TODO: add other rank_genes_groups plots here once they work; """"""\; Checks that passing a negative value for n_genes works, and that passing; var_names as a dict works.; """"""; # Shouldn't be able to pass these together; # add a 'symbols' column; # Wrapped in another fixture to avoid mutation; # ('diffmap', partial(sc.pl.diffmap, components='all', color=['CD3D'])),; # test that the 'groups' parameter sorts; # cells, such that the cells belonging to the groups are; # plotted on top. This new ordering requires that the size; # vector is also ordered (if given).; # matplotlib<3.2; # allowed; """"""Test scatterplot of per-obs points with no basis""""""; # palette only applies to categorical, i.e. color=='bulk_labels'; """"""Test scatterplot of per-var points with no basis""""""; """"""Test scatterplots of raw layer with no basis.""""""; # is equivalent to:; # and also to:; # test that plotting fails with a ValueError if trying to plot; # var_names only found in raw and use_raw is False; # test that p",MatchSource.CODE_COMMENT,tests/test_plotting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_plotting.py
Deployability,update,updated,"# Test images are saved in the directory ./_images/<test-name>/; # If test images need to be updated, simply copy actual.png to expected.png.; # test swap axes; # test heatmap numeric column():; # set as numeric column the vales for the first gene on the matrix; # test var/obs standardization and layer; # test standard_scale_obs; # test var_names as dict; # call umap to trigger colors for the clusters; # test that plot elements are well aligned; # small; # test dotplot dot_min, dot_max, color_map, and var_groups; # test layer, var standardization, smallest_dot,; # color title, size_title return_fig and dot_edge; # only testing stacked_violin, matrixplot and dotplot; # test use of layer; # TODO: Generalize test to more plotting types; # https://github.com/scverse/scanpy/issues/1546; # add gene symbol; """"""Create two anndata objects which are equivalent except for var_names. Both have ensembl ids and hgnc symbols as columns in var. The first has ensembl; ids as var_names, the second has symbols.; """"""; # Cutting down on size for plotting, tracksplot and stacked_violin are slow; # Creating variations; # Computing DE; # TODO: add other rank_genes_groups plots here once they work; # TODO: add other rank_genes_groups plots here once they work; """"""\; Checks that passing a negative value for n_genes works, and that passing; var_names as a dict works.; """"""; # Shouldn't be able to pass these together; # add a 'symbols' column; # Wrapped in another fixture to avoid mutation; # ('diffmap', partial(sc.pl.diffmap, components='all', color=['CD3D'])),; # test that the 'groups' parameter sorts; # cells, such that the cells belonging to the groups are; # plotted on top. This new ordering requires that the size; # vector is also ordered (if given).; # matplotlib<3.2; # allowed; """"""Test scatterplot of per-obs points with no basis""""""; # palette only applies to categorical, i.e. color=='bulk_labels'; """"""Test scatterplot of per-var points with no basis""""""; """"""Test scatterplots of raw layer w",MatchSource.CODE_COMMENT,tests/test_plotting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_plotting.py
Modifiability,variab,variable,"# add a 'symbols' column; # Wrapped in another fixture to avoid mutation; # ('diffmap', partial(sc.pl.diffmap, components='all', color=['CD3D'])),; # test that the 'groups' parameter sorts; # cells, such that the cells belonging to the groups are; # plotted on top. This new ordering requires that the size; # vector is also ordered (if given).; # matplotlib<3.2; # allowed; """"""Test scatterplot of per-obs points with no basis""""""; # palette only applies to categorical, i.e. color=='bulk_labels'; """"""Test scatterplot of per-var points with no basis""""""; """"""Test scatterplots of raw layer with no basis.""""""; # is equivalent to:; # and also to:; # test that plotting fails with a ValueError if trying to plot; # var_names only found in raw and use_raw is False; # test that plotting fails if one axis is a per-var value and the; # other is a per-obs value; """"""Test that `scatter()` raises `ValueError` where appropriate. If `sc.pl.scatter()` receives variable labels that either cannot be; found or are incompatible with one another, the function should; raise a `ValueError`. This test checks that this happens as; expected.; """"""; # TODO: Make more generic; """"""; Test to make sure I can predict when scatter reps should be the same; """"""; # https://github.com/scverse/scanpy/issues/1000; # Tests that plotting functions don't make a copy from a view unless they; # actually have to; # Set colors; # TODO: raises ValueError about empty distance matrix – investigate; # sc.pl.rank_genes_groups_tracksplot,; # the pbmc68k was generated using rank_genes_groups with method='logreg'; # which does not generate 'logfoldchanges', although this field is; # required by `sc.get.rank_genes_groups_df`.; # After updating rank_genes_groups plots to use the latter function; # an error appears. Re-running rank_genes_groups with default method; # solves the problem.; # Only plotting one group at a time to avoid generating dendrogram; # TODO: Generating a dendrogram modifies the object, this should be; # optional a",MatchSource.CODE_COMMENT,tests/test_plotting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_plotting.py
Safety,avoid,avoid,"ell aligned; # small; # test dotplot dot_min, dot_max, color_map, and var_groups; # test layer, var standardization, smallest_dot,; # color title, size_title return_fig and dot_edge; # only testing stacked_violin, matrixplot and dotplot; # test use of layer; # TODO: Generalize test to more plotting types; # https://github.com/scverse/scanpy/issues/1546; # add gene symbol; """"""Create two anndata objects which are equivalent except for var_names. Both have ensembl ids and hgnc symbols as columns in var. The first has ensembl; ids as var_names, the second has symbols.; """"""; # Cutting down on size for plotting, tracksplot and stacked_violin are slow; # Creating variations; # Computing DE; # TODO: add other rank_genes_groups plots here once they work; # TODO: add other rank_genes_groups plots here once they work; """"""\; Checks that passing a negative value for n_genes works, and that passing; var_names as a dict works.; """"""; # Shouldn't be able to pass these together; # add a 'symbols' column; # Wrapped in another fixture to avoid mutation; # ('diffmap', partial(sc.pl.diffmap, components='all', color=['CD3D'])),; # test that the 'groups' parameter sorts; # cells, such that the cells belonging to the groups are; # plotted on top. This new ordering requires that the size; # vector is also ordered (if given).; # matplotlib<3.2; # allowed; """"""Test scatterplot of per-obs points with no basis""""""; # palette only applies to categorical, i.e. color=='bulk_labels'; """"""Test scatterplot of per-var points with no basis""""""; """"""Test scatterplots of raw layer with no basis.""""""; # is equivalent to:; # and also to:; # test that plotting fails with a ValueError if trying to plot; # var_names only found in raw and use_raw is False; # test that plotting fails if one axis is a per-var value and the; # other is a per-obs value; """"""Test that `scatter()` raises `ValueError` where appropriate. If `sc.pl.scatter()` receives variable labels that either cannot be; found or are incompatible with one an",MatchSource.CODE_COMMENT,tests/test_plotting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_plotting.py
Testability,test,test-name,"# Test images are saved in the directory ./_images/<test-name>/; # If test images need to be updated, simply copy actual.png to expected.png.; # test swap axes; # test heatmap numeric column():; # set as numeric column the vales for the first gene on the matrix; # test var/obs standardization and layer; # test standard_scale_obs; # test var_names as dict; # call umap to trigger colors for the clusters; # test that plot elements are well aligned; # small; # test dotplot dot_min, dot_max, color_map, and var_groups; # test layer, var standardization, smallest_dot,; # color title, size_title return_fig and dot_edge; # only testing stacked_violin, matrixplot and dotplot; # test use of layer; # TODO: Generalize test to more plotting types; # https://github.com/scverse/scanpy/issues/1546; # add gene symbol; """"""Create two anndata objects which are equivalent except for var_names. Both have ensembl ids and hgnc symbols as columns in var. The first has ensembl; ids as var_names, the second has symbols.; """"""; # Cutting down on size for plotting, tracksplot and stacked_violin are slow; # Creating variations; # Computing DE; # TODO: add other rank_genes_groups plots here once they work; # TODO: add other rank_genes_groups plots here once they work; """"""\; Checks that passing a negative value for n_genes works, and that passing; var_names as a dict works.; """"""; # Shouldn't be able to pass these together; # add a 'symbols' column; # Wrapped in another fixture to avoid mutation; # ('diffmap', partial(sc.pl.diffmap, components='all', color=['CD3D'])),; # test that the 'groups' parameter sorts; # cells, such that the cells belonging to the groups are; # plotted on top. This new ordering requires that the size; # vector is also ordered (if given).; # matplotlib<3.2; # allowed; """"""Test scatterplot of per-obs points with no basis""""""; # palette only applies to categorical, i.e. color=='bulk_labels'; """"""Test scatterplot of per-var points with no basis""""""; """"""Test scatterplots of raw layer w",MatchSource.CODE_COMMENT,tests/test_plotting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_plotting.py
Usability,simpl,simply,"# Test images are saved in the directory ./_images/<test-name>/; # If test images need to be updated, simply copy actual.png to expected.png.; # test swap axes; # test heatmap numeric column():; # set as numeric column the vales for the first gene on the matrix; # test var/obs standardization and layer; # test standard_scale_obs; # test var_names as dict; # call umap to trigger colors for the clusters; # test that plot elements are well aligned; # small; # test dotplot dot_min, dot_max, color_map, and var_groups; # test layer, var standardization, smallest_dot,; # color title, size_title return_fig and dot_edge; # only testing stacked_violin, matrixplot and dotplot; # test use of layer; # TODO: Generalize test to more plotting types; # https://github.com/scverse/scanpy/issues/1546; # add gene symbol; """"""Create two anndata objects which are equivalent except for var_names. Both have ensembl ids and hgnc symbols as columns in var. The first has ensembl; ids as var_names, the second has symbols.; """"""; # Cutting down on size for plotting, tracksplot and stacked_violin are slow; # Creating variations; # Computing DE; # TODO: add other rank_genes_groups plots here once they work; # TODO: add other rank_genes_groups plots here once they work; """"""\; Checks that passing a negative value for n_genes works, and that passing; var_names as a dict works.; """"""; # Shouldn't be able to pass these together; # add a 'symbols' column; # Wrapped in another fixture to avoid mutation; # ('diffmap', partial(sc.pl.diffmap, components='all', color=['CD3D'])),; # test that the 'groups' parameter sorts; # cells, such that the cells belonging to the groups are; # plotted on top. This new ordering requires that the size; # vector is also ordered (if given).; # matplotlib<3.2; # allowed; """"""Test scatterplot of per-obs points with no basis""""""; # palette only applies to categorical, i.e. color=='bulk_labels'; """"""Test scatterplot of per-var points with no basis""""""; """"""Test scatterplots of raw layer w",MatchSource.CODE_COMMENT,tests/test_plotting.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_plotting.py
Availability,error,error,"# Test base; # Test that we're equivalent for 64 bit; # Test whether ours is more accurate for 32 bit; # now with copy option; # note that sc.pp.normalize_per_cell is also used in; # pl.highest_expr_genes with parameter counts_per_cell_after=100; # now sparse; # This should not throw an error; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; """"""; Test that it doesn't matter where the array being scaled is in the anndata object.; """"""; """"""; Test that running sc.pp.scale on an anndata object and an array returns the same results.; """"""; # These shouldn't throw an error; # results using only one processor; # results using 8 processors; # create a categorical column; # Tests that constant values don't change results; # (since support for constant values is implemented by us); # Just tests for failure for now",MatchSource.CODE_COMMENT,tests/test_preprocessing.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_preprocessing.py
Testability,test,tests,"# Test base; # Test that we're equivalent for 64 bit; # Test whether ours is more accurate for 32 bit; # now with copy option; # note that sc.pp.normalize_per_cell is also used in; # pl.highest_expr_genes with parameter counts_per_cell_after=100; # now sparse; # This should not throw an error; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; """"""; Test that it doesn't matter where the array being scaled is in the anndata object.; """"""; """"""; Test that running sc.pp.scale on an anndata object and an array returns the same results.; """"""; # These shouldn't throw an error; # results using only one processor; # results using 8 processors; # create a categorical column; # Tests that constant values don't change results; # (since support for constant values is implemented by us); # Just tests for failure for now",MatchSource.CODE_COMMENT,tests/test_preprocessing.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_preprocessing.py
Availability,mask,mask,"# We test results for a simple generic example; # Tests are conducted for sparse and non-sparse AnnData objects.; # Due to minor changes in multiplication implementation for sparse and non-sparse objects,; # results differ (very) slightly; # create test object; # adapt marker_genes for cluster (so as to have some form of reasonable input; # Create cluster according to groups; # TODO: Make dask compatible; # Assumption for later checks; # Wilcoxon; # t-test; # https://github.com/scverse/scanpy/issues/1929; """"""tests the sequence log1p→save→load→rank_genes_groups""""""; # Handle scipy versions; # Backwards compat, to drop once we drop scipy < 1.7; """"""\; Check that no. genes in output is; 1. =n_genes when n_genes<sum(mask); 2. =sum(mask) when n_genes>sum(mask); """"""; """"""\; Check that mask is applied successfully to data set \; where test statistics are already available (test stats overwritten).; """"""",MatchSource.CODE_COMMENT,tests/test_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_rank_genes_groups.py
Energy Efficiency,adapt,adapt,"# We test results for a simple generic example; # Tests are conducted for sparse and non-sparse AnnData objects.; # Due to minor changes in multiplication implementation for sparse and non-sparse objects,; # results differ (very) slightly; # create test object; # adapt marker_genes for cluster (so as to have some form of reasonable input; # Create cluster according to groups; # TODO: Make dask compatible; # Assumption for later checks; # Wilcoxon; # t-test; # https://github.com/scverse/scanpy/issues/1929; """"""tests the sequence log1p→save→load→rank_genes_groups""""""; # Handle scipy versions; # Backwards compat, to drop once we drop scipy < 1.7; """"""\; Check that no. genes in output is; 1. =n_genes when n_genes<sum(mask); 2. =sum(mask) when n_genes>sum(mask); """"""; """"""\; Check that mask is applied successfully to data set \; where test statistics are already available (test stats overwritten).; """"""",MatchSource.CODE_COMMENT,tests/test_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_rank_genes_groups.py
Modifiability,adapt,adapt,"# We test results for a simple generic example; # Tests are conducted for sparse and non-sparse AnnData objects.; # Due to minor changes in multiplication implementation for sparse and non-sparse objects,; # results differ (very) slightly; # create test object; # adapt marker_genes for cluster (so as to have some form of reasonable input; # Create cluster according to groups; # TODO: Make dask compatible; # Assumption for later checks; # Wilcoxon; # t-test; # https://github.com/scverse/scanpy/issues/1929; """"""tests the sequence log1p→save→load→rank_genes_groups""""""; # Handle scipy versions; # Backwards compat, to drop once we drop scipy < 1.7; """"""\; Check that no. genes in output is; 1. =n_genes when n_genes<sum(mask); 2. =sum(mask) when n_genes>sum(mask); """"""; """"""\; Check that mask is applied successfully to data set \; where test statistics are already available (test stats overwritten).; """"""",MatchSource.CODE_COMMENT,tests/test_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_rank_genes_groups.py
Performance,load,load,"# We test results for a simple generic example; # Tests are conducted for sparse and non-sparse AnnData objects.; # Due to minor changes in multiplication implementation for sparse and non-sparse objects,; # results differ (very) slightly; # create test object; # adapt marker_genes for cluster (so as to have some form of reasonable input; # Create cluster according to groups; # TODO: Make dask compatible; # Assumption for later checks; # Wilcoxon; # t-test; # https://github.com/scverse/scanpy/issues/1929; """"""tests the sequence log1p→save→load→rank_genes_groups""""""; # Handle scipy versions; # Backwards compat, to drop once we drop scipy < 1.7; """"""\; Check that no. genes in output is; 1. =n_genes when n_genes<sum(mask); 2. =sum(mask) when n_genes>sum(mask); """"""; """"""\; Check that mask is applied successfully to data set \; where test statistics are already available (test stats overwritten).; """"""",MatchSource.CODE_COMMENT,tests/test_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_rank_genes_groups.py
Testability,test,test,"# We test results for a simple generic example; # Tests are conducted for sparse and non-sparse AnnData objects.; # Due to minor changes in multiplication implementation for sparse and non-sparse objects,; # results differ (very) slightly; # create test object; # adapt marker_genes for cluster (so as to have some form of reasonable input; # Create cluster according to groups; # TODO: Make dask compatible; # Assumption for later checks; # Wilcoxon; # t-test; # https://github.com/scverse/scanpy/issues/1929; """"""tests the sequence log1p→save→load→rank_genes_groups""""""; # Handle scipy versions; # Backwards compat, to drop once we drop scipy < 1.7; """"""\; Check that no. genes in output is; 1. =n_genes when n_genes<sum(mask); 2. =sum(mask) when n_genes>sum(mask); """"""; """"""\; Check that mask is applied successfully to data set \; where test statistics are already available (test stats overwritten).; """"""",MatchSource.CODE_COMMENT,tests/test_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_rank_genes_groups.py
Usability,simpl,simple,"# We test results for a simple generic example; # Tests are conducted for sparse and non-sparse AnnData objects.; # Due to minor changes in multiplication implementation for sparse and non-sparse objects,; # results differ (very) slightly; # create test object; # adapt marker_genes for cluster (so as to have some form of reasonable input; # Create cluster according to groups; # TODO: Make dask compatible; # Assumption for later checks; # Wilcoxon; # t-test; # https://github.com/scverse/scanpy/issues/1929; """"""tests the sequence log1p→save→load→rank_genes_groups""""""; # Handle scipy versions; # Backwards compat, to drop once we drop scipy < 1.7; """"""\; Check that no. genes in output is; 1. =n_genes when n_genes<sum(mask); 2. =sum(mask) when n_genes>sum(mask); """"""; """"""\; Check that mask is applied successfully to data set \; where test statistics are already available (test stats overwritten).; """"""",MatchSource.CODE_COMMENT,tests/test_rank_genes_groups.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_rank_genes_groups.py
Testability,log,logreg,"# for method in ['logreg', 't-test']:",MatchSource.CODE_COMMENT,tests/test_rank_genes_groups_logreg.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_rank_genes_groups_logreg.py
Testability,test,test,"# Build files named ""prefix_XXX.xxx"" in a temporary directory.; # Drop genome column for comparing v3; # Check equivalence; # Test that it can be written:; # the test data are such that X is the same shape for both ""genomes"",; # but the values are different; # copy only data, not file metadata; """"""Test checking that read_visium reads the right genome""""""; # Tests that gex option doesn't, say, make the function return None; # Tests the 10x probe barcode matrix is read correctly",MatchSource.CODE_COMMENT,tests/test_read_10x.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_read_10x.py
Testability,test,test,"# test ""data"" for 3 cells * 4 genes; # with gene std 1,0,2,0 and center 0,2,2,0; # with gene std 1,0,1,0 and center 0,2,1,0; # with gene std 1,0,1,0 and center 0,0,0,0; # with gene std 1,0,1,0 and center 0,2,1,0; # test AnnData arguments; # test scaling with default zero_center == True; # test scaling with explicit zero_center == True; # test scaling with explicit zero_center == False; # test bare count arguments, for simplicity only with explicit copy=True; # test scaling with default zero_center == True; # test scaling with explicit zero_center == True; # test scaling with explicit zero_center == False; # test scaling with explicit zero_center == True",MatchSource.CODE_COMMENT,tests/test_scaling.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_scaling.py
Usability,simpl,simplicity,"# test ""data"" for 3 cells * 4 genes; # with gene std 1,0,2,0 and center 0,2,2,0; # with gene std 1,0,1,0 and center 0,2,1,0; # with gene std 1,0,1,0 and center 0,0,0,0; # with gene std 1,0,1,0 and center 0,2,1,0; # test AnnData arguments; # test scaling with default zero_center == True; # test scaling with explicit zero_center == True; # test scaling with explicit zero_center == False; # test bare count arguments, for simplicity only with explicit copy=True; # test scaling with default zero_center == True; # test scaling with explicit zero_center == True; # test scaling with explicit zero_center == False; # test scaling with explicit zero_center == True",MatchSource.CODE_COMMENT,tests/test_scaling.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_scaling.py
Deployability,patch,patched,"""""""; creates a bunch of random gene names (just CAPS letters); """"""; """"""; creates a sparse matrix, with certain amounts of NaN and Zeros; """"""; """"""; creates an AnnData with random data, sparseness and some NaN values; """"""; """"""; Checks if score_genes output agrees with pre-computed reference values.; The reference values had been generated using the same code; and stored as a pickle object in ./data; """"""; # np.testing.assert_allclose(reference, adata.obs[""Test""].to_numpy()); """"""; check the dtype of the scores; check that non-existing genes get ignored; """"""; # TODO: write a test that costs less resources and is more meaningful; # the actual genes names are all 6letters; # create some non-estinsting names with 7 letters:; """"""; check that _sparse_nanmean() is equivalent to np.nanmean(); """"""; # sparse matrix, no NaN; # col/col sum; # sparse matrix with nan; # edge case of only NaNs per row; """"""; TypeError must be thrown when calling _sparse_nanmean with a dense matrix; """"""; """"""; score_genes() should give the same result for dense and sparse matrices; """"""; """"""; deplete some cells from a set of genes.; their score should be <0 since the sum of markers is 0 and; the sum of random genes is >=0. check that for both sparse and dense matrices; """"""; # here's an arbitary gene set; # deplete these genes in 50 cells,; """"""; another check that _sparsemean behaves like np.nanmean!. monkeypatch the _score_genes._sparse_nanmean function to np.nanmean; and check that the result is the same as the non-patched (i.e. sparse_nanmean); function; """"""; # the unpatched, i.e. _sparse_nanmean version; # now patch _sparse_nanmean by np.nanmean inside sc.tools; # These genes have a different length of name; # https://github.com/scverse/scanpy/issues/1395",MatchSource.CODE_COMMENT,tests/test_score_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_score_genes.py
Testability,test,testing,"""""""; creates a bunch of random gene names (just CAPS letters); """"""; """"""; creates a sparse matrix, with certain amounts of NaN and Zeros; """"""; """"""; creates an AnnData with random data, sparseness and some NaN values; """"""; """"""; Checks if score_genes output agrees with pre-computed reference values.; The reference values had been generated using the same code; and stored as a pickle object in ./data; """"""; # np.testing.assert_allclose(reference, adata.obs[""Test""].to_numpy()); """"""; check the dtype of the scores; check that non-existing genes get ignored; """"""; # TODO: write a test that costs less resources and is more meaningful; # the actual genes names are all 6letters; # create some non-estinsting names with 7 letters:; """"""; check that _sparse_nanmean() is equivalent to np.nanmean(); """"""; # sparse matrix, no NaN; # col/col sum; # sparse matrix with nan; # edge case of only NaNs per row; """"""; TypeError must be thrown when calling _sparse_nanmean with a dense matrix; """"""; """"""; score_genes() should give the same result for dense and sparse matrices; """"""; """"""; deplete some cells from a set of genes.; their score should be <0 since the sum of markers is 0 and; the sum of random genes is >=0. check that for both sparse and dense matrices; """"""; # here's an arbitary gene set; # deplete these genes in 50 cells,; """"""; another check that _sparsemean behaves like np.nanmean!. monkeypatch the _score_genes._sparse_nanmean function to np.nanmean; and check that the result is the same as the non-patched (i.e. sparse_nanmean); function; """"""; # the unpatched, i.e. _sparse_nanmean version; # now patch _sparse_nanmean by np.nanmean inside sc.tools; # These genes have a different length of name; # https://github.com/scverse/scanpy/issues/1395",MatchSource.CODE_COMMENT,tests/test_score_genes.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_score_genes.py
Safety,detect,detects,"""""""Check that scrublet runs and detects some doublets.""""""; """"""Test that Scrublet run works with batched data.""""""; # only one in the first batch (<100); # Check that results are independent; """"""Simulate doublets based on the randomly selected parents used previously.""""""; """"""; Test that Scrublet processing is arranged correctly. Check that simulations run on raw data.; """"""; # Run Scrublet and let the main function run simulations; # Now make our own simulated data so we can check the result from function; # is the same, and by inference that the processing steps have not been; # broken; # Replicate the preprocessing steps used by the main function; # Simulate doublets using the same parents; # Apply the same post-normalisation the Scrublet function would; # Require that the doublet scores are the same whether simulation is via; # the main function or manually provided; # Reduce size of input for faster test; """"""; Test that Scrublet args are passed. Check that changes to parameters change scrublet results.; """"""; """"""Check that doublet simulation runs and simulates some doublets.""""""",MatchSource.CODE_COMMENT,tests/test_scrublet.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_scrublet.py
Testability,test,test,"""""""Check that scrublet runs and detects some doublets.""""""; """"""Test that Scrublet run works with batched data.""""""; # only one in the first batch (<100); # Check that results are independent; """"""Simulate doublets based on the randomly selected parents used previously.""""""; """"""; Test that Scrublet processing is arranged correctly. Check that simulations run on raw data.; """"""; # Run Scrublet and let the main function run simulations; # Now make our own simulated data so we can check the result from function; # is the same, and by inference that the processing steps have not been; # broken; # Replicate the preprocessing steps used by the main function; # Simulate doublets using the same parents; # Apply the same post-normalisation the Scrublet function would; # Require that the doublet scores are the same whether simulation is via; # the main function or manually provided; # Reduce size of input for faster test; """"""; Test that Scrublet args are passed. Check that changes to parameters change scrublet results.; """"""; """"""Check that doublet simulation runs and simulates some doublets.""""""",MatchSource.CODE_COMMENT,tests/test_scrublet.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/test_scrublet.py
Deployability,integrat,integrate,"""""""; Test that Harmony integrate works. This is a very simple test that just checks to see if the Harmony; integrate wrapper succesfully added a new field to ``adata.obsm``; and makes sure it has the same dimensions as the original PCA table.; """"""",MatchSource.CODE_COMMENT,tests/external/test_harmony_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/external/test_harmony_integrate.py
Integrability,integrat,integrate,"""""""; Test that Harmony integrate works. This is a very simple test that just checks to see if the Harmony; integrate wrapper succesfully added a new field to ``adata.obsm``; and makes sure it has the same dimensions as the original PCA table.; """"""",MatchSource.CODE_COMMENT,tests/external/test_harmony_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/external/test_harmony_integrate.py
Testability,test,test,"""""""; Test that Harmony integrate works. This is a very simple test that just checks to see if the Harmony; integrate wrapper succesfully added a new field to ``adata.obsm``; and makes sure it has the same dimensions as the original PCA table.; """"""",MatchSource.CODE_COMMENT,tests/external/test_harmony_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/external/test_harmony_integrate.py
Usability,simpl,simple,"""""""; Test that Harmony integrate works. This is a very simple test that just checks to see if the Harmony; integrate wrapper succesfully added a new field to ``adata.obsm``; and makes sure it has the same dimensions as the original PCA table.; """"""",MatchSource.CODE_COMMENT,tests/external/test_harmony_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/external/test_harmony_integrate.py
Deployability,integrat,integration,"""""""; Test that Scanorama integration works. This is a very simple test that just checks to see if the Scanorama; integrate wrapper succesfully added a new field to ``adata.obsm``; and makes sure it has the same dimensions as the original PCA table.; """"""",MatchSource.CODE_COMMENT,tests/external/test_scanorama_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/external/test_scanorama_integrate.py
Integrability,integrat,integration,"""""""; Test that Scanorama integration works. This is a very simple test that just checks to see if the Scanorama; integrate wrapper succesfully added a new field to ``adata.obsm``; and makes sure it has the same dimensions as the original PCA table.; """"""",MatchSource.CODE_COMMENT,tests/external/test_scanorama_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/external/test_scanorama_integrate.py
Testability,test,test,"""""""; Test that Scanorama integration works. This is a very simple test that just checks to see if the Scanorama; integrate wrapper succesfully added a new field to ``adata.obsm``; and makes sure it has the same dimensions as the original PCA table.; """"""",MatchSource.CODE_COMMENT,tests/external/test_scanorama_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/external/test_scanorama_integrate.py
Usability,simpl,simple,"""""""; Test that Scanorama integration works. This is a very simple test that just checks to see if the Scanorama; integrate wrapper succesfully added a new field to ``adata.obsm``; and makes sure it has the same dimensions as the original PCA table.; """"""",MatchSource.CODE_COMMENT,tests/external/test_scanorama_integrate.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/external/test_scanorama_integrate.py
Deployability,install,installed,"# PAGA for hematopoiesis in mouse [(Paul *et al.*, 2015)](https://doi.org/10.1016/j.cell.2015.11.013); # Hematopoiesis: trace myeloid and erythroid differentiation for data of [Paul *et al.* (2015)](https://doi.org/10.1016/j.cell.2015.11.013).; #; # This is the subsampled notebook for testing.; # Preprocessing and Visualization; # See #1262; # TODO: currently needs skip if louvain isn't installed, do major rework; # Clustering and PAGA; # sc.pl.paga(adata, color=['louvain', 'Hba-a2', 'Elane', 'Irf8']); # sc.pl.paga(adata, color=['louvain', 'Itga2b', 'Prss34']); # !!!! no clue why it doesn't produce images with the same shape; # save_and_compare_images('paga'); # slight deviations because of graph drawing; # save_and_compare_images('paga_compare'); # erythroid; # neutrophil; # monocyte; # add a test for this at some point; # data.to_csv('./write/paga_path_{}.csv'.format(descr))",MatchSource.CODE_COMMENT,tests/notebooks/test_paga_paul15_subsampled.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/notebooks/test_paga_paul15_subsampled.py
Testability,test,testing,"# PAGA for hematopoiesis in mouse [(Paul *et al.*, 2015)](https://doi.org/10.1016/j.cell.2015.11.013); # Hematopoiesis: trace myeloid and erythroid differentiation for data of [Paul *et al.* (2015)](https://doi.org/10.1016/j.cell.2015.11.013).; #; # This is the subsampled notebook for testing.; # Preprocessing and Visualization; # See #1262; # TODO: currently needs skip if louvain isn't installed, do major rework; # Clustering and PAGA; # sc.pl.paga(adata, color=['louvain', 'Hba-a2', 'Elane', 'Irf8']); # sc.pl.paga(adata, color=['louvain', 'Itga2b', 'Prss34']); # !!!! no clue why it doesn't produce images with the same shape; # save_and_compare_images('paga'); # slight deviations because of graph drawing; # save_and_compare_images('paga_compare'); # erythroid; # neutrophil; # monocyte; # add a test for this at some point; # data.to_csv('./write/paga_path_{}.csv'.format(descr))",MatchSource.CODE_COMMENT,tests/notebooks/test_paga_paul15_subsampled.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/notebooks/test_paga_paul15_subsampled.py
Availability,avail,available,"# *First compiled on May 5, 2017. Updated August 14, 2018.*; # # Clustering 3k PBMCs following a Seurat Tutorial; #; # This started out with a demonstration that Scanpy would allow to reproduce most of Seurat's; # ([Satija *et al.*, 2015](https://doi.org/10.1038/nbt.3192)) clustering tutorial as described on; # https://satijalab.org/seurat/articles/pbmc3k_tutorial.html (July 26, 2017), which we gratefully acknowledge.; # In the meanwhile, we have added and removed several pieces.; #; # The data consists in *3k PBMCs from a Healthy Donor* and is freely available from 10x Genomics; # ([here](https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz); # from this [webpage](https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k)).; # ensure violin plots and other non-determinstic plots have deterministic behavior; # Preprocessing; # for each cell compute fraction of counts in mito genes vs. all genes; # the `.A1` is only necessary as X is sparse to transform to a dense array after summing; # add the total counts per cell as observations-annotation to adata; # PCA; # UMAP; # sc.tl.umap(adata) # umaps lead to slight variations; # sc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP'], use_raw=False, show=False); # save_and_compare_images('umap_1'); # Clustering the graph; # sc.pl.umap(adata, color=[""leiden"", ""CST3"", ""NKG7""], show=False); # save_and_compare_images(""umap_2""); # Finding marker genes; # Due to incosistency with our test runner vs local, these clusters need to; # be pre-annotated as the numbers for each cluster are not consistent.; # ensure that the column can be sorted for consistent plotting since it is by default unordered; # gives a strange error, probably due to jitter or something; # sc.pl.rank_genes_groups_violin(adata, groups='0', n_genes=8); # save_and_compare_images('rank_genes_groups_4'); # sc.pl.umap(adata, color='leiden', legend_loc='on data', title='', frameon=False, show=False); # save_a",MatchSource.CODE_COMMENT,tests/notebooks/test_pbmc3k.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/notebooks/test_pbmc3k.py
Testability,test,test,"017. Updated August 14, 2018.*; # # Clustering 3k PBMCs following a Seurat Tutorial; #; # This started out with a demonstration that Scanpy would allow to reproduce most of Seurat's; # ([Satija *et al.*, 2015](https://doi.org/10.1038/nbt.3192)) clustering tutorial as described on; # https://satijalab.org/seurat/articles/pbmc3k_tutorial.html (July 26, 2017), which we gratefully acknowledge.; # In the meanwhile, we have added and removed several pieces.; #; # The data consists in *3k PBMCs from a Healthy Donor* and is freely available from 10x Genomics; # ([here](https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz); # from this [webpage](https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k)).; # ensure violin plots and other non-determinstic plots have deterministic behavior; # Preprocessing; # for each cell compute fraction of counts in mito genes vs. all genes; # the `.A1` is only necessary as X is sparse to transform to a dense array after summing; # add the total counts per cell as observations-annotation to adata; # PCA; # UMAP; # sc.tl.umap(adata) # umaps lead to slight variations; # sc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP'], use_raw=False, show=False); # save_and_compare_images('umap_1'); # Clustering the graph; # sc.pl.umap(adata, color=[""leiden"", ""CST3"", ""NKG7""], show=False); # save_and_compare_images(""umap_2""); # Finding marker genes; # Due to incosistency with our test runner vs local, these clusters need to; # be pre-annotated as the numbers for each cluster are not consistent.; # ensure that the column can be sorted for consistent plotting since it is by default unordered; # gives a strange error, probably due to jitter or something; # sc.pl.rank_genes_groups_violin(adata, groups='0', n_genes=8); # save_and_compare_images('rank_genes_groups_4'); # sc.pl.umap(adata, color='leiden', legend_loc='on data', title='', frameon=False, show=False); # save_and_compare_images('umap_3')",MatchSource.CODE_COMMENT,tests/notebooks/test_pbmc3k.py,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/tree/1.10.2/tests/notebooks/test_pbmc3k.py
